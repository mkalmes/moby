// ==========  Accelerate.framework/Headers/Accelerate.h
/*
     File:       Accelerate/Accelerate.h

     Contains:   Master include for all of Accelerate

     Version:    Accelerate-1

     Copyright:  Copyright (c) 2000-2019 by Apple Inc. All rights reserved.

     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:

                     http://developer.apple.com/bugreporter/

*/
#ifndef __ACCELERATE__
#define __ACCELERATE__

#ifndef __VECLIB__
#include "../Frameworks/vecLib.framework/Headers/vecLib.h"
#endif

#ifndef VIMAGE_H
#include <vImage/vImage.h>
#endif

#endif /* __ACCELERATE__ */
// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Morphology.h
/*!
*  @header Morphology.h
*  vImage_Framework
*
*  See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2002-2016 by Apple Inc. All rights reserved.
*
*  @discussion Morphology functions modify the shape of dark and light elements in an image.  They can enlarge
*              light structural elements (Dilate) or make them smaller by making the darker regions larger (Erode).
*              The filters can be used in combination fill in holes (Dilate then Erode) or remove fine structure
*              (Erode then Dilate).  The Erode and Dilate filters can themselves have structure. For example, you
*              can Dilate in the shape of a star, in which case single pixel bright signal (such as stars in a night
*              sky) assume a star shape.  Larger structure elements assume something of a star-like shape, but generally will
*              not become fully recognizable as such until the size of the filter exceeds the size of the image
*              structure element. A small circular filter can turn a rectangle into a larger rectangle with round corners.
*              Many other examples abound.
*
*              Min is a special case for an Erode function with a rectangular kernel that contains all the same value.
*              Max is a special case for a Dilate function with a rectangular kernel that contains all the same value.
*              Min and Max make use of a faster algorithm that can operate at much reduced cost.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*/

#ifndef VIMAGE_MORPHOLOGY_H
#define VIMAGE_MORPHOLOGY_H

#include <vImage/vImage_Types.h>


#ifdef __cplusplus
extern "C" {
#endif


/*!
 *  @functiongroup Dilate
 *  @discussion  A dilate filter uses a shaped probe to trace a 3D surface. Imagine the kernel to define a 2D surface with
 *               the third dimension the values in the kernel.  This is then lowered down over the image, itself treated
 *               as a 3D surface. The result image is the height at which the surface makes contact with the image for
 *               each pixel in the image. In this respect it operates like a scanning electron microscope with an adjustable
 *               probe shape. In code:
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = 0;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = CLIP( r );
 *                  }
 *               @/textblock </pre>
 *               The ARGB variants apply the filter to the four channels in parallel. It should be noted that the application
 *               of the filter to the image causes the structure elements in the filter to be reflected into the image, reversed
 *               top to bottom and left to right. Also, if the center of the kernel is not 0, a general lightening of the image
 *               will occur. Some functions will run much faster if the center of the kernel is 0.
 */

/*!
 *  @function vImageDilate_Planar8
 *  @abstract Apply a dilate filter to a Planar8 image
 *  @discussion  This is a general purpose dilate filter for Planar8 data. It is optimized to handle the special cases that occur
 *               in image masks -- large contiguous regions of all 0xff or 0x0. If your filter is all 0's, you should use vImageMax_Planar8
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = 0;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = CLIP( r );
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the dilate filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageDilate_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const unsigned char *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));



/*!
 *  @function vImageDilate_PlanarF
 *  @abstract Apply a dilate filter to a PlanarF image
 *  @discussion  This is a general purpose dilate filter for Planar8 data. If your filter is all 0's, you should use vImageMax_PlanarF
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float r = -INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the dilate filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place. Floating-point values have host endianness.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageDilate_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageDilate_ARGB8888
 *  @abstract Apply a dilate filter to a ARGB8888 image
 *  @discussion  This is a general purpose dilate filter for ARGB8888 data. If your filter is all 0's, you should use vImageMax_ARGB8888
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int a = 0;
 *                      int r = 0;
 *                      int g = 0;
 *                      int b = 0;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MAX( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] - k[y*kernel_width+x] )
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] - k[y*kernel_width+x] )
 *                              g = MAX( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] - k[y*kernel_width+x] )
 *                              b = MAX( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      a += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      g += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      b += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : CLIP( a );
 *                      result[i][j][1] = CLIP( r );
 *                      result[i][j][2] = CLIP( g );
 *                      result[i][j][3] = CLIP( b );
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the dilate filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place. If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a dilate filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageDilate_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const unsigned char *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageDilate_ARGBFFFF
 *  @abstract Apply a dilate filter to a ARGBFFFF image
 *  @discussion  This is a general purpose dilate filter for ARGBFFFF data. If your filter is all 0's, you should use vImageMax_ARGBFFFF
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float a = -INFINITY;
 *                      float r = -INFINITY;
 *                      float g = -INFINITY;
 *                      float b = -INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MAX( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] - k[y*kernel_width+x] )
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] - k[y*kernel_width+x] )
 *                              g = MAX( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] - k[y*kernel_width+x] )
 *                              b = MAX( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      a += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      g += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      b += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the dilate filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place.  If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first. Floating-point values have host
 *                endianness.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the dilate filter.
 *                              It allows the dilate filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first float in pixel in memory) is copied to the destination
 *                                          without modification, instead of having a dilate filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageDilate_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @functiongroup Erode
 *  @discussion  An erode filter uses a shaped probe to trace a 3D surface. Imagine the kernel to define a 2D surface with
 *               the third dimension the values in the kernel.  This is then elevated from underneath the image, itself treated
 *               as a 3D surface. The result image is the height at which the surface makes contact with the image for
 *               each pixel in the image.
 *
 *               In code:
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = MAX_CHANNEL_VALUE;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = CLIP( r );
 *                  }
 *               @/textblock </pre>
 *               The ARGB variants apply the filter to the four channels in parallel. It should be noted that the application
 *               of the filter to the image causes the structure elements in the filter to be reflected into the image, reversed
 *               top to bottom and left to right. Also, if the center of the kernel is not 0, a general lightening of the image
 *               will occur. Some functions will run much faster if the center of the kernel is 0.
 */


/*!
 *  @function vImageErode_Planar8
 *  @abstract Apply a erode filter to a Planar8 image
 *  @discussion  This is a general purpose erode filter for Planar8 data. If your filter is all 0's, you should use vImageMin_Planar8
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = MAX_CHANNEL_VALUE;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = CLIP( r );
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the erode filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageErode_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const unsigned char *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageErode_PlanarF
 *  @abstract Apply a erode filter to a PlanarF image
 *  @discussion  This is a general purpose erode filter for Planar8 data. If your filter is all 0's, you should use vImageMin_PlanarF
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float r = INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the erode filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place. Floating-point values have host endianness.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageErode_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageErode_ARGB8888
 *  @abstract Apply a erode filter to a ARGB8888 image
 *  @discussion  This is a general purpose erode filter for ARGB8888 data. If your filter is all 0's, you should use vImageMin_ARGB8888
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int a = 255;
 *                      int r = 255;
 *                      int g = 255;
 *                      int b = 255;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MIN( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] - k[y*kernel_width+x] )
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] - k[y*kernel_width+x] )
 *                              g = MIN( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] - k[y*kernel_width+x] )
 *                              b = MIN( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      a += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      g += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      b += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : CLIP( a );
 *                      result[i][j][1] = CLIP( r );
 *                      result[i][j][2] = CLIP( g );
 *                      result[i][j][3] = CLIP( b );
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the erode filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place. If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a erode filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageErode_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const unsigned char *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageErode_ARGBFFFF
 *  @abstract Apply a erode filter to a ARGBFFFF image
 *  @discussion  This is a general purpose erode filter for ARGBFFFF data. If your filter is all 0's, you should use vImageMin_ARGBFFFF
 *               instead.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float a = INFINITY;
 *                      float r = INFINITY;
 *                      float g = INFINITY;
 *                      float b = INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MIN( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] - k[y*kernel_width+x] )
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] - k[y*kernel_width+x] )
 *                              g = MIN( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] - k[y*kernel_width+x] )
 *                              b = MIN( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] - k[y*kernel_width+x] )
 *                          }
 *                      }
 *
 *                      // normalize for kernel center not 0
 *                      a += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      r += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      g += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *                      b += k[y * (kernel_height/2) * kernel_width + kernel_width/2 ];
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the erode filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place.  If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first. Floating-point values have host
 *                endianness.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the erode filter.
 *                              It allows the erode filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel               A pointer to a array of filter values of dimension kernel_height x kernel_width.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first float in pixel in memory) is copied to the destination
 *                                          without modification, instead of having a erode filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                  0                           If kvImageGetTempBufferSize was among the flags, then no work was done.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageErode_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @functiongroup Max
 *  @discussion     A max filter is a special case of a dilate filter, in which the filter elements are all 0.
 *                  This allows a much, much faster algorithm to be used.
 */

/*!
 *  @function   vImageMax_Planar8
 *  @abstract   Apply a max filter to a Planar8 image.
 *  @discussion A max filter is a special case dilate filter, in which the filter elements are all 0. It is much faster than the normal dilate.
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = 0;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *                If only part of the max filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place, unless the kvImageDoNotTile flag is used.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread. It also
 *                                          allows the function to work in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMax_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function   vImageMax_PlanarF
 *  @abstract   Apply a max filter to a PlanarF image.
 *  @discussion A max filter is a special case dilate filter, in which the filter elements are all 0. It is much faster than the normal dilate.
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float r = -INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *                If only part of the max filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place, unless the kvImageDoNotTile flag is used.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread. It also
 *                                          allows the function to work in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return the size of the temp buffer needed.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMax_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageMax_ARGB8888
 *  @abstract Apply a max filter to a ARGB8888 image
 *  @discussion  This is a special purpose dilate filter for ARGB8888 data, for rectangular kernels with value 0. It is much faster than the normal dilate.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int a = 0;
 *                      int r = 0;
 *                      int g = 0;
 *                      int b = 0;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MAX( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] )
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] )
 *                              g = MAX( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] )
 *                              b = MAX( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the max filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *                Does not work in place, unless the kvImageDoNotTile flag is used, in which case it will probably
 *                run more slowly.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *                                          kvImageDoNotTile will also allow the filter to run in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a max filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMax_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageMax_ARGBFFFF
 *  @abstract Apply a max filter to a ARGBFFFF image
 *  @discussion  This is a special purpose dilate filter for ARGBFFFF data, for rectangular kernels with value 0. It is much faster than the normal dilate.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float a = -INFINITY;
 *                      float r = -INFINITY;
 *                      float g = -INFINITY;
 *                      float b = -INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MAX( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] )
 *                              r = MAX( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] )
 *                              g = MAX( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] )
 *                              b = MAX( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the max filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *                Does not work in place, unless the kvImageDoNotTile flag is used, in which case it will probably
 *                run more slowly.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the max filter.
 *                              It allows the max filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *                                          kvImageDoNotTile will also allow the filter to run in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a max filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMax_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @functiongroup Min
 *  @discussion     A min filter is a special case of an erode filter, in which the filter elements are all 0.
 *                  This allows a much, much faster algorithm to be used.
 */

/*!
 *  @function   vImageMin_Planar8
 *  @abstract   Apply a min filter to a Planar8 image.
 *  @discussion A min filter is a special case erode filter, in which the filter elements are all 0.  It is much faster than the normal erode.
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int r = MAX_CHANNEL_VALUE;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *                If only part of the min filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place, unless the kvImageDoNotTile flag is used.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread. It also
 *                                          allows the function to work in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMin_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function   vImageMin_PlanarF
 *  @abstract   Apply a min filter to a PlanarF image.
 *  @discussion A min filter is a special case erode filter, in which the filter elements are all 0. It is much faster than the normal erode.
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float r = INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j] = r;
 *                  }
 *               @/textblock </pre>
 *                If only part of the min filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                Does not work in place, unless the kvImageDoNotTile flag is used.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread. It also
 *                                          allows the function to work in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMin_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageMin_ARGB8888
 *  @abstract Apply a min filter to a ARGB8888 image
 *  @discussion  This is a special purpose erode filter for ARGB8888 data, for rectangular kernels with value 0. It is much faster than the normal erode.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      int a = MAX_CHANNEL_VALUE;
 *                      int r = MAX_CHANNEL_VALUE;
 *                      int g = MAX_CHANNEL_VALUE;
 *                      int b = MAX_CHANNEL_VALUE;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MIN( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] )
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] )
 *                              g = MIN( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] )
 *                              b = MIN( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the min filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *                Does not work in place, unless the kvImageDoNotTile flag is used, in which case it will probably
 *                run more slowly.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *                                          kvImageDoNotTile will also allow the filter to run in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a min filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMin_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageMin_ARGBFFFF
 *  @abstract Apply a min filter to a ARGBFFFF image
 *  @discussion  This is a special purpose erode filter for ARGBFFFF data, for rectangular kernels with value 0. It is much faster than the normal erode.
 *
 *               <pre> @textblock
 *                  for each pixel result[i][j] in the image{
 *                      float a = INFINITY;
 *                      float r = INFINITY;
 *                      float g = INFINITY;
 *                      float b = INFINITY;
 *                      for( y = 0; y < kernel_height; y++ ){
 *                          for( x = 0; x < kernel_width; x++ ){
 *                              a = MIN( a, src[i+y-kernel_height/2][j+x-kernel_width/2][0] )
 *                              r = MIN( r, src[i+y-kernel_height/2][j+x-kernel_width/2][1] )
 *                              g = MIN( g, src[i+y-kernel_height/2][j+x-kernel_width/2][2] )
 *                              b = MIN( b, src[i+y-kernel_height/2][j+x-kernel_width/2][3] )
 *                          }
 *                      }
 *
 *                      // saturate overflow to representable range
 *                      result[i][j][0] = (flags & kvImageLeaveAlphaUnchanged) ? src[i][j] : a;
 *                      result[i][j][1] = r;
 *                      result[i][j][2] = g;
 *                      result[i][j][3] = b;
 *                  }
 *               @/textblock </pre>
 *
 *                If only part of the min filter of the entire image is desired, use srcOffsetToROI_X/Y to set the positioning
 *                of the result tile relative to the src image.
 *
 *                If kvImageLeaveAlphaUnchanged is not used, it works for any channel order.
 *                If kvImageLeaveAlphaUnchanged is used, then the alpha must be first.
 *                Does not work in place, unless the kvImageDoNotTile flag is used, in which case it will probably
 *                run more slowly.
 *
 *  @param src                  The input image
 *  @param dest                 A preallocated buffer to contain the result image
 *  @param tempBuffer           May be NULL. An optional temp buffer in which to store temporary computation. To find the size
 *                              of the temp buffer, call the function with the desired parameters and pass the kvImageGetTempBufferSize
 *                              flag. The size of the temp buffer will be returned from the left hand side of the function in
 *                              place of an error code, and no work will be done on the image data. A temp buffer can provide a speed
 *                              improvement, if it can be allocated once and reused. This saves a bunch of VM faults on first use of
 *                              the newly allocated temp buffer.
 *  @param srcOffsetToROI_X     An offset added to the horizontal position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              left edge of the source image.
 *  @param srcOffsetToROI_Y     An offset added to the vertical position in the src image when calculating the min filter.
 *                              It allows the min filter to operate in a tiled fashion for tiles that do not start on the
 *                              top edge of the source image.
 *  @param kernel_height        The height of the rectangular kernel. Must be an odd number.
 *  @param kernel_width         The width of the rectangular kernel. Must be an odd number.
 *  @param flags                The following flags values are allowed:
 *      <pre> @textblock
 *             kvImageNoFlags               Default operation
 *
 *             kvImageDoNotTile             Turn off internal multithreading. Useful if, for example, you are doing your own
 *                                          multithreading and just want the filter to run local to the current thread.
 *                                          kvImageDoNotTile will also allow the filter to run in place, though more slowly.
 *
 *             kvImageGetTempBufferSize     Return 0.  Do no work.
 *
 *             kvImageLeaveAlphaUnchanged   The alpha channel (first byte of pixel in memory) is copied to the destination
 *                                          without modification, instead of having a min filter applied to it.
 *      @/textblock </pre>
 *  @return The following error codes may result:
 *      <pre> @textblock
 *             kvImageNoError                   Success.
 *                 >=0                          If kvImageGetTempBufferSize was among the flags, then no work was done.
 *                                              Instead, the size of the temp buffer needed is returned.
 *
 *             kvImageRoiLargerThanInputBuffer  dest->width and dest->height must be less than or equal to the corresponding
 *                                              dimensions in the src image
 *
 *             kvImageInvalidKernelSize         The kernel_height and kernel_width must not be evenly divisible by 2.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageMin_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  vImagePixelCount kernel_height, vImagePixelCount kernel_width, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


#ifdef __cplusplus
}
#endif

#endif /* MORPHOLOGY_H */


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Convolution.h
/*!
*  @header Convolution.h
*  vImage_Framework
*
*  See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2002-2016 by Apple Inc. All rights reserved.
*
*  @discussion Convolution is a weighted average between a pixel and its neighboring pixels. By smoothing
*              out high frequency signal, convolution can be used to blur an image. By subtracting out the
*              low frequency signal, it can be used to produce a sharpening effect. Convolution is also used
*              with a variety of filters such as Laplace and Sobel to detect edges in the image. Here we also
*              provide a deconvolution filter that attempts to iteratively undo the effect of a convolution.
*              Where the convolution kernel can be accurately determined, it can be used to remove focus
*              problems, motion blur and distortion introduced by the lens.
*
*      <pre>@textblock
*              =========================================================================
*              ======    MOST VIMAGE CONVOLUTION FUNCTIONS DO NOT WORK IN PLACE    =====
*              =========================================================================
*      @/textblock </pre>
*
*              Since a weighted average of nearby pixels can for some positions extend off the edge of the
*              input image, a variety of edging modes are provided which specify what happens in such
*              cases:
*      <pre>@textblock
*              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
*                                          copy the corresponding source pixel to the destination. This
*                                          will result in a ring on unconvolved content at the edges
*                                          and convolved content in the middle.
*
*              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
*                                          For a blur filter this will look like the edges of the image
*                                          have been blurred into a particular color. This is usually
*                                          appropriate when the color of the surface onto which the image
*                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
*                                          as a background color, and let the downstream image compositor
*                                          blend in the background color. In this case, the result image
*                                          should be treated as a premultiplied image.
*
*              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
*                                          This is better when the image is drawn into a frame or other
*                                          context where the background is not expected to contribute to
*                                          the final content.
*
*              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
*                                          simply ignored, and the bias and divisor are adjusted accordingly.
*                                          Because of the complication at the edges, this edging mode can
*                                          be significantly slower than the others. It can be numerically
*                                          unstable if the sum over any rectangular subsection of the kernel
*                                          is zero, which can result in division by zero during the computation.
*                                          The results for this edging mode are usually quite visually similar
*                                          to kvImageEdgeExtend.
*      @/textblock </pre>
*              Only one edging mode may be active at a time.  Please see the Convolution section in
*              the vImage Programming Guide for a better description of edging modes.
*              (https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/vImage.pdf)
*
*
*              When calling these interfaces from within the context of your own tiling engine,
*              it may be necessary to produce a result tile starting from different points in the
*              input image. Simply repackaging the input image by adjusting the height, width and
*              src->data to achieve this affect will not work correctly because it introduces
*              artificial edges into the input image. (See discussion of edging modes above.)
*              Instead, X and Y offset parameters are provided. Simply pass in the whole input image
*              and use the X and Y offsets to adjust the position of the content used to produce
*              the output image.  The size of the output tile is given by the dest->height and width.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*/


#ifndef VIMAGE_CONVOLUTION_H
#define VIMAGE_CONVOLUTION_H


#ifdef __cplusplus
extern "C" {
#endif

#include <vImage/vImage_Types.h>

/*!
 *  @functiongroup General Convolution
 *  @discussion A general convolve filter allows you to provide an arbitrary set of weights to use
 *              in the weighted average between each pixel and its neighbors. As such, it is a
 *              very flexible tool for achieving a variety of image effects, from blurring to sharpening
 *              to edge detection to sub-pixel image translation and motion blur.
 *
 *              Some filters such as edge detection filters take the first derivative of the image
 *              surface. Since a derivative can be either positive or negative  and some formats such
 *              as Planar8 (unorm8) can only represent positive numbers, the general convolution is also
 *              available in a form which allows you to provide a bias to add to the weighted sum before
 *              the divisor is applied, to shift the values into the positive.  You might provide a
 *              bias of 128 * divisor to move the encoding for 0 to 128 so that negative numbers are
 *              representable, for example. (In this case, -20 would now be encoded as 128-20=108.)
 *
 *              Finally, vImage provides multikernel convolution, which allows a different kernel, bias
 *              and divisor to be used for each color channel. This might allow for the position of the
 *              different color channels to be shifted independently, or for alpha to be blurred differently
 *              from color.
 *      <pre>@textblock
 *              =========================================================================
 *              ======    MOST VIMAGE CONVOLUTION FUNCTIONS DO NOT WORK IN PLACE    =====
 *              =========================================================================
 *      @/textblock </pre>
 */

/*!
 *  @function vImageConvolve_Planar8
 *  @abstract General convolution on a Planar8 image.
 *  @discussion This filter applies a convolution filter of your choosing to a Planar8 image.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sum = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sum = (sum + divisor/2) / divisor;
 *
 *              // write out result
 *              result[y][x] = CLAMP(sum, 0, 255);
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          The kernel values may not sum in any combination to be outside the range
 *                          [-2**23, 2**23), or modulo overflow in the accumulator may result.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param divisor          The weighted sum of nearby pixels is typically a large number, which must be corrected
 *                          to fit back into the image format of the destination image. The correction factor
 *                          is passed in as divisor here, and is divided from the sum before the result is
 *                          returned. Typically, the divisor is the sum over the area of the kernel. If the divisor
 *                          is 0, 1 will be used instead.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolve_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, Pixel_8 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageConvolve_PlanarF
 *  @abstract General convolution on a PlanarF image.
 *  @discussion This filter applies a convolution filter of your choosing to a PlanarF image.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              float sum = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2];
 *              }
 *
 *              // write out result
 *              result[y][x] = sum;
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolve_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, uint32_t kernel_height, uint32_t kernel_width, Pixel_F backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageConvolve_ARGB8888
 *  @abstract General convolution on a ARGB888 image of any channel order.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGB8888 image.
 *              This filter will work on any four-channel, 8-bit per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sumA = 0;
 *              int sumR = 0;
 *              int sumG = 0;
 *              int sumB = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sumA = (sumA + divisor/2) / divisor;
 *              sumR = (sumR + divisor/2) / divisor;
 *              sumG = (sumG + divisor/2) / divisor;
 *              sumB = (sumB + divisor/2) / divisor;
 *
 *              // write out result
 *              result[y][x][0] = CLAMP(sumA, 0, 255);
 *              result[y][x][1] = CLAMP(sumR, 0, 255);
 *              result[y][x][2] = CLAMP(sumG, 0, 255);
 *              result[y][x][3] = CLAMP(sumB, 0, 255);
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          The kernel values may not sum in any combination to be outside the range
 *                          [-2**23, 2**23), or modulo overflow in the accumulator may result.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param divisor          The weighted sum of nearby pixels is typically a large number, which must be corrected
 *                          to fit back into the image format of the destination image. The correction factor
 *                          is passed in as divisor here, and is divided from the sum before the result is
 *                          returned. Typically, the divisor is the sum over the area of the kernel. If the divisor
 *                          is 0, 1 will be used instead.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolve_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageConvolve_ARGBFFFF
 *  @abstract General convolution on a ARGBFFFF image of any channel order.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGBFFFF image.
 *              This filter will work on any four-channel, float per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              float sumA = 0;
 *              float sumR = 0;
 *              float sumG = 0;
 *              float sumB = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *
 *              // write out result
 *              result[y][x][0] = sumA;
 *              result[y][x][1] = sumR;
 *              result[y][x][2] = sumG;
 *              result[y][x][3] = sumB;
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolve_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, uint32_t kernel_height, uint32_t kernel_width, const Pixel_FFFF backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageConvolveWithBias_Planar8
 *  @abstract General convolution on a Planar8 image with added bias.
 *  @discussion This filter applies a convolution filter of your choosing to a Planar8 image.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sum = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sum = (sum + bias) / divisor;
 *
 *              // write out result
 *              result[y][x] = CLAMP(sum, 0, 255);
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          The kernel values may not sum in any combination to be outside the range
 *                          [-2**23, 2**23), or modulo overflow in the accumulator may result.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param divisor          The weighted sum of nearby pixels is typically a large number, which must be corrected
 *                          to fit back into the image format of the destination image. The correction factor
 *                          is passed in as divisor here, and is divided from the sum before the result is
 *                          returned. Typically, the divisor is the sum over the area of the kernel. If the divisor
 *                          is 0, 1 will be used instead.
 *
 *  @param bias             This value is added to the sum of weighted pixels before the divisor is applied.
 *                          It can serve to both control rounding and adjust the brightness of the result.
 *                          A large bias (e.g 128 * divisor) may be required for some kernels to return
 *                          representable results, such as edge detection filters.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveWithBias_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, Pixel_8 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageConvolveWithBias_PlanarF
 *  @abstract General convolution on a PlanarF image with added bias.
 *  @discussion This filter applies a convolution filter of your choosing to a PlanarF image.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              float sum = bias;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2];
 *              }
 *
 *              // write out result
 *              result[y][x] = sum;
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param bias             This value is added to the sum of weighted pixels.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveWithBias_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias, Pixel_F backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageConvolveWithBias_ARGB8888
 *  @abstract General convolution on a ARGB888 image of any channel order with bias.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGB8888 image.
 *              This filter will work on any four-channel, 8-bit per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sumA = 0;
 *              int sumR = 0;
 *              int sumG = 0;
 *              int sumB = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sumA = (sumA + bias[0]) / divisor;
 *              sumR = (sumR + bias[1]) / divisor;
 *              sumG = (sumG + bias[2]) / divisor;
 *              sumB = (sumB + bias[3]) / divisor;
 *
 *              // write out result
 *              result[y][x][0] = CLAMP(sumA, 0, 255);
 *              result[y][x][1] = CLAMP(sumR, 0, 255);
 *              result[y][x][2] = CLAMP(sumG, 0, 255);
 *              result[y][x][3] = CLAMP(sumB, 0, 255);
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          The kernel values may not sum in any combination to be outside the range
 *                          [-2**23, 2**23), or modulo overflow in the accumulator may result.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param divisor          The weighted sum of nearby pixels is typically a large number, which must be corrected
 *                          to fit back into the image format of the destination image. The correction factor
 *                          is passed in as divisor here, and is divided from the sum before the result is
 *                          returned. Typically, the divisor is the sum over the area of the kernel. If the divisor
 *                          is 0, 1 will be used instead.
 *
 *  @param bias             This value is added to the sum of weighted pixels before the divisor is applied.
 *                          It can serve to both control rounding and adjust the brightness of the result.
 *                          A large bias (e.g 128 * divisor) may be required for some kernels, such as edge
 *                          detection filters, to return representable results.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveWithBias_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const int16_t *kernel, uint32_t kernel_height, uint32_t kernel_width, int32_t divisor, int32_t bias, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageConvolveWithBias_ARGBFFFF
 *  @abstract General convolution on a ARGBFFFF image of any channel order with bias.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGBFFFF image.
 *              This filter will work on any four-channel, float per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              float sumA = bias[0];
 *              float sumR = bias[1];
 *              float sumG = bias[2];
 *              float sumB = bias[3];
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *
 *              // write out result
 *              result[y][x][0] = sumA;
 *              result[y][x][1] = sumR;
 *              result[y][x][2] = sumG;
 *              result[y][x][3] = sumB;
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param bias             This value is added to the sum of weighted pixels.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveWithBias_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y,  const float *kernel, uint32_t kernel_height, uint32_t kernel_width, float bias,  const Pixel_FFFF backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageConvolveMultiKernel_ARGB8888
 *  @abstract General convolution on a ARGB8888 image of any channel order with separate bias, kernel and divisor for each channel.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGB8888 image.
 *              This filter will work on any four-channel, 8-bit per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sumA = 0;
 *              int sumR = 0;
 *              int sumG = 0;
 *              int sumB = 0;
 *                const int16_t *kA = kernel[0];
 *                const int16_t *kR = kernel[1];
 *                const int16_t *kG = kernel[2];
 *                const int16_t *kB = kernel[3];
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kA[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kR[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kG[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kB[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sumA = (sumA + bias[0]) / divisor[0];
 *              sumR = (sumR + bias[1]) / divisor[1];
 *              sumG = (sumG + bias[2]) / divisor[2];
 *              sumB = (sumB + bias[3]) / divisor[3];
 *
 *              // write out result
 *              result[y][x][0] = CLAMP(sumA, 0, 255);
 *              result[y][x][1] = CLAMP(sumR, 0, 255);
 *              result[y][x][2] = CLAMP(sumG, 0, 255);
 *              result[y][x][3] = CLAMP(sumB, 0, 255);
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           An array of 4 pointers to weights of dimension kernel_height x kernel_width.
 *                          The kernel values in each array may not sum in any combination to be outside the range
 *                          [-2**23, 2**23), or modulo overflow in the accumulator may result.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param divisor          The weighted sum of nearby pixels is typically a large number, which must be corrected
 *                          to fit back into the image format of the destination image. The correction factor
 *                          is passed in as divisor here, and is divided from the sum before the result is
 *                          returned. Typically, the divisor is the sum over the area of the kernel. If the divisor
 *                          is 0, 1 will be used instead.
 *
 *  @param bias             This array of values is added to the sum of weighted pixels for each channel respectively
 *                          before the divisor is applied. It can serve to both control rounding and adjust the
 *                          brightness of the result. A large bias (e.g 128 * divisor) may be required for some
 *                          kernels, such as edge detection filters, to return representable results.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveMultiKernel_ARGB8888(    const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          void *tempBuffer,
                                                          vImagePixelCount srcOffsetToROI_X,
                                                          vImagePixelCount srcOffsetToROI_Y,
                                                          const int16_t *kernels[4],
                                                          uint32_t kernel_height,
                                                          uint32_t kernel_width,
                                                          const int32_t divisors[4],
                                                          const int32_t biases[4],
                                                          const Pixel_8888 backgroundColor,
                                                          vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageConvolveMultiKernel_ARGBFFFF
 *  @abstract General convolution on a ARGBFFFF image of any channel order with separate bias and kernel for each channel.
 *  @discussion This filter applies a convolution filter of your choosing to a ARGBFFFF image.
 *              This filter will work on any four-channel, float per component image format, not just ARGB.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              float sumA = bias[0];
 *              float sumR = bias[1];
 *              float sumG = bias[2];
 *              float sumB = bias[3];
 *                const float *kA = kernel[0];
 *                const float *kR = kernel[1];
 *                const float *kG = kernel[2];
 *                const float *kB = kernel[3];
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kA[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kR[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kG[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kB[i*kernel_width+j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2] [x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *
 *              // write out result
 *              result[y][x][0] = sumA;
 *              result[y][x][1] = sumR;
 *              result[y][x][2] = sumG;
 *              result[y][x][3] = sumB;
 *          }
 *      @/textblock </pre>
 *              (Above, we imagine the kernel to be a 2D array of size kernel_height by kernel_width.
 *              However, in practice it is passed in as a contiguous 1D array of size kernel_height *
 *              kernel_width.)
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param bias             This value is added to the sum of weighted pixels.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConvolveMultiKernel_ARGBFFFF(    const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          void *tempBuffer,
                                                          vImagePixelCount srcOffsetToROI_X,
                                                          vImagePixelCount srcOffsetToROI_Y,
                                                          const float *kernels[4],
                                                          uint32_t kernel_height,
                                                          uint32_t kernel_width,
                                                          const float biases[4],
                                                          const Pixel_FFFF backgroundColor,
                                                          vImage_Flags flags ) VIMAGE_NON_NULL(1,2,6,9,10) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));




/*!
 * @functiongroup Richardson-Lucy Deconvolution
 *
 * @discussion Richardson-Lucy deconvolution (a.k.a. Lucy-Richardson) is an iterative procedure for estimating what an original image
 *  probably was before a convolution, given the convolution end result and the kernel used to create it.  It is typically used to fix blurring caused
 *  by lens distortion, most famously for the Hubble telescope, but also to improve images in confocal microscopy and other uses. When used
 *  to correct loss of signal due to physical limitations of the imaging system, the point spread function (kernel) is estimated from known
 *  parameters associated with the lensing system. It can also be used to sharpen images that have been digitally blurred, as long as the
 *  original convolution kernel is known or can be estimated.
 *
 *  This routine iteratively uses the following formula:
 *      <pre>@textblock
 *        e[i+1] = e[i] x (psf0 * ( e[0] / (psf1 * e[i]) ) )
 *
 *        where:
 *              e[0] = the observed image (src parameter)
 *              e[n] = the result of the nth iteration
 *              psf  = point spread function (kernel for call to convolution)
 *                x    = multiply operator
 *              '*'  = convolution operator
 *      @/textblock </pre>
 *  As with any sharpening operation, Richardson-Lucy amplifies noise, and at some number of iterations the noise becomes noticeable as artifacts.
 *      <pre>@textblock
 *              =========================================================================
 *              ======    MOST VIMAGE CONVOLUTION FUNCTIONS DO NOT WORK IN PLACE    =====
 *              =========================================================================
 *      @/textblock </pre>
 */


/*!
 * @function vImageRichardsonLucyDeConvolve_Planar8
 * @abstract Perform N iterations of a Lucy-Richardson deconvolution on Planar8 data
 * @discussion
 *  This routine iteratively uses the following formula:
 *      <pre>@textblock
 *        e[i+1] = e[i] x (psf0 * ( e[0] / (psf1 * e[i]) ) )
 *
 *        where:
 *              e[0] = the observed image (src parameter)
 *              e[n] = the result of the nth iteration
 *              psf  = point spread function (kernel for call to convolution)
 *                x    = multiply operator
 *              '*'  = convolution operator
 *      @/textblock </pre>
 *
 *  The work in these functions is currently done internally with floating point precision. If you plan to call this function multiple times
 *  (rather than with iterationCount > 1) on 8-bit per channel images, you can save some computation by converting the 8-bit image data to
 *  single precision floating-point yourself using something like vImageConvert_Planar8toPlanarF and iterating on the appropriate
 *  floating-point Richardson Lucy variant. Convert back, when you are done.
 *
 *  Does not work in place.
 * @param src           The input image
 *
 * @param dest          A preallocated buffer to receive the result image.
 *                      This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          This is psf0 in the formula given in the discussion.
 *
 *  @param kernel2          A pointer to a second 1D array of weights of dimension kernel_height2 x kernel_width2.
 *                          This is psf1 in the formula given in the discussion.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_height2   The height of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param kernel_width2    The width of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param divisor          The divisor to use to correct for the volume under kernel.
 *
 *  @param divisor2         The divisor to use to correct for the volume under kernel2.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param iterationCount   The number of Richardson-Lucy iterations to perform on the data before returning.
 *                          If 0, the src buffer is coped to dest.
 *
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageRichardsonLucyDeConvolve_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, const int16_t *kernel2, uint32_t kernel_height, uint32_t kernel_width, uint32_t kernel_height2, uint32_t kernel_width2, int32_t divisor, int32_t divisor2, Pixel_8 backgroundColor, uint32_t iterationCount, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageRichardsonLucyDeConvolve_PlanarF
 * @abstract Perform N iterations of a Lucy-Richardson deconvolution on PlanarF data
 * @discussion
 *  This routine iteratively uses the following formula:
 *      <pre>@textblock
 *        e[i+1] = e[i] x (psf0 * ( e[0] / (psf1 * e[i]) ) )
 *
 *        where:
 *              e[0] = the observed image (src parameter)
 *              e[n] = the result of the nth iteration
 *              psf  = point spread function (kernel for call to convolution)
 *                x    = multiply operator
 *              '*'  = convolution operator
 *      @/textblock </pre>
 *
 *  Does not work in place.
 * @param src           The input image
 *
 * @param dest          A preallocated buffer to receive the result image.
 *                      This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *                          This is psf0 in the formula given in the discussion.
 *
 *  @param kernel2          A pointer to a second 1D array of weights of dimension kernel_height2 x kernel_width2.
 *                          This is psf1 in the formula given in the discussion.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_height2   The height of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param kernel_width2    The width of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param iterationCount   The number of Richardson-Lucy iterations to perform on the data before returning.
 *                          If 0, the src buffer is coped to dest.
 *
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageRichardsonLucyDeConvolve_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, const float *kernel2, uint32_t kernel_height, uint32_t kernel_width, uint32_t kernel_height2, uint32_t kernel_width2, Pixel_F backgroundColor,  uint32_t iterationCount, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageRichardsonLucyDeConvolve_ARGB8888
 * @abstract Perform N iterations of a Lucy-Richardson deconvolution on ARGB8888 data.
 * @discussion
 *  This routine iteratively uses the following formula:
 *      <pre>@textblock
 *        e[i+1] = e[i] x (psf0 * ( e[0] / (psf1 * e[i]) ) )
 *
 *        where:
 *              e[0] = the observed image (src parameter)
 *              e[n] = the result of the nth iteration
 *              psf  = point spread function (kernel for call to convolution)
 *                x    = multiply operator
 *              '*'  = convolution operator
 *      @/textblock </pre>
 *  The channels are operated on independently of one another. Consequently, this function will work on
 *  any 4-channel interleaved 8-bit per component format (e.g. RGBA, BGRA...), not just ARGB.
 *
 *  The work in these functions is currently done internally with floating point precision. If you plan to call this function multiple times
 *  (rather than with iterationCount > 1) on 8-bit per channel images, you can save some computation by converting the 8-bit image data to
 *  single precision floating-point yourself using something like vImageConvert_Planar8toPlanarF and iterating on the appropriate
 *  floating-point Richardson Lucy variant. Convert back, when you are done.
 *
 *  Does not work in place.
 *
 * @param src           The input image
 *
 * @param dest          A preallocated buffer to receive the result image.
 *                      This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const int16_t kernel[9] = { 1, 2, 1,
 *                                                      2, 4, 2,
 *                                                      1, 2, 1 };
 *                          @/textblock </pre>
 *
 *                          This is psf0 in the formula given in the discussion.
 *
 *  @param kernel2          A pointer to a second 1D array of weights of dimension kernel_height2 x kernel_width2.
 *                          This is psf1 in the formula given in the discussion.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_height2   The height of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param kernel_width2    The width of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param divisor          The divisor to use to correct for the volume under kernel.
 *
 *  @param divisor2         The divisor to use to correct for the volume under kernel2.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param iterationCount   The number of Richardson-Lucy iterations to perform on the data before returning.
 *                          If 0, the src buffer is coped to dest.
 *
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *              kvImageLeaveAlphaUnchanged  Operate only on the last 3 channels in memory. Leave the first channel
 *                                          unmodified.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageRichardsonLucyDeConvolve_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const int16_t *kernel, const int16_t *kernel2, uint32_t kernel_height, uint32_t kernel_width, uint32_t kernel_height2, uint32_t kernel_width2, int32_t divisor, int32_t divisor2, const Pixel_8888 backgroundColor,  uint32_t iterationCount, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageRichardsonLucyDeConvolve_ARGBFFFF
 * @abstract Perform N iterations of a Lucy-Richardson deconvolution on ARGBFFFF data
 * @discussion
 *  This routine iteratively uses the following formula:
 *      <pre>@textblock
 *        e[i+1] = e[i] x (psf0 * ( e[0] / (psf1 * e[i]) ) )
 *
 *        where:
 *              e[0] = the observed image (src parameter)
 *              e[n] = the result of the nth iteration
 *              psf  = point spread function (kernel for call to convolution)
 *                x    = multiply operator
 *              '*'  = convolution operator
 *      @/textblock </pre>
 *
 *  The channels are operated on independently of one another. Consequently, this function will work on
 *  any 4-channel interleaved 8-bit per component format (e.g. RGBA, BGRA...), not just ARGB.
 *
 *  Does not work in place.
 * @param src           The input image
 *
 * @param dest          A preallocated buffer to receive the result image.
 *                      This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel           A pointer to a 1D array of weights of dimension kernel_height x kernel_width.
 *                          For example, for a simple 3x3 blur, it might be:
 *                          <pre>@textblock
 *                          const float kernel[9] =   { 1./16, 2./16, 1./16,
 *                                                      2./16, 4./16, 2./16,
 *                                                      1./16, 2./16, 1./16 };
 *                          @/textblock </pre>
 *
 *                          This is psf0 in the formula given in the discussion.
 *
 *  @param kernel2          A pointer to a second 1D array of weights of dimension kernel_height2 x kernel_width2.
 *                          This is psf1 in the formula given in the discussion.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel1. It must be an odd number.
 *
 *  @param kernel_height2   The height of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param kernel_width2    The width of the 2D table of weights passed in as kernel2. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param iterationCount   The number of Richardson-Lucy iterations to perform on the data before returning.
 *                          If 0, the src buffer is coped to dest.
 *
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *              kvImageLeaveAlphaUnchanged  Operate only on the last 3 channels in memory. Leave the first channel
 *                                          unmodified.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageNullPointerArgument  kernel may not be NULL
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageRichardsonLucyDeConvolve_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, const float *kernel, const float *kernel2, uint32_t kernel_height, uint32_t kernel_width, uint32_t kernel_height2, uint32_t kernel_width2, const Pixel_FFFF backgroundColor,  uint32_t iterationCount, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @functiongroup Box Convolve
 *  @discussion  A box convolve is a special case of the standard convolution, which has a much faster algorithm.
 *               A box kernel is a kernel populated with all 1's. This returns the unweighted average of the pixels
 *               near the result pixel. Unlike general purpose convolution, the cost of a box convolve algorithm
 *               is theoretically constant regardless of kernel size, so it can be an inexpensive choice for very
 *               large blurs. The box convolve behavior may not be as desired, however, because the blur effect is
 *               rectangular in appearance, whereas most expect it to be gaussian or roughly circular. A tent blur has
 *               similar rectangular character, but because the weighting is non uniform, it is less obvious. A
 *               tent blur is also constant cost per pixel, but larger constant cost.
 *
 *               Two box blur passes of the same kernel size results in a tent blur.  To see this we can take a 1-d starting image:
 *                <pre> @textblock
 *                      p0          p1          p2          p3          p4          p5          p6
 *                @/textblock </pre>
 *               and blur it once with a 3 wide box blur, ignoring for the moment what happens at the edges:
 *                <pre> @textblock
 *                     (edge)    p0+p1+p2    p1+p2+p3    p2+p3+p4    p3+p4+p5    p4+p5+p6    p5+p6+p7
 *                @/textblock </pre>
 *               and a second time:
 *                <pre> @textblock
 *                     (edge)           p0+2p1+3p2+2p3+p4       p2+2p3+3p4+2p5+p6       p4+2p5+3p6+2p7+p8 .....
 *                               (edge)             p1+2p2+3p3+2p4+p5       p3+2p4+3p5+2p6+p7
 *                @/textblock </pre>
 *
 *                If this process is continued, the shape converges towards a gaussian over multiple passes.
 *      <pre>@textblock
 *              =========================================================================
 *              ======    MOST VIMAGE CONVOLUTION FUNCTIONS DO NOT WORK IN PLACE    =====
 *              =========================================================================
 *      @/textblock </pre>
 */

/*!
 *  @function vImageBoxConvolve_Planar8
 *  @abstract Special purpose box convolution on a Planar8 image.
 *  @discussion This filter applies a box filter to a Planar8 image.  A box filter uses a much faster algorithm
 *              than a standard convolution, and may be a good solution for real time application of large blur
 *              radii against images.
 *              For each pixel:
 *      <pre>@textblock
 *          vImagePixelCount kernel_area = kernel_height * kernel_width;
 *          for each pixel[y][x] in image{
 *              int sum = 0;
 *
 *              // Calculate unweighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                              [x+j+srcOffsetToROI_X- kernel_width/2];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sum = (sum + kernel_area/2) / kernel_area;
 *
 *              // write out result
 *              result[y][x] = CLAMP(sum, 0, 255);
 *          }
 *      @/textblock </pre>
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageBoxConvolve_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageBoxConvolve_ARGB8888
 *  @abstract Special purpose box convolution on a 4-channel interleaved, 8-bit per channel image.
 *  @discussion This filter applies a box filter to a 4-channel interleaved, 8-bit per channel imagee.
 *              A box filter uses a much faster algorithm than a standard convolution, and may be a good
 *              solution for real time application of large blur radii against images.
 *              For each pixel:
 *      <pre>@textblock
 *          vImagePixelCount kernel_area = kernel_height * kernel_width;
 *          for each pixel[y][x] in image{
 *              int sumA = 0;
 *              int sumR = 0;
 *              int sumG = 0;
 *              int sumB = 0;
 *
 *              // Calculate unweighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2][3];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sumA = (sumA + kernel_area/2) / kernel_area;
 *              sumR = (sumR + kernel_area/2) / kernel_area;
 *              sumG = (sumG + kernel_area/2) / kernel_area;
 *              sumB = (sumB + kernel_area/2) / kernel_area;
 *
 *              // write out result
 *              result[y][x][0] = CLAMP(sumA, 0, 255);
 *              result[y][x][1] = CLAMP(sumR, 0, 255);
 *              result[y][x][2] = CLAMP(sumG, 0, 255);
 *              result[y][x][3] = CLAMP(sumB, 0, 255);
 *          }
 *      @/textblock </pre>
 *
 *              This filter does not work in place.
 *
 *              This filter will work without modification for other byte orders such as RGBA, BGRA, AGBR, CMYK, etc.
 *              The image should be non-premultiplied to avoid odd results in non-opaque regions.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageLeaveAlphaUnchanged  Apply the convolution to the last three channels in memory, only.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageBoxConvolve_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @functiongroup Tent Convolve
 *  @discussion  A tent convolve is a special case of the standard convolution, which has a much faster algorithm.
 *               A tent kernel is a kernel populated with slope of 1 rising towards the center. 2D kernels
 *               are calculated as the product of the appropriate 1D kernels. e.g.
 *               <pre> @textblock
 *                  {1,2,3,2,1}
 *               @/textblock </pre>
 *               or
 *               <pre> @textblock
 *               |1|                { {1,2,1},
 *               |2| *  {1,2,1} =     {2,4,2},
 *               |1|                  {1,2,1}   }
 *               @/textblock </pre>
 *               This returns a blur that is more heavily weighted towards the center pixel than a box blur.
 *
 *               Unlike general purpose convolution, the cost of a tent convolve algorithm is theoretically constant
 *               regardless of kernel size, so it can be an inexpensive choice for very large blurs. The tent convolve
 *               behavior may not be as desired, however, because the blur effect is rectangular in appearance, whereas
 *               most expect a blurring effect to be gaussian or roughly circular. The rectangularity of the tent blur
 *               is less obvious than the box blur.  A tent blur has somewhat larger constant cost than a box blur.
 *
 *      <pre>@textblock
 *              =========================================================================
 *              ======    MOST VIMAGE CONVOLUTION FUNCTIONS DO NOT WORK IN PLACE    =====
 *              =========================================================================
 *      @/textblock </pre>
 */

/*!
 *  @function vImageTentConvolve_Planar8
 *  @abstract Special purpose tent convolution on a Planar8 image.
 *  @discussion This filter applies a tent filter to a Planar8 image.  A tent filter uses a much faster algorithm
 *              than a standard convolution, and may be a good solution for real time application of large blur
 *              radii against images.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sum = 0;
 *              int divisor = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sum += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2]
 *                               [x+j+srcOffsetToROI_X- kernel_width/2];
 *                  divisor += kernel_element[i][j];
 *              }
 *
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sum = (sum + divisor/2) / divisor;
 *
 *              // write out result
 *              result[y][x] = CLAMP(sum, 0, 255);
 *          }
 *      @/textblock </pre>
 *
 *              This filter does not work in place.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageTentConvolve_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, Pixel_8 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));




/*!
 *  @function vImageTentConvolve_ARGB8888
 *  @abstract Special purpose tent convolution on a 4-channel interleaved, 8-bit per channel image.
 *  @discussion This filter applies a tent filter to a 4-channel interleaved, 8-bit per channel imagee.
 *              A tent filter uses a much faster algorithm than a standard convolution, and may be a good
 *              solution for real time application of large blur radii against images.
 *              For each pixel:
 *      <pre>@textblock
 *          for each pixel[y][x] in image{
 *              int sumA = 0;
 *              int sumR = 0;
 *              int sumG = 0;
 *              int sumB = 0;
 *              int divisor = 0;
 *
 *              // Calculate weighted average over kernel area
 *              for each kernel_element[i][j] in kernel{
 *                  sumA += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][0];
 *                  sumR += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][1];
 *                  sumG += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][2];
 *                  sumB += kernel_element[i][j] *
 *                          pixel[y+i+srcOffsetToROI_Y-kernel_height/2][x+j+srcOffsetToROI_X- kernel_width/2][3];
 *                  divisor += kernel_element[i][j];
 *              }
 *              // Correct for the scaling introduced by multiplying by the weights table
 *              sumA = (sumA + divisor/2) / divisor;
 *              sumR = (sumR + divisor/2) / divisor;
 *              sumG = (sumG + divisor/2) / divisor;
 *              sumB = (sumB + divisor/2) / divisor;
 *
 *              // write out result
 *              result[y][x][0] = CLAMP(sumA, 0, 255);
 *              result[y][x][1] = CLAMP(sumR, 0, 255);
 *              result[y][x][2] = CLAMP(sumG, 0, 255);
 *              result[y][x][3] = CLAMP(sumB, 0, 255);
 *          }
 *      @/textblock </pre>
 *
 *              This filter does not work in place.
 *
 *              This filter will work without modification for other byte orders such as RGBA, BGRA, AGBR, CMYK, etc.
 *              The image should be non-premultiplied to avoid odd results in non-opaque regions.
 *
 *  @param src              The input image
 *
 *  @param dest             A pointer to a preallocated vImage_Buffer to receive the result image.
 *                          This may not alias the src image.
 *
 *  @param tempBuffer       An optional pointer to a region of memory to use as a working area during
 *                          computation.  The size of the tempBuffer is given by calling the function
 *                          with the same parameters and the kvImageGetTempBufferSize flag, in which case
 *                          the size is returned instead of an error code from the left hand side.
 *                          You may pass NULL here, in which case a region of memory of similar size
 *                          will be allocated by the function and freed before it returns.  Temp Buffers
 *                          are a way of avoiding lost time due to VM faults to initialize newly allocated
 *                          buffers. If you will be calling this function repeatedly with similar parameters
 *                          you should use a temp buffer. If the function is called from different threads
 *                          concurrently, a different temp buffer should be used for each.
 *
 *  @param srcOffsetToROI_X An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the left edge of the image.
 *
 *  @param srcOffsetToROI_Y An offset used in tiling to shift the position of the destination image
 *                          relative to the src image. Typically this is 0. Non-Zero values are needed
 *                          when the destination tile is not aligned with the top edge of the image.
 *
 *  @param kernel_height    The height of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param kernel_width     The width of the 2D table of weights passed in as kernel. It must be an odd number.
 *
 *  @param backgroundColor  When the kvImageBackgroundColorFill edging mode is active, the backgroundColor
 *                          parameter provides the background color to be used for missing pixels when the
 *                          kernel extends off the edge of the image.
 *
 *  @param flags            The following flags are allowed:
 *      <pre>@textblock
 *              kvImageCopyInPlace          If any pixels covered by the kernel do no not exist, simply
 *                                          copy the corresponding source pixel to the destination. This
 *                                          will result in a ring on unconvolved content at the edges
 *                                          and convolved content in the middle.
 *
 *              kvImageBackgroundColorFill  Substitute in the provided background color for missing pixels.
 *                                          For a blur filter this will look like the edges of the image
 *                                          have been blurred into a particular color. This is usually
 *                                          appropriate when the color of the surface onto which the image
 *                                          will be drawn is known.  You can also use {Alpha = 0, color = 0}
 *                                          as a background color, and let the downstream image compositor
 *                                          blend in the background color. In this case, the result image
 *                                          should be treated as a premultiplied image.
 *
 *              kvImageEdgeExtend           Substitute in the nearest defined pixel for missing pixels.
 *                                          This is better when the image is drawn into a frame or other
 *                                          context where the background is not expected to contribute to
 *                                          the final content.
 *
 *              kvImageTruncateKernel       This is similar to kvImageEdgeExtend, except that edge pixels are
 *                                          simply ignored, and the bias and divisor are adjusted accordingly.
 *                                          Because of the complication at the edges, this edging mode can
 *                                          be significantly slower than the others. It can be numerically
 *                                          unstable if the sum over any rectangular subsection of the kernel
 *                                          is zero, which can result in division by zero during the computation.
 *                                          The results for this edging mode are usually quite visually similar
 *                                          to kvImageEdgeExtend.
 *
 *              kvImageGetTempBufferSize    Instead of calculating the convolution of the image, return the
 *                                          size of the temp buffer needed for this set of parameters. Does
 *                                          not touch the src or dest image.
 *
 *              kvImageLeaveAlphaUnchanged  Apply the convolution to the last three channels in memory, only.
 *
 *              kvImageDoNotTile            Disable internal multithreading.
 *
 *      @/textblock </pre>
 *                          The first four flags listed are edging modes. One and only one edging mode must be
 *                          provided. If no edging mode is provided (e.g. kvImageNoFlags), then an error will
 *                          be returned.
 *
 *  @return One of the following error codes may be returned:
 *      <pre>@textblock
 *          kvImageNoError              Success.
 *              >= 0                    If kvImageGetTempBufferSize is passed, this is the size of
 *                                      the temp buffer to use with this function and this set of
 *                                      parameters.
 *
 *          kvImageInvalidEdgeStyle     One and only one of the following flags must be provided:
 *                                         { kvImageCopyInPlace, kvImageBackgroundColorFill,
 *                                           kvImageEdgeExtend,  kvImageTruncateKernel }
 *
 *          kvImageRoiLargerThanInputBuffer The dest->width and height must be less than or equal
 *                                          to corresponding dimensions of the source buffer.
 *
 *          kvImageInvalidOffset_X      The destination width + srcOffsetToROI_X > src->width
 *
 *          kvImageInvalidOffset_Y      The destination height + srcOffsetToROI_Y > src->height
 *
 *          kvImageMemoryAllocationError Could not allocate memory to serve as a temp buffer.
 *
 *          kvImageInvalidKernelSize    The kernel height and width must be odd numbers.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageTentConvolve_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, uint32_t kernel_height, uint32_t kernel_width, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


#ifdef __cplusplus
}
#endif

#endif


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Alpha.h
/*!
*  @header Alpha.h
*  vImage Framework
*
*  @copyright Copyright (c) 2003-2016 by Apple Inc. All rights reserved.
*
*  See vImage/vImage.h for more on how to better view the headerdoc documentation for functions declared herein.
*
*  @discussion
*  Alpha compositing is the practice of blending one image into another using a coverage component, commonly called
*  alpha, to indicate how opaque each pixel is.  A value of 1.0 (or 255 if an 8-bit image) indicates the
*  pixel is fully opaque. A value of 0 indicates that it is fully transparent. Values in beetween establish a linear scale
*  of partial opacity. Alpha can be used to slowly fade one image into another. Some images such as icons, have opqaue
*  portions and transparent portions. A blending operation based on alpha is required to draw these over a background
*  image.
*
*  In its simplest form, an alpha blend is a linear interpolation between pixel of two images:
* <pre>@textblock
*      float: new color =  top color * alpha + (1-alpha) * bottom color
*      8-bit: new color =  (top color * alpha + (255-alpha) * bottom color + 127) / 255
* @/textblock </pre>
*  However, these formulations are a bit too simplistic because they do not account for the possibility that
*  the bottom layer may have some transparency to it too. One subtlety of adding transparency to the bottom image
*  is that the result image has an alpha component to it too that should be divided out:
* <pre>@textblock
*      float: new alpha =  top alpha + (1-top alpha) * bottom alpha
*      float: new color =  (top color * top alpha + (1- top alpha) * bottom color * bottom alpha ) / new alpha
*
*      8-bit: new alpha =  top alpha + ((255-top alpha) * bottom alpha + 127)/255
*      8-bit: new color =  (255 * top color * top alpha + (255- top alpha) * bottom color * bottom alpha + (255*255)/2) / (255 * 255 * new alpha)
* @/textblock </pre>
* As might be imagined, this extra complication may become costly.  At the cost of a small amount of accuracy in non-opaque regions
* of the image, it is common to leave the alpha multiplied into the image. This avoids the division at the end. In addition,
* if the result is used in another blend, we don't have to multiply the alpha back in at that stage, which simplifies things
* all around. Such images with the alpha channel multiplied into the other channels are called pre-multiplied images.
*
* This gives us the following for premultiplied alpha blends:
* <pre>@textblock
*      float: new alpha               = top alpha +               (1 - top alpha) * bottom alpha
*      float: new premultiplied color = top premultiplied color + (1 - top alpha) * bottom premultiplied color
*
*      8-bit: new alpha               =  top alpha +               ((255 - top alpha) * bottom alpha + 127)/255
*      8-bit: new premultiplied color =  top premultiplied color + ((255 - top alpha) * bottom premultiplied color + 127)/255
* @/textblock </pre>
*
* Some images are known to be fully opaque. For such images, alpha is 1.0 (or 255) for all the pixels, and consequently
* do not require any computation to be done to interconvert between premultiplied and non-premultiplied states. That is,
* since x/1 = x  and x*1 = x, they are non-premultiplied and premultiplied at the same time, and can serve in either
* capacity as needed.  Because of this special status, CoreGraphics labels them kCGImageAlphaNoneSkipFirst/Last. vImage
* does similar things in vImage_Utilities.h. Labelling opaque buffers as such enables cheaper alpha blending and will frequently
* will let us avoid blending all-together as a simple copy operation will do.
*
* This header provides compositing (blending) operations for both premultiplied and non-premultiplied images, functions
* to premultiply or unpremultiply images, and some functions to make sure that the color channel values do not exceed
* the alpha value, which sometimes can happen after a convolution, morphological or resampling operation.
*
*    These alpha compositing functions assume that the floating point range is 0.0 (black) to 1.0 (full intensity color).
*    While values outside these ranges do function correctly (you can have 110% intensity color or -50% color, for example),
*    the calculation is done assuming 1.0 represents full intensity color, and 0.0 represents the absence of color. If these
*    assumptions are not correct, then the calculation will produce incorrect results. Apple does not currently provide
*    alpha compositing functions that work with any floating point range in vImage.
*
*    8 bit functionality assumes that 0 is no color, and 255 is full color. 8 bit functions do saturated clipping before
*    converting from internal precision back to 8 bits to make sure that modulo overflow does not occur. The internal
*  calculation is done with higher precision.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*/

#ifndef VIMAGE_ALPHA_H
#define VIMAGE_ALPHA_H

#include <vImage/vImage_Types.h>

#ifdef __cplusplus
extern "C" {
#endif

/*!
 * @functiongroup Alpha Blend
 * @discussion The Alpha Blend functions composite two non-premultiplied images together to produce a non-premultiplied result.
 *             They are in general more expensive than their premultiplied counterparts, because the alpha has to be
 *             divided out of the result at the end.
 */

/*!
 * @function vImageAlphaBlend_Planar8
 * @abstract Composite two non-premultiplied planar 8-bit images, to produce a non-premultiplied result.
 * @discussion
 *
 *      For each color channel:
 * <pre>@textblock
 *          float destColor = (  srcTopColor * srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha * srcBottomColor ) / alpha
 * @/textblock </pre>
 *      alpha (the new alpha value for that pixel) is calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock </pre>
 *      For planar data, you need to calculate alpha yourself ahead of time and provide that as an argument to this function.
 *      This can be done using:
 * <pre>@textblock
 *          vImagePremultipliedAlphaBlend_Planar8( srcTopAlpha, srcTopAlpha, srcBottomAlpha, alpha, kvImageNoFlags );
 * @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The ARGB versions work for all 4 channel 8-bit / channel image formats with alpha first in memory.
 *
 * @param srcTop        The color image that is composited on top of the bottom image
 * @param srcTopAlpha   The alpha channel corresponding to the srcTop image
 * @param srcBottom     The color image that is below the srcTop image, into which it is blended
 * @param srcBottomAlpha The alpha channel corresponding to the srcBottom image
 * @param alpha         The alpha channel for the destination image. You need to calculate this ahead of time as:
 * <pre>@textblock
 *          vImagePremultipliedAlphaBlend_Planar8( srcTopAlpha, srcTopAlpha, srcBottomAlpha, alpha, kvImageNoFlags );
 * @/textblock </pre>
 * @oaram dest          The non-premultiplied result will be written here.
 *
 * @param flags         The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Disables internal multithreading. This may be useful if you are writing your own multithreaded tiling engine.
 *  @/textblock</pre>
 * @result              The following result codes may be returned:
 *  <pre>@textblock
 *          kvImageNoError                      Success!
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer must be no larger than srcTop, srcBottom, srcTopAlpha, srcBottomAlpha and alpha.
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_Planar8( const vImage_Buffer *srcTop, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *srcBottomAlpha, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags )  VIMAGE_NON_NULL(1,2,3,4,5,6)  API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_PlanarF
 * @abstract Composite two non-premultiplied planar floating-point images, to produce a non-premultiplied result.
 * @discussion
 *
 *      For each color channel:
 * <pre>@textblock
 *          float destColor = (  srcTopColor * srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha * srcBottomColor ) / alpha
 * @/textblock </pre>
 *      alpha (the new alpha value for that pixel) is calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock </pre>
 *      For planar data, you need to calculate alpha yourself ahead of time and provide that as an argument to this function.
 *      This can be done using:
 * <pre>@textblock
 *          vImagePremultipliedAlphaBlend_PlanarF( srcTopAlpha, srcTopAlpha, srcBottomAlpha, alpha, kvImageNoFlags );
 * @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The ARGB versions work for all 4 channel 8-bit / channel image formats with alpha first in memory.
 *
 * @param srcTop        The color image that is composited on top of the bottom image
 * @param srcTopAlpha   The alpha channel corresponding to the srcTop image
 * @param srcBottom     The color image that is below the srcTop image, into which it is blended
 * @param srcBottomAlpha The alpha channel corresponding to the srcBottom image
 * @param alpha         The alpha channel for the destination image. You need to calculate this ahead of time as:
 * <pre>@textblock
 *                          vImagePremultipliedAlphaBlend_PlanarF( srcTopAlpha, srcTopAlpha, srcBottomAlpha, alpha, kvImageNoFlags );
 * @/textblock </pre>
 * @oaram dest          The non-premultiplied result will be written here.
 *
 * @param flags         The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Disables internal multithreading. This may be useful if you are writing your own multithreaded tiling engine.
 *  @/textblock</pre>
 * @result              The following result codes may be returned:
 *  <pre>@textblock
 *          kvImageNoError                      Success!
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer must be no larger than srcTop, srcBottom, srcTopAlpha, srcBottomAlpha and alpha.
 *  @/textblock</pre>
 */

VIMAGE_PF vImage_Error    vImageAlphaBlend_PlanarF( const vImage_Buffer *srcTop, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *srcBottomAlpha, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags )  VIMAGE_NON_NULL(1,2,3,4,5,6)  API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_ARGB8888
 * @abstract Composite two non-premultiplied ARGB8888 images, to produce a non-premultiplied result.
 * @discussion
 *
 *      For each color channel:
 * <pre>@textblock
 *          float destColor = (  srcTopColor * srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha * srcBottomColor ) / alpha
 * @/textblock </pre>
 *      alpha (the new alpha value for that pixel) is calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock </pre>
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The ARGB versions work for all 4 channel 8-bit / channel image formats with alpha first in memory.
 *
 * @param srcTop        The image that is composited on top of the bottom image. The alpha channel must appear first.
 * @param srcBottom     The image that is below the srcTop image, into which it is blended. The alpha channel must appear first.
 * @oaram dest          The non-premultiplied result will be written here.
 *
 * @param flags         The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Disables internal multithreading. This may be useful if you are writing your own multithreaded tiling engine.
 *  @/textblock</pre>
 * @result              The following result codes may be returned:
 *  <pre>@textblock
 *          kvImageNoError                      Success!
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer must be no larger than srcTop, srcBottom, srcTopAlpha, srcBottomAlpha and alpha.
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_ARGB8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_ARGBFFFF
 * @abstract Composite two non-premultiplied ARGBFFFF images, to produce a non-premultiplied result.
 * @discussion
 *
 *      For each color channel:
 * <pre>@textblock
 *          float destColor = (  srcTopColor * srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha * srcBottomColor ) / alpha
 * @/textblock </pre>
 *      alpha (the new alpha value for that pixel) is calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock </pre>
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The ARGB versions work for all 4 channel float / channel image formats with alpha first in memory.
 *
 * @param srcTop        The image that is composited on top of the bottom image. The alpha channel must appear first.
 * @param srcBottom     The image that is below the srcTop image, into which it is blended. The alpha channel must appear first.
 * @oaram dest          The non-premultiplied result will be written here.
 *
 * @param flags         The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Disables internal multithreading. This may be useful if you are writing your own multithreaded tiling engine.
 *  @/textblock</pre>
 * @result              The following result codes may be returned:
 *  <pre>@textblock
 *          kvImageNoError                      Success!
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer must be no larger than srcTop, srcBottom, srcTopAlpha, srcBottomAlpha and alpha.
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_ARGBFFFF( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)  API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @functiongroup Alpha Blend (Premultiplied)
 * @discussion  Premultiplied alpha blends composite two premultiplied images to produce a premultiplied result.
 */

/*!
 * @function vImagePremultipliedAlphaBlend_Planar8
 * @abstract blend two premultiplied Planar8 images to produce a premultiplied Planar8 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = srcTopColor  + ((255 - srcTopAlpha) * srcBottomColor + 127)/255;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 * @param srcTop            The top image
 * @param srcTopAlpha       The coverage component for the top image (alpha)
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_Planar8( const vImage_Buffer *srcTop,
                                                                const vImage_Buffer *srcTopAlpha,
                                                                const vImage_Buffer *srcBottom,
                                                                const vImage_Buffer *dest,
                                                                vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedAlphaBlend_PlanarF
 * @abstract blend two premultiplied PlanarF images to produce a premultiplied PlanarF result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          float destColor = srcTopColor  + (1.0 - srcTopAlpha) * srcBottomColor;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0.0f, 1.0f].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 * @param srcTop            The top image
 * @param srcTopAlpha       The coverage component for the top image (alpha)
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */

VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_PlanarF( const vImage_Buffer *srcTop,
                                                                const vImage_Buffer *srcTopAlpha,
                                                                const vImage_Buffer *srcBottom,
                                                                const vImage_Buffer *dest,
                                                                vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedAlphaBlend_ARGB8888
 * @abstract blend two premultiplied ARGB8888 images to produce a premultiplied ARGB8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = srcTopColor  + ((255 - srcTopAlpha) * srcBottomColor + 127)/255;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha first in memory, not just ARGB.
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_ARGB8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedAlphaBlend_BGRA8888
 * @abstract blend two premultiplied BGRA8888 images to produce a premultiplied BGRA8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = srcTopColor  + ((255 - srcTopAlpha) * srcBottomColor + 127)/255;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha last in memory, not just BGRA.
 *      Also available as vImagePremultipliedAlphaBlend_RGBA8888().
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_BGRA8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImagePremultipliedAlphaBlend_RGBA8888( _srcTop, _srcBottom, _dest, _flags )    vImagePremultipliedAlphaBlend_BGRA8888((_srcTop), (_srcBottom), (_dest), (_flags))

/*!
 * @function vImagePremultipliedAlphaBlend_ARGBFFFF
 * @abstract blend two premultiplied ARGBFFFF images to produce a premultiplied ARGBFFFF result.
 * @discussion
 * <pre>@textblock
 *          float destColor = srcTopColor  + (1.0 - srcTopAlpha) * srcBottomColor;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0.0f, 1.0f].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel float / channel image formats with alpha first in memory, not just ARGB.
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_ARGBFFFF( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedAlphaBlend_BGRAFFFF
 * @abstract blend two premultiplied ARGBFFFF images to produce a premultiplied BGRAFFFF result.
 * @discussion
 * <pre>@textblock
 *          float destColor = srcTopColor  + (1.0 - srcTopAlpha) * srcBottomColor;
 * @/textblock</pre>
 *      Similarly, the output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          float alpha =  srcTopAlpha + (1.0 - srcTopAlpha) * srcBottomAlpha
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0.0f, 1.0f].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel float / channel image formats with alpha first in memory, not just BGRA.
 *      Also available as vImagePremultipliedAlphaBlend_RGBAFFFF.
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlend_BGRAFFFF( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImagePremultipliedAlphaBlend_RGBAFFFF( _srcTop, _srcBottom, _dest, _flags )    vImagePremultipliedAlphaBlend_BGRAFFFF((_srcTop), (_srcBottom), (_dest), (_flags))

/*!
 * @function vImagePremultipliedAlphaBlendWithPermute_ARGB8888
 * @abstract Reorder the channels of the top 8-bit 4 channel premultiplied image, blend into a bottom premultiplied ARGB8888 image.
 * @discussion
 *  This function does 3 things.
 * <pre>@textblock
 *      1. Changes the order of channels of srcTop according to permuteMap.
 *      2. A premultiplied alpha compositing.
 *      3. Set destA to 0xFF when makeDestAlphaOpaque is true.
 *
 *      permuteMap[i] = 0, 1, 2, or 3 to specify how we permute each channel in srcTop.
 *
 *      permuteMap[0] tells which channel in srcTop XXXX8888 will be used as A.
 *      permuteMap[1] tells which channel in srcTop XXXX8888 will be used as R.
 *      permuteMap[2] tells which channel in srcTop XXXX8888 will be used as G.
 *      permuteMap[3] tells which channel in srcTop XXXX8888 will be used as B.
 * @/textblock</pre>
 *
 *  This permuteMap lets us to cover any channel order for the top and bottom images.
 *  For example,
 * <pre>@textblock
 *      PremultipliedAlphaBlend(srcTop_ARGB8888, srcBottom_ARGB8888) -> dest_ARGB8888
 *                     will be covered by permuteMap[4] = {0, 1, 2, 3}
 *
 *      PremultipliedAlphaBlend(srcTop_RGBA8888, srcBottom_ARGB8888) -> dest_ARGB8888
 *                     will be covered by permuteMap[4] = {3, 0, 1, 2}
 *
 *      PremultipliedAlphaBlend(srcTop_ABGR8888, srcBottom_ARGB8888) -> dest_ARGB8888
 *                     will be covered by permuteMap[4] = {0, 3, 2, 1}
 *
 *      PremultipliedAlphaBlend(srcTop_BGRA8888, srcBottom_ARGB8888) -> dest_ARGB8888
 *                     will be covered by permuteMap[4] = {3, 2, 1, 0}
 * @/textblock</pre>
 *
 *  srcBottom will have the same pixel format (ARGB8888) as dest.
 *
 *
 *  The per-pixel operation is:
 *
 * <pre>@textblock
 *  uint8_t *srcTop, *srcBottom, *dest;
 *  uint8_t srcTopA, srcTopR, srcTopG, srcTopB;
 *  uint8_t srcBottomA, srcBottomR, srcBottomG, srcBottomB;
 *  uint8_t destA, destR, destG, destB;
 *
 *  srcTopA = srcTop[ permuteMap[0] ];
 *  srcTopR = srcTop[ permuteMap[1] ];
 *  srcTopG = srcTop[ permuteMap[2] ];
 *  srcTopB = srcTop[ permuteMap[3] ];
 *
 *  srcBottomA = srcBottom[ 0 ];
 *  srcBottomR = srcBottom[ 1 ];
 *  srcBottomG = srcBottom[ 2 ];
 *  srcBottomB = srcBottom[ 3 ];
 *
 *  destR = (srcTopR * 255 + (255 - srcTopA) * srcBottomR + 127) / 255;
 *  destG = (srcTopG * 255 + (255 - srcTopA) * srcBottomG + 127) / 255;
 *  destB = (srcTopB * 255 + (255 - srcTopA) * srcBottomB + 127) / 255;
 *
 *  if(makeDestAlphaOpaque)
 *  {
 *
 *      dest[0] = 0xFF;
 *      dest[1] = destR;
 *      dest[2] = destG;
 *      dest[3] = destB;
 *  }
 *  else
 *  {
 *      destA = (srcTopA * 255 + (255 - srcTopA) * srcBottomA + 127) / 255;
 *
 *      dest[0] = destA;
 *      dest[1] = destR;
 *      dest[2] = destG;
 *      dest[3] = destB;
 *  }
 * @/textblock</pre>
 *
 *  This function can work in place.
 *
 *  @param srcTop       A pointer to vImage_Buffer that references 8-bit XXXX interleaved source top image.
 *  @param srcBottom    A pointer to vImage_Buffer that references 8-bit ARGB interleaved source bottom image.
 *  @param dest         A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination image.
 *  @param permuteMap   Values that can be used to switch the channel order of the source top image.
 *  @param makeDestAlphaOpaque  A boolean to set destA into 0xFF when it's true.
 *  @param flags        The following flags are allowed:
 * <pre>@textblock
 *      kvImageGetTempBufferSize    Returns 0. Does no work.
 *      kvImageDoNotTile            Disables internal multithreading, if any.
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *      kvImageNoError                  Is returned when there was no error.
 *      kvImageRoiLargerThanInputBuffer The destination buffers are larger than the source buffer.
 *      kvImageBufferSizeMismatch       Is returned when there is a mismatch in width & height of srcTop and srcBottom.
 *      kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 *      kvImageInvalidParameter         Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error vImagePremultipliedAlphaBlendWithPermute_ARGB8888(const vImage_Buffer *srcTop,
                                                                         const vImage_Buffer *srcBottom,
                                                                         const vImage_Buffer *dest,
                                                                         const uint8_t permuteMap[4],
                                                                         bool makeDestAlphaOpaque,
                                                                         vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*!
 * @function vImagePremultipliedAlphaBlendWithPermute_RGBA8888
 * @abstract Reorder the channels of the top 8-bit 4 channel premultiplied image, blend into a bottom premultiplied RGBA8888 image.
 * @discussion
 *  This function does 3 things.
 * <pre>@textblock
 *      1. Changes the order of channels of srcTop according to permuteMap.
 *      2. A premultiplied alpha compositing.
 *      3. Set destA to 0xFF when makeDestAlphaOpaque is true.
 *
 *      permuteMap[i] = 0, 1, 2, or 3 to specify how we permute each channel in srcTop.
 *
 *      permuteMap[0] tells which channel in srcTop XXXX8888 will be used as A.
 *      permuteMap[1] tells which channel in srcTop XXXX8888 will be used as R.
 *      permuteMap[2] tells which channel in srcTop XXXX8888 will be used as G.
 *      permuteMap[3] tells which channel in srcTop XXXX8888 will be used as B.
 * @/textblock</pre>
 *
 *  This permuteMap lets us to cover any channel order for the top and bottom images.
 *  For example,
 * <pre>@textblock
 *      PremultipliedAlphaBlend(srcTop_RGBA8888, srcBottom_RGBA8888) -> dest_RGBA8888
 *                     will be covered by permuteMap[4] = {0, 1, 2, 3}
 *
 *      PremultipliedAlphaBlend(srcTop_ARGB8888, srcBottom_RGBA8888) -> dest_RGBA8888
 *                     will be covered by permuteMap[4] = {1, 2, 3, 0}
 *
 *      PremultipliedAlphaBlend(srcTop_ABGR8888, srcBottom_RGBA8888) -> dest_RGBA8888
 *                     will be covered by permuteMap[4] = {3, 2, 1, 0}
 *
 *      PremultipliedAlphaBlend(srcTop_BGRA8888, srcBottom_RGBA8888) -> dest_RGBA8888
 *                     will be covered by permuteMap[4] = {2, 1, 0, 3}
 * @/textblock</pre>
 *
 *  srcBottom will have the same pixel format (RGBA8888) as dest.
 *
 *
 *  The per-pixel operation is:
 *
 * <pre>@textblock
 *  uint8_t *srcTop, *srcBottom, *dest;
 *  uint8_t srcTopA, srcTopR, srcTopG, srcTopB;
 *  uint8_t srcBottomA, srcBottomR, srcBottomG, srcBottomB;
 *  uint8_t destA, destR, destG, destB;
 *
 *  srcTopR = srcTop[ permuteMap[0] ];
 *  srcTopG = srcTop[ permuteMap[1] ];
 *  srcTopB = srcTop[ permuteMap[2] ];
 *  srcTopA = srcTop[ permuteMap[3] ];
 *
 *  srcBottomR = srcBottom[ 0 ];
 *  srcBottomG = srcBottom[ 1 ];
 *  srcBottomB = srcBottom[ 2 ];
 *  srcBottomA = srcBottom[ 3 ];
 *
 *  destR = (srcTopR * 255 + (255 - srcTopA) * srcBottomR + 127) / 255;
 *  destG = (srcTopG * 255 + (255 - srcTopA) * srcBottomG + 127) / 255;
 *  destB = (srcTopB * 255 + (255 - srcTopA) * srcBottomB + 127) / 255;
 *
 *  if(makeDestAlphaOpaque)
 *  {
 *
 *      dest[0] = 0xFF;
 *      dest[1] = destR;
 *      dest[2] = destG;
 *      dest[3] = destB;
 *  }
 *  else
 *  {
 *      destA = (srcTopA * 255 + (255 - srcTopA) * srcBottomA + 127) / 255;
 *
 *      dest[0] = destA;
 *      dest[1] = destR;
 *      dest[2] = destG;
 *      dest[3] = destB;
 *  }
 * @/textblock</pre>
 *
 *  This function can work in place.
 *
 *  @param srcTop       A pointer to vImage_Buffer that references 8-bit XXXX interleaved source top image.
 *  @param srcBottom    A pointer to vImage_Buffer that references 8-bit RGBA interleaved source bottom image.
 *  @param dest         A pointer to vImage_Buffer that references 8-bit RGBA interleaved destination image.
 *  @param permuteMap   Values that can be used to switch the channel order of the source top image.
 *  @param makeDestAlphaOpaque  A boolean to set destA into 0xFF when it's true.
 *  @param flags        The following flags are allowed:
 * <pre>@textblock
 *      kvImageGetTempBufferSize    Returns 0. Does no work.
 *      kvImageDoNotTile            Disables internal multithreading, if any.
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *      kvImageNoError                  Is returned when there was no error.
 *      kvImageRoiLargerThanInputBuffer The destination buffers are larger than the source buffer.
 *      kvImageBufferSizeMismatch       Is returned when there is a mismatch in width & height of srcTop and srcBottom.
 *      kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 *      kvImageInvalidParameter         Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error vImagePremultipliedAlphaBlendWithPermute_RGBA8888(const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, const uint8_t permuteMap[4], bool makeDestAlphaOpaque, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 *  @functiongroup Alpha Blend (Premultiplied, other blend modes)
 *  @discussion These alternative blend modes correspond to the blend modes required for SVG
 *              (http://www.w3.org/TR/SVG/filters.html, see feBlend)
 */

/*!
 * @function vImagePremultipliedAlphaBlendMultiply_RGBA8888
 * @abstract blend two premultiplied RGBA8888 images using the Multiply blend mode to produce a premultiplied RGBA8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor =((255 -    srcTopAlpha) * srcBottomColor +
 *                              (255 - srcBottomAlpha) * srcTopColor +
 *                               srcTopColor * srcBottomColor + 127)/255;
 * @/textblock</pre>
 *      The output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha last in memory, not just RGBA.
 *      This function corresponds to the multiply blend mode in feBlend in the SVG standard. http://www.w3.org/TR/SVG/filters.html)
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlendMultiply_RGBA8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*!
 * @function vImagePremultipliedAlphaBlendScreen_RGBA8888
 * @abstract blend two premultiplied RGBA8888 images using the Screen blend mode to produce a premultiplied RGBA8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = CLAMP( srcTopColor + srcBottomcolor - (srcTopColor * srcBottomColor + 127)/255, 0, 255);
 * @/textblock</pre>
 *      The output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha last in memory, not just RGBA.
 *      This function corresponds to the screen blend mode in feBlend in the SVG standard. http://www.w3.org/TR/SVG/filters.html)
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlendScreen_RGBA8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*!
 * @function vImagePremultipliedAlphaBlendDarken_RGBA8888
 * @abstract blend two premultiplied RGBA8888 images using the Darken blend mode to produce a premultiplied RGBA8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = MIN( topColor +    ((255 - srcTopAlpha) *  srcBotomColor + 127) / 255,
 *                                   bottomColor + ((255 - srcBottomAlpha) * srcTopColor + 127) / 255);
 * @/textblock</pre>
 *      The output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha last in memory, not just RGBA.
 *      This function corresponds to the darken blend mode in feBlend in the SVG standard. http://www.w3.org/TR/SVG/filters.html)
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlendDarken_RGBA8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*!
 * @function vImagePremultipliedAlphaBlendLighten_RGBA8888
 * @abstract blend two premultiplied RGBA8888 images using the Lighten blend mode to produce a premultiplied RGBA8888 result.
 * @discussion
 *      For each color channel:
 * <pre>@textblock
 *          uint8_t destColor = MAX( topColor +    ((255 - srcTopAlpha) *  srcBotomColor + 127) / 255,
 *                                   bottomColor + ((255 - srcBottomAlpha) * srcTopColor + 127) / 255);
 * @/textblock</pre>
 *      The output alpha channel (the new alpha value for that pixel) can be calculated as:
 * <pre>@textblock
 *          uint8_t alpha =  srcTopAlpha + ((255 - srcTopAlpha) * srcBottomAlpha + 127)/255;
 * @/textblock</pre>
 *      The alpha values are presumed to be normalized over the range [0, 255].
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel 8-bit / channel image formats with alpha last in memory, not just RGBA.
 *      This function corresponds to the lighten blend mode in feBlend in the SVG standard. http://www.w3.org/TR/SVG/filters.html)
 *
 * @param srcTop            The top image
 * @param srcBottom         The bottom image
 * @param dest              The result image is written here. This buffer must be preallocated before the function is called.
 * @param flags             The following flags may be used:
 * <pre>@textblock
 *      kvImageNoFlags      Default operation
 *
 *      kvImageDoNotTile    Disable internal multithreading. You might want to do that if you are calling
 *                          this in the context of your own multithreaded tiling engine.
 * @/textblock </pre>
 *
 * @result  The following error codes may occur:
 * <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     The height and width of the result must be less than or equal to each of the input buffers.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultipliedAlphaBlendLighten_RGBA8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));




/*!
 *  @functiongroup Premultiply Data
 *  @discussion  Multiply a non-premultiplied (normal) image with its
 *               alpha channel to produce a premultiplied image.
 */

/*!
 *  @function vImagePremultiplyData_Planar8
 *  @abstract Multiply a Planar8 color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          uint8_t destColor = (src * alpha + 127) / 255;
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *  @/textblock </pre>
 *
 *  @param src      The color data to multiply with alpha
 *  @param alpha    The alpha data to multiply against src
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already  multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_Planar8( const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImagePremultiplyData_PlanarF
 *  @abstract Multiply a PlanarF color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          float destColor = src * alpha;
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *  @/textblock </pre>
 *
 *  @param src      The color data to multiply with alpha
 *  @param alpha    The alpha data to multiply against src
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already  multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_PlanarF( const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImagePremultiplyData_ARGB8888
 *  @abstract Multiply a ARGB8888 color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          uint8_t destColor = (src * alpha + 127) / 255;
 *          uint8_t destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel 8-bit / channel image formats with alpha first in memory.
 *      It does not have to be ARGB.
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImagePremultiplyData_ARGBFFFF
 *  @abstract Multiply a ARGBFFFF color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          float destColor = src * alpha;
 *          float destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel float / channel image formats with alpha first in memory.
 *      It does not have to be ARGB.
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImagePremultiplyData_RGBA8888
 *  @abstract Multiply a RGBA8888 color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          uint8_t destColor = (src * alpha + 127) / 255;
 *          uint8_t destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel 8-bit / channel image formats with alpha last in memory.
 *      It does not have to be RGBA. Also available as vImagePremultiplyData_BGRA8888().
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_RGBA8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)   API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImagePremultiplyData_BGRA8888( _src, _dest, _flags )           vImagePremultiplyData_RGBA8888((_src), (_dest), (_flags))

/*!
 *  @function vImagePremultiplyData_RGBAFFFF
 *  @abstract Multiply a RGBAFFFF color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          float destColor = src * alpha;
 *          float destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel float / channel image formats with alpha first in memory.
 *      It does not have to be RGBA. Also available as vImagePremultiplyData_BGRAFFFF().
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_RGBAFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)   API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImagePremultiplyData_BGRAFFFF( _src, _dest, _flags )           vImagePremultiplyData_RGBAFFFF((_src), (_dest), (_flags))

/*!
 *  @function vImagePremultiplyData_ARGB16U
 *  @abstract Multiply a unsigned 16-bit ARGB color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          uint16_t destColor = (src * alpha + 32767) / 65535;
 *          uint16_t destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel uint16_t / channel image formats with alpha first in memory.
 *      It does not have to be ARGB.
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));

/*!
 *  @function vImagePremultiplyData_RGBA16U
 *  @abstract Multiply a unsigned 16-bit RGBA color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *  <pre>@textblock
 *      For each color channel:
 *
 *          uint16_t destColor = (src * alpha + 32767) / 65535;
 *          uint16_t destAlpha = alpha;
 *
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *  @/textblock </pre>
 *
 *      This function will for all 4 channel uint16_t / channel image formats with alpha last in memory.
 *      It does not have to be RGBA. Also available as vImagePremultiplyData_BGRA16U().
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_RGBA16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define         vImagePremultiplyData_BGRA16U( _src, _dest, _flags )           vImagePremultiplyData_RGBA16U((_src), (_dest), (_flags))


/*!
 *  @function vImagePremultiplyData_ARGB16Q12
 *  @abstract Multiply a signed 16Q12 fixed-point ARGB color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *      For each color in each pixel:
 *  <pre>@textblock
 *          int16_t destColor = CLAMP((src * alpha + 2048) / 4096, INT16_MIN, INT16_MAX);
 *          int16_t destAlpha = alpha;
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *
 *      This function will for other 4 channel 16Q12 / channel image formats with alpha first in memory.
 *      It does not have to be ARGB.
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_ARGB16Q12( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 *  @function vImagePremultiplyData_RGBA16Q12
 *  @abstract Multiply a signed 16Q12 RGBA color channel by its corresponding alpha
 *  @discussion
 *  This function multiplies color channels by the alpha channel.
 *      For each color in each pixel:
 *  <pre>@textblock
 *          int16_t destColor = CLAMP((src * alpha + 2048) / 4096, INT16_MIN, INT16_MAX);
 *          int16_t destAlpha = alpha;
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          If src overlaps with dest, src->data must be equal to dest->data
 *          If src also has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *                  (It is not necessary to pass kvImageDoNotTile if src and dest do not overlap.)
 *
 *      This function will for all 4 channel 16Q12 / channel image formats with alpha last in memory.
 *      It does not have to be RGBA.
 *
 *  @param src      The color data to multiply with alpha
 *  @param dest     A preallocated vImage_Buffer where the results are written
 *  @param flags    The following flags are allowed:
 *  <pre>@textblock
 *          kvImageNoFlags                      Default operation
 *
 *          kvImageDoNotTile                    Turn off internal multithreading. This might be useful if you are already multithreading
 *                                              the work in your own tiling engine.
 *  @/textblock </pre>
 *  @result         The following result codes may occur:
 *  <pre>@textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->width and dest->height must be less than or equal to corresponding
 *                                              dimensions in src and alpha
 *  @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultiplyData_RGBA16Q12( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 *  @functiongroup Unpremultiply Data
 *  @discussion Divide a premultiplied image by its alpha channel to produce a non-premultiplied image.
 */

/*!
 *  @function vImageUnpremultiplyData_Planar8
 *  @abstract Divide alpha from a premultiplied Planar8 images
 *
 *  @discussion This function divides color channels by the alpha channel.
 *      For each color channel:
 *  <pre>@textblock
 *          uint8_t destColor = ( MIN(src_color, alpha) * 255 + alpha/2) / alpha;
 *  @/textblock </pre>
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data.
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *  @param src          The planar8 input color channel
 *  @param alpha        The planar8 input alpha channel
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_Planar8( const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageUnpremultiplyData_PlanarF
 *  @abstract Divide alpha from a premultiplied PlanarF images
 *
 *  @discussion This function divides color channels by the alpha channel.
 *      For each color channel:
 *  <pre>@textblock
 *          float destColor = destColor / alpha;   // according to current rounding mode
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data.
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *  @param src          The planarF input color channel
 *  @param alpha        The planarF input alpha channel
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_PlanarF( const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageUnpremultiplyData_ARGB8888
 *  @abstract Divide the alpha channel from the color channels in a ARGB8888 image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          uint8_t destColor = ( MIN(src_color, alpha) * 255 + alpha/2) / alpha;
 *          uint8_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha first.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageUnpremultiplyData_RGBA8888
 *  @abstract Divide the alpha channel from the color channels in a RGBA8888 image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          uint8_t destColor = ( MIN(src_color, alpha) * 255 + alpha/2) / alpha;
 *          uint8_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha last. The function is also available as vImageUnpremultiplyData_BGRA8888.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_RGBA8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageUnpremultiplyData_BGRA8888( _src, _dest, _flags )             vImageUnpremultiplyData_RGBA8888((_src), (_dest), (_flags))

/*!
 *  @function vImageUnpremultiplyData_ARGBFFFF
 *  @abstract Divide the alpha channel from the color channels in a ARGBFFFF image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          float destColor = destColor / alpha;
 *          float destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha first.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @function vImageUnpremultiplyData_RGBAFFFF
 *  @abstract Divide the alpha channel from the color channels in a RGBAFFFF image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          float destColor = destColor / alpha;
 *          float destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha last. It is also available as vImageUnpremultiplyData_BGRAFFFF().
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_RGBAFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageUnpremultiplyData_BGRAFFFF( _src, _dest, _flags )             vImageUnpremultiplyData_RGBAFFFF((_src), (_dest), (_flags))


/*!
 *  @function vImageUnpremultiplyData_ARGB16U
 *  @abstract Divide the alpha channel from the color channels in a ARGB16U image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          uint16_t destColor = ( MIN(src_color, alpha) * 65535 + alpha/2) / alpha;
 *          uint16_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha first.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));


/*!
 *  @function vImageUnpremultiplyData_RGBA16U
 *  @abstract Divide the alpha channel from the color channels in a RGBA16U image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          uint16_t destColor = ( MIN(src_color, alpha) * 65535 + alpha/2) / alpha;
 *          uint16_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha last. The function is also available as vImageUnpremultiplyData_BGRA16U.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_RGBA16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define         vImageUnpremultiplyData_BGRA16U( _src, _dest, _flags )             vImageUnpremultiplyData_RGBA16U((_src), (_dest), (_flags))

/*!
 *  @function vImageUnpremultiplyData_ARGB16Q12
 *  @abstract Divide the alpha channel from the color channels in a ARGB16Q12 image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          int16_t destColor = ( MIN(src_color, alpha) * 4096 + alpha/2) / alpha;
 *          int16_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha first.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_ARGB16Q12( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 *  @function vImageUnpremultiplyData_RGBA16Q12
 *  @abstract Divide the alpha channel from the color channels in a RGBA16Q12 image
 *  @discussion
 *  This function divides color channels by the alpha channel.
 *      For each color channel:
 *      <pre>@textblock
 *          int16_t destColor = ( MIN(src_color, alpha) * 4096 + alpha/2) / alpha;
 *          int16_t destAlpha = alpha;
 *      @/textblock </pre>
 *
 *      ...which is the nearest unpremultiplied result, with clamping to ensure no modulo overflow in cases where srcColor > srcAlpha.
 *      In the division by zero case, the returned color value is 0.
 *
 *      The positioning of only the alpha channel is important for interleaved formats for these functions.
 *      This function will work with other channel orders that have alpha last.
 *
 *      This function can work in place provided the following are true:
 *          src->data must be equal to dest->data
 *          src->rowBytes must be equal to dest->rowBytes
 *
 *  @param src          The input inmage
 *  @param dest         A preallocated planar8 destination buffer into which the result will be written.'
 *  @param flags        The following flags are allowed:
 *      <pre>@textblock
 *                      kvImageNoFlags          Default operation
 *
 *                      kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *
 *  @result The following error codes may be returned:
 *      <pre>@textblock
 *                      kvImageNoError                      Success
 *
 *                      kvImageRoiLargerThanInputBuffer     dest->height or width is larger than the corresponding src or alpha dimension
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageUnpremultiplyData_RGBA16Q12( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 * @functiongroup Alpha Blend (Premultiplied with additional constant alpha)
 * @discussion  A premultiplied alpha blend with an extra constant alpha applied to the top image to allow
 *              it to be faded in our out.
 */

/*!
 * @function vImagePremultipliedConstAlphaBlend_Planar8
 * @abstract Blend two Planar8 premultiplied images with an extra image-wide alpha for the top image
 * @discussion  This is a premultiplied alpha compositing function using a constant for alpha over the whole image.
 *  Color data from both images is presumed to be already premultiplied by its own per-pixel alpha.
 *      For calculations involving 8-bit integer data, the calculation is done with an additional rounding step
 *      followed by division by 255:
 * <pre>@textblock
 *          uint8_t destColor = (srcTopColor * constAlpha * 255  + (255*255 - srcTopAlpha * constAlpha) * srcBottomColor + 127*255) / (255*255);
 *          uint8_t destAlpha =  (srcTopAlpha * constAlpha * 255 + (255*255 - srcTopAlpha * constAlpha) * srcBottomAlpha + 127*255 ) / (255*255);
 * @/textblock</pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 * @param srcTop        The top image
 * @param constAlpha    An extra alpha to apply to the entire top image
 * @param srcTopAlpha   The alpha channel for the top image
 * @param srcBottom     The bottom image
 * @param dest          A preallocate vImage_Buffer where the result will be written
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Turn off internal multithreading
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *          kvImageNoError                      Success
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer height or width is larger than the corresponding dimension in
 *                                              srcTop, srcTopAlpha or srcBottom
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultipliedConstAlphaBlend_Planar8( const vImage_Buffer *srcTop, Pixel_8 constAlpha, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,3,4,5)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedConstAlphaBlend_PlanarF
 * @abstract Blend two PlanarF premultiplied images with an extra image-wide alpha for the top image
 * @discussion  This is a premultiplied alpha compositing function using a constant for alpha over the whole image.
 *  Color data from both images is presumed to be already premultiplied by its own per-pixel alpha.
 * <pre>@textblock
 *          float destColor = srcTopColor * constAlpha  + (1.0 - srcTopAlpha  * constAlpha) * srcBottomColor;
 *          float alpha =  srcTopAlpha * constAlpha + (1.0 - srcTopAlpha * constAlpha) * srcBottomAlpha
 * @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 * @param srcTop        The top image
 * @param constAlpha    An extra alpha to apply to the entire top image
 * @param srcTopAlpha   The alpha channel for the top image
 * @param srcBottom     The bottom image
 * @param dest          A preallocate vImage_Buffer where the result will be written
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Turn off internal multithreading
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *          kvImageNoError                      Success
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer height or width is larger than the corresponding dimension in
 *                                              srcTop, srcTopAlpha or srcBottom
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultipliedConstAlphaBlend_PlanarF( const vImage_Buffer *srcTop, Pixel_F constAlpha, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,3,4,5)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedConstAlphaBlend_ARGB8888
 * @abstract Blend two ARGB8888 premultiplied images with an extra image-wide alpha for the top image
 * @discussion  This is a premultiplied alpha compositing function using a constant for alpha over the whole image.
 *  Color data from both images is presumed to be already premultiplied by its own per-pixel alpha.
 *      For calculations involving 8-bit integer data, the calculation is done with an additional rounding step
 *      followed by division by 255:
 * <pre>@textblock
 *          uint8_t destColor = (srcTopColor * constAlpha * 255  + (255*255 - srcTopAlpha * constAlpha) * srcBottomColor + 127*255) / (255*255);
 *          uint8_t destAlpha =  (srcTopAlpha * constAlpha * 255 + (255*255 - srcTopAlpha * constAlpha) * srcBottomAlpha + 127*255 ) / (255*255);
 * @/textblock</pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The  function will work for all 4 channel 8-bit / channel image formats with alpha first in memory, not just ARGB.
 * @param srcTop        The top image
 * @param constAlpha    An extra alpha to apply to the entire top image
 * @param srcBottom     The bottom image
 * @param dest          A preallocate vImage_Buffer where the result will be written
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Turn off internal multithreading
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *          kvImageNoError                      Success
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer height or width is larger than the corresponding dimension in
 *                                              srcTop, srcTopAlpha or srcBottom
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultipliedConstAlphaBlend_ARGB8888( const vImage_Buffer *srcTop, Pixel_8 constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,3,4)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImagePremultipliedConstAlphaBlend_ARGBFFFF
 * @abstract Blend two ARGBFFFF premultiplied images with an extra image-wide alpha for the top image
 * @discussion  This is a premultiplied alpha compositing function using a constant for alpha over the whole image.
 *  Color data from both images is presumed to be already premultiplied by its own per-pixel alpha.
 *      For calculations involving 8-bit integer data, the calculation is done with an additional rounding step
 *      followed by division by 255:
 * <pre>@textblock
 *          float destColor = srcTopColor * constAlpha  + (1.0 - srcTopAlpha  * constAlpha) * srcBottomColor;
 *          float alpha =  srcTopAlpha * constAlpha + (1.0 - srcTopAlpha * constAlpha) * srcBottomAlpha
 * @/textblock</pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      The function will work for all 4 channel float / channel image formats with alpha first in memory, not just ARGB.
 * @param srcTop        The top image
 * @param constAlpha    An extra alpha to apply to the entire top image
 * @param srcBottom     The bottom image
 * @param dest          A preallocate vImage_Buffer where the result will be written
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageNoFlags          Default operation
 *
 *          kvImageDoNotTile        Turn off internal multithreading
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *          kvImageNoError                      Success
 *
 *          kvImageRoiLargerThanInputBuffer     The destination buffer height or width is larger than the corresponding dimension in
 *                                              srcTop, srcTopAlpha or srcBottom
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error    vImagePremultipliedConstAlphaBlend_ARGBFFFF( const vImage_Buffer *srcTop, Pixel_F constAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,3,4)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @functiongroup Alpha Blend (Non-premultiplied to premultiplied)
 *  @discussion    These functions blend a non-premultiplied top image into a premultiplied bottom image
 *                 to produce a premultiplied result.
 */

/*!
 * @function vImageAlphaBlend_NonpremultipliedToPremultiplied_Planar8
 * @abstract Blend a non-premultiplied top Planar8 image into a premultiplied Planar8 bottom image and return a premultiplied Planar8 result.
 * @discussion Top buffer is non-premultiplied. Bottom buffer is premultiplied. Dest buffer is premultiplied. Works in place.
 * <pre>@textblock
 *      result = (srcTop * srctopAlpha + (255 - srcTopAlpha) * bottomAlpha + 127 ) / 255;
 * @/textblock </pre>
 * This function will work in place as long as the src and dest buffer overlap exactly.
 * The src buffers must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 *
 *  To calculate the alpha result for the Planar cases, use
 * <pre>@textblock
 *      vImagePremultipliedAlphaBlend_Planar8( srcTopAlpha, srcTopAlpha, srcBottomAlpha, dest, flags );
 * @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 * @param srcTop        Planar8 non-premultiplied color channel for top image
 * @param srcTopAlpha   Planar8 alpha channel for top image
 * @param srcBottom     Planar8 premultiplied color channel for bottom image
 * @param dest          Planar8 premultiplied result. Must be preallocated before the call is made.
 * @param flags         The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_NonpremultipliedToPremultiplied_Planar8( const vImage_Buffer *srcTop, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_NonpremultipliedToPremultiplied_PlanarF
 * @abstract Blend a non-premultiplied top PlanarF image into a premultiplied PlanarF bottom image and return a premultiplied PlanarF result.
 * @discussion Top buffer is non-premultiplied. Bottom buffer is premultiplied. Dest buffer is premultiplied. Works in place.
 * <pre>@textblock
 *      result = srcTop * srcTopAlpha + (1 - srcTopAlpha) * srcBottom
 * @/textblock </pre>
 * This function will work in place as long as the src and dest buffer overlap exactly.
 * The src buffers must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 *
 *  To calculate the alpha result for the Planar cases, use
 * <pre>@textblock
 *      vImagePremultipliedAlphaBlend_PlanarF( srcTopAlpha, srcTopAlpha, srcBottomAlpha, dest, flags );
 * @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 * @param srcTop        Planar8 non-premultiplied color channel for top image
 * @param srcTopAlpha   Planar8 alpha channel for top image
 * @param srcBottom     Planar8 premultiplied color channel for bottom image
 * @param dest          Planar8 premultiplied result. Must be preallocated before the call is made.
 * @param flags         The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_NonpremultipliedToPremultiplied_PlanarF( const vImage_Buffer *srcTop, const vImage_Buffer *srcTopAlpha, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888
 * @abstract Blend a non-premultiplied top ARGB8888 image into a premultiplied ARGB8888 bottom image and return a premultiplied ARGB8888 result.
 * @discussion Top buffer is non-premultiplied. Bottom buffer is premultiplied. Dest buffer is premultiplied. Works in place.
 * <pre>@textblock
 *      result = (srcTop * srctopAlpha + (255 - srcTopAlpha) * bottomAlpha + 127 ) / 255;
 * @/textblock </pre>
 * This function will work in place as long as the src and dest buffer overlap exactly.
 * The src buffers must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *  This function will work with any channel order with alpha first, not just ARGB.
 *
 * @param srcTop        Planar8 non-premultiplied color channel for top image
 * @param srcBottom     Planar8 premultiplied color channel for bottom image
 * @param dest          Planar8 premultiplied result. Must be preallocated before the call is made.
 * @param flags         The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGB8888( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF
 * @abstract Blend a non-premultiplied top ARGBFFFF image into a premultiplied ARGBFFFF bottom image and return a premultiplied ARGBFFFF result.
 * @discussion Top buffer is non-premultiplied. Bottom buffer is premultiplied. Dest buffer is premultiplied. Works in place.
 * <pre>@textblock
 *      result = srcTop * srcTopAlpha + (1 - srcTopAlpha) * srcBottom
 * @/textblock </pre>
 * This function will work in place as long as the src and dest buffer overlap exactly.
 * The src buffers must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 *
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *  This function will work with any channel order with alpha first, not just ARGB.
 *
 * @param srcTop        Planar8 non-premultiplied color channel for top image
 * @param srcBottom     Planar8 premultiplied color channel for bottom image
 * @param dest          Planar8 premultiplied result. Must be preallocated before the call is made.
 * @param flags         The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error    vImageAlphaBlend_NonpremultipliedToPremultiplied_ARGBFFFF( const vImage_Buffer *srcTop, const vImage_Buffer *srcBottom, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @functiongroup Clip To Alpha
 * @discusssion  If a premultiplied image color channel has a value greater than alpha it is super-opaque.  This may
 *               cause problems later in compositing operations. Alpha > color can happen normally as a result of
 *               convolutions, resampling, and morphological operations. While such operations should generally be done
 *               on non-premultiplied content, sometimes premultiplied content slips through. vImage provides a clamping
 *               function vImageClipToAlpha which identify color channels that are greater than alpha and clamp them to be
 *               equal to alpha.
 */

/*!
 *  @function vImageClipToAlpha_Planar8
 *  @abstract Clamp a Planar8 color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *  @param src      The color image to clip
 *  @param alpha    The alpha channel
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_Planar8( const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageClipToAlpha_PlanarF
 *  @abstract Clamp a PlanarF color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *  @param src      The color image to clip
 *  @param alpha    The alpha channel
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_PlanarF( const vImage_Buffer *src,  const vImage_Buffer *alpha,  const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageClipToAlpha_ARGB8888
 *  @abstract Clamp a ARGB8888 color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          alpha_result = alpha
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      This function will work for any channel order with alpha first, not just ARGB.
 *
 *  @param src      The color image to clip
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageClipToAlpha_ARGBFFFF
 *  @abstract Clamp a ARGBFFFF color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          alpha_result = alpha
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      This function will work for any channel order with alpha first, not just ARGB.
 *
 *  @param src      The color image to clip
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 *  @function vImageClipToAlpha_RGBA8888
 *  @abstract Clamp a RGBA8888 color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          alpha_result = alpha
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      This function will work for any channel order with alpha last, not just RGBA. Also available as vImageClipToAlpha_BGRA8888().
 *
 *  @param src      The color image to clip
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_RGBA8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define vImageClipToAlpha_BGRA8888(_src, _dest, _flags)   vImageClipToAlpha_RGBA8888((_src), (_dest), (_flags))

/*!
 *  @function vImageClipToAlpha_RGBAFFFF
 *  @abstract Clamp a RGBAFFFF color buffer to be less than or equal to alpha
 *  @discussion
 *  For each pixel, each color channel shall be set to the smaller of the color channel or alpha value for that pixel.
 *  <pre>@textblock
 *          alpha_result = alpha
 *          color_result = MIN( color, alpha )
 *  @/textblock </pre>
 *      This function can work in place provided the following are true:
 *          For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data
 *          If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 *
 *      This function will work for any channel order with alpha last, not just RGBA. Also available as vImageClipToAlpha_BGRAFFFF().
 *
 *  @param src      The color image to clip
 *  @param dest     A preallocated buffer to receive the results.
 *  @param flags    The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags          Default operation.
 *
 *          kvImageDoNotTile        Disable internal multithreading.
 *      @/textblock </pre>
 *  @result     The following error codes may occur:
 *      <pre> @textblock
 *          kvImageNoError                      Success.
 *
 *          kvImageRoiLargerThanInputBuffer     dest->height and dest->width must be less than or equal to corresponding
 *                                              dimensions in srcTop, srcTopAlpha and srcBottom.
 *      @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageClipToAlpha_RGBAFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define vImageClipToAlpha_BGRAFFFF(_src, _dest, _flags)   vImageClipToAlpha_RGBAFFFF((_src), (_dest), (_flags))

/*
 * See also the vImageFlatten APIs in Conversion.h for compositing an image against a constant color opaque background.
 */

#ifdef __cplusplus
}
#endif

#endif /* vImage_ALPHA_H */


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Conversion.h
/*!
@header Conversion.h
vImage_Framework

See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.

@discussion  This header lists conversions between the many different image formats supported by vImage.  The core formats:
@code
Planar 8        Planar 16U      Planar 16S      Float
ARGB8888        ARGB16U         ARGB16S         ARGBFFFF        (also available in other channel orderings)
@endcode
are supported generally throughout vImage. Other formats may need to be converted to a core format before much can be
done with them. Many conversions between core formats as well as those between core formats and non-core formats are available here.
Please also see vImage_Utilities.h and vImage_CVUtilities.h for interfaces that allow for conversion between various CoreGraphics
and CoreVideo formats.  In many cases, those interfaces can serve as a simpler entrypoint into these APIs. They should be strongly
considered in cases where your code has to handle a variety of different image formats. These interfaces provide direct access to
the low-level workhorse functions for the case when you know exactly what image formats you are working with and want to just call
the right conversion directly.

Generally speaking, conversions are much faster than other image filters. When tiled correctly, they do not add a lot of extra cost
in cases where you find that your image format is not directly supported by other image filters. They are usually fast enough that
their performance is bottlenecked by throughput to L2, L3, etc. caches.  They benefit greatly from tiling, usually performing best
with wide tile sizes that are less than half the size of the L1 cache. Because of the L1 cache residency requirement for best performance,
they are often not internally multithreaded (it wouldn't help for out-of-cache performance and would cause slowdowns for in-cache)
and may need  to be used within the context of your tiling engine (or vImageConvert_AnyToAny) to reach peak performance. If you are calling
vImage functions within your own multithreaded tiling engine, it is recommended that you use the kvImageDoNotTile flag to make sure that
vImage does its work on the calling thread. This will help ensure that the data just produced on that thread by a previous pass
is resident in the correct L1 cache for the next pass.

@copyright Copyright (c) 2003-2016 by Apple Inc. All rights reserved.

@ignorefuncmacro VIMAGE_NON_NULL
*/


#ifndef VIMAGE_CONVERSION_H
#define VIMAGE_CONVERSION_H

#include <vImage/vImage_Types.h>

#ifdef __cplusplus
extern "C" {
#endif

/*!
 @function vImageClip_PlanarF
 
 @abstract Clips the pixel values of an image in PlanarF format, using the provided minimum and maximum values.
 
 @discussion For each pixel, do the following:
 @code
 if( pixel > maxFloat )
 pixel = maxFloat;
 if( pixel < minFloat )
 pixel = minFloat;
 @endcode
 
 This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src
 A pointer to a vImage buffer structure that contains the source image whose data you want to clip.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat
 A maximum pixel value. The function clips larger values to this value in the destination image.
 
 @param minFloat
 A minimum pixel value. The function clips smaller values to this value in the destination image.
 
 @param flags
 \p kvImageNoFlags      Default operation.
 \p kvImageDoNotTile    Disable internal multithreading.
 
 @return kvImageNoError                      Success
 @return kvImageRoiLargerThanInputBuffer     The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note This function may be used for multichannel image formats, such as ARGBFFFF.
 Scale the vImage_Buffer.width to compensate for the extra channels.
 */
VIMAGE_PF vImage_Error vImageClip_PlanarF(const vImage_Buffer* src, const vImage_Buffer* dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags)  VIMAGE_NON_NULL(1,2)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_Planar8toPlanarF
 
 @abstract Convert an array of 8 bit integer data to floating point data.
 
 @discussion For each pixel, do the following:
 @code
 float result = (maxFloat - minFloat) * (float) srcPixel / 255.0  + minFloat
 @endcode
 
 You can use this for ARGB8888 -> ARGBFFFF conversions by simply multiplying the width of the vImage_Buffer by 4 (for 4 channels)
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat
 A maximum float value.
 
 @param minFloat
 A minimum float value.
 
 @param flags
 \p kvImageNoFlags                      Default operation.
 \p kvImageDoNotTile                    Disable internal multithreading.
 \p kvImagePrintDiagnosticsToConsole    Directs the function to print diagnostic information to the console in the event of failure.
 
 @return kvImageNoError                     Success
 @return kvImageUnknownFlagsBit             Not all vImage flags are understood by this function. See description of flags parameter for supported flags.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar8toPlanarF(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_PlanarFtoPlanar8
 
 @abstract Convert an array of floating point data to 8 bit integer data.
 
 @discussion For each pixel, do the following:
 @code
 uint8_t result = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel - minFloat ) / (maxFloat - minFloat) + 0.5f );
 @endcode
 
 You can use this for ARGBFFFF -> ARGB8888 conversions by simply multiplying the width of the vImage_Buffer by 4 (for 4 channels)
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat
 A maximum float value.
 
 @param minFloat
 A minimum float value.
 
 @param flags
 \p kvImageNoFlags      Default operation
 \p kvImageDoNotTile    Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_PlanarFtoPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, vImage_Flags flags) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_PlanarFtoPlanar8_dithered
 
 @abstract Convert an array of floating point data to 8 bit integer data with dithering.
 
 @discussion For each pixel, do the following:
 @code
 uint8_t result = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel - minFloat ) / (maxFloat - minFloat) + random_float[0,1) );
 @endcode
 
 The \p dither parameter must be one of the following flags:
 \p kvImageConvert_DitherNone Same as vImageConvert_PlanarFtoPlanar8(). Rounds to nearest.
 
 \p kvImageConvert_DitherOrdered Pre-computed blue noise is added to the image before rounding to the values in
 the destination format.  The offset into this blue noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on sequential frames of video.
 
 \p kvImageConvert_DitherOrderedReproducible Pre-computed blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the blue noise is the same for every call to allow users
 to get reproducible results. Fine for still images. For video kvImageConvert_DitherOrdered is a better choice.
 
 The ordered dither methods may be further influenced by shaping the distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageConvert_DitherOrderedReproducible:
 
 \p kvImageConvert_OrderedGaussianBlue When using an ordered dither pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --  less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and kvImageConvert_DitherOrderedReproducible.
 
 \p kvImageConvert_OrderedUniformBlue When using an ordered dither pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 @note "Blue" noise does not look blue, nor does it operate solely on the blue color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 @warning Unlike vImageConvert_PlanarFtoPlanar8, vImageConvert_PlanarFtoPlanar8_dithered usually should not be used for
 multichannel data. Otherwise the dithering will occur in the chrominance dimensions and the noise will cause
 grain with varying hue.
 
 @note This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes.
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param src               A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest              A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat          The encoding for 1.0 in the src buffer, full intensity. Typically, this is 1.0 for floating-point data in the range[0,1] but if your data is [0,65535] then you would pass 65535.0f here.
 
 @param minFloat          The encoding for 0.0 in the src buffer, no light.  Typically this is 0.0 for floating-point data in the range [0,1], but if your data is [-.5,0.5] then you would pass -0.5f here.
 
 @param dither            The type of random noise to use for the dither. See discussion for more details.
 
 @param flags                   The following flags are honored:
 \p kvImageNoFlags              Default operation.
 \p kvImageDoNotTile            Disable internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns 0. Does no work. Does not touch data.
 
 @return \p kvImageNoError                      Success
 @return \p kvImageRoiLargerThanInputBuffer     The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 @return \p kvImageUnknownFlagsBit              Not all vImage flags are understood by this function. See description of flags parameter for supported flags.
 @return \p kvImageInvalidParameter             An unknown / unsupported dithering mode was requested.
 
 @seealso vImageConvert_PlanarFtoPlanar8
 */
VIMAGE_PF vImage_Error vImageConvert_PlanarFtoPlanar8_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, Pixel_F maxFloat, Pixel_F minFloat, int dither, vImage_Flags flags) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_RGBFFFtoRGB888_dithered
 
 @abstract Convert an array of floating point data to 8 bit integer data with dithering.
 
 @discussion For each pixel, do the following:
 @code
 // convert to uint8_t
 result[0] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[0] - minFloat[0] ) / (maxFloat[0] - minFloat[0]) + random_float[0,1) );
 result[1] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[1] - minFloat[1] ) / (maxFloat[1] - minFloat[1]) + random_float[0,1) );
 result[2] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[2] - minFloat[2] ) / (maxFloat[2] - minFloat[2]) + random_float[0,1) );
 @endcode
 
 This function will work for other channel orders, such as BGR, and other colorspaces such as L*a*b*.
 If you need to change channel orders, please see vImagePermuteChannels_RGB888().
 
 The \p dither parameter must be one of the following flags:
 \p kvImageConvert_DitherNone Same as vImageConvert_PlanarFtoPlanar8(). Rounds to nearest.
 
 \p kvImageConvert_DitherOrdered Pre-computed blue noise is added to the image before rounding to the values in
 the destination format.  The offset into this blue noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on sequential frames of video.
 
 \p kvImageConvert_DitherOrderedReproducible Pre-computed blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the blue noise is the same for every call to allow users
 to get reproducible results. Fine for still images. For video kvImageConvert_DitherOrdered is a better choice.
 
 The ordered dither methods may be further influenced by shaping the distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageConvert_DitherOrderedReproducible:
 
 \p kvImageConvert_OrderedGaussianBlue When using an ordered dither pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --  less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and kvImageConvert_DitherOrderedReproducible.
 
 \p kvImageConvert_OrderedUniformBlue When using an ordered dither pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 @note "Blue" noise does not look blue, nor does it operate solely on the blue color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 @note This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes.
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src               A pointer to a valid and initialized vImage_Buffer struct that points to a buffer containing the source pixels.
 
 @param dest              A pointer to a valid and initialized vImage_Buffer struct that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat          The encoding for 1.0 in the src buffer, full intensity.
 Typically, this is 1.0 for floating-point data in the range[0,1]
 but if your data is [0,65535] then you would pass 65535.0f here.
 A separate value is provided for each of the three channels.
 
 @param minFloat          The encoding for 0.0 in the src buffer, no light.
 Typically this is 0.0 for floating-point data in the range [0,1],
 but if your data is [-.5,0.5] then you would pass -0.5f here.
 A separate value is provided for each of the three channels.
 
 @param dither            The type of random noise to use for the dither. See discussion for more details.
 
 @param flags                   The following flags are honored:
 \p kvImageNoFlags              Default operation.
 \p kvImageDoNotTile            Disable internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns 0. Does no work. Does not touch data.
 
 @return \p kvImageNoError                      Success
 @return \p kvImageRoiLargerThanInputBuffer     The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 @return \p kvImageUnknownFlagsBit              Not all vImage flags are understood by this function. See description of flags parameter for supported flags.
 @return \p kvImageInvalidParameter             An unknown / unsupported dithering mode was requested.
 
 @seealso vImageConvert_RGBFFFtoRGB888
 @seealso vImagePermuteChannels_RGB888
 */
VIMAGE_PF vImage_Error vImageConvert_RGBFFFtoRGB888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_F maxFloat[3], const Pixel_F minFloat[3], int dither, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4)    API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGBFFFFtoARGB8888_dithered
 
 @abstract Convert an array of floating point data to 8 bit integer data with dithering.
 
 @discussion For each pixel, do the following:
 @code
 // convert to uint8_t
 Pixel_8888 temp;
 temp[0] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[0] - minFloat[0] ) / (maxFloat[0] - minFloat[0]) + random_float[0,1) );
 temp[1] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[1] - minFloat[1] ) / (maxFloat[1] - minFloat[1]) + random_float[0,1) );
 temp[2] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[2] - minFloat[2] ) / (maxFloat[2] - minFloat[2]) + random_float[0,1) );
 temp[3] = SATURATED_CLIP_0_to_255( 255.0f * ( srcPixel[3] - minFloat[3] ) / (maxFloat[3] - minFloat[3]) + random_float[0,1) );
 
 // place in requested output order
 Pixel_8888 result;
 result[0] = temp[permuteMap[0]];
 result[1] = temp[permuteMap[1]];
 result[2] = temp[permuteMap[2]];
 result[3] = temp[permuteMap[3]];
 @endcode
 
 The \p dither parameter must be one of the following flags:
 \p kvImageConvert_DitherNone Same as vImageConvert_PlanarFtoPlanar8(). Rounds to nearest.
 
 \p kvImageConvert_DitherOrdered Pre-computed blue noise is added to the image before rounding to the values in
 the destination format.  The offset into this blue noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on sequential frames of video.
 
 \p kvImageConvert_DitherOrderedReproducible Pre-computed blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the blue noise is the same for every call to allow users
 to get reproducible results. Fine for still images. For video kvImageConvert_DitherOrdered is a better choice.
 
 The ordered dither methods may be further influenced by shaping the distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageConvert_DitherOrderedReproducible:
 
 \p kvImageConvert_OrderedGaussianBlue When using an ordered dither pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --  less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and kvImageConvert_DitherOrderedReproducible.
 
 \p kvImageConvert_OrderedUniformBlue When using an ordered dither pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 @note "Blue" noise does not look blue, nor does it operate solely on the blue color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 @note This function will work for other channel orders, such as RGBA and BGRA.
 
 @note This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src         A pointer to a valid and initialized vImage_Buffer struct that points to a buffer containing the source pixels.
 
 @param dest        A pointer to a valid and initialized vImage_Buffer struct that points to a allocated buffer to receive the result pixels.
 
 @param maxFloat    The encoding for 1.0 in the src buffer, full intensity.
 Typically, this is 1.0 for floating-point data in the range[0,1]
 but if your data is [0,65535] then you would pass 65535.0f here.
 A separate value is provided for each of the three channels.
 
 @param minFloat    The encoding for 0.0 in the src buffer, no light.
 Typically this is 0.0 for floating-point data in the range [0,1],
 but if your data is [-.5,0.5] then you would pass -0.5f here.
 A separate value is provided for each of the three channels.
 
 @param dither      The type of random noise to use for the dither. See discussion for more details.
 
 @param permuteMap  A 4 element array giving the order of the result channels.
 This allows you to convert a ARGB float buffer to a BGRA result
 buffer by providing the order {3,2,1,0}.
 
 @param flags                   The following flags are honored:
 \p kvImageNoFlags              Default operation.
 \p kvImageDoNotTile            Disable internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns 0. Does no work. Does not touch data.
 
 @return \p kvImageNoError                      Success
 @return \p kvImageRoiLargerThanInputBuffer     The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 @return \p kvImageUnknownFlagsBit              Not all vImage flags are understood by this function. See description of flags parameter for supported flags.
 @return \p kvImageInvalidParameter             An unknown / unsupported dithering mode was requested.
 
 @seealso vImageConvert_RGBFFFtoRGB888
 @seealso vImagePermuteChannels_RGB888
 */
VIMAGE_PF vImage_Error vImageConvert_ARGBFFFFtoARGB8888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, int dither, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4)    API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_Planar8toARGB8888
 
 @abstract Interleave 4 planar 8 bit integer buffers to make an interleaved 4 channel ARGB8888 buffer.
 
 @discussion For each pixel in { srcA, srcR, srcG, srcB }, do the following:
 @code
 Pixel_88888 result = { pixelFromSrcA, pixelFromSrcR, pixelFromSrcG, pixelFromSrcB };
 @endcode
 
 This function may be used to create other channel orderings such as RGBA8888 by passing in the planar8 images in the alternate order.
 
 @param srcA
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A source pixels.
 
 @param srcR
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R source pixels.
 
 @param srcG
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G source pixels.
 
 @param srcB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param flags
 \p kvImageNoFlags      Default operation
 \p kvImageDoNotTile    Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar8toARGB8888(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_PlanarFtoARGBFFFF
 
 @abstract Interleave 4 planar floating point buffers to make an interleaved 4 channel ARGBFFFF buffer.
 
 @discussion For each pixel in { srcA, srcR, srcG, srcB }, do the following:
 @code
 Pixel_FFFF result = { pixelFromSrcA, pixelFromSrcR, pixelFromSrcG, pixelFromSrcB };
 @endcode
 
 This function may be used to create other channel orderings such as RGBAFFFF by passing in the planar8 images in the alternate order.
 
 @param srcA
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A source pixels.
 
 @param srcR
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R source pixels.
 
 @param srcG
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G source pixels.
 
 @param srcB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a allocated buffer to receive the result pixels.
 
 @param flags
 \p kvImageNoFlags      Default operation
 \p kvImageDoNotTile    Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_PlanarFtoARGBFFFF(const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_ARGB8888toPlanar8
 
 @abstract Deinterleave an ARGB8888 interleaved vImage_Buffer to form 4 planar 8-bit integer buffers.
 
 @discussion For each pixel in srcARGB, do the following:
 @code
 Pixel_8 destAResult = srcARGBPixel[0];
 Pixel_8 destRResult = srcARGBPixel[1];
 Pixel_8 destGResult = srcARGBPixel[2];
 Pixel_8 destBResult = srcARGBPixel[3];
 @endcode
 
 This function may be used to deinterleave other channel orderings such as RGBA8888 by passing in the planar8 images in the alternate order.
 
 @param srcARGB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixels.
 
 @param destA
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A destination pixels.
 
 @param destR
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R destination pixels.
 
 @param destG
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G destination pixels.
 
 @param destB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B destination pixels.
 
 @param flags
 \p kvImageNoFlags      Default operation
 \p kvImageDoNotTile    Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888toPlanar8(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
#define vImageConvert_RGBA8888toPlanar8(_src, _red, _green, _blue, _alpha, _flags) \
vImageConvert_ARGB8888toPlanar8((_src), (_red), (_green), (_blue), (_alpha), (_flags))
#define vImageConvert_BGRA8888toPlanar8(_src, _blue, _green, _red, _alpha, _flags) \
vImageConvert_ARGB8888toPlanar8((_src), (_blue), (_green), (_red), (_alpha), (_flags))

/*!
 @function vImageConvert_ARGBFFFFtoPlanarF
 
 @abstract Deinterleave an ARGBFFFF interleaved vImage_Buffer to form 4 planar floating point buffers.
 
 @discussion For each pixel in srcARGB, do the following:
 @code
 Pixel_F destAResult = srcARGBPixel[0];
 Pixel_F destRResult = srcARGBPixel[1];
 Pixel_F destGResult = srcARGBPixel[2];
 Pixel_F destBResult = srcARGBPixel[3];
 @endcode
 
 This function may be used to deinterleave other channel orderings such as RGBAFFFF by passing in the planar8 images in the alternate order.
 
 @param srcARGB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixels.
 
 @param destA
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A destination pixels.
 
 @param destR
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R destination pixels.
 
 @param destG
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G destination pixels.
 
 @param destB
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B destination pixels.
 
 @param flags
 \p kvImageNoFlags      Default operation
 \p kvImageDoNotTile    Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGBFFFFtoPlanarF(const vImage_Buffer *srcARGB, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
#define vImageConvert_RGBAFFFFtoPlanarF(_src, _red, _green, _blue, _alpha, _flags) \
vImageConvert_ARGBFFFFtoPlanarF((_src), (_red), (_green), (_blue), (_alpha), (_flags))
#define vImageConvert_BGRAFFFFtoPlanarF(_src, _blue, _green, _red, _alpha, _flags) \
vImageConvert_ARGBFFFFtoPlanarF((_src), (_blue), (_green), (_red), (_alpha), (_flags))

/*
 vImageConvert_ChunkyToPlanar8
 vImageConvert_PlanarToChunky8
 vImageConvert_ChunkyToPlanarF
 vImageConvert_PlanarToChunkyF
 
 These functions convert between nearly arbitrary interleaved data formats that vImage
 doesn't support to/from planar data formats that it does support.
 N planar arrays are interleaved to make (or are made made by deinterleaving)
 a N channel packed interleaved buffer with pixels that are {srcStrideBytes, destStrideBytes}
 bytes in size. Valid data channels are assumed to be at the front of each interleaved pixel.
 
 Will not work in place
 
 Performance advisory:
 =====================
 These functions are too flexible to vectorize every case. When appropriate, it is suggested you use the other
 special purpose conversion functions. If you know what your data formats are ahead of time, it is
 likely you can write your own special purpose conversion function that is faster, even in scalar code.
 These functions are provided as a convenience.
 */
VIMAGE_PF vImage_Error vImageConvert_ChunkyToPlanar8( const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_PlanarToChunky8( const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_ChunkyToPlanarF( const void *srcChannels[], const vImage_Buffer *destPlanarBuffers[], unsigned int channelCount, size_t srcStrideBytes, vImagePixelCount srcWidth, vImagePixelCount srcHeight, size_t srcRowBytes, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_PlanarToChunkyF( const vImage_Buffer *srcPlanarBuffers[], void *destChannels[], unsigned int channelCount, size_t destStrideBytes, vImagePixelCount destWidth, vImagePixelCount destHeight, size_t destRowBytes, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_16SToF
 
 @abstract Convert a planar vImage_Buffer of 16 bit signed integers to a buffer containing floating point values.
 
 @discussion For each 16 bit pixel in src, do the following:
 @code
 float result = (float) srcPixel * scale + offset;
 @endcode
 
 To convert 4 channel interleaved signed 16 bit data to ARGBFFFF, simply multiply the vImage_Buffer.width by 4.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param scale
 A scale value in the conversion.
 
 @param offset
 A offset value in the conversion.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                       Success
 @return kvImageRoiLargerThanInputBuffer      The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_16SToF( const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_16UToF
 
 @abstract Convert a planar vImage_Buffer of 16 bit unsigned integers to a buffer containing floating point values.
 
 @discussion For each 16 bit pixel in src, do the following:
 @code
 float result = (float) srcPixel * scale + offset;
 @endcode
 
 To convert 4 channel interleaved signed 16 bit data to ARGBFFFF, simply multiply the vImage_Buffer.width by 4.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param scale
 A scale value in the conversion.
 
 @param offset
 A offset value in the conversion.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                       Success
 @return kvImageRoiLargerThanInputBuffer      The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Does not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_16UToF( const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_FTo16S
 
 @abstract Convert a planar vImage_Buffer of floating point values to a buffer of 16 bit signed integers.
 
 @discussion For each floating point pixel in src, do the following:
 @code
 int16_t result = SATURATED_CLIP_SHRT_MIN_to_SHRT_MAX( (srcPixel - offset) / scale  + 0.5f);
 @endcode
 
 Programmer's note:
 The scale and offset here are designed to be the same offset and scale used for the vImageConvert_16SToF conversion.
 For a lossless round trip (within the limits of floating point precision), use the same scale and offset values
 in both directions:
 
 vImageConvert_16SToF( int16_buffer, float_buffer, myOffset, myScale, kvImageNoFlags );   //Convert to float
 vImageConvert_FTo16S( float_buffer, int16_buffer, myOffset, myScale, kvImageNoFlags );   //Convert back to int16_t
 
 @note Works in place, as long as src->data == dest->data and src->rowBytes == dest->rowBytes.
 @note To convert multichannel interleaved floating point formats (e.g. ARGBFFFF) to a multichannel 16-bit image format with the same channel ordering, simply multiply the vImage_Buffer.width by the number of channels.
 
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param scale             A scale value in the conversion.
 
 @param offset            A offset value in the conversion.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_FTo16S( const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_FTo16U
 
 @abstract Convert a planar vImage_Buffer of floating point values to a buffer of 16 bit unsigned integers.
 
 @discussion For each floating point pixel in src, do the following:
 @code
 uint16_t result = SATURATED_CLIP_0_to_USHRT_MAX( (srcPixel - offset) / scale  + 0.5f);
 @endcode
 
 Programmer's note:
 The scale and offset here are designed to be the same offset and scale used for the vImageConvert_16UToF conversion.
 For a lossless round trip (within the limits of floating point precision), use the same scale and offset values
 in both directions:
 
 vImageConvert_16UToF( int16_buffer, float_buffer, myOffset, myScale, kvImageNoFlags );   //Convert to float
 vImageConvert_FTo16U( float_buffer, int16_buffer, myOffset, myScale, kvImageNoFlags );   //Convert back to uint16_t
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param scale             A scale value in the conversion.
 
 @param offset            A offset value in the conversion.
 
 @param flags
 \p kvImageNoFlags                     Default operation.
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @note Works in place, as long as src->data == dest->data and src->rowBytes == dest->rowBytes.
 
 @note To convert multichannel interleaved floating point formats (e.g. ARGBFFFF) to a multichannel 16-bit image format with the same channel ordering, simply multiply the vImage_Buffer.width by the number of channels.
 */
VIMAGE_PF vImage_Error vImageConvert_FTo16U( const vImage_Buffer *src, const vImage_Buffer *dest, float offset, float scale, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 @function vImageConvert_16Uto16F
 
 @abstract Convert from 16 bit unsigned integer to 16 bit float format.
 
 @discussion For each floating point pixel in src, do the following:
 @code
 destPixel[x] = ConvertToPlanar16F(srcPixel[x]);
 @endcode
 
 The 16 bit floating point format is half-precision floating point
 (a.k.a.  IEEE-754 binary16, OpenCL half, GL_ARB_half_float_pixel, OpenEXR half).
 It has a layout as follows:
 
 16 bits:  seeeeemmmmmmmmmm
 
 1-bit sign | 5 bits of exponent, with a bias of 15 | 10 bits of significand
 (with 11 bits of significance due to the implicit 1 bit)
 
 NaNs, Infinities and denormals are supported.
 Per IEEE-754, all signaling NaNs are quieted during the conversion. (OpenEXR-1.2.1 converts SNaNs to SNaNs.)
 To set/inspect the current IEEE-754 rounding mode, please see appropriate utilities in fenv.h.
 
 @param src
 A pointer to a vImage_Buffer that references the source pixels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination pixels.
 
 @param flags
 \p kvImageDoNotTile    Turns off internal multithreading.
 
 @return \p kvImageNoError                      Success!
 @return \p kvImageRoiLargerThanInputBuffer     The source buffer must have a height and
 width at least as large as the destination buffer.
 @return \p kvImageNullPointerArgument          src or dest pointer is NULL.
 @return \p kvImageUnknownFlagsBit              Unknown flag was passed.
 
 @note This routine will work in place provided that src.data == dest.data
 and src.rowBytes >= dest.rowBytes. However, when src.rowBytes > dest.rowBytes
 in-place will only work if you pass kvImageDoNotTile.
 
 @note To use this with interleaved data, multiply vImage_Buffer.width by 4.
 */
VIMAGE_PF vImage_Error vImageConvert_16Uto16F( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags )
VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_16Fto16U
 
 @abstract Convert from 16 bit float to 16 bit unsigned integer format.
 
 @discussion For each floating point pixel in src, do the following:
 @code
 destPixel[x] = ConvertToPlanar16U(srcPixel[x]);
 @endcode
 
 The 16 bit floating point format is half-precision floating point
 (a.k.a.  IEEE-754 binary16, OpenCL half, GL_ARB_half_float_pixel, OpenEXR half).
 It has a layout as follows:
 
 16 bits:  seeeeemmmmmmmmmm
 
 1-bit sign | 5 bits of exponent, with a bias of 15 | 10 bits of significand
 (with 11 bits of significance due to the implicit 1 bit)
 
 NaNs, Infinities and denormals are supported.
 Per IEEE-754, all signaling NaNs are quieted during the conversion. (OpenEXR-1.2.1 converts SNaNs to SNaNs.)
 To set/inspect the current IEEE-754 rounding mode, please see appropriate utilities in fenv.h
 
 @param src
 A pointer to a vImage_Buffer that references the source pixels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination pixels.
 
 @param flags
 \p kvImageDoNotTile    Turns off internal multithreading.
 
 @return \p kvImageNoError              Success!
 @return \p kvImageRoiLargerThanInputBuffer   The source buffer must have a height and
 width at least as large as the destination buffer.
 @return \p kvImageNullPointerArgument  src or dest pointer is NULL.
 @return \p kvImageUnknownFlagsBit      Unknown flag was passed.
 
 @note This routine will work in place provided that src.data == dest.data
 and src.rowBytes >= dest.rowBytes. However, when src.rowBytes > dest.rowBytes
 in-place will only work if you pass kvImageDoNotTile.
 
 @note To use this with interleaved data, multiply vImage_Buffer.width by 4
 */
VIMAGE_PF vImage_Error vImageConvert_16Fto16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags )
VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 @function vImageConvert_12UTo16U
 
 @abstract Converts 12U to 16U
 
 @discussion For each floating point pixel in src, do the following:
 @code
 uint8_t *srcRow = srcData;
 uint16_t *destRow = destData;
 
 //Load 2 12-bit values
 t0 = (srcRow[0] << 16) | (srcRow[1] << 8) | srcRow [2];
 srcRow += 3;
 
 //Separate each of 12-bit
 t1 = t0 & 0xfff;
 t0 >>= 12;
 
 //Convert and store
 destRow[0] = (t0 * 65535U + (t0 << 4) + 2055U) >> 12;
 destRow[1] = (t1 * 65535U + (t1 << 4) + 2055U) >> 12;
 destRow += 2;
 @endcode
 
 @param src
 A pointer to a vImage_Buffer that references 12-bit source pixels
 
 @param dest
 A pointer to a vImage_Buffer that references 16-bit destination pixels.
 
 @param flags
 \p kvImageDoNotTile    Turns off internal multithreading.
 
 @return \p kvImageNoError                      Success!
 @return \p kvImageRoiLargerThanInputBuffer     The source buffer must have a height and
 width at least as large as the destination buffer.
 @return \p kvImageNullPointerArgument          src, dest or table pointer is NULL.
 
 @note This routine will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_12UTo16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_16UTo12U
 
 @abstract Converts 16U to 12U
 
 @discussion For each floating point pixel in src, do the following:
 @code
 uint16_t *srcRow = srcData;
 uint8_t *destRow = destData;
 
 // 2 16-bit in 4-bytes
 t0 = srcRow[0];
 t1 = srcRow[1];
 srcRow += 2;
 
 t0 = (t0 * 4095 + 32767 + (t0 >> 4)) >> 16;
 t1 = (t1 * 4095 + 32767 + (t1 >> 4)) >> 16;
 
 t0 <<= 12;
 t0 |= t1;
 
 // 2 12-bit in 3-bytes
 destRow[0] = t0 >> 16;
 destRow[1] = t0 >> 8;
 destRow[2] = t0;
 destRow += 3;
 @endcode
 
 @param src
 A pointer to a vImage_Buffer that references 12-bit source pixels
 
 @param dest
 A pointer to a vImage_Buffer that references 16-bit destination pixels.
 
 @param flags
 \p kvImageDoNotTile         Turns off internal multithreading.
 
 @return \p kvImageNoError                      Success!
 @return \p kvImageRoiLargerThanInputBuffer     The source buffer must have a height and
 width at least as large as the destination buffer.
 @return \p kvImageNullPointerArgument          src, dest or table pointer is NULL.
 
 @note This routine will not work in place.
 
 */
VIMAGE_PF vImage_Error vImageConvert_16UTo12U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageTableLookUp_ARGB8888
 
 @abstract Transforms an ARGB8888 image by substituting pixel values with pixel values provided by four lookup tables.
 
 @discussion For each pixel in src, do the following:
 Use a lookup table to remap 0...255 values in the source image to a different set of 0...255 values in the destination.
 A different lookup table is used for each channel in the ARGB image.
 
 This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @note This function may be used to do table lookups on other 4 channel 8-bit/channel formats (e.g. RGBA8888) by adjusting the order of the tables
 passed into the function accordingly.
 
 @note Performance Advisory:   For 8-bit monochrome -> ARGB8888  or 8-bit indexed -> ARGB8888 conversions,
 it is likely significantly faster to use vImageLookupTable_Planar8toPlanarF. Use the desired
 ARGB8888 (32 bits/pixel) pixels in place of the planar 32-bit floats in the lookup table.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param alphaTable
 A table to remap A values.
 
 @param redTable
 A table to remap R values.
 
 @param greenTable
 A table to remap G values.
 
 @param blueTable
 A table to remap B values.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageTableLookUp_ARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 alphaTable[256], const Pixel_8 redTable[256], const  Pixel_8 greenTable[256], const  Pixel_8 blueTable[256], vImage_Flags flags) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageTableLookUp_ARGB8888
 
 @abstract Transforms an Planar8 image by substituting pixel values with pixel values provided by four lookup tables.
 
 @discussion For each pixel in src, do the following:
 Use a lookup table to remap 0...255 values in the source image to a different set of 0...255 values in the destination.
 
 This function can work in place provided the following are true:
 If src overlaps with dest, src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If an overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param table
 A table to remap the values in src.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageTableLookUp_Planar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 table[256], vImage_Flags flags)  VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 Additional Lookup table functions are available in Transform.h :
 vImageLookupTable_Planar8toPlanar16,
 vImageLookupTable_Planar8toPlanarF,
 vImageLookupTable_8to64U,
 vImageLookupTable_PlanarFtoPlanar8
 */

/*!
 @function vImageOverwriteChannels_ARGB8888
 
 @abstract Overwrites one or more planes of an ARGB8888 image buffer with the provided planar buffer.
 
 @discussion For each pixel in src, do the following:
 @code
 // Set up a uint32_t mask - 0xFF where the pixels should be conserved
 // Load and splat the src pixel
 uint32_t srcPixel = newSrc->data[x];
 uint32_t result = origSrc->data[x];
 srcPixel |= srcPixel << 8;
 srcPixel |= srcPixel << 16;
 
 // Select for the channels based on the mask
 srcPixel &= ~mask;
 result &= mask;
 
 // combine the two and store
 dest->data[x] = srcPixel | result;
 @endcode
 
 origSrc and dest may overlap, if they share the same origin.
 origSrc should be at least as big as dest
 
 origSrc and dest can be the same buffer
 This function may be used with other channel orderings (e.g. origSrc -> a RGBA8888 buffer) by adjusting the order of the bits in the copyMask.
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param newSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing planar source pixel that we will overwrite with.
 
 @param origSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannels_ARGB8888(    const vImage_Buffer *newSrc,       /* A planar buffer */
                                                        const vImage_Buffer *origSrc,      /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                        uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                        vImage_Flags    flags ) VIMAGE_NON_NULL(1,2,3)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannels_ARGBFFFF
 
 @abstract Overwrites one or more planes of an ARGBFFFF image buffer with the provided planar buffer.
 
 @discussion For each pixel in src, do the following:
 @code
 // Set up a uint32_t mask for which channels to use -1U where the pixels should not be conserved
 uint32_t    a = origSrc->data[0] & maskA;
 uint32_t    r = origSrc->data[1] & maskR;
 uint32_t    g = origSrc->data[2] & maskG;
 uint32_t    b = origSrc->data[3] & maskB;
 uint32_t    colorA = newSrc->data[0];
 uint32_t    colorR = colorA & ~maskR;
 uint32_t    colorG = colorA & ~maskG;
 uint32_t    colorB = colorA & ~maskB;
 colorA &= ~maskA;
 
 dest->data[0] = colorA | a;
 dest->data[1] = colorR | r;
 dest->data[2] = colorG | g;
 dest->data[3] = colorB | b;
 @endcode
 
 origSrc and dest may overlap, if they share the same origin.
 origSrc should be at least as big as dest
 
 origSrc and dest can be the same buffer
 This function may be used with other channel orderings (e.g. origSrc -> a RGBAFFFF buffer) by adjusting the order of the bits in the copyMask.
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param newSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing planar source pixel that we will overwrite with.
 
 @param origSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannels_ARGBFFFF(    const vImage_Buffer *newSrc,       /* A planar buffer */
                                                        const vImage_Buffer *origSrc,      /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                        uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                        vImage_Flags    flags ) VIMAGE_NON_NULL(1,2,3)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithScalar_Planar8
 
 @abstract Fill the dest buffer with the scalar value.
 
 @param scalar
 A scalar value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_Planar8(    Pixel_8     scalar,
                                                                 const vImage_Buffer *dest,      /* A planar buffer */
                                                                 vImage_Flags    flags ) VIMAGE_NON_NULL(2)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithScalar_PlanarF
 
 @abstract Fill the dest buffer with the scalar value.
 
 @param scalar
 A scalar value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_PlanarF( Pixel_F     scalar,
                                                                 const vImage_Buffer *dest,      /* A planar buffer */
                                                                 vImage_Flags    flags ) VIMAGE_NON_NULL(2)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithScalar_Planar16S
 
 @abstract Fill the dest buffer with the scalar value.
 
 @param scalar
 A scalar value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_Planar16S(    Pixel_16S     scalar,
                                                                   const vImage_Buffer *dest,      /* A planar buffer */
                                                                   vImage_Flags    flags ) VIMAGE_NON_NULL(2)        API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageOverwriteChannelsWithScalar_Planar16U
 
 @abstract Fill the dest buffer with the scalar value.
 
 @param scalar
 A scalar value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_Planar16U(    Pixel_16U     scalar,
                                                                   const vImage_Buffer *dest,      /* A planar buffer */
                                                                   vImage_Flags    flags ) VIMAGE_NON_NULL(2)        API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 @function vImageExtractChannel_ARGB8888
 
 @abstract Extract one channel from a 4-channel interleaved 8-bit per component buffer
 
 @discussion  This is the opposite operation from vImageOverwriteChannels_ARGB8888. It reads one component
 from the four channel 8-bit per component buffer and writes it into a Planar8 buffer.
 
 For each pixel i in src:
 @code
 Pixel_8888 *src_pixel;
 Pixel_8 *dest_pixel;
 
 dest_pixel[i] = src_pixel[i][channelIndex];
 @endcode
 
 @param src
 A valid pointer to a vImage_Buffer struct which describes a 8-bit per component, four channel buffer.
 It does not have to be ARGB8888. It can be BGRA, RGBA, CMYK, etc.
 
 @param dest
 A valid pointer to a vImage_Buffer struct which describes a 8-bit per component, one channel buffer.
 The buffer pointed to by dest should be allocated by you. It will be overwritten with one of the
 channels.  This function does work in place, so long as the rowBytes is the same for src and dest
 images and the start address also matches.
 
 @param channelIndex
 The index of the channel to extract. For alpha in a ARGB image, this is 0.  For alpha in a BGRA image, this is 3.
 
 @param flags
 \p kvImageDoNotTile
 \p kvImageGetTempBufferSize
 \p kvImagePrintDiagnosticsToConsole
 
 @return \p kvImageNoError                      Success. However, see also 0 below, if the kvImageGetTempBufferSize flag is passed.
 @return \p 0                                   If the kvImageGetTempBufferSize flag is passed, this function returns 0 and does no work.
 @return \p kvImageRoiLargerThanInputBuffer     The destination height or width is larger than the src height or width, respectively.
 @return \p kvImageUnknownFlagsBit              A flag was used which was not among the approved set of flags. See flags param description above.
 @return \p kvImageInvalidParameter             channelIndex must be in the range [0,3]
 */
VIMAGE_PF vImage_Error vImageExtractChannel_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, long channelIndex, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageExtractChannel_ARGB16U
 
 @abstract Extract one channel from a 4-channel interleaved 16-bit per component buffer.
 
 @discussion  vImageExtractChannel_ARGB16U reads one component from the four channel 16-bit per component buffer
 and writes it into a Planar16U buffer.  Since this just copies data around, the data may be any
 16-bit per component data type, including signed 16 bit integers and half-precision floating point,
 of any endianness. Likewise, the channel order does not need to be ARGB. RGBA, BGRA, CMYK, etc. all work.
 
 For each pixel i in src:
 @code
 Pixel_ARGB_16U *src_pixel;
 Pixel_16U *dest_pixel;
 
 dest_pixel[i] = src_pixel[i][channelIndex];
 @endcode
 
 @param src
 A valid pointer to a vImage_Buffer struct which describes a 16-bit per component, four channel buffer.
 It does not have to be ARGB16U. It can be BGRA, RGBA, CMYK, etc. The data can be any 16-bit per component
 type such as int16_t or half-precision floating-point. Data must be at least 2-byte aligned.
 
 @param dest
 A valid pointer to a vImage_Buffer struct which describes a 16-bit per component, one channel buffer.
 The buffer pointed to by dest should be allocated by you. It will be overwritten with one of the
 channels.  This function does work in place, so long as the rowBytes is the same for src and dest
 images and the start address also matches. The data returned will be in the same format (uint16_t,
 int16_t, half-float, etc.) as the data provided in the src format, except that only a single channel
 is present. Data must be at least 2-byte aligned.
 
 @param channelIndex
 The index of the channel to extract. For alpha in a ARGB image, this is 0.  For alpha in a BGRA image, this is 3.
 
 @param flags
 The following flags are allowed:  kvImageDoNotTile, kvImageGetTempBufferSize, kvImageNoFlags, kvImagePrintDiagnosticsToConsole
 
 @result kvImageNoError                     Success. However, see also 0 below, if the kvImageGetTempBufferSize flag is passed.
 @result 0                                  If the kvImageGetTempBufferSize flag is passed, this function returns 0 and does no work.
 @result kvImageRoiLargerThanInputBuffer    The destination height or width is larger than the src height or width, respectively.
 @result kvImageUnknownFlagsBit             A flag was used which was not among the approved set of flags. See flags param description above.
 @result kvImageInvalidParameter            channelIndex must be in the range [0,3]
 */
VIMAGE_PF vImage_Error vImageExtractChannel_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, long channelIndex, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageExtractChannel_ARGBFFFF
 
 @abstract Extract one channel from a 4-channel interleaved 32-bit per component buffer
 
 @discussion  This is the opposite operation from vImageOverwriteChannels_ARGBFFFF. It reads one component
 from the four channel 32-bit per component buffer and writes it into a PlanarF buffer. NaNs and
 and sNaNs are not modified. Sign of zero shall be preserved.
 
 For each pixel i in src:
 @code
 Pixel_FFFF *src_pixel;
 Pixel_F *dest_pixel;
 
 dest_pixel[i] = src_pixel[i][channelIndex];
 @endcode
 
 @param src
 A valid pointer to a vImage_Buffer struct which describes a 32-bit per component, four channel buffer.
 It does not have to be ARGBFFFF. It can be BGRA, RGBA, CMYK, etc. of any endianness. Data must be at
 least 4-byte aligned.
 
 @param dest
 A valid pointer to a vImage_Buffer struct which describes a 32-bit per component, one channel buffer.
 The buffer pointed to by dest should be allocated by you. It will be overwritten with one of the
 channels.  This function does work in place, so long as the rowBytes is the same for src and dest
 images and the start address also matches. Data must be at least 4 byte aligned.
 
 @param channelIndex
 The index of the channel to extract. For alpha in a ARGB image, this is 0.  For alpha in a BGRA image, this is 3.
 
 @param flags
 The following flags are allowed:  kvImageDoNotTile, kvImageGetTempBufferSize, kvImageNoFlags, kvImagePrintDiagnosticsToConsole
 
 @result kvImageNoError                     Success. However, see also 0 below, if the kvImageGetTempBufferSize flag is passed.
 @result 0                                  If the kvImageGetTempBufferSize flag is passed, this function returns 0 and does no work.
 @result kvImageRoiLargerThanInputBuffer    The destination height or width is larger than the src height or width, respectively.
 @result kvImageUnknownFlagsBit             A flag was used which was not among the approved set of flags. See flags param description above.
 @result kvImageInvalidParameter            channelIndex must be in the range [0,3]
 */
VIMAGE_PF vImage_Error vImageExtractChannel_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, long channelIndex, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*
 * Fill a buffer with a color. Use vImageOverwriteChannelsWithScalar_* to fill planar buffers with a color.
 * These functions work for any 2-channel, 4-channel 8-bit/channel, 16-bit/channel or floating-point format, such as RGBAFFFF or BGRA8888.
 */

/*!
 @function vImageBufferFill_ARGB8888
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                    Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageBufferFill_ARGB8888( const vImage_Buffer *dest, const Pixel_8888 color, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageBufferFill_ARGB16U
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageBufferFill_ARGB16U( const vImage_Buffer *dest, const Pixel_ARGB_16U color, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageBufferFill_ARGB16S
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                    Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageBufferFill_ARGB16S( const vImage_Buffer *dest, const Pixel_ARGB_16S color, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageBufferFill_ARGBFFFF
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                    Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageBufferFill_ARGBFFFF( const vImage_Buffer *dest, const Pixel_FFFF color, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageBufferFill_CbCr8
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error
vImageBufferFill_CbCr8(
                       const vImage_Buffer *dest,
                       const Pixel_88 color,
                       vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!
 @function vImageBufferFill_CbCr16U
 
 @abstract Fill the dest buffer with the pixel value.
 
 @param color
 A pixel value to fill the destination buffer.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error
vImageBufferFill_CbCr16U(
                         const vImage_Buffer *dest,
                         const Pixel_16U16U color,
                         vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));


/*!
 @function vImageOverwriteChannelsWithScalar_ARGB8888
 
 @abstract Overwrites the pixels of one or more planes of an ARGB8888 image buffer with the provided scalar value.
 
 @discussion Fill the color channels (as indicated by copyMask) with the scalar value.
 
 For each pixel in src:
 @code
 Pixel_8888 srcPixel, destPixel;
 int mask;
 int i;
 
 mask = 0x8;
 for( i = 0; i < 4; i++ )
 {
 if( copyMask & mask )
 destPixel[i] = scalar;
 else
 destPixel[i] = srcPixel[i]
 
 mask = mask >> 1;
 }
 @endcode
 
 Bits 0-27 of copyMask must be 0.
 This function can work in place provided the following are true:
 src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @note These functions may be used for images with other channel orderings such as RGBA8888 by adjusting the ordering of the bits in copyMask.
 
 @param scalar
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing scalar value that we will overwrite with.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageInvalidParameter                When copyMask > 15 which is invalid.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @seealso vImageOverwriteChannelsWithPixel_ARGB8888
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_ARGB8888(    Pixel_8     scalar,
                                                                  const vImage_Buffer *src,      /* A ARGB interleaved buffer */
                                                                  const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                                  uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                  vImage_Flags    flags ) VIMAGE_NON_NULL(2,3)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithScalar_ARGBFFFF
 
 @abstract Overwrites the pixels of one or more planes of an ARGBFFFF image buffer with the provided scalar value.
 
 @discussion Fill the color channels (as indicated by copyMask) with the scalar value.
 
 For each pixel in src:
 @code
 Pixel_FFFF srcPixel, destPixel;
 int mask;
 int i;
 
 mask = 0x8;
 for( i = 0; i < 4; i++ )
 {
 if( copyMask & mask )
 destPixel[i] = scalar;
 else
 destPixel[i] = srcPixel[i]
 
 mask = mask >> 1;
 }
 @endcode
 
 Bits 0-27 of copyMask must be 0.
 This function can work in place provided the following are true:
 src->data must be equal to dest->data and src->rowBytes >= dest->rowBytes
 If overlapping src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @note These functions may be used for images with other channel orderings such as RGBAFFFF by adjusting the ordering of the bits in copyMask.
 
 @param scalar
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing scalar value that we will overwrite with.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageInvalidParameter                When copyMask > 15 which is invalid.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @seealso vImageOverwriteChannelsWithPixel_ARGBFFFF
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithScalar_ARGBFFFF(    Pixel_F     scalar,
                                                                  const vImage_Buffer *src,      /* A ARGB interleaved buffer */
                                                                  const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                                  uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                  vImage_Flags    flags ) VIMAGE_NON_NULL(2,3)        API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImagePermuteChannels_ARGB8888
 
 @abstract Reorder color channels within the buffer according to the permute map.
 
 @discussion For each pixel in src, do the following:
 @code
 Pixel_8888 srcPixel, result;
 for( int i = 0; i < 4; i++ )
 result[i] = srcPixel[ permuteMap[i] ];
 @endcode
 
 The src buffer must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 This function may be used with any 4 channel 8-bit/channel format, such as RGBA8888, BGRA8888 or AYUV8888.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3, which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannels_ARGB8888(    const vImage_Buffer *src,
                                                      const vImage_Buffer *dest,
                                                      const uint8_t       permuteMap[4],
                                                      vImage_Flags        flags )    VIMAGE_NON_NULL(1,2,3)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImagePermuteChannels_ARGB16U
 
 @abstract Reorder color channels within the buffer according to the permute map.
 
 @discussion For each pixel in src, do the following:
 @code
 Pixel_ARGB_16U srcPixel, result;
 for( int i = 0; i < 4; i++ )
 result[i] = srcPixel[ permuteMap[i] ];
 @endcode
 
 The src buffer must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 This function may be used with any 4 channel 16-bit/channel format, such as RGBA16U, BGRA16U or AYUV16U.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3, which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannels_ARGB16U(     const vImage_Buffer *src,
                                                     const vImage_Buffer *dest,
                                                     const uint8_t       permuteMap[4],
                                                     vImage_Flags        flags )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 @function vImagePermuteChannels_ARGBFFFF
 
 @abstract Reorder color channels within the buffer according to the permute map.
 
 @discussion For each pixel in src, do the following:
 @code
 Pixel_FFFF srcPixel, result;
 for( int i = 0; i < 4; i++ )
 result[i] = srcPixel[ permuteMap[i] ];
 @endcode
 
 The src buffer must be at least as large as the dest buffer in each dimension. (src.height >= dest.height && src.width >= dest.width)
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 This function may be used with any 4 channel 32-bit/channel format, such as 16S and 16F.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param flags
 \p kvImageNoFlags                     Default operation
 \p kvImageDoNotTile                   Disable internal multithreading.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3, which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannels_ARGBFFFF(    const vImage_Buffer *src,
                                                      const vImage_Buffer *dest,
                                                      const uint8_t       permuteMap[4],
                                                      vImage_Flags        flags )    VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));



/*!
 @function vImagePermuteChannelsWithMaskedInsert_ARGB8888
 
 @discussion This is in principle vImagePermuteChannels_ARGB8888, followed by vImageOverwriteChannelsWithScalar_ARGB8888.  The fused operation is
 provided because it allows you to set different channels to different values (a weakness in vImageOverwriteChannelsWithScalar_ARGB8888)
 and because neither the Permute or Overwrite functions alone saturate the vector ALU on most architectures, so we think we can get
 the extra work done in the compound operation for free.
 
 For each pixel in src, do the following:
 @code
 Pixel_8888 srcPixel,  destPixel;
 uint8_t mask = 0x8;
 
 for( int i = 0; i < 4; i++ )
 {
 result[i] = srcPixel[ permuteMap[i] ];
 if( mask & copyMask )
 result[i] = backgroundColor[i];
 mask = mask >> 1;
 }
 @endcode
 
 If you intend to just set the entire image to just the backgroundColor, we will detect this case and reroute to vImageBufferFill_ARGB8888.
 If it isn't obvious, this will of course work with other non-ARGB channel orderings. You'll need to adjust copyMask accordingly. The
 backgroundColor should be in the output format.
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest,
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param   flags
 \p kvImageNoFlags                  Default operation
 \p kvImageDoNotTile                Disable internal multithreading.
 \p kvImageGetTempBufferSize        Does no work and returns zero, as this function does not use a temp buffer.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3 which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannelsWithMaskedInsert_ARGB8888(    const vImage_Buffer *src,
                                                                      const vImage_Buffer *dest,
                                                                      const uint8_t permuteMap[4],
                                                                      uint8_t copyMask, /* Copy backgroundColor into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                      const Pixel_8888 backgroundColor,
                                                                      vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImagePermuteChannelsWithMaskedInsert_ARGB16U
 
 @discussion This is in principle vImagePermuteChannels_ARGB16U, followed by vImageOverwriteChannelsWithScalar_ARGB16U.  The fused operation is
 provided because it allows you to set different channels to different values (a weakness in vImageOverwriteChannelsWithScalar_ARGB16U)
 and because neither the Permute or Overwrite functions alone saturate the vector ALU on most architectures, so we think we can get
 the extra work done in the compound operation for free.
 
 For each pixel in src, do the following:
 @code
 Pixel_ARGB_16U srcPixel,  destPixel;
 uint8_t mask = 0x8;
 
 for( int i = 0; i < 4; i++ )
 {
 result[i] = srcPixel[ permuteMap[i] ];
 if( mask & copyMask )
 result[i] = backgroundColor[i];
 mask = mask >> 1;
 }
 @endcode
 
 If you intend to just set the entire image to just the backgroundColor, we will detect this case and reroute to vImageBufferFill_ARGB16U.
 If it isn't obvious, this will of course work with other non-ARGB channel orderings. You'll need to adjust copyMask accordingly. The
 backgroundColor should be in the output format.
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest,
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param   flags
 \p kvImageNoFlags                  Default operation
 \p kvImageDoNotTile                Disable internal multithreading.
 \p kvImageGetTempBufferSize        Does no work and returns zero, as this function does not use a temp buffer.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3 which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannelsWithMaskedInsert_ARGB16U(const vImage_Buffer *src,
                                                                     const vImage_Buffer *dest,
                                                                     const uint8_t permuteMap[4],
                                                                     uint8_t copyMask,
                                                                     const Pixel_ARGB_16U backgroundColor,
                                                                     vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImagePermuteChannelsWithMaskedInsert_ARGBFFFF
 
 @discussion This is in principle vImagePermuteChannels_ARGBFFFF, followed by vImageOverwriteChannelsWithScalar_ARGBFFFF.  The fused operation is
 provided because it allows you to set different channels to different values (a weakness in vImageOverwriteChannelsWithScalar_ARGBFFFF)
 and because neither the Permute or Overwrite functions alone saturate the vector ALU on most architectures, so we think we can get
 the extra work done in the compound operation for free.
 
 For each pixel in src, do the following:
 @code
 Pixel_FFFF srcPixel,  destPixel;
 uint8_t mask = 0x8;
 
 for( int i = 0; i < 4; i++ )
 {
 result[i] = srcPixel[ permuteMap[i] ];
 if( mask & copyMask )
 result[i] = backgroundColor[i];
 mask = mask >> 1;
 }
 @endcode
 
 If you intend to just set the entire image to just the backgroundColor, we will detect this case and reroute to vImageBufferFill_ARGBFFFF.
 If it isn't obvious, this will of course work with other non-ARGB channel orderings. You'll need to adjust copyMask accordingly. The
 backgroundColor should be in the output format.
 
 This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest,
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing the source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param permuteMap
 The map describing the permutation of the 4 color channels.
 Each value in the map must be 0,1,2, or 3.  A map of 0,1,2,3
 is a copy from src->dest while a map of 3,2,1,0 is permutes
 ARGB -> BGRA.  Providing a map value greater than 3 will
 result in the return of error kvImageInvalidParameter.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param   flags
 \p kvImageNoFlags                  Default operation
 \p kvImageDoNotTile                Disable internal multithreading.
 \p kvImageGetTempBufferSize        Does no work and returns zero, as this function does not use a temp buffer.
 
 @return kvImageNoError                     Success
 @return kvImageInvalidParameter            When permuteMap > 3 which is invalid.
 @return kvImageRoiLargerThanInputBuffer    The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImagePermuteChannelsWithMaskedInsert_ARGBFFFF(    const vImage_Buffer *src,
                                                                      const vImage_Buffer *dest,
                                                                      const uint8_t permuteMap[4],
                                                                      uint8_t copyMask, /* Copy backgroundColor into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                      const Pixel_FFFF backgroundColor,
                                                                      vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 @function vImageConvert_ARGB8888toPlanarF
 
 @abstract Convert a packed (interleaved) 4-channel 8-bit unsigned buffer to planar float buffers.
 
 @discussion For each pixel in src, do the following:
 @code
 float  alpha = (maxFloat[0] - minFloat[0]) * (float) src[0] / 255.0  + minFloat[0];
 float    red = (maxFloat[1] - minFloat[1]) * (float) src[1] / 255.0  + minFloat[1];
 float  green = (maxFloat[2] - minFloat[2]) * (float) src[2] / 255.0  + minFloat[2];
 float   blue = (maxFloat[3] - minFloat[3]) * (float) src[3] / 255.0  + minFloat[3];
 @endcode
 
 @note This routine will not work in place.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param alpha
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A destination pixels.
 
 @param red
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R destination pixels.
 
 @param green
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G destination pixels.
 
 @param blue
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B destination pixels.
 
 @param maxFloat
 A maxFloat value in the above formula.
 
 @param minFloat
 A minFloat value in the above formula.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageBufferSizeMismatch              When the dimension of alpha / red / green / blue are not same.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGB8888toPlanarF(
                                                          const vImage_Buffer *src,
                                                          const vImage_Buffer *alpha,
                                                          const vImage_Buffer *red,
                                                          const vImage_Buffer *green,
                                                          const vImage_Buffer *blue,
                                                          const Pixel_FFFF maxFloat,
                                                          const Pixel_FFFF minFloat,
                                                          vImage_Flags flags ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_ARGBFFFFtoPlanar8
 
 @abstract Convert a packed (interleaved) 4-channel floating point buffer to planar 8-bit unsigned integer buffers.
 
 @discussion For each pixel in src, do the following:
 @code
 uint8_t alpha = ROUND_TO_INTEGER( CLAMP(0, 255.0f * ( src[0] - minFloat[0], 255 ) / (maxFloat[0] - minFloat[0]) ));
 uint8_t   red = ROUND_TO_INTEGER( CLAMP(0, 255.0f * ( src[1] - minFloat[1], 255 ) / (maxFloat[1] - minFloat[1]) ));
 uint8_t green = ROUND_TO_INTEGER( CLAMP(0, 255.0f * ( src[2] - minFloat[2], 255 ) / (maxFloat[2] - minFloat[2]) ));
 uint8_t  blue = ROUND_TO_INTEGER( CLAMP(0, 255.0f * ( src[3] - minFloat[3], 255 ) / (maxFloat[3] - minFloat[3]) ));
 @endcode
 
 @note This routine will not work in place.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param alpha
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing A destination pixels.
 
 @param red
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing R destination pixels.
 
 @param green
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing G destination pixels.
 
 @param blue
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing B destination pixels.
 
 @param maxFloat
 A maxFloat value in the above formula.
 
 @param minFloat
 A minFloat value in the above formula.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageBufferSizeMismatch              When the dimension of alpha / red / green / blue are not same.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGBFFFFtoPlanar8(
                                                          const vImage_Buffer *src,
                                                          const vImage_Buffer *alpha,
                                                          const vImage_Buffer *red,
                                                          const vImage_Buffer *green,
                                                          const vImage_Buffer *blue,
                                                          const Pixel_FFFF maxFloat,
                                                          const Pixel_FFFF minFloat,
                                                          vImage_Flags flags ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_ARGBFFFFtoRGBFFF
 
 @abstract Convert 4-channel ARGB buffer to a 3-channel RGB one, by removing the alpha (1st) channel.
 @note This routine will work in place.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGBFFFFtoRGBFFF(
                                                         const vImage_Buffer *src,
                                                         const vImage_Buffer *dest,
                                                         vImage_Flags flags ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_RGBAFFFFtoRGBFFF
 
 @abstract Convert 4-channel RGBA buffer to a 3-channel RGB one, by removing the alpha (last) channel.
 @note This routine will work in place.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_RGBAFFFFtoRGBFFF(
                                                         const vImage_Buffer *src,
                                                         const vImage_Buffer *dest,
                                                         vImage_Flags flags ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_BGRAFFFFtoRGBFFF
 
 @abstract Convert 4-channel BGRA buffer to a 3-channel RGB one, by removing the alpha (last) channel and reordering the remaining..
 @note This routine will work in place.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing source pixels.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_BGRAFFFFtoRGBFFF(
                                                         const vImage_Buffer *src,
                                                         const vImage_Buffer *dest,
                                                         vImage_Flags flags ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*!
 @function vImageConvert_RGBFFFtoARGBFFFF
 
 @abstract Convert/pack a float RGB buffer with a corresponding alpha channel buffer or an alpha factor into a ARGB buffer.
 
 @discussion
 a = a corresponding pixel from the alpha channel buffer, if it exists, or a fixed factor
 For each pixel (r,g,b) in src with alpha a, do the following:
 @code
 if (premultiply!=0) dest = (a,r*a,g*a,b*a);
 else dest = (a,r,g,b);
 @endcode
 
 @param rgbSrc
 A pointer to a vImage_Buffer that references the source RGB pixels.  Unchanged on exit.
 
 @param aSrc
 A pointer to a vImage_Buffer that references the source alpha channel. Unchanged on exit.
 
 @param alpha
 If aSrc is non-NULL, this parameter is ignored.
 If aSrc is NULL, the value is used for the alpha channel of every pixel.
 
 @param argbDest
 A pointer to a vImage_Buffer that references where to write the converted ARGB data.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                     Success!
 @return kvImageNullPointerArgument         rgbSrc or argbDest is NULL.
 @return kvImageUnknownFlagsBit             Unknown flag was passed to the function.
 @return kvImageRoiLargerThanInputBuffer    The source buffer must have a size (in both height and width)
 no less than the destination buffers.
 
 @note This routine will not work in place.
 */
VIMAGE_PF vImage_Error
vImageConvert_RGBFFFtoARGBFFFF(
                               const vImage_Buffer* /* rgbSrc      */,
                               const vImage_Buffer* /* aSrc        */,
                               Pixel_F              /* alpha       */,
                               const vImage_Buffer* /* argbDest    */,
                               bool                 /* premultiply */,  /* Boolean 1 or 0 */
                               vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_RGBFFFtoRGBAFFFF
 
 @abstract Convert/pack a float RGB buffer with a corresponding alpha channel buffer or an alpha factor into a RGBA buffer.
 
 @discussion
 a = a corresponding pixel from the alpha channel buffer, if it exists, or a fixed factor
 For each pixel (r,g,b) in src with alpha a, do the following:
 @code
 if (premultiply!=0) dest = (r*a,g*a,b*a,a);
 else dest = (r,g,b,a);
 @endcode
 
 @param rgbSrc
 A pointer to a vImage_Buffer that references the source RGB pixels.  Unchanged on exit.
 
 @param aSrc
 A pointer to a vImage_Buffer that references the source alpha channel. Unchanged on exit.
 
 @param alpha
 If aSrc is non-NULL, this parameter is ignored.
 If aSrc is NULL, the value is used for the alpha channel of every pixel.
 
 @param rgbaDest
 A pointer to a vImage_Buffer that references where to write the converted RGBA data.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                     Success!
 @return kvImageNullPointerArgument         rgbSrc or rgbaDest is NULL.
 @return kvImageUnknownFlagsBit             Unknown flag was passed to the function.
 @return kvImageRoiLargerThanInputBuffer    The source buffer must have a size (in both height and width)
 no less than the destination buffers.
 
 @note This routine will not work in place.
 */
VIMAGE_PF vImage_Error
vImageConvert_RGBFFFtoRGBAFFFF(
                               const vImage_Buffer* /* rgbSrc      */,
                               const vImage_Buffer* /* aSrc        */,
                               Pixel_F              /* alpha       */,
                               const vImage_Buffer* /* rgbaDest    */,
                               bool                 /* premultiply */,  /* Boolean 1 or 0 */
                               vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_RGBFFFtoBGRAFFFF
 
 @abstract Convert/pack a float RGB buffer with a corresponding alpha channel buffer or an alpha factor into a BGRA buffer.
 
 @discussion
 a = a corresponding pixel from the alpha channel buffer, if it exists, or a fixed factor
 For each pixel (r,g,b) in src with alpha a, do the following:
 @code
 if (premultiply!=0) dest = (b*a,g*a,r*a,a);
 else dest = (b,g,r,a);
 @endcode
 
 @param rgbSrc
 A pointer to a vImage_Buffer that references the source RGB pixels.  Unchanged on exit.
 
 @param aSrc
 A pointer to a vImage_Buffer that references the source alpha channel. Unchanged on exit.
 
 @param alpha
 If aSrc is non-NULL, this parameter is ignored.
 If aSrc is NULL, the value is used for the alpha channel of every pixel.
 
 @param bgraDest
 A pointer to a vImage_Buffer that references where to write the converted BGRA data.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                     Success!
 @return kvImageNullPointerArgument         rgbSrc or bgraDest is NULL.
 @return kvImageUnknownFlagsBit             Unknown flag was passed to the function.
 @return kvImageRoiLargerThanInputBuffer    The source buffer must have a size (in both height and width)
 no less than the destination buffers.
 
 @note This routine will not work in place.
 */
VIMAGE_PF vImage_Error
vImageConvert_RGBFFFtoBGRAFFFF(
                               const vImage_Buffer* /* rgbSrc      */,
                               const vImage_Buffer* /* aSrc        */,
                               Pixel_F              /* alpha       */,
                               const vImage_Buffer* /* bgraDest    */,
                               bool                 /* premultiply */,  /* Boolean 1 or 0 */
                               vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

#define vImageConvert_BGRFFFtoBGRAFFFF( _bgrSrc, _aSrc, _alpha, _bgraDest, _premultiply, _flags )   \
vImageConvert_RGBFFFtoRGBAFFFF((_bgrSrc), (_aSrc), (_alpha), (_bgraDest), (_premultiply), (_flags) )
#define vImageConvert_BGRFFFtoRGBAFFFF( _bgrSrc, _aSrc, _alpha, _rgbaDest, _premultiply, _flags )   \
vImageConvert_RGBFFFtoBGRAFFFF((_bgrSrc), (_aSrc), (_alpha), (_rgbaDest), (_premultiply), (_flags) )


/*!
 @function vImageConvert_ARGB1555toPlanar8
 
 @abstract Convert from 16 bit/pixel ARGB1555 to 8-bit/channel Planar8 format.
 
 @discussion
 For each pixel x in src->data:
 @code
 destA->data[x] =  1bitAlphaChannel * 255;
 destR->data[x] = (5bitRedChannel   * 255 + 15) / 31;
 destG->data[x] = (5bitGreenChannel * 255 + 15) / 31;
 destB->data[x] = (5bitBlueChannel  * 255 + 15) / 31;
 @endcode
 
 @note This function will not work in place.
 
 @param src
 A pointer to a vImage_Buffer that references the ARGB source channels.
 
 @param destA
 A pointer to a vImage_Buffer that references the destination planar 8-bit alpha channel.
 
 @param destR
 A pointer to a vImage_Buffer that references the destination planar 8-bit R channel.
 
 @param destG
 A pointer to a vImage_Buffer that references the destination planar 8-bit G channel.
 
 @param destB
 A pointer to a vImage_Buffer that references the destination planar 8-bit B channel.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageBufferSizeMismatch              When the dimension of alpha / red / green / blue are not same.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGB1555toPlanar8( const vImage_Buffer *src, const vImage_Buffer *destA, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags )    VIMAGE_NON_NULL(1,2,3,4,5)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_ARGB1555toARGB8888
 
 @abstract Convert from 16 bit/pixel ARGB1555 to 32 bit/pixel ARGB8888 format.
 
 @discussion
 For each pixel x in src:
 @code
 Pixel8 alpha =  1bitAlphaChannel * 255;
 Pixel8 red   = (5bitRedChannel   * 255 + 15) / 31;
 Pixel8 green = (5bitGreenChannel * 255 + 15) / 31;
 Pixel8 blue  = (5bitBlueChannel  * 255 + 15) / 31;
 dest->data[x] = {alpha, red, green, blue};
 @endcode
 
 @note This function will not work in place.
 
 @param src
 A pointer to a vImage_Buffer that references the ARGB source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination ARGB channels.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGB1555toARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_Planar8toARGB1555
 
 @abstract Convert from 8-bit/channel Planar8 to 16 bit/pixel ARGB1555 format.
 
 @discussion
 For each pixel x:
 @code
 uint32_t alpha = (srcA->data[x]      + 127) / 255;
 uint32_t red   = (srcR->data[x] * 31 + 127) / 255;
 uint32_t green = (srcG->data[x] * 31 + 127) / 255;
 uint32_t blue  = (srcB->data[x] * 31 + 127) / 255;
 dest->data[x] =  (alpha << 15) | (red << 10) | (green << 5) | blue;
 @endcode
 
 @note This function will not work in place.
 
 @param srcA
 A pointer to a vImage_Buffer that references the 8-bit alpha source channel.
 
 @param srcR
 A pointer to a vImage_Buffer that references the 8-bit R source channel.
 
 @param srcG
 A pointer to a vImage_Buffer that references the 8-bit G source channel.
 
 @param srcB
 A pointer to a vImage_Buffer that references the 8-bit B source channel.
 
 @param dest
 A pointer to a vImage_Buffer that references the ARGB destination channels.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageBufferSizeMismatch              When the dimension of alpha / red / green / blue are not same.
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_Planar8toARGB1555( const vImage_Buffer *srcA, const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags )    VIMAGE_NON_NULL(1,2,3,4,5)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_ARGB8888toARGB1555
 
 @abstract Convert between 32 bit/pixel ARGB8888 to 16 bit/pixel ARGB1555 format.
 
 @discussion
 For each pixel x in src:
 @code
 uint32_t alpha = (8bitAlphaChannel      + 127) / 255;
 uint32_t red   = (8bitRedChannel   * 31 + 127) / 255;
 uint32_t green = (8bitGreenChannel * 31 + 127) / 255;
 uint32_t blue  = (8bitBlueChannel  * 31 + 127) / 255;
 dest->data[x] =  (alpha << 15) | (red << 10) | (green << 5) | blue;
 @endcode
 
 @note This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param src
 A pointer to a vImage_Buffer that references the source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination channels.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_ARGB8888toARGB1555( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_RGBA5551toRGBA8888
 
 @abstract Convert from 16 bit/pixel RGBA5551 to 32 bit/pixel RGBA8888 format.
 
 @discussion
 For each pixel x in src:
 @code
 Pixel8 red   = (5bitRedChannel   * 255 + 15) / 31;
 Pixel8 green = (5bitGreenChannel * 255 + 15) / 31;
 Pixel8 blue  = (5bitBlueChannel  * 255 + 15) / 31;
 Pixel8 alpha =  1bitAlphaChannel * 255;
 dest->data[x] = {red, green, blue, alpha};
 @endcode
 
 @note This function will not work in place.
 
 @param src
 A pointer to a vImage_Buffer that references the RGBA source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination RGBA channels.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_RGBA5551toRGBA8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
#define         vImageConvert_BGRA5551toBGRA8888( _src, _dest, _flags)  vImageConvert_RGBA5551toRGBA8888( _src, _dest, _flags )

/*!
 @function vImageConvert_RGBA8888toRGBA5551
 
 @abstract Convert between 32 bit/pixel RGBA8888 to 16 bit/pixel RGBA5551 format.
 
 @discussion
 For each pixel x in src:
 @code
 uint32_t red   = (8bitRedChannel   * 31 + 127) / 255;
 uint32_t green = (8bitGreenChannel * 31 + 127) / 255;
 uint32_t blue  = (8bitBlueChannel  * 31 + 127) / 255;
 uint32_t alpha = (8bitAlphaChannel      + 127) / 255;
 dest->data[x] =  (red << 11) | (green << 6) | (blue << 1) | alpha;
 @endcode
 
 @note This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param src
 A pointer to a vImage_Buffer that references the source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination channels.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_RGBA8888toRGBA5551( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
#define         vImageConvert_BGRA8888toBGRA5551( _src, _dest, _flags)  vImageConvert_RGBA8888toRGBA5551( _src, _dest, _flags )

/*!
 @function vImageConvert_ARGB8888toARGB1555_dithered
 
 @abstract Convert between 32 bit/pixel ARGB8888 to 16 bit/pixel ARGB1555 format with dithering.
 
 @discussion Similar to vImageConvert_ARGB8888toARGB1555, except the result is dithered instead of round to nearest.
 This method should provide more accurate (overall) color reproduction and less banding in low-frequency regions of the image.
 
 @note This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param src
 A pointer to a vImage_Buffer that references the source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination channels.
 
 @param dither
 A dithering method which should be kvImageConvert_DitherOrdered or kvImageConvert_DitherOrderedReproducible.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 
 @seealso vImageConvert_ARGB8888toARGB1555
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888toARGB1555_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageConvert_RGBA8888toRGBA5551_dithered
 
 @abstract Convert between 32 bit/pixel RGBA8888 to 16 bit/pixel RGBA5551 format with dithering.
 
 @discussion Similar to vImageConvert_RGBA8888toRGBA5551, except the result is dithered instead of round to nearest.
 This method should provide more accurate (overall) color reproduction and less banding in low-frequency regions of the image.
 
 @note This function can work in place provided the following are true:
 For each buffer "buf" that overlaps with dest, buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes.
 If an overlapping buffer has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags.
 
 @param src
 A pointer to a vImage_Buffer that references the source channels.
 
 @param dest
 A pointer to a vImage_Buffer that references the destination channels.
 
 @param dither
 A dithering method which should be kvImageConvert_DitherOrdered or kvImageConvert_DitherOrderedReproducible.
 
 @param flags
 \p kvImageDoNotTile            Turns off internal multithreading.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_RGBA8888toRGBA5551_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
#define      vImageConvert_BGRA8888toBGRA5551_dithered( _src, _dest, _tempBuffer, _dither, _flags)  vImageConvert_RGBA8888toRGBA5551_dithered( _src, _dest, _tempBuffer, _dither, _flags )

/*  Convert from 16 bit/pixel RGB565 to 32 bit/pixel ARGB8888 or RGBA8888 or
 BGRA8888 formats.  For each pixel:
 
 Pixel8 alpha = alpha;
 Pixel8 red   = (5bitRedChannel   * 255 + 15) / 31;
 Pixel8 green = (6bitGreenChannel * 255 + 31) / 63;
 Pixel8 blue  = (5bitBlueChannel  * 255 + 15) / 31;
 
 The following flags are allowed:
 kvImageDoNotTile            Disables internal threading.  You may want
 to use this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize    Does no work and returns zero, as this
 function does not use a temp buffer.
 
 Return values:
 kvImageNoError              Success
 kvImageUnknownFlagsBit      No work was done because an unknown bit was
 set in the flags parameter.
 kvImageBufferSizeMismatch   No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 
 These functions do not operate in place.
 */

VIMAGE_PF vImage_Error vImageConvert_RGB565toARGB8888(Pixel_8 alpha, const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_RGB565toRGBA8888(Pixel_8 alpha, const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGB565toBGRA8888(Pixel_8 alpha, const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGB565toRGB888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags )  VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*  Convert from 32 bit/pixel ARGB8888 or RGBA8888 or BGRA8888 formats
 to 16 bit/pixel RGB565. for each pixel:
 
 uint32_t red   = (8bitRedChannel   * 31 + 127) / 255;
 uint32_t green = (8bitGreenChannel * 63 + 127) / 255;
 uint32_t blue  = (8bitBlueChannel  * 31 + 127) / 255;
 uint16_t RGB565pixel =  (red << 11) | (green << 5) | blue;
 
 The following flags are allowed:
 kvImageDoNotTile            Disables internal threading.  You may want
 to use this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize    Does no work and returns zero, as this
 function does not use a temp buffer.
 
 Return values:
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 These functions operate in place, provided that src->data == dest->data,
 src->rowBytes >= dest->rowBytes, and the kvImageDoNotTile flag is used
 if src->rowBytes > dest->rowBytes.
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888toRGB565(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_RGBA8888toRGB565(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_BGRA8888toRGB565(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_RGB888toRGB565_dithered
 vImageConvert_ARGB8888toRGB565_dithered
 vImageConvert_RGBA8888toRGB565_dithered
 vImageConvert_BGRA8888toRGB565_dithered
 
 @param dither     A dithering method which should be kvImageConvert_DitherOrdered or kvImageConvert_DitherOrderedReproducible.
 
 Convert from RGB888 or ARGB8888 or RGBA8888 or BGRA8888 formats
 to 16 bit/pixel RGB565 with dithering. For each pixel:
 
 uint32_t red   = (8bitRedChannel   * (31 << 8) + 127) / 255;
 uint32_t green = (8bitGreenChannel * (63 << 8) + 127) / 255;
 uint32_t blue  = (8bitBlueChannel  * (31 << 8) + 127) / 255;
 red   += dither_noise + (1 << 7);
 green += dither_noise + (1 << 7);
 blue  += dither_noise + (1 << 7);
 red    = red >> 8;
 green  = green >> 8;
 blue   = blue >> 8;
 uint16_t RGB565pixel =  (red << 11) | (green << 5) | blue;
 
 The following flags are allowed:
 kvImageDoNotTile            Disables internal threading.  You may want
 to use this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize    Does no work and returns zero, as this
 function does not use a temp buffer.
 
 Return values:
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 These functions operate in place, provided that src->data == dest->data,
 src->rowBytes >= dest->rowBytes, and the kvImageDoNotTile flag is used
 if src->rowBytes > dest->rowBytes.
 */

VIMAGE_PF vImage_Error vImageConvert_RGB888toRGB565_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB8888toRGB565_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGBA8888toRGB565_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_BGRA8888toRGB565_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*  Convert between 16 bit/pixel RGB565 and Planar8 formats.  For the forward
 conversion, for each pixel:
 
 Pixel8 red   = (5bitRedChannel   * 255 + 15) / 31;
 Pixel8 green = (6bitGreenChannel * 255 + 31) / 63;
 Pixel8 blue  = (5bitBlueChannel  * 255 + 15) / 31;
 
 For the reverse conversion, for each pixel:
 
 uint32_t red   = (8bitRedChannel   * 31 + 127) / 255;
 uint32_t green = (8bitGreenChannel * 63 + 127) / 255;
 uint32_t blue  = (8bitBlueChannel  * 31 + 127) / 255;
 uint16_t RGB565pixel =  (red << 11) | (green << 5) | blue;
 
 The following flags are allowed:
 kvImageDoNotTile            Disables internal threading.  You may want
 to use this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize    Does no work and returns zero, as this
 function does not use a temp buffer.
 
 Return values:
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 These functions do not operate in place.
 */

VIMAGE_PF vImage_Error vImageConvert_RGB565toPlanar8(const vImage_Buffer *src, const vImage_Buffer *destR, const vImage_Buffer *destG, const vImage_Buffer *destB, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_Planar8toRGB565(const vImage_Buffer *srcR, const vImage_Buffer *srcG, const vImage_Buffer *srcB, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageConvert_RGBA5551toRGB565
 @abstract Convert from RGBA5551 to RGB565 image format
 @discussion  Convert (with loss of alpha) from RGBA5551 to RGB565 format.
 If you need something fancier done with alpha first, such as unpremultiplication or flattening, convert to 8 bit per channel first.
 Both RGB565 and RGBA5551 are defined by vImage to be host-endian formats. On Intel and ARM and other little endian systems, these are
 little endian uint16_t's in memory. On a big endian system, these are big endian uint16_t's.
 
 @param src           A pointer to a vImage_Buffer struct which describes a memory region full of RGBA5551 pixels
 
 @param dest          A pointer to a vImage_Buffer struct which describes a preallocated memory region to be overwritten by RGB565 pixels
 
 @param flags           The following flags are understood by this function:
 
 <pre>
 @textblock
 kvImageNoFlags                      Default operation.
 
 kvImageDoNotTile                    Turn internal multithreading off. This may be helpful in cases where you already have
 many such operations going concurrently, and in cases where it is desirable to keep
 CPU utilization to a single core.
 
 kvImageGetTempBufferSize            Returns 0.  Reads and writes no pixels.
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                         Success
 
 kvImageRoiLargerThanInputBuffer        dest->height > src->height OR dest->width > src->width.  There are not enough pixels to fill the destination buffer.
 @/textblock
 </pre>
 */
VIMAGE_PF vImage_Error vImageConvert_RGBA5551toRGB565( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB1555toRGB565
 @abstract Convert from ARGB1555 to RGB565 image format
 @discussion  Convert (with loss of alpha) from ARGB1555 to RGB565 format.
 If you need something fancier done with alpha first, such as unpremultiplication or flattening, convert to 8 bit per channel first.
 Both RGB565 and ARGB1555 are defined by vImage to be host-endian formats. On Intel and ARM and other little endian systems, these are
 little endian uint16_t's in memory. On a big endian system, these are big endian uint16_t's.
 
 @param src           A pointer to a vImage_Buffer struct which describes a memory region full of ARGB1555 pixels
 
 @param dest          A pointer to a vImage_Buffer struct which describes a preallocated memory region to be overwritten by RGB565 pixels
 
 @param flags           The following flags are understood by this function:
 
 <pre>
 @textblock
 kvImageNoFlags                      Default operation.
 
 kvImageDoNotTile                    Turn internal multithreading off. This may be helpful in cases where you already have
 many such operations going concurrently, and in cases where it is desirable to keep
 CPU utilization to a single core.
 
 kvImageGetTempBufferSize            Returns 0.  Reads and writes no pixels.
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                         Success
 
 kvImageRoiLargerThanInputBuffer        dest->height > src->height OR dest->width > src->width.  There are not enough pixels to fill the destination buffer.
 @/textblock
 </pre>
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB1555toRGB565( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_RGB565toRGBA5551
 @abstract Convert from RGB565 to RGBA5551 image format
 @discussion  Convert from RGB565 to RGBA5551 format.  The new alpha is set to 1.
 Both RGB565 and RGBA5551 are defined by vImage to be host-endian formats. On Intel and ARM and other little endian systems, these are
 little endian uint16_t's in memory. On a big endian system, these are big endian uint16_t's.
 
 @param src           A pointer to a vImage_Buffer struct which describes a memory region full of RGB565 pixels
 
 @param dest          A pointer to a vImage_Buffer struct which describes a preallocated memory region to be overwritten by RGBA5551 pixels
 
 @param dither        A dithering method for the green channel.   Options:
 
 kvImageConvert_DitherNone - apply no dithering; input values
 are rounded to the nearest value representable in the
 destination format.
 kvImageConvert_DitherOrdered - pre-computed blue noise is
 added to the image before rounding to the values in
 the destination format.  The offset into this blue
 noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on
 sequential frames of video.
 kvImageConvert_DitherOrderedReproducible - pre-computed
 blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the
 blue noise is the same for every call to allow users
 to get reproducible results.
 
 The ordered dither methods may be further influenced by shaping the
 distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageCon-
 vert_DitherOrderedReproducible:
 
 kvImageConvert_OrderedGaussianBlue - when using an ordered dither
 pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --
 less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone
 and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and
 kvImageConvert_DitherOrderedReproducible.
 
 kvImageConvert_OrderedUniformBlue - when using an ordered dither
 pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more
 obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other
 circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image
 is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little
 inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 To clarify: "Blue" noise is not blue, nor does it operate solely on the blue
 color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency
 than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have
 visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 @param flags           The following flags are understood by this function:
 
 <pre>
 @textblock
 kvImageNoFlags                      Default operation.
 
 kvImageDoNotTile                    Turn internal multithreading off. This may be helpful in cases where you already have
 many such operations going concurrently, and in cases where it is desirable to keep
 CPU utilization to a single core.
 
 kvImageGetTempBufferSize            Returns 0.  Reads and writes no pixels.
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                          Success
 
 kvImageRoiLargerThanInputBuffer         dest->height > src->height OR dest->width > src->width.  There are not enough pixels to fill the destination buffer.
 
 kvImageInvalidParameter                 Invalid / unknown dither value
 @/textblock
 </pre>
 */
VIMAGE_PF vImage_Error vImageConvert_RGB565toRGBA5551( const vImage_Buffer *src, const vImage_Buffer *dest, int dither, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_RGB565toARGB1555
 @abstract Convert from RGB565 to ARGB1555 image format
 @discussion  Convert from RGB565 to ARGB1555 format.  The new alpha is set to 1.
 Both RGB565 and ARGB1555 are defined by vImage to be host-endian formats. On Intel and ARM and other little endian systems, these are
 little endian uint16_t's in memory. On a big endian system, these are big endian uint16_t's.
 
 @param src           A pointer to a vImage_Buffer struct which describes a memory region full of RGB565 pixels
 
 @param dest          A pointer to a vImage_Buffer struct which describes a preallocated memory region to be overwritten by ARGB1555 pixels
 
 @param dither        A dithering method for the green channel.   Options:
 
 kvImageConvert_DitherNone - apply no dithering; input values
 are rounded to the nearest value representable in the
 destination format.
 kvImageConvert_DitherOrdered - pre-computed blue noise is
 added to the image before rounding to the values in
 the destination format.  The offset into this blue
 noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on
 sequential frames of video.
 kvImageConvert_DitherOrderedReproducible - pre-computed
 blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the
 blue noise is the same for every call to allow users
 to get reproducible results.
 
 The ordered dither methods may be further influenced by shaping the
 distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageCon-
 vert_DitherOrderedReproducible:
 
 kvImageConvert_OrderedGaussianBlue - when using an ordered dither
 pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --
 less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone
 and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and
 kvImageConvert_DitherOrderedReproducible.
 
 kvImageConvert_OrderedUniformBlue - when using an ordered dither
 pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more
 obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other
 circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image
 is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little
 inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 To clarify: "Blue" noise is not blue, nor does it operate solely on the blue
 color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency
 than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have
 visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 @param flags           The following flags are understood by this function:
 
 <pre>
 @textblock
 kvImageNoFlags                      Default operation.
 
 kvImageDoNotTile                    Turn internal multithreading off. This may be helpful in cases where you already have
 many such operations going concurrently, and in cases where it is desirable to keep
 CPU utilization to a single core.
 
 kvImageGetTempBufferSize            Returns 0.  Reads and writes no pixels.
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                          Success
 
 kvImageRoiLargerThanInputBuffer               dest->height > src->height OR dest->width > src->width.  There are not enough pixels to fill the destination buffer.
 
 kvImageInvalidParameter                 Invalid / unknown dither value
 @/textblock
 </pre>
 */
VIMAGE_PF vImage_Error vImageConvert_RGB565toARGB1555( const vImage_Buffer *src, const vImage_Buffer *dest, int dither, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*
 Convert between 16 bit floats to 32 bit float format.
 The 16 bit floating point format is identical to OpenEXR. It has a layout as follows:
 
 16 bits:  seeeeemmmmmmmmmm
 
 1-bit sign | 5 bits of exponent, with a bias of 15 | 10 bits of significand (with 11 bits of significance due to the implicit 1 bit)
 
 NaNs, Infinities and denormals are supported.
 Per IEEE-754, all signaling NaNs are quieted during the conversion. (OpenEXR-1.2.1 converts SNaNs to SNaNs.)
 In the float->16 bit float direction, rounding occurs according to the IEEE-754 standard and current IEEE-754 rounding mode.
 To set/inspect the current IEEE-754 rounding mode, please see appropriate utilities in fenv.h
 
 vImageConvert_Planar16FtoPlanarF does not work in place.
 vImageConvert_PlanarFtoPlanar16F does work in place, though the contents of the unused half of the buffer are undefined
 In which case:
 src->data must be equal to dest->data  and src->rowBytes >= dest->rowBytes
 If src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 These functions may also be used with multichannel images formats, such as RGBAFFFF by scaling the width by the number of channels.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_Planar16FtoPlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_PlanarFtoPlanar16F( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*
 vImageConvert_Planar8toPlanar16F
 
 Convert from 8 bit integer to 16 bit float format.
 
 destPixel[x] = ConvertToPlanar16F(srcPixel[x]);
 
 The range for conversion is [0,255] -> (half) [0.0, 1.0]
 
 The 16 bit floating point format is half-precision floating point
 (a.k.a.  IEEE-754 binary16, OpenCL half, GL_ARB_half_float_pixel, OpenEXR half).
 It has a layout as follows:
 
 16 bits:  seeeeemmmmmmmmmm
 
 1-bit sign | 5 bits of exponent, with a bias of 15 | 10 bits of significand
 (with 11 bits of significance due to the implicit 1 bit)
 
 Operands:
 ---------
 src             A pointer to a vImage_Buffer that references the source pixels
 
 dest            A pointer to a vImage_Buffer that references the destination pixels
 
 flags           The following flags are allowed:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 Return Value:
 -------------
 kvImageNoError                  Success!
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height.
 kvImageNullPointerArgument      src or dest pointer is NULL.
 kvImageUnknownFlagsBit            Unexpected flag was passed.
 
 This routine will work in place as long as the scan lines overlap exactly.
 
 You can use this for ARGB8888 -> ARGB16F conversions by simply multiplying
 the width of the vImage_Buffer by 4 (for 4 channels)
 */
VIMAGE_PF vImage_Error
vImageConvert_Planar8toPlanar16F(
                                 const vImage_Buffer *src,
                                 const vImage_Buffer *dest,
                                 vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Planar16FtoPlanar8
 
 Convert from 16 bit float to 8 bit integer format.
 
 destPixel[x] = ROUND_TO_INTEGER( SATURATED_CLAMP_0_to_255( 255.0f * (srcPixel[x])));
 
 The ROUND_TO_INTEGER function is round to nearest integer (ties go to the even result).
 
 The 16 bit floating point format is half-precision floating point
 (a.k.a.  IEEE-754 binary16, OpenCL half, GL_ARB_half_float_pixel, OpenEXR half).
 It has a layout as follows:
 
 16 bits:  seeeeemmmmmmmmmm
 
 1-bit sign | 5 bits of exponent, with a bias of 15 | 10 bits of significand
 (with 11 bits of significance due to the implicit 1 bit)
 
 All NaNs are converted to 0.
 To set/inspect the current IEEE-754 rounding mode, please see appropriate utilities in fenv.h
 
 Operands:
 ---------
 src             A pointer to a vImage_Buffer that references the source pixels
 
 dest            A pointer to a vImage_Buffer that references the destination pixels
 
 flags           The following flags are allowed:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 Return Value:
 -------------
 kvImageNoError                  Success!
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height.
 kvImageNullPointerArgument      src or dest pointer is NULL.
 kvImageUnknownFlagsBit            Unexpected flag was passed.
 
 This routine will work in place as long as the scan lines overlap exactly.
 
 You can use this for ARGB16F -> ARGB8888 conversions by simply multiplying
 the width of the vImage_Buffer by 4 (for 4 channels)
 */
VIMAGE_PF vImage_Error
vImageConvert_Planar16FtoPlanar8(
                                 const vImage_Buffer *src,
                                 const vImage_Buffer *dest,
                                 vImage_Flags flags ) VIMAGE_NON_NULL(1,2)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageConvert_16UToPlanar8
 Convert a planar (or interleaved -- multiply vImage_Buffer.width by 4) vImage_Buffer of 16 bit unsigned shorts to a buffer containing 8 bit integer values.
 For each 16 bit pixel in src:
 
 uint8_t result = (srcPixel * 255 + 32767) / 65535;
 
 To convert 4 channel interleaved unsigned 16 bit data to ARGB_8888, simply multiply the vImage_Buffer.width by 4.
 
 This can work in place, though the contents of the unused half of the buffer are undefined
 In which case:
 src->data must be equal to dest->data  and src->rowBytes >= dest->rowBytes
 If src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 This function may also be used with multichannel images formats, such as RGBA16U -> RGBA8888 by scaling the width by the number of channels.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_16UToPlanar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*
 vImageConvert_Planar8To16U:
 Convert a planar (or interleaved -- multiply vImage_Buffer.width by 4) vImage_Buffer of 8-bit integer values to a buffer of 16 bit unsigned ints.
 For each 8-bit pixel in src:
 
 uint16_t result = (srcPixel * 65535 + 127 ) / 255;
 
 will not work in place.
 This function may also be used with multichannel images formats, such as RGBA8888 -> RGBA16U  by scaling the width by the number of channels.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar8To16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*
 Convert RGB to ARGB
 
 If premultiply != 0, then
 
 r = (a * r + 127) / 255
 g = (a * g + 127) / 255
 b = (a * b + 127) / 255
 
 will not work in place.
 Pass NULL for aSrc to use alpha instead.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_RGB888toARGB8888(   const vImage_Buffer* /* rgbSrc */,
                                                      const vImage_Buffer* /* aSrc */,
                                                      Pixel_8 /* alpha */,
                                                      const vImage_Buffer* /*argbDest*/,
                                                      bool /* premultiply */,  /* Boolean 1 or 0 */
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_RGB888toRGBA8888(    const vImage_Buffer * /* rgbSrc */,
                                                      const vImage_Buffer * /* aSrc */,
                                                      Pixel_8 /* alpha */,
                                                      const vImage_Buffer * /* rgbaDest */,
                                                      bool /* premultiply */,  /* Boolean 1 or 0 */
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error vImageConvert_RGB888toBGRA8888(    const vImage_Buffer * /* rgbSrc, */,
                                                      const vImage_Buffer * /* aSrc */,
                                                      Pixel_8 /* alpha */,
                                                      const vImage_Buffer * /* bgraDest */,
                                                      bool /* premultiply */,  /* Boolean 1 or 0 */
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define vImageConvert_BGR888toBGRA8888( _bgrSrc, _aSrc, _alpha, _bgraDest, _premultiply, _flags )   vImageConvert_RGB888toRGBA8888((_bgrSrc), (_aSrc), (_alpha), (_bgraDest), (_premultiply), (_flags) )
#define vImageConvert_BGR888toRGBA8888( _bgrSrc, _aSrc, _alpha, _rgbaDest, _premultiply, _flags )   vImageConvert_RGB888toBGRA8888((_bgrSrc), (_aSrc), (_alpha), (_rgbaDest), (_premultiply), (_flags) )

/*
 Convert 4 channel buffer to a 3 channel one, by removing the 1st channel.
 The R,G and B channels are simply copied into the new buffer.
 
 This can work in place, though the contents of the unused half of the buffer are undefined
 In which case:
 src->data must be equal to dest->data  and src->rowBytes >= dest->rowBytes
 If src has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888toRGB888(    const vImage_Buffer * /*argbSrc*/,
                                                      const vImage_Buffer * /*rgbDest*/,
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_RGBA8888toRGB888(    const vImage_Buffer * /*rgbaSrc*/,
                                                      const vImage_Buffer * /*rgbDest*/,
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error vImageConvert_BGRA8888toRGB888(    const vImage_Buffer * /* bgraSrc */,
                                                      const vImage_Buffer * /* rgbDest */,
                                                      vImage_Flags /* flags */ ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(6.0), watchos(1.0), tvos(6.0));
#define vImageConvert_RGBA8888toBGR888( _rgbaSrc, _bgrDest, _flags )    vImageConvert_BGRA8888toRGB888((_rgbaSrc), (_bgrDest), (_flags))
#define vImageConvert_BGRA8888toBGR888( _bgraSrc, _bgrDest, _flags )    vImageConvert_RGBA8888toRGB888((_bgraSrc), (_bgrDest), (_flags))

/*
 Flatten a ARGB8888 image to a RGB888 image against an opaque background of a certain color.
 The calculation for each {R,G,B} channel is done as:
 
 8-bit:
 if( isImagePremultiplied )
 color = (color * 255 + (255 - alpha) * backgroundColor + 127) / 255
 else
 color = (color * alpha + (255 - alpha) * backgroundColor + 127) / 255
 
 floating point:
 if( isImagePremultiplied )
 color = color + (1.0f - alpha) * backgroundColor
 else
 color = color * alpha + (1.0f - alpha) * backgroundColor
 
 backgroundColor.alpha is ignored.
 These functions will work in place provided that rowBytes and the position of row starts is the same between src and dest images.
 
 Note: that regardless of the value of isImagePremultiplied, the result image is premultiplied by alpha. If the backgroundColor
 was not opaque and you want a non-premultiplied image, you will still need to unpremultiply the result.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageNullPointerArgument             If src or dest == NULL
 kvImageUnknownFlagsBit                 kvImageDoNotTile and kvImageNoFlags are only flags allowed for these function.
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error  vImageFlatten_ARGB8888ToRGB888(
                                                       const vImage_Buffer * /* argb8888Src */,
                                                       const vImage_Buffer * /* rgb888dest */,
                                                       const Pixel_8888   /* backgroundColor */,    /* background color is assumed to have a 255 alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error  vImageFlatten_ARGBFFFFToRGBFFF(
                                                       const vImage_Buffer * /* argbFFFFSrc */,
                                                       const vImage_Buffer * /* rgbFFFdest */,
                                                       const Pixel_FFFF   /* backgroundColor */,    /* background color is assumed to have a 1.0f alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error  vImageFlatten_RGBA8888ToRGB888(
                                                       const vImage_Buffer * /* rgba8888Src */,
                                                       const vImage_Buffer * /* rgb888dest */,
                                                       const Pixel_8888   /* backgroundColor */,    /* background color is assumed to have a 255 alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error  vImageFlatten_RGBAFFFFToRGBFFF(
                                                       const vImage_Buffer * /* rgbaFFFFSrc */,
                                                       const vImage_Buffer * /* rgbFFFdest */,
                                                       const Pixel_FFFF   /* backgroundColor */,    /* background color is assumed to have a 1.0f alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error  vImageFlatten_BGRA8888ToRGB888(
                                                       const vImage_Buffer * /* bgra8888Src */,
                                                       const vImage_Buffer * /* rgb888dest */,
                                                       const Pixel_8888   /* backgroundColor */,    /* background color is assumed to have a 255 alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error  vImageFlatten_BGRAFFFFToRGBFFF(
                                                       const vImage_Buffer * /* bgraFFFFSrc */,
                                                       const vImage_Buffer * /* rgbFFFdest */,
                                                       const Pixel_FFFF   /* backgroundColor */,    /* background color is assumed to have a 1.0f alpha channel */
                                                       bool      /* isImagePremultiplied */,
                                                       vImage_Flags     /* flags */
) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#define vImageFlatten_BGRA8888ToBGR888( _bgra8888Src, _bgr888Dest, _backgroundColor, _isImagePremultiplied, _flags )        \
vImageFlatten_RGBA8888ToRGB888( (_bgra8888Src), (_bgr888Dest), (_backgroundColor), (_isImagePremultiplied), (_flags) )
#define vImageFlatten_RGBA8888ToBGR888( _rgba8888Src, _bgr888Dest, _backgroundColor, _isImagePremultiplied, _flags )        \
vImageFlatten_BGRA8888ToRGB888( (_rgba8888Src), (_bgr888Dest), (_backgroundColor), (_isImagePremultiplied), (_flags) )
#define vImageFlatten_BGRAFFFFToBGRFFF( _bgraFFFFSrc, _bgrFFFDest, _backgroundColor, _isImagePremultiplied, _flags )        \
vImageFlatten_RGBAFFFFToRGBFFF( (_bgraFFFFSrc), (_bgrFFFDest), (_backgroundColor), (_isImagePremultiplied), (_flags) )
#define vImageFlatten_RGBAFFFFToBGRFFF( _rgbaFFFFSrc, _bgrFFFDest, _backgroundColor, _isImagePremultiplied, _flags )        \
vImageFlatten_BGRAFFFFToRGBFFF( (_rgbaFFFFSrc), (_bgrFFFDest), (_backgroundColor), (_isImagePremultiplied), (_flags) )


/*
 Convert 3 planar buffers to a 3 channel interleave buffer.
 
 Does not work in place
 This may be used to produce other channel orderings by changing the order of the planar buffers passed into the function.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar8toRGB888( const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_PlanarFtoRGBFFF( const vImage_Buffer *planarRed, const vImage_Buffer *planarGreen, const vImage_Buffer *planarBlue, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*
 Convert a 3 channel interleave buffer to 3 planar buffers.
 
 Does not work in place
 This may be used to consume other channel orderings by changing the order of the planar buffers passed into the function.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageConvert_RGB888toPlanar8( const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error vImageConvert_RGBFFFtoPlanarF( const vImage_Buffer *rgbSrc, const vImage_Buffer *redDest, const vImage_Buffer *greenDest, const vImage_Buffer *blueDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 @function vImageSelectChannels_ARGB8888
 
 @abstract Does the same thing as vImageOverwriteChannels_ARGB8888 except that the newSrc buffer is in ARGB8888.
 
 @discussion For each pixel in src, do the following:
 @code
 // Generate intMask to be 0xff for the channels that we want copy from newSrc to origSrc.
 uint32_t    t = *(uint32_t*)newSrc;
 uint32_t    b = *(uint32_t*)origSrc;
 
 t  = (t & intMask ) | (b & ~intMask );
 
 *(uint32_t*)dest = t;
 @endcode
 
 If the appropriate copyMask bit is set, then the color channel from newSrc is used. Otherwise the color channel from origSrc is used.
 We note that functions of this kind only exist for interleaved buffers. If you had been using planar data, this would just be a pointer swap.
 This will work for other channel orderings, such as RGBA8888.  You need to adjust the ordering of the bits in copyMask to compensate.
 This can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param newSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite with.
 
 @param origSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageSelectChannels_ARGB8888( const vImage_Buffer *newSrc,       /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *origSrc,      /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                        uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                        vImage_Flags    flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.5), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageSelectChannels_ARGBFFFF
 
 @abstract Does the same thing as vImageOverwriteChannels_ARGBFFFF except that the newSrc buffer is in ARGBFFFF
 
 @discussion For each pixel in src, do the following:
 @code
 // Generate intMask to be 0xffffffff for the channels that we want copy from newSrc to origSrc.
 float    t = *(float*)newSrc;
 float    b = *(float*)origSrc;
 
 t  = (t & intMask ) | (b & ~intMask );
 
 *(float*)dest = t;
 @endcode
 
 If the appropriate copyMask bit is set, then the color channel from newSrc is used. Otherwise the color channel from origSrc is used.
 We note that functions of this kind only exist for interleaved buffers. If you had been using planar data, this would just be a pointer swap.
 This will work for other channel orderings, such as RGBAFFFF.  You need to adjust the ordering of the bits in copyMask to compensate.
 This can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param newSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite with.
 
 @param origSrc
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel that we will overwrite into.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageSelectChannels_ARGBFFFF( const vImage_Buffer *newSrc,       /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *origSrc,      /* A ARGB interleaved buffer */
                                                        const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                        uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                        vImage_Flags    flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.5), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithPixel_ARGB8888
 
 @abstract Like vImageOverwriteChannelsWithScalar_ARGB8888, with a ARGB input pixel, instead of a planar one.
 
 @discussion For each pixel in src, do the following:
 @code
 // Set up a uint32_t mask - 0xFF where the pixels should be conserved
 destRow[x] = (srcRow[x] & mask) | the_pixel;
 @endcode
 
 This will work for other channel orderings, such as RGBA8888. You will need to adjust the ordering of bits in copyMask to compensate.
 This can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param the_pixel
 A pointer to Pixel_8888 that references ARGB pixel.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageInvalidParameter                When copyMask > 0x0F
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithPixel_ARGB8888( const Pixel_8888     the_pixel,
                                                                 const vImage_Buffer *src,      /* A ARGB interleaved buffer */
                                                                 const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                                 uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                 vImage_Flags    flags ) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.5), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 @function vImageOverwriteChannelsWithPixel_ARGB16U
 
 @abstract Like vImageOverwriteChannelsWithScalar_ARGB16U, with a ARGB input pixel, instead of a planar one.
 
 @discussion For each pixel in src, do the following:
 @code
 // Set up a uint32_t mask - 0xFFFF where the pixels should be conserved
 destRow[x] = (srcRow[x] & mask) | the_pixel;
 @endcode
 
 This will work for other channel orderings, such as RGBA16U. You will need to adjust the ordering of bits in copyMask to compensate.
 This can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param the_pixel
 A pointer to Pixel_ARGB_16U that references ARGB pixel.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageInvalidParameter                When copyMask > 0x0F
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithPixel_ARGB16U( const Pixel_ARGB_16U     the_pixel,
                                                                const vImage_Buffer *src,      /* A ARGB interleaved buffer */
                                                                const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                                uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                vImage_Flags    flags ) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 @function vImageOverwriteChannelsWithPixel_ARGBFFFF
 
 @abstract Like vImageOverwriteChannelsWithScalar_ARGBFFFF, with a ARGB input pixel, instead of a planar one.
 
 @discussion For each pixel in src, do the following:
 @code
 // Set up a uint32_t mask - 0xFFFFFFFF where the pixels should be conserved
 destRow[x] = (srcRow[x] & mask) | the_pixel;
 @endcode
 
 This will work for other channel orderings, such as RGBAFFFF. You will need to adjust the ordering of bits in copyMask to compensate.
 This can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 @param the_pixel
 A pointer to Pixel_FFFF that references ARGB pixel.
 
 @param src
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB source pixel.
 
 @param dest
 A pointer to a valid and initialized vImage_Buffer struct, that points to a buffer containing ARGB destination pixels.
 
 @param copyMask
 A mask to copy plane : 0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue
 
 @param flags
 \p kvImageNoFlags          Default operation
 \p kvImageDoNotTile        Disable internal multithreading.  You should use this if you are doing your own threading / tiling.
 
 @return kvImageNoError                         Success
 @return kvImageInvalidParameter                When copyMask > 0x0F
 @return kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error vImageOverwriteChannelsWithPixel_ARGBFFFF( const Pixel_FFFF     the_pixel,
                                                                 const vImage_Buffer *src,      /* A ARGB interleaved buffer */
                                                                 const vImage_Buffer *dest,      /* A ARGB interleaved buffer */
                                                                 uint8_t copyMask,               /* Copy plane into  0x8  -- alpha, 0x4 -- red, 0x2 --- green, 0x1 --- blue */
                                                                 vImage_Flags    flags ) VIMAGE_NON_NULL(2,3) API_AVAILABLE(macos(10.5), ios(5.0), watchos(1.0), tvos(5.0));


/*
 The following functions interleave the planar buffers pointed to by red, green and blue, with the scalar value in alpha, to
 create a ARGB, BGRA or RGBA  four channel interleaved buffer.  These functions do not work in place.  Per all vImage functions
 channel order is defined as memory order.
 
 Flags:
 kvImageGetTempBufferSize    returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Other flags cause the function to return kvImageUnknownFlagsBit.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_Planar8ToXRGB8888( Pixel_8 alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(2,3,4,5)    API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_Planar8ToBGRX8888( const vImage_Buffer *blue, const vImage_Buffer *green, const vImage_Buffer *red, Pixel_8 alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5)    API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageConvert_Planar8ToRGBX8888( _red, _green, _blue, _alpha, _dest, _flags )   vImageConvert_Planar8ToBGRX8888((_red), (_green), (_blue), (_alpha), (_dest), (_flags))
VIMAGE_PF vImage_Error    vImageConvert_PlanarFToXRGBFFFF( Pixel_F alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(2,3,4,5)    API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_PlanarFToBGRXFFFF( const vImage_Buffer *blue, const vImage_Buffer *green, const vImage_Buffer *red, Pixel_F alpha, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5)    API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageConvert_PlanarFToRGBXFFFF( _red, _green, _blue, _alpha, _dest, _flags )   vImageConvert_PlanarFToBGRXFFFF( (_red), (_green), (_blue), (_alpha), (_dest), (_flags))

/*
 The following functions de-interleave a XRGB, BGRX, or RGBX four-channel
 buffer to create three planar buffers red, green, and blue, discarding
 the fourth channel.  These functions do not work in place.  As with all
 vImage functions, channel order is memory order.
 
 Flags:
 kvImageGetTempBufferSize    Returns 0, does nothing.
 kvImageDoNotTile            Disables internal multithreading.
 
 Other flags cause the function to return kvImageUnknownFlagsBit.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageBufferSizeMismatch              When the size of destination dimensions are different.
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */

VIMAGE_PF vImage_Error vImageConvert_XRGB8888ToPlanar8(const vImage_Buffer *src,
                                                       const vImage_Buffer *red,
                                                       const vImage_Buffer *green,
                                                       const vImage_Buffer *blue,
                                                       vImage_Flags flags)
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImageConvert_BGRX8888ToPlanar8(const vImage_Buffer *src,
                                                       const vImage_Buffer *blue,
                                                       const vImage_Buffer *green,
                                                       const vImage_Buffer *red,
                                                       vImage_Flags flags)
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

#define vImageConvert_RGBX8888ToPlanar8(_src, _red, _green, _blue, _flags) \
vImageConvert_BGRX8888ToPlanar8((_src), (_red), (_green), (_blue), (_flags))

VIMAGE_PF vImage_Error vImageConvert_XRGBFFFFToPlanarF(const vImage_Buffer *src,
                                                       const vImage_Buffer *red,
                                                       const vImage_Buffer *green,
                                                       const vImage_Buffer *blue,
                                                       vImage_Flags flags)
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImageConvert_BGRXFFFFToPlanarF(const vImage_Buffer *src,
                                                       const vImage_Buffer *blue,
                                                       const vImage_Buffer *green,
                                                       const vImage_Buffer *red,
                                                       vImage_Flags flags)
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

#define vImageConvert_RGBXFFFFToPlanarF(_src, _red, _green, _blue, _flags) \
vImageConvert_BGRXFFFFToPlanarF((_src), (_red), (_green), (_blue), (_flags))

/*
 vImageConvert_Planar8ToARGBFFFF
 
 Conversion routines to convert planar 8-bit buffers to packed (interleaved) 4 channel floating point format.
 
 The meaning of maxFloat and minFloat here is substatially the same as for vImageConvert_Planar8toPlanarF. The difference is that since this is a four channel
 image, we use four channel min and max values so that different mins and maxes can be used for each channel. The channels in minFloat and maxFloat are in the
 same order as the output pixels. MaxFloat is allowed to be less than MinFloat, in which case the image will come out looking like a photographic negative.
 (That is, you get a free contrast + brightness adjustment as part of this conversion.)
 
 The vImageConvert_Planar8ToARGBFFFF function is the workhorse of this family. The other three are provided as a convenience. You can actually substitute
 any of the channels with a constant by replacing the vImage_Buffer passed in for that channel with one from another color channel, and set
 maxFloat.color = minFloat.color = desired color for that channel. (In the particular case where you want all four channels to be constant, please call
 vImageBufferFill_ARGBFFFF instead.) Likewise, though these API names appear to correspond to particular color channel orders, vImage has no way of knowing what
 is actually red or green or cyan, so you can use them for other color spaces and other packing orders. Thus, vImageConvert_Planar8ToARGBFFFF should be
 properly considered a somewhat color space agnostic <=4 planar 8-bit channel to 4 channel packed float conversion function.
 
 Performance data on Intel Core2 and G5 suggest that like most simple conversion functions, these functions only perform well if the data is already in the cache.
 Optimum tile sizes are between ~200 pixels and somewhere in the 50000 to 200000 pixel range, depending on cache size. If the image is larger than that, this
 function is limited by the throughput of the machine's front side bus and will run anywhere from 3 to 10 times slower. For well sized images / times, we observe
 that the vector code is 3-12x faster than scalar code. For images that don't fit in cache or that are not in cache, the vector code is perhaps only 10% faster
 than scalar.
 
 Flags:
 kvImageGetTempBufferSize    returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Other flags cause the function to return kvImageUnknownFlagsBit.
 
 These routines do not work in place.
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_Planar8ToARGBFFFF( const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_Planar8ToXRGBFFFF( Pixel_F alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(2,3,4,5) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_Planar8ToBGRXFFFF(const vImage_Buffer *blue, const vImage_Buffer *green, const vImage_Buffer *red, Pixel_F alpha, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageConvert_Planar8ToRGBXFFFF(_red, _green, _blue, _alpha, _dest, _maxFloat, _minFloat, _flags)  vImageConvert_Planar8ToBGRXFFFF((_red), (_green), (_blue), (_alpha), (_dest), (_maxFloat), (_minFloat), (_flags))

/*
 vImageConvert_PlanarFToARGB8888
 
 Conversion routines to convert planar float buffers to packed (interleaved) 4 channel 8-bit unsigned format.
 
 The meaning of maxFloat and minFloat here is similar to vImageConvert_PlanarFtoPlanar8. That is, the max and min define the pixel values for
 100% and 0% light saturation respectively. (Pixel values outside that range are clamped into that range before we do arithmetic.) Since this function works on
 multichannel data, we pass in multichannel maxes and mins. The channel order in maxFloat and minFloat is the same as for the desired result pixel. The calculation
 is as follows for each channel:
 
 uint8_t result = ROUND_TO_INTEGER( SATURATED_CLAMP_0_to_255( 255.0f * ( srcPixel[channel] - minFloat[channel] ) / (maxFloat[channel] - minFloat[channel]) ));
 
 The ROUND_TO_INTEGER function here behaves identically to the C99 lrintf() function. It rounds to integer using the prevailing floating point rounding mode.
 By default, that is round to nearest integer (ties go to the even result).  You can change the rounding mode using interfaces in fenv.h, specfically fesetround()
 and fegetround(), if you prefer that the calculation round up, or down.
 
 It is allowed that maxFloat[channel] < minFloat[channel], in which case you'll get an image that appears a bit like a photgraphic negative. The astute reader will
 note that these routines provide a free per-channel brightness and contrast adjustment.
 
 In the special case that maxFloat[channel] == minFloat[channel], then the calculation is instead as follows:
 
 uint8_t result = ROUND_TO_INTEGER( SATURATED_CLIP_0_to_255( 255.0f * maxFloat[channel] ) );
 
 This serves two purposes. It guarantees we can't get a division by zero. It also provides a way to set certain channels to a constant, rather than use
 a source buffer. You'll still need to pass in a source buffer for that channel, the data also needs to be valid, but it wont actually affect the result. It should
 be sufficient to substitute one of the other source buffers in for the color channel that you wish to be constant. If you intend to produce a buffer full of a
 solid color, please use vImageBufferFill_ARGB8888 instead. Two special purpose routines are provided for constant alpha in either the first or last position
 in the pixel, vImageConvert_PlanarFToXRGB8888, vImageConvert_PlanarFToBGRX8888. On some platforms, these may be a little bit faster.
 
 Numerics note: We don't actually do the full caculation described above. The constant terms are precalculated. We are really doing the following:
 
 float a = 255.0f / (maxFloat[channel] - minFloat[channel]);
 float b = minFloat[channel] * -a;
 
 uint8_t result = ROUND_TO_INTEGER( SATURATED_CLAMP( srcPixel[channel], minFloat[channel], maxFloat[channel]) * a  + b ));
 
 This may cause slightly different rounding when scaling the pixel to the 0..255 range. In very rare cases, that may produce a result pixel that differs from the
 true calculation by 1. NaNs produce 0.
 
 Flags:
 kvImageGetTempBufferSize    returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Other flags cause the function to return kvImageUnknownFlagsBit.
 
 These can work in place provided that for each buffer "buf" that overlaps with dest:
 buf->data must be equal to dest->data and buf->rowBytes >= dest->rowBytes
 If buf has a different rowBytes from dest, kvImageDoNotTile must be also passed in the flags
 
 Return Value:
 -------------
 kvImageNoError                         Success
 kvImageRoiLargerThanInputBuffer        The height and width of the destination must be less than or equal to the height and width of the src buffer, respectively.
 */
VIMAGE_PF vImage_Error    vImageConvert_PlanarFToARGB8888( const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4,5,6,7) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_PlanarFToXRGB8888( Pixel_8 alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(2,3,4,5,6,7) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageConvert_PlanarFToBGRX8888( const vImage_Buffer *blue, const vImage_Buffer *green, const vImage_Buffer *red, Pixel_8 alpha, const vImage_Buffer *dest, const Pixel_FFFF maxFloat, const Pixel_FFFF minFloat, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5,6,7) API_AVAILABLE(macos(10.6), ios(5.0), watchos(1.0), tvos(5.0));
#define         vImageConvert_PlanarFToRGBX8888( _red, _green, _blue, _alpha, _dest, _maxFloat, _minFloat, _flags )     vImageConvert_PlanarFToBGRX8888((_red), (_green), (_blue), (_alpha), (_dest), (_maxFloat), (_minFloat), (_flags))

/*
 vImageConvert_RGB16UtoARGB16U
 
 Convert RGB16U -> ARGB16U
 
 
 vImageConvert_RGB16UtoRGBA16U
 
 Convert RGB16U -> RGBA16U
 
 
 vImageConvert_RGB16UtoBGRA16U
 
 Convert RGB16U -> BGRA16U
 
 
 Each channel is 16 bit unsigned.
 3-channel interleaved pixel buffers that contains RGB and 1 planar pixel buffer for A or
 'alpha' value are combined to create 4-channel interleaved pixel buffer.
 'premultiply' determines if this function will premultiply alpha value(s) to RGB values.
 
 if (aSrc != NULL)
 {
 if (premultiply)
 {
 r = (aSrc[i] * rgb[i*3+0] + 32767) / 65535
 g = (aSrc[i] * rgb[i*3+1] + 32767) / 65535
 b = (aSrc[i] * rgb[i*3+2] + 32767) / 65535
 
 argbDest[i*4+0] = aSrc[i];
 argbDest[i*4+1] = r;
 argbDest[i*4+2] = g;
 argbDest[i*4+3] = b;
 }
 else
 {
 argbDest[i*4+0] = aSrc[i];
 argbDest[i*4+1] = rgb[i*3+0];
 argbDest[i*4+2] = rgb[i*3+1];
 argbDest[i*4+3] = rgb[i*3+2];
 }
 }
 else
 {
 if (premultiply)
 {
 r = (alpha * rgb[i*3+0] + 32767) / 65535
 g = (alpha * rgb[i*3+1] + 32767) / 65535
 b = (alpha * rgb[i*3+2] + 32767) / 65535
 
 argbDest[i*4+0] = alpha;
 argbDest[i*4+1] = r;
 argbDest[i*4+2] = g;
 argbDest[i*4+3] = b;
 }
 else
 {
 argbDest[i*4+0] = alpha;
 argbDest[i*4+1] = rgb[i*3+0];
 argbDest[i*4+2] = rgb[i*3+1];
 argbDest[i*4+3] = rgb[i*3+2];
 }
 }
 
 Operands:
 ---------
 rgbSrc : A pointer to vImage_Buffer that references RGB interleaved source pixels
 
 aSrc : A pointer to vImage_Buffer that references A planar source pixels
 
 alpha : A single alpha value
 
 argbDest / rgbaDest / bgraDest : A pointer to vImage_Buffer that references ARGB / RGBA / BGRA interleaved destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_RGB16UtoARGB16U( const vImage_Buffer *rgbSrc, const vImage_Buffer *aSrc, Pixel_16U alpha, const vImage_Buffer *argbDest, bool premultiply, vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGB16UtoRGBA16U( const vImage_Buffer *rgbSrc, const vImage_Buffer *aSrc, Pixel_16U alpha, const vImage_Buffer *rgbaDest, bool premultiply, vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGB16UtoBGRA16U( const vImage_Buffer *rgbSrc, const vImage_Buffer *aSrc, Pixel_16U alpha, const vImage_Buffer *bgraDest, bool premultiply, vImage_Flags flags ) VIMAGE_NON_NULL(1,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_ARGB16UtoRGB16U
 
 Convert ARGB16U -> RGB16U
 
 
 vImageConvert_RGBA16UtoRGB16U
 
 Convert RGBA16U -> RGB16U
 
 
 vImageConvert_BGRA16UtoRGB16U
 
 Convert BGRA16U -> RGB16U
 
 
 Each channel is 16-bit unsigned
 4-channel interleaved pixel buffer becomes 3-channel interleaved pixel buffer by skipping to copy
 the first channel in 4-channel interleaved pixel buffer.
 
 rgbDest[i*3+0] = argbSrc[i*4+1];
 rgbDest[i*3+1] = argbSrc[i*4+2];
 rgbDest[i*3+2] = argbSrc[i*4+3];
 
 Operands:
 ---------
 argbSrc / rgbaSrc / bgraSrc : A pointer to vImage_Buffer that references ARGB / RGBA / BGRA interleaved source pixels
 
 rgbDest : A pointer to vImage_Buffer that references RGB interleaved destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 vImageConvert_RGBA16UtoRGB16U() can work in place if (argbSrc.data == rgbDest.data) && (argbSrc.rowBytes == rgbDest.rowBytes)
 Rest of cases in these 3 functions will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB16UtoRGB16U( const vImage_Buffer *argbSrc, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGBA16UtoRGB16U( const vImage_Buffer *rgbaSrc, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_BGRA16UtoRGB16U( const vImage_Buffer *bgraSrc, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Planar16UtoARGB16U
 
 Convert Planar16U -> ARGB16U
 
 
 Each channel is 16-bit unsigned
 4 planar pixel buffers are combined to create 4-channel interleaved pixel buffers.
 
 argbDest[i*4+0] = aSrc[i];
 argbDest[i*4+1] = rSrc[i];
 argbDest[i*4+2] = gSrc[i];
 argbDest[i*4+3] = bSrc[i];
 
 This function can be used to create any channel order from 4 planar pixel buffers to interleaved
 pixel buffers.
 
 Operands:
 ---------
 aSrc : A pointer to vImage_Buffer that references A planar source pixels
 
 rSrc : A pointer to vImage_Buffer that references R planar source pixels
 
 gSrc : A pointer to vImage_Buffer that references G planar source pixels
 
 bSrc : A pointer to vImage_Buffer that references B planar source pixels
 
 argbDest : A pointer to vImage_Buffer that references ARGB interleaved destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar16UtoARGB16U(const vImage_Buffer *aSrc, const vImage_Buffer *rSrc, const vImage_Buffer *gSrc, const vImage_Buffer *bSrc, const vImage_Buffer *argbDest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_ARGB16UtoPlanar16U
 
 Convert ARGB16U -> Planar16U
 
 
 Each channel is 16-bit unsigned
 Each channel of 4-channel interleaved pixel buffer are copied into its planar pixel buffer.
 
 aDest[i] = argbSrc[i*4+0];
 rDest[i] = argbSrc[i*4+1];
 gDest[i] = argbSrc[i*4+2];
 bDest[i] = argbSrc[i*4+3];
 
 This function can be used to create any channel order from interleaved pixel buffers to 4 planar
 pixel buffers.
 
 Operands:
 ---------
 argbSrc : A pointer to vImage_Buffer that references ARGB interleaved source pixels
 
 aDest : A pointer to vImage_Buffer that references A planar destination pixels
 
 rDest : A pointer to vImage_Buffer that references R planar destination pixels
 
 gDest : A pointer to vImage_Buffer that references G planar destination pixels
 
 bDest : A pointer to vImage_Buffer that references B planar destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 vImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB16UtoPlanar16U(const vImage_Buffer *argbSrc, const vImage_Buffer *aDest, const vImage_Buffer *rDest, const vImage_Buffer *gDest, const vImage_Buffer *bDest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageConvert_Planar16UtoRGB16U
 
 Planar16U -> RGB16U
 
 
 Each channel is 16-bit unsigned
 3 planar pixel buffers are combined to create 3-channel interleaved pixel buffer.
 
 rgbDest[i*3+0] = rSrc[i];
 rgbDest[i*3+1] = gSrc[i];
 rgbDest[i*3+2] = bSrc[i];
 
 This function can be used to create any channel order from 3 planar pixel buffers to interleaved
 pixel buffers.
 
 Operands:
 ---------
 rSrc : A pointer to vImage_Buffer that references R planar source pixels
 
 gSrc : A pointer to vImage_Buffer that references G planar source pixels
 
 bSrc : A pointer to vImage_Buffer that references B planar source pixels
 
 rgbDest : A pointer to vImage_Buffer that references RGB interleaved destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar16UtoRGB16U( const vImage_Buffer *rSrc, const vImage_Buffer *gSrc, const vImage_Buffer *bSrc, const vImage_Buffer *rgbDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_RGB16UtoPlanar16U
 
 Planar16U <- RGB16U
 
 Each channel is 16-bit unsigned
 Each channel of 3-channel interleaved pixel buffer are copied into its planar pixel buffer.
 
 rDest[i] = rgbSrc[i*3+0];
 gDest[i] = rgbSrc[i*3+1];
 bDest[i] = rgbSrc[i*3+2];
 
 This function can be used to create any channel order from interleaved pixel buffers to 3 planar
 pixel buffers.
 
 Operands:
 ---------
 rgbSrc : A pointer to vImage_Buffer that references RGB interleaved source pixels
 
 rDest : A pointer to vImage_Buffer that references R planar destination pixels
 
 gDest : A pointer to vImage_Buffer that references G planar destination pixels
 
 bDest : A pointer to vImage_Buffer that references B planar destination pixels
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function will not work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_RGB16UtoPlanar16U( const vImage_Buffer *rgbSrc, const vImage_Buffer *rDest, const vImage_Buffer *gDest, const vImage_Buffer *bDest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


VIMAGE_PF vImage_Error vImageConvert_Planar16UtoPlanar8_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, int dither, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_RGB16UtoRGB888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, int dither, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB16UtoARGB8888_dithered(const vImage_Buffer *src, const vImage_Buffer *dest, int dither, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*
 vImageConvert_ARGB16UToARGB8888
 
 This function does 3 things.
 1. Converts 16-bit unsigned ARGB interleaved pixels into 8-bit unsigned ARGB interleaved pixels.
 2. Changes the order of channels according to permuteMap
 3. Selects between the result pixel and the backgroundColor according to the bits in copyMask.
 
 permuteMap[i] = 0, 1, 2, or 3 to specify how we permute each channel.
 
 permuteMap[0] tells which channel in ARGB16U will be used as the 1st channel (A) in the result ARGB8888.
 permuteMap[1] tells which channel in ARGB16U will be used as the 2nd channel (R) in the result ARGB8888.
 permuteMap[2] tells which channel in ARGB16U will be used as the 3rd channel (G) in the result ARGB8888.
 permuteMap[3] tells which channel in ARGB16U will be used as the 4th channel (B) in the result ARGB8888.
 
 copyMask = _ _ _ _  ; 4 digit binary
 1000 tells if we want to use 1st channel in backgroundColor as the value of channel A in the result.
 0100 tells if we want to use 2nd channel in backgroundColor as the value of channel R in the result.
 0010 tells if we want to use 3rd channel in backgroundColor as the value of channel G in the result.
 0001 tells if we want to use 4th channel in backgroundColor as the value of channel B in the result.
 
 The per-pixel operation is:
 
 Pixel_16U *srcPixel;
 Pixel_8888 *destPixel;
 uint16_t result16;
 uint8_t result[4];
 uint8_t mask = 0x8;
 
 for( int i = 0; i < 4; i++ )
 {
 result16 = srcPixel[ permuteMap[i] ];
 result[i] = (result16 * 255U + 32767U) / 65535U;
 if( mask & copyMask )
 result[i] = backgroundColor[i];
 mask = mask >> 1;
 }
 srcPixel += 4;
 
 destPixel[0] = result[0];
 destPixel[1] = result[1];
 destPixel[2] = result[2];
 destPixel[3] = result[3];
 destPixel += 4;
 
 Operands:
 ---------
 src : A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 permuteMap : Values that can be used to switch the channel order as the above example.
 copyMask : A mask to choose between the result and backgroundColor.
 backgroudColor : A pointer to Pixel_8888 that references 8-bit ARGB values which can replace the result pixels with. Pixel_8888 backgroundColor = {alpha, red, green, blue};
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageInvalidParameter            Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 kvImageNullPointerArgument         Is returned when backgroundColorPtr is NULL.
 
 This function can work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB16UToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageConvert_ARGB8888ToARGB16U
 
 This function does 3 things.
 1. Converts 8-bit unsigned ARGB interleaved pixels into 16-bit unsigned ARGB interleaved pixels.
 2. Changes the order of channels according to permuteMap
 3. Selects between the result pixel and the backgroundColor according to the bits in copyMask.
 
 permuteMap[i] = 0, 1, 2, or 3 to specify how we permute each channel.
 
 permuteMap[0] tells which channel in ARGB8888 will be used as the 1st channel (A) in the result ARGB16U.
 permuteMap[1] tells which channel in ARGB8888 will be used as the 2nd channel (R) in the result ARGB16U.
 permuteMap[2] tells which channel in ARGB8888 will be used as the 3rd channel (G) in the result ARGB16U.
 permuteMap[3] tells which channel in ARGB8888 will be used as the 4th channel (B) in the result ARGB16U.
 
 copyMask = _ _ _ _  ; 4 digit binary
 1000 tells if we want to use 1st channel in backgroundColor as the value of channel A in the result.
 0100 tells if we want to use 2nd channel in backgroundColor as the value of channel R in the result.
 0010 tells if we want to use 3rd channel in backgroundColor as the value of channel G in the result.
 0001 tells if we want to use 4th channel in backgroundColor as the value of channel B in the result.
 
 The per-pixel operation is:
 
 Pixel_8888 *srcPixel;
 Pixel_16U *destPixel;
 uint8_t result8;
 uint16_t result[4];
 uint8_t mask = 0x8;
 
 for( int i = 0; i < 4; i++ )
 {
 result8 = srcPixel[ permuteMap[i] ];
 result[i] = (result8 * 65535U + 127U) / 255U;
 if( mask & copyMask )
 result[i] = backgroundColor[i];
 mask = mask >> 1;
 }
 srcPixel += 4;
 
 destPixel[0] = result[0];
 destPixel[1] = result[1];
 destPixel[2] = result[2];
 destPixel[3] = result[3];
 destPixel += 4;
 
 Operands:
 ---------
 src : A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 permuteMap : Values that can be used to switch the channel order as the above example.
 copyMask : A mask to choose between the result and backgroundColor.
 backgroundColor : A pointer to Pixel_ARGB_16U that references 16-bit ARGB values which can replace the result pixels with. Pixel_ARGB_16U backgroundColor = {alpha, red, green, blue};
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageInvalidParameter            Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 kvImageNullPointerArgument         Is returned when backgroundColorPtr is NULL.
 
 This function doesn't work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_ARGB_16U backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageConvert_RGB16UToARGB8888
 
 This function does 3 things.
 1. Converts 16-bit unsigned RGB interleaved pixels into 8-bit unsigned ARGB interleaved pixels. (255 or A from backgroundColor will be used as A.)
 2. Changes the order of channels according to permuteMap.
 3. Selects between the result pixel and the backgroundColor according to the bits in copyMask.
 
 permuteMap[x] = 0, 1, or 2 to specify how we permute each channel.
 
 255 will be used as a default alpha value.
 permuteMap[0] tells which channel in 255RGB16U will be used as the 1st channel (A) in the result ARGB8888.
 permuteMap[1] tells which channel in 255RGB16U will be used as the 2nd channel (R) in the result ARGB8888.
 permuteMap[2] tells which channel in 255RGB16U will be used as the 3rd channel (G) in the result ARGB8888.
 permuteMap[3] tells which channel in 255RGB16U will be used as the 4th channel (B) in the result ARGB8888.
 
 copyMask = _ _ _ _  ; 4 digit binary
 1000 tells if we want to use 1st channel in backgroundColor which can be A in this function.
 0100 tells if we want to use 2nd channel in backgroundColor which can be R in this function.
 0010 tells if we want to use 3rd channel in backgroundColor which can be G in this function.
 0001 tells if we want to use 4th channel in backgroundColor which can be B in this function.
 
 The per-pixel operation is:
 
 Pixel_16U  *srcPixel;
 Pixel_8888 *destPixel;
 uint16_t   r16, g16, b16;
 uint8_t    result[4], a, r, g, b;
 uint8_t    mask = 0x8;
 
 r16 = srcPixel[0];
 g16 = srcPixel[1];
 b16 = srcPixel[2];
 srcPixel += 3;
 
 //Convert
 result[0] = 255;
 result[1] = (r16 * 255U + 32767U) / 65535U;
 result[2] = (g16 * 255U + 32767U) / 65535U;
 result[3] = (b16 * 255U + 32767U) / 65535U;
 
 //Permute
 a = result[permuteMap[0]];
 r = result[permuteMap[1]];
 g = result[permuteMap[2]];
 b = result[permuteMap[3]];
 
 //Select
 if( mask & copyMask )
 a = backgroundColor[0];
 mask = mask >> 1;
 if( mask & copyMask )
 r = backgroundColor[1];
 mask = mask >> 1;
 if( mask & copyMask )
 g = backgroundColor[2];
 mask = mask >> 1;
 if( mask & copyMask )
 b = backgroundColor[3];
 
 destPixel[0] = a;
 destPixel[1] = r;
 destPixel[2] = g;
 destPixel[3] = b;
 destPixel += 4;
 
 Operands:
 ---------
 src : A pointer to vImage_Buffer that references 16-bit RGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 permuteMap : Values that can be used to switch the channel order as the above example. permuteMap[0] is for A in the dest ARGB8888, permuteMap[1] is for R in the dest ARGB8888,  permuteMap[2] is for G in the dest ARGB8888, and  permuteMap[3] is for B in the dest ARGB8888.
 copyMask : A mask to choose between the result and backgroundColor.
 backgroudColor : A pointer to Pixel_8888 that references 8-bit ARGB values which can replace the result pixels with. Pixel_8888 backgroundColor = {alpha, red, green, blue};
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageInvalidParameter            Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 kvImageNullPointerArgument         Is returned when backgroundColorPtr is NULL.
 
 This function can work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_RGB16UToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[4], uint8_t copyMask, const Pixel_8888 backgroundColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageConvert_ARGB8888ToRGB16U
 
 This function does 3 things.
 1. Converts 8-bit unsigned ARGB interleaved pixels into 16-bit unsigned RGB interleaved pixels.
 2. Changes the order of channels according to permuteMap.
 3. Selects between the result pixel and the backgroundColor according to the bits in copyMask.
 
 permuteMap[i] = 0, 1, 2, or 3 to specify how we permute each channel.
 
 permuteMap[0] tells which channel in ARGB8888 will be used as the 1st channel (R) in the result RGB16U.
 permuteMap[1] tells which channel in ARGB8888 will be used as the 2nd channel (G) in the result RGB16U.
 permuteMap[2] tells which channel in ARGB8888 will be used as the 3rd channel (B) in the result RGB16U.
 
 copyMask = _ _ _ _  ; 3 digit binary
 100 tells if we want to use 1st channel in backgroundColor as the value of channel R in the result.
 010 tells if we want to use 2nd channel in backgroundColor as the value of channel G in the result.
 001 tells if we want to use 3rd channel in backgroundColor as the value of channel B in the result.
 
 The per-pixel operation is:
 
 Pixel_8888 *srcPixel;
 Pixel_16U *destPixel;
 uint8_t result8, r8, g8, b8;
 uint16_t result[3];
 uint8_t mask = 0x4;
 
 //Permute
 r8 = srcPixel[permuteMap[0]];
 g8 = srcPixel[permuteMap[1]];
 b8 = srcPixel[permuteMap[2]];
 srcPixel += 4;
 
 //Convert
 result[0] = (r8 * 65535U + 127U) / 255U;
 result[1] = (g8 * 65535U + 127U) / 255U;
 result[2] = (b8 * 65535U + 127U) / 255U;
 
 //Select
 if( mask & copyMask )
 result[0] = backgroundColor[0];
 mask = mask >> 1;
 if( mask & copyMask )
 result[1] = backgroundColor[1];
 mask = mask >> 1;
 if( mask & copyMask )
 result[2] = backgroundColor[2];
 
 destPixel[0] = result[0];
 destPixel[1] = result[1];
 destPixel[2] = result[2];
 destPixel += 3;
 
 Operands:
 ---------
 src : A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 16-bit RGB interleaved destination pixels.
 permuteMap : Values that can be used to switch the channel order as the above example. permuteMap[0] is for R in the dest ARGB8888,  permuteMap[1] is for G in the dest ARGB8888, and  permuteMap[2] is for B in the dest ARGB8888.
 copyMask : A mask to choose between the result and backgroundColor.
 backgroudColor : A pointer to Pixel_16U that references 16-bit ARGB values which can replace the result pixels with. Pixel_16U backgroundColor[3] = {red, green, blue};
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageInvalidParameter            Is returned when the values in permuteMap[i] is not one of 0, 1, 2, or 3.
 kvImageNullPointerArgument         Is returned when backgroundColorPtr is NULL.
 
 This function doesn't work in place.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888ToRGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[3], uint8_t copyMask, const Pixel_16U backgroundColor[3], vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 vImageByteSwap_Planar16U
 
 This function does byteswap 16-bit pixel.
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 16-bit source pixels.
 dest : A pointer to vImage_Buffer that references 16-bit destination pixels.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 
 This function can work in place.
 */

VIMAGE_PF vImage_Error vImageByteSwap_Planar16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*
 vImageFlatten_ARGB8888
 
 This function does a flattening operation on ARGB8888.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = (pixelAlpha * 255 + (255 - pixelAlpha) * backgroundAlpha + 127) / 255
 if(isImagePremultiplied)
 {
 resultColor = (pixelColor * 255 + (255 - pixelAlpha) * backgroundColor + 127) / 255
 }
 else
 {
 resultColor = (pixelColor * pixelAlpha + (255 - pixelAlpha) * backgroundColor + 127) / 255
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_8888 that references 8-bit premultiplied ARGB background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when argbBackgroundColorPtr is NULL.
 
 This function can work in place.
 */

VIMAGE_PF vImage_Error  vImageFlatten_ARGB8888(const vImage_Buffer *argbSrc, const vImage_Buffer *argbDst, const Pixel_8888 argbBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageFlatten_RGBA8888
 
 This function does flattening operation on RGBA8888.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = (pixelAlpha * 255 + (255 - pixelAlpha) * backgroundAlpha + 127) / 255
 if(isImagePremultiplied)
 {
 resultColor = (pixelColor * 255 + (255 - pixelAlpha) * backgroundColor + 127) / 255
 }
 else
 {
 resultColor = (pixelColor * pixelAlpha + (255 - pixelAlpha) * backgroundColor + 127) / 255
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 8-bit RGBA interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 8-bit RGBA interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_8888 that references 8-bit premultiplied RGBA background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when rgbaBackgroundColorPtr is NULL.
 
 This function can work in place.
 BGRA format can be used as well.
 */

VIMAGE_PF vImage_Error  vImageFlatten_RGBA8888(const vImage_Buffer *rgbaSrc, const vImage_Buffer *rgbaDst, const Pixel_8888 rgbaBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageFlatten_ARGB16U
 
 This function does flattening operation on ARGB16U.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = (pixelAlpha * 65535 + (65535 - pixelAlpha) * backgroundAlpha + 32767) / 65535
 if(isImagePremultiplied)
 {
 resultColor = (pixelColor * 65535 + (65535 - pixelAlpha) * backgroundColor + 32767) / 65535
 }
 else
 {
 resultColor = (pixelColor * pixelAlpha + (65535 - pixelAlpha) * backgroundColor + 32767) / 65535
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_ARGB_16U that references 16-bit ARGB premultiplied background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when argbBackgroundColorPtr is NULL.
 
 This function can work in place.
 */

VIMAGE_PF vImage_Error  vImageFlatten_ARGB16U(const vImage_Buffer *argbSrc, const vImage_Buffer *argbDst, const Pixel_ARGB_16U argbBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageFlatten_RGBA16U
 
 This function does flattening operation on RGBA16U.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = (pixelAlpha * 65535 + (65535 - pixelAlpha) * backgroundAlpha + 32767) / 65535
 if(isImagePremultiplied)
 {
 resultColor = (pixelColor * 65535 + (65535 - pixelAlpha) * backgroundColor + 32767) / 65535
 }
 else
 {
 resultColor = (pixelColor * pixelAlpha + (65535 - pixelAlpha) * backgroundColor + 32767) / 65535
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 16-bit RGBA interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 16-bit RGBA interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_ARGB_16U that references 16-bit RGBA premultiplied background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when rgbaBackgroundColorPtr is NULL.
 
 This function can work in place.
 BGRA format can be used as well.
 */

VIMAGE_PF vImage_Error  vImageFlatten_RGBA16U(const vImage_Buffer *rgbaSrc, const vImage_Buffer *rgbaDst, const Pixel_ARGB_16U rgbaBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*
 vImageFlatten_ARGB16Q12
 
 This function does flattening operation on ARGB16Q12.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 pixelAlpha = CLAMP( 0, pixelAlpha, 4096);
 resultAlpha = (pixelAlpha * 4096 + (4096 - pixelAlpha) * backgroundAlpha + 2048) >> 12;
 if(isImagePremultiplied)
 {
 resultColor = (pixelColor * 4096 + (4096 - pixelAlpha) * backgroundColor + 2048) >> 12
 }
 else
 {
 resultColor = (pixelColor * pixelAlpha + (4096 - pixelAlpha) * backgroundColor + 2048) >> 12
 }
 
 Whether the function attempts to clamp the case when the |resultColor| >= 8.0 is undefined.
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references 16Q12 ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references 16Q12 ARGB interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_ARGB_16S that references 16-bit ARGB16Q12 premultiplied background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when argbBackgroundColorPtr is NULL.
 
 This function can work in place. Pixels are assumed to be in native-endian byte order.
 */
VIMAGE_PF vImage_Error  vImageFlatten_ARGB16Q12(const vImage_Buffer *argbSrc, const vImage_Buffer *argbDst, const Pixel_ARGB_16S argbBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error  vImageFlatten_RGBA16Q12(const vImage_Buffer *argbSrc, const vImage_Buffer *argbDst, const Pixel_ARGB_16S argbBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*
 vImageFlatten_ARGBFFFF
 
 This function does flattening operation on ARGBFFFF.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = pixelAlpha + (1 - pixelAlpha) * backgroundAlpha
 if(isImagePremultiplied)
 {
 resultColor = pixelColor + (1 - pixelAlpha) * backgroundColor
 }
 else
 {
 resultColor = pixelColor * pixelAlpha + (1 - pixelAlpha) * backgroundColor
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references float ARGB interleaved source pixels.
 dest : A pointer to vImage_Buffer that references float ARGB interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_FFFF that references float premultiplied ARGB background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when argbBackgroundColorPtr is NULL.
 
 This function can work in place.
 */

VIMAGE_PF vImage_Error  vImageFlatten_ARGBFFFF(const vImage_Buffer *argbSrc, const vImage_Buffer *argbDst, const Pixel_FFFF argbBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageFlatten_RGBAFFFF
 
 This function does flattening operation on RGBAFFFF.
 A flattening operation is an alpha composite against a solid background color.
 
 The per-pixel operation is:
 
 resultAlpha = pixelAlpha + (1 - pixelAlpha) * backgroundAlpha
 if(isImagePremultiplied)
 {
 resultColor = pixelColor + (1 - pixelAlpha) * backgroundColor
 }
 else
 {
 resultColor = pixelColor * pixelAlpha + (1 - pixelAlpha) * backgroundColor
 }
 
 Operands:
 ---------
 src  : A pointer to vImage_Buffer that references float RGBA interleaved source pixels.
 dest : A pointer to vImage_Buffer that references float RGBA interleaved destination pixels.
 backgroudColorPtr : A pointer to Pixel_FFFF that references float RGBA premultiplied background color.
 isImagePremultiplied : True means input colors are premultiplied. False means input colors are not premultiplied.
 
 Flags:
 ------
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 Return Value:
 -------------
 kvImageNoError                     Is returned when there was no error.
 kvImageRoiLargerThanInputBuffer    Is returned when the height and width of the source are less than the height and width of the destination buffer, respectively.
 kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 kvImageNullPointerArgument         Is returned when rgbaBackgroundColorPtr is NULL.
 
 This function can work in place.
 BGRA format can be used as well.
 */

VIMAGE_PF vImage_Error  vImageFlatten_RGBAFFFF(const vImage_Buffer *rgbaSrc, const vImage_Buffer *rgbaDst, const Pixel_FFFF rgbaBackgroundColorPtr, bool isImagePremultiplied, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Planar1toPlanar8
 vImageConvert_Planar2toPlanar8
 vImageConvert_Planar4toPlanar8
 
 These functions convert from 1-, 2-, or 4-bit per pixel to 8-bit per pixel
 planar formats.
 
 Function Arguments:
 -------------------
 src         Pointer to the source vImage_Buffer object.  Because the
 source pixel format is smaller than a byte, there are
 multiple pixels in each byte of the data buffer.  These
 pixels are interpreted as being in big endian order (i.e.
 the low-indexed pixel is in the high-order bits of the
 byte).
 
 Sub-byte indexing of scanlines is unsupported, because the
 data and rowBytes fields of the buffer are specified in
 whole bytes.
 
 Widths, however, are measured in pixels, so a scanline may
 end in the middle of a byte.  If this occurs, the contents
 of any unused bits of the final byte are ignored.
 
 dest        Pointer to the destination vImage_Buffer object.
 
 flags       The following flags are allowed:
 
 kvImageDoNotTile - disables internal threading.  You may
 want to specify this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize - does no work and returns zero,
 as these functions do not use temp buffers.
 
 Operation:
 ----------
 For each pixel in the destination image, the resulting value is the
 corresponding pixel value from the source image multiplied by 255, 85,
 or 17 (for Planar1, Planar2, or Planar4, respectively).
 
 
 Return values:
 --------------
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 These functions do not work in place.
 */

VIMAGE_PF vImage_Error vImageConvert_Planar1toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar2toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar4toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Indexed1toPlanar8
 vImageConvert_Indexed2toPlanar8
 vImageConvert_Indexed4toPlanar8
 
 These functions convert from 1-, 2-, or 4-bit per pixel indexed to 8-bit
 per pixel planar format.
 
 Function Arguments:
 -------------------
 src         Pointer to the source vImage_Buffer object.  Because the
 source pixel format is smaller than a byte, there are
 multiple pixels in each byte of the data buffer.  These
 pixels are interpreted as being in big endian order (i.e.
 the low-indexed pixel is in the high-order bits of the
 byte).
 
 Sub-byte indexing of scanlines is unsupported, because the
 data and rowBytes fields of the buffer are specified in
 whole bytes.
 
 Widths, however, are measured in pixels, so a scanline may
 end in the middle of a byte.  If this occurs, the contents
 of any unused bits of the final byte are ignored.
 
 dest        Pointer to the destination vImage_Buffer object.
 
 colors      Color table in which to lookup pixel values for destination
 image.
 
 flags       The following flags are allowed:
 
 kvImageDoNotTile - disables internal threading.  You may
 want to specify this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize - does no work and returns zero,
 as these functions do not use temp buffers.
 
 Operation:
 ----------
 For each pixel in the destination image, the value is looked up in the
 color table using the corresponding pixel value from the source image
 as an index.
 
 Return values:
 --------------
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 
 These functions do not work in place.
 */


VIMAGE_PF vImage_Error vImageConvert_Indexed1toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 colors[2], const vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Indexed2toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 colors[4], const vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Indexed4toPlanar8(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_8 colors[16], const vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Planar8toPlanar1
 vImageConvert_Planar8toPlanar2
 vImageConvert_Planar8toPlanar4
 
 These functions convert from 8-bit per pixel to 1-, 2-, or 4-bit per pixel
 planar formats.
 
 Function Arguments:
 -------------------
 src         Pointer to the source vImage_Buffer object
 
 dest        Pointer to the destination vImage_Buffer object.  Because
 the destination pixel format is smaller than a byte, there
 are multiple pixels in each byte of the data buffer.  These
 pixels are interpreted as being in big endian order (i.e.
 the low-indexed pixel is in the high-order bits of the
 byte).
 
 Sub-byte indexing of scanlines is unsupported, because the
 data and rowBytes fields of the buffer are specified in
 whole bytes.
 
 Widths, however, are measured in pixels, so a scanline may
 end in the middle of a byte.  If this occurs, the contents
 of any unused bits of the final byte are unspecified.
 
 tempBuffer  Pointer to temporary buffer for the routine to use for
 scratch space.  If non-NULL, the buffer must be at least
 as large as the value returned by calling this function
 with the kvImageGetTempBufferSize flag.  If NULL, this
 function will still work, but may allocate and free a
 scratch buffer internally.
 
 dither      Type of dithering to apply to the image, if any.  Options:
 
 kvImageConvert_DitherNone - apply no dithering; input values
 are rounded to the nearest value representable in the
 destination format.
 kvImageConvert_DitherOrdered - pre-computed blue noise is
 added to the image before rounding to the values in
 the destination format.  The offset into this blue
 noise is randomized per-call to avoid visible artifacts
 if you do your own tiling or call the function on
 sequential frames of video.
 kvImageConvert_DitherOrderedReproducible - pre-computed
 blue noise is added to the image before rounding to the
 values in the destination format.  The offset into the
 blue noise is the same for every call to allow users
 to get reproducible results.
 kvImageConvert_DitherFloydSteinberg - Floyd-Steinberg
 dithering is applied to the image.
 kvImageConvert_DitherAtkinson - Atkinson dithering is
 applied to the image, for the old timers.
 
 The ordered dither methods may be further influenced by shaping the
 distribution of the noise using the gaussian and uniform options below.
 These options are OR-ed with kvImageConvert_DitherOrdered / kvImageCon-
 vert_DitherOrderedReproducible:
 
 kvImageConvert_OrderedGaussianBlue - when using an ordered dither
 pattern, distribute the noise according to a gaussian
 distribution. This generally gives more pleasing images --
 less noisy and perhaps a little more saturated -- but color
 fidelity can suffer. Its effect is between kvImageConvert_DitherNone
 and kvImageConvert_DitherOrdered | kvImageConvert_DitherUniform.
 This is the default for kvImageConvert_DitherOrdered and
 kvImageConvert_DitherOrderedReproducible.
 
 kvImageConvert_OrderedUniformBlue - when using an ordered dither
 pattern, distribute the noise uniformly. This generally gives
 best color fidelity, but the resulting image is noisier and more
 obviously dithered. This is usually the best choice when low
 bitdepth content is drawn next to high bitdepth content and in other
 circumstances where subtle changes to color arising from the conversion
 could be easily noticed. It may be a poor choice when the image
 is likely to be enlarged -- this would cause the noise to become
 more evident-- and for very flat / synthetic content with little
 inherent noise. The enlargement problem may be avoided by enlarging
 first at high bitdepth, then convert to lower bitdepth.
 
 To clarify: "Blue" noise is not blue, nor does it operate solely on the blue
 color channel. Blue noise is monochrome noise that is added to all color
 channels equally. The name arises from blue light, which has a higher frequency
 than other colors of visible light. Thus, blue noise is noise which is
 weighted heavily towards high frequencies. Low frequency noise tends to have
 visible shapes in it that would become apparent in an image if it was added in,
 so it is excluded from the dither pattern.
 
 flags       The following flags are allowed:
 
 kvImageDoNotTile - disables internal threading.  You may
 want to specify this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize - does no work, but returns the
 required size of tempBuffer for this routine.
 
 Operation:
 ----------
 For a brief description of each dither method, see dither above.
 
 Notes:
 The results for Floyd-Steinberg and Atkinson dithering methods depend on the results
 of pixels around each result pixel. Consequently, these methods do not tile well. (Tiling
 can cause visible artifacts at tile boundaries because residuals are not propagated to the
 next tile in a manner consistent with how it is propagated within a tile.) You should pass
 the entire image in a single call for good results with Atkinson and Floyd-Steinberg.
 
 The ordered dithering method is suitable for tiled / multithreaded / scanline-at-a-time
 execution. In addition to being spatially random, the ordered dithering pattern is also
 temporally random. This is necessary to prevent stripes when it is invoked scanline-at-a-time.
 Temporal randomness also prevents visible walking of the dither pattern across the image
 when used for video. Because it is temporally random, results from this filter are not
 reproducible.
 
 To get reproducible results with ordered dithering, use the OrderedReproducible
 dithering attribute.
 
 The dither none option will produce reproducible results in all calling contexts, but
 is prone to obvious banding in images, especially in regions of an image with a smooth
 gradient.
 
 Return values:
 --------------
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 kvImageInvalidParameter            An unrecognized or unsupported value was
 specified for dither.
 
 These functions do not work in place.
 */

/* See description above for the meaning of these contants */
enum {
    kvImageConvert_DitherNone                   = 0,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_DitherOrdered                = 1,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_DitherOrderedReproducible    = 2,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_DitherFloydSteinberg         = 3,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_DitherAtkinson               = 4,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    
    /* Gaussian vs. uniform distribution for ordered dithering patterns. By default ordered dithering uses gaussian blue noise. */
    kvImageConvert_OrderedGaussianBlue          = 0,         /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_OrderedUniformBlue           = (1<<28),   /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
    kvImageConvert_OrderedNoiseShapeMask        = (0xfU<<28) /* __OSX_AVAILABLE_STARTING(__MAC_10_9, __IPHONE_7_0) */
};

VIMAGE_PF vImage_Error vImageConvert_Planar8toPlanar1(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar8toPlanar2(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar8toPlanar4(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_Planar8toIndexed1
 vImageConvert_Planar8toIndexed2
 vImageConvert_Planar8toIndexed4
 
 These functions convert from 8-bit per pixel planar to 1-, 2-, or 4-bit
 per pixel indexed formats.
 
 Function Arguments:
 -------------------
 src         Pointer to the source vImage_Buffer object
 
 dest        Pointer to the destination vImage_Buffer object.  Because
 the destination pixel format is smaller than a byte, there
 are multiple pixels in each byte of the data buffer.  These
 pixels are interpreted as being in big endian order (i.e.
 the low-indexed pixel is in the high-order bits of the
 byte).
 
 Sub-byte indexing of scanlines is unsupported, because the
 data and rowBytes fields of the buffer are specified in
 whole bytes.
 
 Widths, however, are measured in pixels, so a scanline may
 end in the middle of a byte.  If this occurs, the contents
 of any unused bits of the final byte are unspecified.
 
 tempBuffer  Pointer to temporary buffer for the routine to use for
 scratch space.  If non-NULL, the buffer must be at least
 as large as the value returned by calling this function
 with the kvImageGetTempBufferSize flag.  If NULL, this
 function will still work, but may allocate and free a
 scratch buffer internally.
 
 colors      Color table to use for the destination.
 
 If you want us to compute a color table for you, initialize
 the table to all zeros:
 
 Pixel_8 colors[4] = { 0 };
 vImageConvert_Planar8toIndexed2(src, dest, colors, dither, flags);
 
 We will then compute an appropriate color table for the
 image before performing the conversion.
 
 dither      Type of dithering to apply to the image, if any.  See the
 discussion for vImageConvert_Planar8toPlanar1 for more
 details.
 
 flags       The following flags are allowed:
 
 kvImageDoNotTile - disables internal threading.  You may
 want to specify this if you have your own threading
 scheme and need to avoid interference.
 
 kvImageGetTempBufferSize - does no work, but returns the
 required size of tempBuffer for this routine.
 
 Operation:
 ----------
 See dither above.
 
 Return values:
 --------------
 kvImageNoError                     Success
 kvImageUnknownFlagsBit             No work was done because an unknown bit was
 set in the flags parameter.
 kvImageRoiLargerThanInputBuffer    No work was done because the source
 image isn't large enough to cover the
 destination image.
 kvImageInvalidParameter            A non-zero color table was not ordered
 correctly, or an unrecognized or
 unsupported value was specified for dither.
 
 These functions do not work in place.
 */

VIMAGE_PF vImage_Error vImageConvert_Planar8toIndexed1(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, Pixel_8 colors[2], int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar8toIndexed2(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, Pixel_8 colors[4], int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar8toIndexed4(const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, Pixel_8 colors[16], int dither, const vImage_Flags flags) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*  vImageConvert_8to16Q12
 vImageConvert_RGB888toPlanar16Q12
 vImageConvert_ARGB8888toPlanar16Q12
 
 All three functions convert 8-bit pixel format to 16-bit format with 12
 fractional bits.  The conversion is performed as follows:
 
 uint16_t dest = ((src << 12) + 127)/255;
 
 Source pixel values of 0 are mapped to 0, and source pixel values of 255
 are mapped to 0x1000 (4096).  No larger values are produced by this
 conversion, which provides some headroom to help subsequent operations
 avoid overflow or clipping.
 
 vImageConvert_8to16Q12 takes a single source buffer and a single
 destination buffer and simply does the conversion.
 
 vImageConvert_RGB888toPlanar16Q12 has three destination buffers, and
 de-interleaves while it converts.
 
 vImageConvert_ARGB8888toPlanar16Q12 takes four destination buffers, and
 also de-interleaves.  Any channel order may be handled by the latter two
 functions by permuting the order in which the destination buffers are
 passed as arguments.
 
 
 
 The only supported flags are:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 kvImageGetTempBufferSize    Returns zero, as the routine does not use
 a temp buffer.
 
 Passing any other flag will result in no work being done and an error
 code being returned.
 
 Return Value:
 -------------
 kvImageNoError                  is returned when there was no error.
 kvImageBufferSizeMismatch       the destination buffers do not have the
 same size as each other
 kvImageRoiLargerThanInputBuffer The destination buffers are larger than the
 source buffer.
 kvImageUnknownFlagsBit          is returned when there is a unknown flag.
 
 This function will not operate in place.
 */

VIMAGE_PF vImage_Error vImageConvert_8to16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_RGB888toPlanar16Q12(const vImage_Buffer *src, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB8888toPlanar16Q12(const vImage_Buffer *src, const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*  vImageConvert_16Q12to8
 vImageConvert_Planar16Q12toRGB888
 vImageConvert_Planar16Q12toARGB8888
 
 All three functions convert 16-bit format with 12 fractional bits to 8-bit.
 The conversion is performed as follows:
 
 uint8_t dest = clamp(src, 0, 4096)*255 + 2048 >> 12
 
 Source pixel values of 0 are mapped to 0, and source pixel values of 4088
 or greater are mapped to 255.
 
 vImageConvert_Planar16Q12toRGB888 and vImageConvert_Planar16Q12toARGB8888
 interleave data from three or four buffers respectively while performing
 the conversion.
 
 The only supported flags are:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 kvImageGetTempBufferSize    Returns zero, as the routine does not use
 a temp buffer.
 
 Passing any other flag will result in no work being done and an error
 code being returned.
 
 Return Value:
 -------------
 kvImageNoError                  is returned when there was no error.
 kvImageBufferSizeMismatch       the destination buffers do not have the
 same size as each other
 kvImageRoiLargerThanInputBuffer The destination buffers are larger than the
 source buffer.
 kvImageUnknownFlagsBit          is returned when there is a unknown flag.
 
 This function will not operate in place.
 */

VIMAGE_PF vImage_Error vImageConvert_16Q12to8(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar16Q12toRGB888(const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Planar16Q12toARGB8888(const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 @function vImageConvert_16Q12to16F
 @abstract Convert 16Q12 (16-bit format with 12 fractional bits) to half-precision floating-point.
 
 @discussion Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16Q12) 4096
 are mapped to (Pixel_16F) 1.0f.
 @note Works in place provided that src->data == dest->data && src->rowBytes == dest->rowBytes.
 
 @param src              The input image.
 @param dest             A pointer to a preallocated vImage_Buffer to receive the resulting image.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns zero, as the routine does not use a temp buffer.
 
 @return kvImageNoError                      There was no error.
 @return kvImageBufferSizeMismatch           The destination buffers do not have the
 same size as each other
 @return kvImageRoiLargerThanInputBuffer     The destination buffers are larger than the
 source buffer.
 @return kvImageUnknownFlagsBit              Unknown flag(s) provided.
 */
VIMAGE_PF vImage_Error vImageConvert_16Q12to16F(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!
 @function vImageConvert_16Q12toRGB16F
 @abstract Convert 16Q12 and interleave (16-bit format with 12 fractional bits) to half-precision floating-point.
 
 @discussion Interleaves data from three source buffers while performing the format conversion.
 Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16Q12) 4096
 are mapped to (Pixel_16F) 1.0f.
 @note Does not work in place.
 
 @param red              The red channel of the input image.
 @param green            The green channel of the input image.
 @param blue             The blue channel of the input image.
 @param dest             A pointer to a preallocated vImage_Buffer to receive the resulting chunky image.
 
 @param flags
 \p kvImageDoNotTile             Disables internal multithreading, if any.
 \p kvImageGetTempBufferSize     Returns zero, as the routine does not use a temp buffer.
 
 @return kvImageNoError                      There was no error.
 @return kvImageBufferSizeMismatch           The destination buffers do not have the
 same size as each other
 @return kvImageRoiLargerThanInputBuffer     The destination buffers are larger than the
 source buffer.
 @return kvImageUnknownFlagsBit              Unknown flag(s) provided.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar16Q12toRGB16F(const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!
 @function vImageConvert_Planar16Q12toARGB16F
 @abstract Convert and interleave 16Q12 (16-bit format with 12 fractional bits) to half-precision floating-point.
 
 @discussion Interleaves data from four source buffers while performing the format conversion.
 Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16Q12) 4096
 are mapped to (Pixel_16F) 1.0f.
 @note Does not work in place.
 
 @param alpha            The alpha channel of the input image.
 @param red              The red channel of the input image.
 @param green            The green channel of the input image.
 @param blue             The blue channel of the input image.
 @param dest             A pointer to a preallocated vImage_Buffer to receive the resulting chunky image.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns zero, as the routine does not use a temp buffer.
 
 @return kvImageNoError                      There was no error.
 @return kvImageBufferSizeMismatch           The destination buffers do not have the
 same size as each other
 @return kvImageRoiLargerThanInputBuffer     The destination buffers are larger than the
 source buffer.
 @return kvImageUnknownFlagsBit              Unknown flag(s) provided.
 */
VIMAGE_PF vImage_Error vImageConvert_Planar16Q12toARGB16F(const vImage_Buffer *alpha, const vImage_Buffer *red, const vImage_Buffer *green, const vImage_Buffer *blue, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!
 @function vImageConvert_16Fto16Q12
 @abstract Convert half-precision floating-point to 16Q12 (16-bit format with 12 fractional bits).
 
 @discussion Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16F) 1.0f
 are mapped to (Pixel_16Q12) 4096.
 @note Works in place provided that src->data == dest->data && src->rowBytes == dest->rowBytes.
 
 @param src              The input image.
 @param dest             A pointer to a preallocated vImage_Buffer to receive the resulting image.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns zero, as the routine does not use a temp buffer.
 
 @return kvImageNoError                      There was no error.
 @return kvImageBufferSizeMismatch           The destination buffers do not have the
 same size as each other
 @return kvImageRoiLargerThanInputBuffer     The destination buffers are larger than the
 source buffer.
 @return kvImageUnknownFlagsBit              Unknown flag(s) provided.
 */
VIMAGE_PF vImage_Error vImageConvert_16Fto16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));


/*
 vImageConvert_16Q12toF
 vImageConvert_Fto16Q12
 
 Convert 16-bit format with 12 fractional bits to floating-point.
 The conversion is performed as follows:
 
 float vImageConvert_16Q12toF_result = sample_16Q12 / 4096.0f
 Pixel_16Q12 vImageConvert_Fto16Q12_result = CLAMP( -32768, lrintf(sample_float * 4096.0f), 32767)
 
 Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16Q12) 4096
 is mapped to (Pixel_F) 1.0f.
 
 The only supported flags are:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 kvImageGetTempBufferSize    Returns zero, as the routine does not use
 a temp buffer. Does no work
 
 Passing any other flag will result in no work being done and an error
 code being returned.
 
 Return Value:
 -------------
 kvImageNoError                  is returned when there was no error.
 kvImageRoiLargerThanInputBuffer one or more source buffers is smaller than the destination buffer.
 kvImageUnknownFlagsBit          is returned when there is a unknown flag.
 
 These functions will operate in place provided that src->data == dest->data && src->rowBytes == dest->rowBytes.
 
 */

VIMAGE_PF vImage_Error vImageConvert_16Q12toF(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_Fto16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 vImageConvert_16Q12to16U
 vImageConvert_16Uto16Q12
 
 Convert 16-bit format with 12 fractional bits to floating-point.
 The conversion is performed as follows:
 
 Pixel_16U vImageConvert_16Q12to16U_result = CLAMP( 0, (sample_16Q12 * 65535 + 2048) >> 12, 65535 )
 Pixel_16Q12 vImageConvert_16Uto16Q12_result = (sample_16U * 4096 + 32767) / 65535;
 
 Source pixel values of 0 are mapped to 0, and source pixel values of (Pixel_16Q12) 4096
 is mapped to (Pixel_16U) 65535.
 
 The only supported flags are:
 
 kvImageDoNotTile            Turns off internal multithreading. You may
 wish to do this if you have your own
 multithreading scheme to avoid having the
 two interfere with one another.
 
 kvImageGetTempBufferSize    Returns zero, as the routine does not use
 a temp buffer. Does no work
 
 Passing any other flag will result in no work being done and an error
 code being returned.
 
 Return Value:
 -------------
 kvImageNoError                  is returned when there was no error.
 kvImageRoiLargerThanInputBuffer one or more source buffers is smaller than the destination buffer.
 kvImageUnknownFlagsBit          is returned when there is a unknown flag.
 
 These functions will operate in place provided that src->data == dest->data && src->rowBytes == dest->rowBytes.
 
 */

VIMAGE_PF vImage_Error vImageConvert_16Q12to16U(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageConvert_16Uto16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 
 YpCbCr image formats:
 =====================
 Some details about video formats are provided below to provide some context for interpreting parameters passed to YpCbCr conversion functions.
 
 Luminance and Chrominance
 -------------------------
 Luminance is the brightness in an image. An example might be the image visible on a black and white television. Luminance (Y, Y' or Yp)
 is defined as a weighted sum of the color components, usually weighted according to the relative ability of red, green and blue to cause
 perceived brightness changes in an image. For example for ITU-Recommendation BT.709 images, the Y component is given as:
 
 Y = 0.2126*R + 0.7152*G + 0.0722*B
 
 Chrominance (Cb and Cr) is the correction applied to the luminance to add color information. For ITU-R BT.709 they would be:
 
 Cb  = 0.5389*(B-Y)
 Cr  = 0.6350*(R-Y)
 
 Together, these can be written as an invertible 3x3 color matrix that converts {R,G,B} to {Yp, Cb, Cr}.
 
 Since chrominance is calculated as the difference between two unsigned values, it itself is signed. Typically, a large bias is added to
 the chrominance value when it is encoded as an unsigned integer to make negative values representable. For example, 8-bit chrominance
 values typically have an implicit +128 in them. To convert a full range 8-bit chroma value to an unbiased floating point representation,
 it would be:
 
 Pb = (Cb - 128) / 127
 
 and so a full range chrominance value actually varies in the range [1,255] indicating [-1.0, 1.0]. See also Video Range.
 
 YpCbCr video color representations are typically specified in terms of a RGB colorspace and a typically 3x3 matrix to convert RGB to YpCbCr.
 The RGB colorspace itself is also defined relative to an absolute XYZ colorspace.  This is typically done using some x,y,z (little xyz!)
 primaries defining the size of the color cube and a transfer function that describes the conversion from linear color to non-linear RGB. The
 transfer function is usually of a form that can be evaluated with vImagePiecewiseGamma_<fmt> (see vImage/Transform.h). It should be noted that
 both of these are customarily defined in the opposite direction from an ICC profile. Whereas an ICC profile is conversion from display colorspace
 to linear XYZ, video formats are from XYZ to YpCbCr. Thus, the matrices and the gamma are inverted.
 
 Chroma subsampling
 ------------------
 Since the human eye is better at getting positional information from luminance changes than chrominance changes, the chrominance signal is
 often subsampled relative to luminance. (For each chrominance sample, there may be 2 or 4 luminance ones.) This saves memory. See
 [sampling ratio] below.
 
 Chroma Siting
 -------------
 Chroma subsampling also brings with it the possibility of differences in how the chroma samples are positioned relative to the luminance samples.
 For example for 422, they may overlap the left one, or the right one, or be positioned between them. This is called chroma siting. Many video
 formats are available for a diversity of siting options. However, for OS X.10 and iOS 8, vImage does only the centered variant
 (kCVImageBufferChromaLocation_Center). Please file a bug report to request other siting modes where they are found to be necessary.
 
 Video Range
 -----------
 Video formats commonly do not use the entire representable range available to them to encode video data. A 8-bit video range signal may use
 the range [16,235] instead of [0,255] to represent the range of signal strengths [0,1.0] for luminance, and [16,240] for chroma. A video signal
 that uses the full [0,255] ([1,255] for chroma) is said to be full range.
 
 Pixel Blocks
 -------------
 Since some channels are sampled at a higher rate than other channels, it is common to group multiple pixels together in a block which is
 encoded or decoded as a unit. Blocks are useful because they are an integer multiple of a byte in size and describe a particular
 repeating component order that can be serviced in a loop. See [sampling ratio] below for common ways in which multiple YpCbCr pixels fit
 together in a block. An example might be a 2vuy image (422CbYpCrYp8 to vImage) which has luminance and chrominance for two pixels packed
 together in a block as {Cb, Yp, Cr, Yp}.  The chrominance is shared between the two pixels, but each has its own luminance.
 
 In all cases, the beginning of a scanline must also be the beginning of a block, and must be at least byte aligned.  We do not support cases where
 a block spans multiple block rows. If the width of an image is not a multiple of the block width (or height is not a multiple of a block height)
 and you are writing to a YpCbCr format, then the edge pixels are in effect duplicated (see kvImageEdgeExtend) until the image is a multiple of
 the block size and the larger image is encoded to YpCbCr. If you are converting from a subsampled YpCbCr image to a non-subsampled format, then
 we only write out the pixels that fit in the vImage_Buffer.height and vImage_Buffer.width provided. You should be careful to keep the original
 height and width, since edges of the wider images may look like image artifacts if they appear onscreen.
 
 Nomenclature used in vImage
 ---------------------------
 To cover the many different packing orders, chroma subsamplings and separate image planes, we adopt the following naming convention here
 as an extension of that commonly used here for RGB images:
 
 [sampling ratio][order of appearance of components in block][bitdepth](_[sampling ratio][order of appearance of components in block] for plane 2)...
 
 Examples:
 
 444YpCbCrA8        Each chunk is one pixel. Channels are 8-bit interleaved in the order A, Yp, Cb, Cr.  kCVPixelFormatType_4444YpCbCrA8
 422CbYpCrYp8        Each chunk is two horizontally adjacent pixels. Channels are 8-bit interleaved in the order Cb, Yp0, Cr, Yp0. (2vuy)
 422CbYpCrYp8_AA8   Like 422CbYpCrYp8 but with a separate alpha channel added. (kCVPixelFormatType_422YpCbCr_4A_8BiPlanar)
 
 The fields and behavior details of such formats are described below:
 
 [sampling ratio]
 ----------------
 The format begins with a three digit code commonly used for video formats that describes the relative frequencies with which luminance
 (Yp) and chrominance (Cb, Cr) are sampled, to enable rapid indexing of the formats for experienced professionals. For the rest of us,
 these are:
 
 444  Luminance and chrominance are sampled at the same rate
 422  Luminance is sampled at twice the rate of chrominance components in the horizontal dimension
 420  Luminance is sampled at twice the rate of chrominance components in both dimensions.
 ...
 
 If there is an alpha channel, it is assumed to be sampled at the same rate as luminance.
 
 [order of appearance of components in block]:
 --------------------------------------------
 For a 444 block, each component appears once and the block holds one pixel, so you
 just see the order of the components such as AYpCbCr much like we write ARGB. For a 422 block, there are two luminance for each Cb or Cr, so
 you will see a block order like YpCbYpCr and the block represents two pixels in a fully decoded image. Noticed that Yp is repeated twice.
 The pixels have different luminance but share the same chrominance values. In the case of 420 and 411, there is also vertical subsampling.
 In this case, the vImage_Buffer.rowBytes describes the stride from one block row to the next. (Each block row is two pixels tall.)
 
 In extreme cases with blocks that span more than two pixels, the block packing can become complicated, resulting in an exceptionally long name.
 This can happen for bit depths that are not a multiple of a byte, where the block size not only accommodates subsampling, but also byte alignment.
 Such cases may be abbreviated with the 'P' character, indicating Packed. An example is 422PCbYpCrYp10, which would otherwise have to be named
 422CrYpCbYpCbYpCbYpCrYpCrYp10. In such cases, the actual layout is given in the comments.
 
 
 [bitdepth]
 ----------
 This is the number of bits per component. Typically, all the channels have the same bit depth so it just appears at the end of the plane name.
 If there is a heterogeneous format, the bit depths are inserted between the channels where they change (e.g. 444Yp12CbCr10) for 12 bit Yp and
 10 bit CbCr in a 32-bit 444 block.
 
 [additional planes]
 -------------------
 Some YpCbCr formats have multiple planes. (That is, multiple vImage_Buffers.) It is common to have the Y data in one plane and CbCr packed in
 another plane, or perhaps alpha stored  separately. An underbar '_' shall separate the planes and the new plane will be named according to order
 of appearance of the block components. Subsampling may cause components in a plane to appear multiple times.  For example: 411YpYpCbYpYpCr8_AAAA8
 is a 411 format with alpha in a separate plane.  We see AAAA instead of A because each block holds 4 pixels and so the block contains alpha four
 times in that plane. Blocks in all planes have the same height and width for a particular format.
 
 
 Conversion to RGB and other colorspaces
 ----------------------------------------
 
 Unfortunately, the RGB colorspaces that commonly underlie YpCbCr formats are generally not the same as ones commonly used for RGB imaging in
 CoreGraphics. For example, ITU-R BT.709 is similar to sRGB but not the same. Consequently, simple conversion from YpCbCr data to RGB using
 the matrix functions provided here may not be the only step required for good color fidelity. A RGB -> RGB colorspace conversion may be
 required too.  Typically, this involves converting to linear color, applying a RGB->RGB conversion matrix, and then applying the new gamma.
 This can be done using vImagePiecewiseGamma_<fmt> and vImageMatrixMultiply_<fmt>. (We recommend planar 16Q12 for 8- and 10-bpc formats and
 floating point for higher bit depths.) However, some may find it simpler to use the interfaces on vImage_CVUtilities.h instead, which handle
 such details for you.
 
 */


/*!
 @function vImageConvert_YpCbCrToARGB_GenerateConversion
 
 @abstract Generates the conversion from a YpCbCr to a ARGB pixel format.
 
 @param matrix
 A pointer to vImage_YpCbCrToARGBMatrix that contains the matrix coefficients for the conversion
 from a YpCbCr to a ARGB pixel format.
 
 @param pixelRange
 A pointer to vImage_YpCbCrPixelRange that contains the pixel range information for the conversion
 from a YpCbCr to a ARGB pixel format.
 
 @param outInfo
 A pointer to vImage_YpCbCrToRGB will be initialized with the information for the conversion function
 will use later.
 
 @param inYpCbCrType
 A YpCbCrType to specify the input YpCbCr format.
 
 @param outARGBType
 A ARGBType to specify the output ARGB format.
 
 @param flags
 kvImagePrintDiagnosticsToConsole   Directs the function to print diagnostic information to the console in the event of failure.
 
 @discussion This function is used to create the vImage_YpCbCrToARGB conversion information necessary for all
 of YUV -> RGB conversion functions.
 
 For example, if we want to prepare for the conversion from 'yuvs' with ITU 601 video range to ARGB8888, then we
 need to do the following:
 
 <pre> @textblock
 VIMAGE_PF vImage_Error err = kvImageNoError;
 vImage_Flags flags = kvImageNoFlags;
 vImage_YpCbCrPixelRange pixelRange;
 vImage_YpCbCrToARGB outInfo;
 
 pixelRange.Yp_bias         =   16;     // encoding for Y' = 0.0
 pixelRange.CbCr_bias       =  128;     // encoding for CbCr = 0.0
 pixelRange.YpRangeMax      =  235;     // encoding for Y'= 1.0
 pixelRange.CbCrRangeMax    =  240;     // encoding for CbCr = 0.5
 pixelRange.YpMax           =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest. Use pixelRange.YpRangeMax if you don't want Y' > 1.
 pixelRange.YpMin           =    0;     // a clamping limit below which the value is not allowed to go. 0 is fastest. Use pixelRange.Yp_bias if you don't want Y' < 0.
 pixelRange.CbCrMax         =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest.  Use pixelRange.CbCrRangeMax, if you don't want CbCr > 0.5
 pixelRange.CbCrMin         =    0;     // a clamping limit above which the value is not allowed to go. 0 is fastest.  Use (2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax), if you don't want CbCr < -0.5
 //                ( pixelRange.CbCr_bias - (pixelRange.CbCrRangeMax - pixelRange.CbCr_bias) = 2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax )
 
 err = vImageConvert_YpCbCrToARGB_GenerateConversion(kvImageITU601_YpCbCrToARGBMatrix, &pixelRange, &outInfo, kvImage422YpCbYpCr8, kvImageARGB8888, flags);
 @/textblock </pre>
 
 If we want to define our own conversion coefficents, then we can do
 
 <pre> @textblock
 vImage_YpCbCrToARGBMatrix matrix;
 vImage_YpCbCrPixelRange pixelRange;
 
 matrix.Yp                  =  1.0f;
 matrix.Cb_G                = -0.3441f;
 matrix.Cb_B                =  1.772f;
 matrix.Cr_R                =  1.402f;
 matrix.Cr_G                = -0.7141f;
 
 pixelRange.Yp_bias         =   16;     // encoding for Y' = 0.0
 pixelRange.CbCr_bias       =  128;     // encoding for CbCr = 0.0
 pixelRange.YpRangeMax      =  235;     // encoding for Y'= 1.0
 pixelRange.CbCrRangeMax    =  240;     // encoding for CbCr = 0.5
 pixelRange.YpMax           =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest. Use pixelRange.YpRangeMax if you don't want Y' > 1.
 pixelRange.YpMin           =    0;     // a clamping limit below which the value is not allowed to go. 0 is fastest. Use pixelRange.Yp_bias if you don't want Y' < 0.
 pixelRange.CbCrMax         =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest.  Use pixelRange.CbCrRangeMax, if you don't want CbCr > 0.5
 pixelRange.CbCrMin         =    0;     // a clamping limit above which the value is not allowed to go. 0 is fastest.  Use (2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax), if you don't want CbCr < -0.5
 //                ( pixelRange.CbCr_bias - (pixelRange.CbCrRangeMax - pixelRange.CbCr_bias) = 2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax )
 
 err = vImageConvert_YpCbCrToARGB_GenerateConversion(&matrix, &pixelRange, &outInfo, kvImage422YpCbYpCr8, kvImageARGB8888, flags).
 @/textblock </pre>
 
 
 vImage_YpCbCrToARGB created may be reused multiple times from multiple threads concurrently.
 
 Here are the conversions available currently.
 
 <pre>
 @textblock
 RGB8   RGB16Q12    RGB16
 YUV8     Y        N          N
 YUV10    Y        Y          N
 YUV12    Y        Y          N
 YUV14    Y        N          Y
 YUV16    Y        N          Y
 @/textblock
 </pre>
 
 @return  The following return codes may occur:
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageUnsupportedConversion    Is returned when there is no conversion in vImage for inYpCbCrType & outARGBType.
 @/textblock  </pre>
 
 */

VIMAGE_PF vImage_Error vImageConvert_YpCbCrToARGB_GenerateConversion(const vImage_YpCbCrToARGBMatrix *matrix, const vImage_YpCbCrPixelRange *pixelRange, vImage_YpCbCrToARGB *outInfo, vImageYpCbCrType inYpCbCrType, vImageARGBType outARGBType, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGBToYpCbCr_GenerateConversion
 
 @abstract Generates the conversion from a ARGB to a YpCbCr pixel format.
 
 @param matrix
 A pointer to vImage_ARGBToYpCbCrMatrix that contains the matrix coefficients for the conversion
 from a ARGB to a YpCbCr pixel format.
 
 @param pixelRange
 A pointer to vImage_YpCbCrPixelRange that contains the pixel range information for the conversion
 from a ARGB to a YpCbCr pixel format.
 
 @param outInfo
 A pointer to vImage_ARGBToYpCbCr will be initialized with the information for the conversion function
 will use later.
 
 @param inARGBType
 A ARGBType to specify the output ARGB format.
 
 @param outYpCbCrType
 A YpCbCrType to specify the input YpCbCr format.
 
 @param flags
 kvImagePrintDiagnosticsToConsole   Directs the function to print diagnostic information to the console in the event of failure.
 
 @discussion This function is used to create the vImage_ARGBToYpCbCr conversion information necessary for all
 of RGB -> YUV conversion functions.
 
 For example, if we want to prepare for the conversion from ARGB8888 'yuvs' with ITU 601 video range, then we
 need to do the following:
 
 <pre> @textblock
 VIMAGE_PF vImage_Error err = kvImageNoError;
 vImage_Flags flags = kvImageNoFlags;
 vImage_YpCbCrPixelRange pixelRange;
 vImage_ARGBToYpCbCr outInfo;
 
 pixelRange.Yp_bias         =   16;     // encoding for Y' = 0.0
 pixelRange.CbCr_bias       =  128;     // encoding for CbCr = 0.0
 pixelRange.YpRangeMax      =  235;     // encoding for Y'= 1.0
 pixelRange.CbCrRangeMax    =  240;     // encoding for CbCr = 0.5
 pixelRange.YpMax           =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest. Use pixelRange.YpRangeMax if you don't want Y' > 1.
 pixelRange.YpMin           =    0;     // a clamping limit below which the value is not allowed to go. 0 is fastest. Use pixelRange.Yp_bias if you don't want Y' < 0.
 pixelRange.CbCrMax         =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest.  Use pixelRange.CbCrRangeMax, if you don't want CbCr > 0.5
 pixelRange.CbCrMin         =    0;     // a clamping limit above which the value is not allowed to go. 0 is fastest.  Use (2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax), if you don't want CbCr < -0.5
 
 err = vImageConvert_ARGBToYpCbCr_GenerateConversion(kvImage_ARGBToYpCbCrMatrix_ITU_R_601_4, &pixelRange, &outInfo, kvImageARGB8888, kvImage422YpCbYpCr8, flags);
 
 
 If we want to define our own conversion coefficents, then we can do
 
 vImage_ARGBToYpCbCrMatrix matrix;;
 vImage_YpCbCrPixelRange pixelRange;
 
 matrix.R_Yp          =  0.2989f;
 matrix.G_Yp          =  0.5866f;
 matrix.B_Yp          =  0.1144f;
 matrix.R_Cb          = -0.1688f;
 matrix.G_Cb          = -0.3312f;
 matrix.B_Cb_R_Cr     =  0.5f;
 matrix.G_Cr          = -0.4183f;
 matrix.B_Cr          = -0.0816f;
 pixelRange.Yp_bias         =   16;     // encoding for Y' = 0.0
 pixelRange.CbCr_bias       =  128;     // encoding for CbCr = 0.0
 pixelRange.YpRangeMax      =  235;     // encoding for Y'= 1.0
 pixelRange.CbCrRangeMax    =  240;     // encoding for CbCr = 0.5
 pixelRange.YpMax           =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest. Use pixelRange.YpRangeMax if you don't want Y' > 1.
 pixelRange.YpMin           =    0;     // a clamping limit below which the value is not allowed to go. 0 is fastest. Use pixelRange.Yp_bias if you don't want Y' < 0.
 pixelRange.CbCrMax         =  255;     // a clamping limit above which the value is not allowed to go. 255 is fastest.  Use pixelRange.CbCrRangeMax, if you don't want CbCr > 0.5
 pixelRange.CbCrMin         =    0;     // a clamping limit above which the value is not allowed to go. 0 is fastest.  Use (2*pixelRange.CbCr_bias - pixelRange.CbCrRangeMax), if you don't want CbCr < -0.5
 
 err = vImageConvert_ARGBToYpCbCr_GenerateConversion(&matrix, &pixelRange, &outInfo, kvImageARGB8888, kvImage422YpCbYpCr8, flags);
 
 
 vImage_ARGBToYpCbCr created may be reused multiple times from multiple threads concurrently.
 
 Here are the conversions available currently.
 
 RGB8   RGB16Q12    RGB16
 YUV8     Y        N          N
 YUV10    Y        Y          N
 YUV12    Y        Y          N
 YUV14    Y        N          Y
 YUV16    Y        N          Y
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageUnsupportedConversion    Is returned when there is no conversion in vImage for inARGBType & outYpCbCrType.
 @/textblock </pre>
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGBToYpCbCr_GenerateConversion(const vImage_ARGBToYpCbCrMatrix *matrix, const vImage_YpCbCrPixelRange *pixelRange, vImage_ARGBToYpCbCr *outInfo, vImageARGBType inARGBType, vImageYpCbCrType outYpCbCrType, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 422YpCbYpCr8 ('yuvs' and 'yuvf') */

/*!
 @function vImageConvert_422YpCbYpCr8ToARGB8888
 
 @abstract Convert YUV 422YpCbYpCr8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 422YpCbYpCr8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 
 @discussion Convert YUV 422YpCbYpCr8 format to ARGB8888
 
 
 Yp0 Cb0 Yp1 Cr0  =>  A0 R0 G0 B0  A1 R1 G1 B1
 
 
 YUV 422YpCbYpCr8 can be used for 'yuvs' and 'yuvf' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'yuvs' with ITU 601 video range, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion().
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 Yp0 = srcPixel[0];
 Cb0 = srcPixel[1];
 Yp1 = srcPixel[2];
 Cr0 = srcPixel[3];
 srcPixel += 4;
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 
 uint8_t ARGB[8];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel += 8;
 @/textblock
 </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422YpCbYpCr8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To422YpCbYpCr8
 
 @abstract Convert ARGB8888 to YUV 422YpCbYpCr8 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422YpCbYpCr8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 422YpCbYpCr8 format
 
 A0 R0 G0 B0  A1 R1 G1 B1 => Yp0 Cb0 Yp1 Cr0
 
 
 YUV 422YpCbYpCr8 can be used for 'yuvs' and 'yuvf' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'yuvs' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = Yp0;
 destPixel[1] = Cb0;
 destPixel[2] = Yp1;
 destPixel[3] = Cr0;
 destPixel += 4;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To422YpCbYpCr8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 422CbYpCrYp8 ('2vuy' and '2vuf') */

/*!
 @function vImageConvert_422CbYpCrYp8ToARGB8888
 
 @abstract Convert YUV 422CbYpCrYp8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 422CbYpCrYp8 format to ARGB8888
 
 
 Cb0 Yp0 Cr0 Yp1  =>  A0 R0 G0 B0  A1 R1 G1 B1
 
 
 YUV 422CbYpCrYp8 can be used for '2vuy' and '2vuf' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert '2vuy' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 Cb0 = srcPixel[0];
 Yp0 = srcPixel[1];
 Cr0 = srcPixel[2];
 Yp1 = srcPixel[3];
 srcPixel += 4;
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                            ), 255 )
 
 uint8_t ARGB[8];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel += 8;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */
VIMAGE_PF vImage_Error vImageConvert_422CbYpCrYp8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 @function vImageConvert_ARGB8888To422CbYpCrYp8
 
 @abstract Convert ARGB8888 to YUV 422CbYpCrYp8 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once in the ordering.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 422CbYpCrYp8 format. Can be used for 2vuy.
 
 
 A0 R0 G0 B0  A1 R1 G1 B1 => Cb0 Yp0 Cr0 Yp1
 
 
 YUV 422CbYpCrYp8 can be used for '2vuy' and '2vuf' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to '2vuy' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = Cb0;
 destPixel[1] = Yp0;
 destPixel[2] = Cr0;
 destPixel[3] = Yp1;
 destPixel += 4;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */


VIMAGE_PF vImage_Error vImageConvert_ARGB8888To422CbYpCrYp8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 422CbYpCrYp8_AA8 ('a2vy') */

/*!
 @function vImageConvert_422CbYpCrYp8_AA8ToARGB8888
 
 @abstract Convert YUV 422CbYpCrYp8_AA8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp8_AA8 source pixels.
 
 @param srcA
 A pointer to vImage_Buffer that references 8-bit alpha source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 422CbYpCrYp8_AA8 format to ARGB8888
 
 
 Cb0 Yp0 Cr0 Yp1  =>  A0 R0 G0 B0  A1 R1 G1 B1
 
 A0 A1
 
 
 YUV 422CbYpCrYp8_AA8 can be used for 'a2vy' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'a2vy' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 Cb0 = srcPixel[0];
 Yp0 = srcPixel[1];
 Cr0 = srcPixel[2];
 Yp1 = srcPixel[3];
 srcPixel += 4;
 
 uint8_t *alpha = srcA.data;
 A0 = alpha[0];
 A1 = alpha[1];
 alpha += 2;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[8];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel += 8;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422CbYpCrYp8_AA8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *srcA, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To422CbYpCrYp8_AA8
 
 @abstract Convert ARGB8888 to YUV 422CbYpCrYp8_AA8 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp8_AA8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears exactly once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 422CbYpCrYp8_AA8 format
 
 
 A0 R0 G0 B0  A1 R1 G1 B1 => Cb0 Yp0 Cr0 Yp1
 
 A0 A1
 
 
 For example, if we want to use this function to convert ARGB8888 to 'a2vy' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = Cb0;
 destPixel[1] = Yp0;
 destPixel[2] = Cr0;
 destPixel[3] = Yp1;
 destPixel += 4;
 
 uint8_t *alpha = destA.data;
 alpha[0] = A0;
 alpha[1] = A1;
 alpha += 2;
 
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 <pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To422CbYpCrYp8_AA8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_Buffer *destA, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 444AYpCbCr8 ('r408' and 'y408') */

/*!
 @function vImageConvert_444AYpCbCr8ToARGB8888
 
 @abstract Convert YUV 444AYpCbCr8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 444AYpCbCr8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444AYpCbCr8 format to ARGB8888
 
 
 A0 Yp0 Cb0 Cr0  =>  A0 R0 G0 B0
 
 
 YUV 444AYpCbCr8 can be used for 'r408' and 'y408' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'y408' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 
 uint8_t *srcPixel = src.data;
 A0  = srcPixel[0];
 Yp0 = srcPixel[1];
 Cb0 = srcPixel[2];
 Cr0 = srcPixel[3];
 srcPixel += 4;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[4];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 
 @/textblock
 <pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444AYpCbCr8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To444AYpCbCr8
 
 @abstract Convert ARGB8888 to YUV 444AYpCbCr8 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444AYpCbCr8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears exactly once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 444AYpCbCr8 format
 
 
 A0 R0 G0 B0  =>  A0 Yp0 Cb0 Cr0
 
 
 YUV 444AYpCbCr8 can be used for 'r408' and 'y408' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'y408' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = A0;
 destPixel[1] = Yp0;
 destPixel[2] = Cb0;
 destPixel[3] = Cr0;
 destPixel += 4;
 @/textblock
 </pre>
 
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 <pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To444AYpCbCr8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 444CbYpCrA8 ('v408') */
/*!
 @function vImageConvert_444CbYpCrA8ToARGB8888
 
 @abstract Convert YUV 444CbYpCrA8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 444CbYpCrA8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444CbYpCrA8 format to ARGB8888
 
 
 Cb0 Yp0 Cr0 A0  =>  A0 R0 G0 B0
 
 
 YUV 444CbYpCrA8 can be used for 'v408' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'v408' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 
 <pre> @textblock
 uint8_t *srcPixel = src.data;
 Cb0 = srcPixel[0];
 Yp0 = srcPixel[1];
 Cr0 = srcPixel[2];
 A0  = srcPixel[3];
 srcPixel += 4;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[4];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock    </pre>
 
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444CbYpCrA8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To444CbYpCrA8
 
 @abstract Convert ARGB8888 to YUV 444CbYpCrA8 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444CbYpCrA8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, provided that each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 444CbYpCrA8 format
 
 
 A0 R0 G0 B0  =>  Cb0 Yp0 Cr0 A0
 
 
 YUV 444CbYpCrA8 can be used for 'v408' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'v408' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 
 <pre> @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = Cb0;
 destPixel[1] = Yp0;
 destPixel[2] = Cr0;
 destPixel[3] = A0;
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To444CbYpCrA8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 444CrYpCb8 ('v308')  */

/*!
 @function vImageConvert_444CrYpCb8ToARGB8888
 
 @abstract Convert YUV 444CrYpCb8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 444CrYpCb8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444CrYpCb8 format to ARGB8888
 
 
 Cr0 Yp0 Cb0  =>  A0 R0 G0 B0
 
 
 YUV 444CrYpCb8 can be used for 'v308' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'v308' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre>
 @textblock
 
 uint8_t *srcPixel = src.data;
 Cr0 = srcPixel[0];
 Yp0 = srcPixel[1];
 Cb0 = srcPixel[2];
 srcPixel += 3;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock
 </pre>
 
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444CrYpCb8ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To444CrYpCb8
 
 @abstract Convert ARGB8888 to YUV 444CrYpCb8 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444CrYpCb8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, provided that each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 444CrYpCb8 format
 
 
 A0 R0 G0 B0  =>  Cr0 Yp0 Cb0
 
 
 YUV 444CrYpCb8 can be used for 'v308' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'v308' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 
 <pre>
 @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = Cr0;
 destPixel[1] = Yp0;
 destPixel[2] = Cb0;
 destPixel += 3;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To444CrYpCb8(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 420Yp8_Cb8_Cr8 ('y420' and 'f420') */

/*!
 @function vImageConvert_420Yp8_Cb8_Cr8ToARGB8888
 
 @abstract Convert YUV 420Yp8_Cb8_Cr8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 420Yp8_Cb8_Cr8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 420Yp8_Cb8_Cr8 format to ARGB8888
 
 
 Ypt0 Ypt1  =>  At0 Rt0 Gt0 Bt0  At1 Rt1 Gt1 Bt1
 
 Ypb0 Ypb1      Ab0 Rb0 Gb0 Bb0  Ab1 Rb1 Gb1 Bb1
 
 Cb0
 
 Cr0
 
 
 YUV 420Yp8_Cb8_Cr8 can be used for 'y420' and 'f420' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'y420' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre>
 @textblock
 
 uint8_t *srcYtPixel = srcY.data;
 uint8_t *srcYbPixel = srcY.data + srcY.rowBytes;
 Ypt0 = srcYtPixel[0];
 Ypt1 = srcYtPixel[1];
 srcYtPixel += 2;
 Ypb0 = srcYbPixel[0];
 Ypb1 = srcYbPixel[1];
 srcYbPixel += 2;
 
 uint8_t *srcCbPixel = srcCb.data;
 uint8_t *srcCrPixel = srcCr.data;
 Cb0 = srcCbPixel[0];
 srcCbPixel += 1;
 Cr0 = srcCrPixel[0];
 srcCrPixel += 1;
 
 At0 = alpha
 Rt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 + Yp_bias) * Yp                               + (Cr0 + CbCr_bias) * Cr_R), 255 )
 Gt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_G + (Cr0 + CbCr_bias) * Cr_G), 255 )
 Bt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_B                              ), 255 )
 At1 = alpha
 Rt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 + Yp_bias) * Yp                               + (Cr0 + CbCr_bias) * Cr_R), 255 )
 Gt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_G + (Cr0 + CbCr_bias) * Cr_G), 255 )
 Bt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_B                              ), 255 )
 Ab0 = alpha
 Rb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 + Yp_bias) * Yp                               + (Cr0 + CbCr_bias) * Cr_R), 255 )
 Gb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_G + (Cr0 + CbCr_bias) * Cr_G), 255 )
 Bb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_B                              ), 255 )
 Ab1 = alpha
 Rb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 + Yp_bias) * Yp                               + (Cr0 + CbCr_bias) * Cr_R), 255 )
 Gb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_G + (Cr0 + CbCr_bias) * Cr_G), 255 )
 Bb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 + Yp_bias) * Yp + (Cb0 + CbCr_bias) * Cb_B                              ), 255 )
 
 uint8_t ARGB[16];
 ARGB[0]  = At0;
 ARGB[1]  = Rt0;
 ARGB[2]  = Gt0;
 ARGB[3]  = Bt0;
 ARGB[4]  = At1;
 ARGB[5]  = Rt1;
 ARGB[6]  = Gt1;
 ARGB[7]  = Bt1;
 ARGB[8]  = Ab0;
 ARGB[9]  = Rb0;
 ARGB[10] = Gb0;
 ARGB[11] = Bb0;
 ARGB[12] = Ab1;
 ARGB[13] = Rb1;
 ARGB[14] = Gb1;
 ARGB[15] = Bb1;
 
 uint8_t *destTPixel = dest.data;
 destTPixel[0]  = ARGB[permuteMap[0]];
 destTPixel[1]  = ARGB[permuteMap[1]];
 destTPixel[2]  = ARGB[permuteMap[2]];
 destTPixel[3]  = ARGB[permuteMap[3]];
 destTPixel[4]  = ARGB[permuteMap[0]+4];
 destTPixel[5]  = ARGB[permuteMap[1]+4];
 destTPixel[6]  = ARGB[permuteMap[2]+4];
 destTPixel[7]  = ARGB[permuteMap[3]+4];
 destTPixel += 8;
 uint8_t *destBPixel = dest.data + dest.rowBytes;
 destBPixel[0]  = ARGB[permuteMap[0]+8];
 destBPixel[1]  = ARGB[permuteMap[1]+8];
 destBPixel[2]  = ARGB[permuteMap[2]+8];
 destBPixel[3]  = ARGB[permuteMap[3]+8];
 destBPixel[4]  = ARGB[permuteMap[0]+12];
 destBPixel[5]  = ARGB[permuteMap[1]+12];
 destBPixel[6]  = ARGB[permuteMap[2]+12];
 destBPixel[7]  = ARGB[permuteMap[3]+12];
 destBPixel += 8;
 
 @/textblock
 </pre>
 
 @return
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_420Yp8_Cb8_Cr8ToARGB8888(const vImage_Buffer *srcYp, const vImage_Buffer *srcCb, const vImage_Buffer *srcCr, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To420Yp8_Cb8_Cr8
 
 @abstract Convert ARGB8888 to YUV 420Yp8_Cb8_Cr8 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 420Yp8_Cb8_Cr8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, provided that each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 420Yp8_Cb8_Cr8 format
 
 
 <pre> @textblock
 At0 Rt0 Gt0 Bt0  At1 Rt1 Gt1 Bt1  =>  Ypt0 Ypt1
 
 Ab0 Rb0 Gb0 Bb0  Ab1 Rb1 Gb1 Bb1      Ypb0 Ypb1
 
 Cb0
 
 Cr0
 
 @/textblock </pre>
 
 YUV 420Yp8_Cb8_Cr8 can be used for 'y420' and 'f420' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'y420' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 <pre>
 @textblock
 uint8_t *srcTPixel = src.data;
 At0 = srcTPixel[permuteMap[0]];
 Rt0 = srcTPixel[permuteMap[1]];
 Gt0 = srcTPixel[permuteMap[2]];
 Bt0 = srcTPixel[permuteMap[3]];
 srcTPixel += 4;
 At1 = srcTPixel[permuteMap[0]];
 Rt1 = srcTPixel[permuteMap[1]];
 Gt1 = srcTPixel[permuteMap[2]];
 Bt1 = srcTPixel[permuteMap[3]];
 srcTPixel += 4;
 uint8_t *srcBPixel = src.data + src.rowBytes;
 Ab0 = srcBPixel[permuteMap[0]];
 Rb0 = srcBPixel[permuteMap[1]];
 Gb0 = srcBPixel[permuteMap[2]];
 Bb0 = srcBPixel[permuteMap[3]];
 srcBPixel += 4;
 Ab1 = srcBPixel[permuteMap[0]];
 Rb1 = srcBPixel[permuteMap[1]];
 Gb1 = srcBPixel[permuteMap[2]];
 Bb1 = srcBPixel[permuteMap[3]];
 srcBPixel += 4;
 
 Ypt0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rt0 * R_Yp      + Gt0 * G_Yp + Bt0 * B_Yp     )
 Ypt1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rt1 * R_Yp      + Gt1 * G_Yp + Bt1 * B_Yp     )
 Ypb0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rb0 * R_Yp      + Gb0 * G_Yp + Bb0 * B_Yp     )
 Ypb1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rb1 * R_Yp      + Gb1 * G_Yp + Bb1 * B_Yp     )
 Cb0  = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( Rt0 * R_Cb      + Gt0 * G_Cb + Bt0 * B_Cb_R_Cr
 +   Rt1 * R_Cb      + Gt1 * G_Cb + Bt1 * B_Cb_R_Cr
 +   Rb0 * R_Cb      + Gb0 * G_Cb + Bb0 * B_Cb_R_Cr
 +   Rb1 * R_Cb      + Gb1 * G_Cb + Bb1 * B_Cb_R_Cr) / 4 )
 Cr0  = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( Rt0 * B_Cb_R_Cr + Gt0 * G_Cr + Bt0 * B_Cr
 +   Rt1 * B_Cb_R_Cr + Gt1 * G_Cr + Bt1 * B_Cr
 +   Rb0 * B_Cb_R_Cr + Gb0 * G_Cr + Bb0 * B_Cr
 +   Rb1 * B_Cb_R_Cr + Gb1 * G_Cr + Bb1 * B_Cr     ) / 4 )
 
 uint8_t *destYptPixel = destYp.data;
 uint8_t *destYpbPixel = destYp.data + destYp.rowBytes;
 destYptPixel[0] = Ypt0;
 destYptPixel[1] = Ypt1;
 destYpbPixel[0] = Ypb0;
 destYpbPixel[1] = Ypb1;
 destYptPixel += 2;
 destYpbPixel += 2;
 
 uint8_t *destCbPixel = destCb.data;
 uint8_t *destCrPixel = destCr.data;
 destCbPixel[0] = Cb0;
 destCrPixel[0] = Cr0;
 destCbPixel += 1;
 destCrPixel += 1;
 @/textblock
 </pre>
 
 @return
 <pre>
 @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To420Yp8_Cb8_Cr8(const vImage_Buffer *src, const vImage_Buffer *destYp, const vImage_Buffer *destCb, const vImage_Buffer *destCr, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 420Yp8_CbCr8 ('420v' and '420f') */

/*!
 @function vImageConvert_420Yp8_CbCr8ToARGB8888
 
 @abstract Convert YUV 420Yp8_CbCr8 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 420Yp8_CbCr8 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 420Yp8_CbCr8 format to ARGB8888
 
 
 Ypt0 Ypt1  =>  At0 Rt0 Gt0 Bt0  At1 Rt1 Gt1 Bt1
 
 Ypb0 Ypb1      Ab0 Rb0 Gb0 Bb0  Ab1 Rb1 Gb1 Bb1
 
 Cb0 Cr0
 
 
 YUV 420Yp8_CbCr8 can be used for '420v' and '420f' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert '420v' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre>
 @textblock
 
 uint8_t *srcYtPixel = srcY.data;
 uint8_t *srcYbPixel = srcY.data + srcY.rowBytes;
 Ypt0 = srcYtPixel[0];
 Ypt1 = srcYtPixel[1];
 srcYtPixel += 2;
 Ypb0 = srcYbPixel[0];
 Ypb1 = srcYbPixel[1];
 srcYbPixel += 2;
 
 uint8_t *srcCbCrPixel = srcCbCr.data;
 Cb0 = srcCbCrPixel[0];
 Cr0 = srcCbCrPixel[1];
 srcCrPixel += 2;
 
 At0 = alpha
 Rt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 Gt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 Bt0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 At1 = alpha
 Rt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 Gt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 Bt1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypt1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 Ab0 = alpha
 Rb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 Gb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 Bb0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 Ab1 = alpha
 Rb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 Gb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 Bb1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Ypb1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                              ), 255 )
 
 uint8_t ARGB[16];
 ARGB[0]  = At0;
 ARGB[1]  = Rt0;
 ARGB[2]  = Gt0;
 ARGB[3]  = Bt0;
 ARGB[4]  = At1;
 ARGB[5]  = Rt1;
 ARGB[6]  = Gt1;
 ARGB[7]  = Bt1;
 ARGB[8]  = Ab0;
 ARGB[9]  = Rb0;
 ARGB[10] = Gb0;
 ARGB[11] = Bb0;
 ARGB[12] = Ab1;
 ARGB[13] = Rb1;
 ARGB[14] = Gb1;
 ARGB[15] = Bb1;
 
 uint8_t *destTPixel = dest.data;
 destTPixel[0]  = ARGB[permuteMap[0]];
 destTPixel[1]  = ARGB[permuteMap[1]];
 destTPixel[2]  = ARGB[permuteMap[2]];
 destTPixel[3]  = ARGB[permuteMap[3]];
 destTPixel[4]  = ARGB[permuteMap[0]+4];
 destTPixel[5]  = ARGB[permuteMap[1]+4];
 destTPixel[6]  = ARGB[permuteMap[2]+4];
 destTPixel[7]  = ARGB[permuteMap[3]+4];
 destTPixel += 8;
 uint8_t *destBPixel = dest.data + dest.rowBytes;
 destBPixel[0]  = ARGB[permuteMap[0]+8];
 destBPixel[1]  = ARGB[permuteMap[1]+8];
 destBPixel[2]  = ARGB[permuteMap[2]+8];
 destBPixel[3]  = ARGB[permuteMap[3]+8];
 destBPixel[4]  = ARGB[permuteMap[0]+12];
 destBPixel[5]  = ARGB[permuteMap[1]+12];
 destBPixel[6]  = ARGB[permuteMap[2]+12];
 destBPixel[7]  = ARGB[permuteMap[3]+12];
 destBPixel += 8;
 
 @/textblock
 </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_420Yp8_CbCr8ToARGB8888(const vImage_Buffer *srcYp, const vImage_Buffer *srcCbCr, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To420Yp8_CbCr8
 
 @abstract Convert ARGB8888 to YUV 420Yp8_CbCr8 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 420Yp8_CbCr8 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as the channels don't repeat. For example, the pirate colorspace {0,1,1,1} (ARRR) is not supported, because the red channel appears more than once. (Pirates see red.)
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 420Yp8_CbCr8 format
 
 
 <pre> @textblock
 At0 Rt0 Gt0 Bt0  At1 Rt1 Gt1 Bt1  =>  Ypt0 Ypt1
 
 Ab0 Rb0 Gb0 Bb0  Ab1 Rb1 Gb1 Bb1      Ypb0 Ypb1
 
 Cb0 Cr0
 @/textblock </pre>
 
 
 YUV 420Yp8_CbCr8 can be used for '420v' and '420f' that are defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to '420v' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 <pre> @textblock
 uint8_t *srcTPixel = src.data;
 At0 = srcTPixel[permuteMap[0]];
 Rt0 = srcTPixel[permuteMap[1]];
 Gt0 = srcTPixel[permuteMap[2]];
 Bt0 = srcTPixel[permuteMap[3]];
 srcTPixel += 4;
 At1 = srcTPixel[permuteMap[0]];
 Rt1 = srcTPixel[permuteMap[1]];
 Gt1 = srcTPixel[permuteMap[2]];
 Bt1 = srcTPixel[permuteMap[3]];
 srcTPixel += 4;
 uint8_t *srcBPixel = src.data + src.rowBytes;
 Ab0 = srcBPixel[permuteMap[0]];
 Rb0 = srcBPixel[permuteMap[1]];
 Gb0 = srcBPixel[permuteMap[2]];
 Bb0 = srcBPixel[permuteMap[3]];
 srcBPixel += 4;
 Ab1 = srcBPixel[permuteMap[0]];
 Rb1 = srcBPixel[permuteMap[1]];
 Gb1 = srcBPixel[permuteMap[2]];
 Bb1 = srcBPixel[permuteMap[3]];
 srcBPixel += 4;
 
 Ypt0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rt0 * R_Yp      + Gt0 * G_Yp + Bt0 * B_Yp     )
 Ypt1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rt1 * R_Yp      + Gt1 * G_Yp + Bt1 * B_Yp     )
 Ypb0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rb0 * R_Yp      + Gb0 * G_Yp + Bb0 * B_Yp     )
 Ypb1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   Rb1 * R_Yp      + Gb1 * G_Yp + Bb1 * B_Yp     )
 Cb0  = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( Rt0 * R_Cb      + Gt0 * G_Cb + Bt0 * B_Cb_R_Cr
 +   Rt1 * R_Cb      + Gt1 * G_Cb + Bt1 * B_Cb_R_Cr
 +   Rb0 * R_Cb      + Gb0 * G_Cb + Bb0 * B_Cb_R_Cr
 +   Rb1 * R_Cb      + Gb1 * G_Cb + Bb1 * B_Cb_R_Cr) / 4 )
 Cr0  = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( Rt0 * B_Cb_R_Cr + Gt0 * G_Cr + Bt0 * B_Cr
 +   Rt1 * B_Cb_R_Cr + Gt1 * G_Cr + Bt1 * B_Cr
 +   Rb0 * B_Cb_R_Cr + Gb0 * G_Cr + Bb0 * B_Cr
 +   Rb1 * B_Cb_R_Cr + Gb1 * G_Cr + Bb1 * B_Cr     ) / 4 )
 
 uint8_t *destYptPixel = destYp.data;
 uint8_t *destYpbPixel = destYp.data + destYp.rowBytes;
 destYptPixel[0] = Ypt0;
 destYptPixel[1] = Ypt1;
 destYpbPixel[0] = Ypb0;
 destYpbPixel[1] = Ypb1;
 destYptPixel += 2;
 destYpbPixel += 2;
 
 uint8_t *destCbCrPixel = destCbCr.data;
 destCbCrPixel[0] = Cb0;
 destCbCrPixel[1] = Cr0;
 destCbCrPixel += 2;
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock
 </pre>
 
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To420Yp8_CbCr8(const vImage_Buffer *src, const vImage_Buffer *destYp, const vImage_Buffer *destCbCr, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 444AYpCbCr16 ('y416') */

/*!
 @function vImageConvert_444AYpCbCr16ToARGB8888
 
 @abstract Convert YUV 444AYpCbCr16 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 444AYpCbCr16 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444AYpCbCr16 format to ARGB8888
 
 
 A0 Yp0 Cb0 Cr0  =>  A0 R0 G0 B0
 
 
 YUV 444AYpCbCr8 can be used for 'y416' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'y416' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint16_t *srcPixel = src.data;
 A0  = srcPixel[0];
 Yp0 = srcPixel[1];
 Cb0 = srcPixel[2];
 Cr0 = srcPixel[3];
 srcPixel += 4;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[4];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @result
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444AYpCbCr16ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To444AYpCbCr16
 
 @abstract Convert ARGB8888 to YUV 444AYpCbCr16 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444AYpCbCr16 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as the values are unique.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 444AYpCbCr16 format
 
 
 A0 R0 G0 B0  =>  A0 Yp0 Cb0 Cr0
 
 
 YUV 444AYpCbCr8 can be used for 'y416' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'y416' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = A0;
 destPixel[1] = Yp0;
 destPixel[2] = Cb0;
 destPixel[3] = Cr0;
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To444AYpCbCr16(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_444AYpCbCr16ToARGB16U
 
 @abstract Convert YUV 444AYpCbCr16 format to ARGB16U
 
 @param src
 A pointer to vImage_Buffer that references YUV 444AYpCbCr16 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444AYpCbCr16 format to ARGB16U
 
 
 A0 Yp0 Cb0 Cr0  =>  A0 R0 G0 B0
 
 
 YUV 444AYpCbCr8 can be used for 'y416' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'y416' with ITU 601 video range to ARGB16U, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint16_t *srcPixel = src.data;
 A0  = srcPixel[0];
 Yp0 = srcPixel[1];
 Cb0 = srcPixel[2];
 Cr0 = srcPixel[3];
 srcPixel += 4;
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 65535 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 65535 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 65535 )
 
 uint16_t ARGB[4];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444AYpCbCr16ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16UTo444AYpCbCr16
 
 @abstract Convert ARGB8888 to YUV 444AYpCbCr16 format
 
 @param src
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444AYpCbCr16 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 Any ordering of channels is supported as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB16U to YUV 444AYpCbCr16 format
 
 
 A0 R0 G0 B0  =>  A0 Yp0 Cb0 Cr0
 
 
 YUV 444AYpCbCr8 can be used for 'y416' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16U to 'y416' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint16_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = A0;
 destPixel[1] = Yp0;
 destPixel[2] = Cb0;
 destPixel[3] = Cr0;
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16UTo444AYpCbCr16(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup 444CrYpCb10 ('v410') */

/*!
 @function vImageConvert_444CrYpCb10ToARGB8888
 
 @abstract Convert YUV 444CrYpCb10 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 444CrYpCb10 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444CrYpCb10 format to ARGB8888
 
 
 3 10-bit unsigned components are packed into a 32-bit little-endian word.
 
 <pre> @textblock
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr            10-bit Yp            10-bit Cb                   =>  A0 R0 G0 B0
 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  X X
 @/textblock </pre>
 
 
 YUV 444CrYpCb10 can be used for 'v410' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'v410' with ITU 601 video range to ARGB8888, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint32_t *srcPixel = src.data;
 uint32_t pixel = *srcPixel;
 srcPixel += 1;
 
 Yp0 = getYpFromv410(pixel);
 Cb0 = getCbFromv410(pixel);
 Cr0 = getCrFromv410(pixel);
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444CrYpCb10ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To444CrYpCb10
 
 @abstract Convert ARGB8888 to YUV 444CrYpCb10 format
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444CrYpCb10 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion Convert ARGB8888 to YUV 444CrYpCb10 format
 
 
 3 10-bit unsigned components are packed into a 32-bit little-endian word.
 
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 A0 R0 G0 B0  =>  10-bit Cr            10-bit Yp            10-bit Cb
 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  X X
 
 
 YUV 444CrYpCb10 can be used for 'v410' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'v410' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint8_t *srcPixel = src.data;
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 pixel = makev410(Yp0, Cb0, Cr0);
 
 uint32_t *destPixel = dest.data;
 *destPixel = pixel;
 destPixel += 1;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To444CrYpCb10(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_444CrYpCb10ToARGB16Q12
 
 @abstract Convert YUV 444CrYpCb10 format to ARGB16Q12
 
 @param src
 A pointer to vImage_Buffer that references YUV 444CrYpCb10 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 
 @param alpha
 A 16Q12 value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 444CrYpCb10 format to ARGB16Q12
 
 
 3 10-bit unsigned components are packed into a 32-bit little-endian word.
 
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr            10-bit Yp            10-bit Cb                   =>  A0 R0 G0 B0
 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  X X
 
 
 YUV 444CrYpCb10 can be used for 'v410' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert 'v410' with ITU 601 video range to ARGB16Q12, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint32_t *srcPixel = src.data;
 uint32_t pixel = *srcPixel;
 srcPixel += 1;
 
 Yp0 = getYpFromv410(pixel);
 Cb0 = getCbFromv410(pixel);
 Cr0 = getCrFromv410(pixel);
 
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 4096 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 4096 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                            ), 4096 )
 
 uint16_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_444CrYpCb10ToARGB16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const Pixel_16Q12 alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16Q12To444CrYpCb10
 
 @abstract Convert ARGB16Q12 to YUV 444CrYpCb10 format
 
 @param src
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 444CrYpCb10 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 Any ordering of channels is supported as long as each channel appears only once.
 
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB16Q12 to YUV 444CrYpCb10 format
 
 
 3 10-bit unsigned components are packed into a 32-bit little-endian word.
 <pre> @textblock
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 A0 R0 G0 B0  =>  10-bit Cr            10-bit Yp            10-bit Cb
 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  X X
 @/textblock </pre>
 
 
 YUV 444CrYpCb10 can be used for 'v410' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16Q12 to 'v410' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint16_t *srcPixel = src.data;
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   + R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr)
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr     )
 
 pixel = makev410(Yp0, Cb0, Cr0);
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = pixel;
 destPixel += 1;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16Q12To444CrYpCb10(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 422CrYpCbYpCbYpCbYpCrYpCrYp10 ('v210')   */

/*!
 @function vImageConvert_422CrYpCbYpCbYpCbYpCrYpCrYp10ToARGB8888
 
 @abstract Convert YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format to ARGB8888
 
 
 6 packed YUV pixels are getting mapped into 6 ARGB8888 pixels.
 
 12 10-bit unsigned components are packed into 4 32-bit little-endian words.
 <pre> @textblock
 
 Word0
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr0           10-bit Y0            10-bit Cb0
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word1
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y2            10-bit Cb1           10-bit Y1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word2
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cb2           10-bit Y3            10-bit Cr1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word3
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y5            10-bit Cr2           10-bit Y4
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 
 =>  A0 R0 G0 B0  A1 R1 G1 B1  A2 R2 G2 B2  A3 R3 G3 B3  A4 R4 G4 B4  A5 R5 G5 B5
 @/textblock </pre>
 
 YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 can be used for 'v210' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'v210' with ITU 601 video range, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 uint32_t *srcPixel = src.data;
 pixel0 = srcPixel[0];
 pixel1 = srcPixel[1];
 pixel2 = srcPixel[2];
 pixel3 = srcPixel[3];
 srcPixel += 4;
 
 Yp0 = getYp0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp1 = getYp1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp2 = getYp2Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp3 = getYp3Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp4 = getYp4Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp5 = getYp5Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb0 = getCb0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb1 = getCb1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb2 = getCb2Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr0 = getCr0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr1 = getCr1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr2 = getCr2Fromv210(pixel0, pixel1, pixel2, pixel3);
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 A2 = alpha
 R2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp                            + (Cr1 - CbCr_bias) * Cr_R), 255 )
 G2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_G + (Cr1 - CbCr_bias) * Cr_G), 255 )
 B2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_B                           ), 255 )
 A3 = alpha
 R3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp                            + (Cr1 - CbCr_bias) * Cr_R), 255 )
 G3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_G + (Cr1 - CbCr_bias) * Cr_G), 255 )
 B3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_B                           ), 255 )
 A4 = alpha
 R4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp                            + (Cr2 - CbCr_bias) * Cr_R), 255 )
 G4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_G + (Cr2 - CbCr_bias) * Cr_G), 255 )
 B4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_B                           ), 255 )
 A5 = alpha
 R5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp                            + (Cr2 - CbCr_bias) * Cr_R), 255 )
 G5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_G + (Cr2 - CbCr_bias) * Cr_G), 255 )
 B5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[24];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 ARGB[8] = A2;
 ARGB[9] = R2;
 ARGB[10] = G2;
 ARGB[11] = B2;
 ARGB[12] = A3;
 ARGB[13] = R3;
 ARGB[14] = G3;
 ARGB[15] = B3;
 ARGB[16] = A4;
 ARGB[17] = R4;
 ARGB[18] = G4;
 ARGB[19] = B4;
 ARGB[20] = A5;
 ARGB[21] = R5;
 ARGB[22] = G5;
 ARGB[23] = B5;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel[8] = ARGB[permuteMap[0]+8];
 destPixel[9] = ARGB[permuteMap[1]+8];
 destPixel[10] = ARGB[permuteMap[2]+8];
 destPixel[11] = ARGB[permuteMap[3]+8];
 destPixel[12] = ARGB[permuteMap[0]+12];
 destPixel[13] = ARGB[permuteMap[1]+12];
 destPixel[14] = ARGB[permuteMap[2]+12];
 destPixel[15] = ARGB[permuteMap[3]+12];
 destPixel[16] = ARGB[permuteMap[0]+16];
 destPixel[17] = ARGB[permuteMap[1]+16];
 destPixel[18] = ARGB[permuteMap[2]+16];
 destPixel[19] = ARGB[permuteMap[3]+16];
 destPixel[20] = ARGB[permuteMap[0]+20];
 destPixel[21] = ARGB[permuteMap[1]+20];
 destPixel[22] = ARGB[permuteMap[2]+20];
 destPixel[23] = ARGB[permuteMap[3]+20];
 destPixel += 24;
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422CrYpCbYpCbYpCbYpCrYpCrYp10ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To422CrYpCbYpCbYpCbYpCrYpCrYp10
 
 @abstract Convert ARGB8888 to YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format
 
 
 6 ARGB8888 pixels are getting mapped into 6 packed YUV pixels.
 
 A0 R0 G0 B0  A1 R1 G1 B1  A2 R2 G2 B2  A3 R3 G3 B3  A4 R4 G4 B4  A5 R5 G5 B5  =>
 
 12 10-bit unsigned components are packed into 4 32-bit little-endian words.
 
 <pre> @textblock
 Word0
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr0           10-bit Y0            10-bit Cb0
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word1
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y2            10-bit Cb1           10-bit Y1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word2
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cb2           10-bit Y3            10-bit Cr1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word3
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y5            10-bit Cr2           10-bit Y4
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 @/textblock </pre>
 
 YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 can be used for 'v210' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 'v210' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 <pre> @textblock
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A2 = srcPixel[permuteMap[0]];
 R2 = srcPixel[permuteMap[1]];
 G2 = srcPixel[permuteMap[2]];
 B2 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A3 = srcPixel[permuteMap[0]];
 R3 = srcPixel[permuteMap[1]];
 G3 = srcPixel[permuteMap[2]];
 B3 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A4 = srcPixel[permuteMap[0]];
 R4 = srcPixel[permuteMap[1]];
 G4 = srcPixel[permuteMap[2]];
 B4 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A5 = srcPixel[permuteMap[0]];
 R5 = srcPixel[permuteMap[1]];
 G5 = srcPixel[permuteMap[2]];
 B5 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 Yp2 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R2 * R_Yp      + G2 * G_Yp + B2 * B_Yp     )
 Yp3 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R3 * R_Yp      + G3 * G_Yp + B3 * B_Yp     )
 Cb1 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R2 * R_Cb      + G2 * G_Cb + B2 * B_Cb_R_Cr
 +   R3 * R_Cb      + G3 * G_Cb + B3 * B_Cb_R_Cr) / 2 )
 Cr1 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R2 * B_Cb_R_Cr + G2 * G_Cr + B2 * B_Cr
 +   R3 * B_Cb_R_Cr + G3 * G_Cr + B3 * B_Cr     ) / 2 )
 Yp4 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R4 * R_Yp      + G4 * G_Yp + B4 * B_Yp     )
 Yp5 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R5 * R_Yp      + G5 * G_Yp + B5 * B_Yp     )
 Cb2 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R4 * R_Cb      + G4 * G_Cb + B4 * B_Cb_R_Cr
 +   R5 * R_Cb      + G5 * G_Cb + B5 * B_Cb_R_Cr) / 2 )
 Cr2 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R4 * B_Cb_R_Cr + G4 * G_Cr + B4 * B_Cr
 +   R5 * B_Cb_R_Cr + G5 * G_Cr + B5 * B_Cr     ) / 2 )
 
 uint32_t *destPixel = dest.data;
 packv210AndStore(destPixel, Yp0, Yp1, Yp2, Yp3, Yp4, Yp5, Cb0, Cb1, Cb2, Cr0, Cr1, Cr2);
 destPixel += 4;
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To422CrYpCbYpCbYpCbYpCrYpCrYp10(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_422CrYpCbYpCbYpCbYpCrYpCrYp10ToARGB16Q12
 
 @abstract Convert YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format to ARGB16Q12
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format to ARGB16Q12
 
 
 6 packed YUV pixels are getting mapped into 6 ARGB16Q12 pixels.
 
 12 10-bit unsigned components are packed into 4 32-bit little-endian words.
 <pre> @textblock
 Word0
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr0           10-bit Y0            10-bit Cb0
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word1
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y2            10-bit Cb1           10-bit Y1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word2
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cb2           10-bit Y3            10-bit Cr1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word3
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y5            10-bit Cr2           10-bit Y4
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 
 =>  A0 R0 G0 B0  A1 R1 G1 B1  A2 R2 G2 B2  A3 R3 G3 B3  A4 R4 G4 B4  A5 R5 G5 B5
 @/textblock </pre>
 
 YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 can be used for 'v210' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16Q12 to 'v210' with ITU 601 video range, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint32_t *srcPixel = src.data;
 pixel0 = srcPixel[0];
 pixel1 = srcPixel[1];
 pixel2 = srcPixel[2];
 pixel3 = srcPixel[3];
 srcPixel += 4;
 
 Yp0 = getYp0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp1 = getYp1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp2 = getYp2Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp3 = getYp3Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp4 = getYp4Fromv210(pixel0, pixel1, pixel2, pixel3);
 Yp5 = getYp5Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb0 = getCb0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb1 = getCb1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cb2 = getCb2Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr0 = getCr0Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr1 = getCr1Fromv210(pixel0, pixel1, pixel2, pixel3);
 Cr2 = getCr2Fromv210(pixel0, pixel1, pixel2, pixel3);
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 4096 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 4096 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 4096 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 4096 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 4096 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 4096 )
 A2 = alpha
 R2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp                            + (Cr1 - CbCr_bias) * Cr_R), 4096 )
 G2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_G + (Cr1 - CbCr_bias) * Cr_G), 4096 )
 B2 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp2 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_B                           ), 4096 )
 A3 = alpha
 R3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp                            + (Cr1 - CbCr_bias) * Cr_R), 4096 )
 G3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_G + (Cr1 - CbCr_bias) * Cr_G), 4096 )
 B3 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp3 - Yp_bias) * Yp + (Cb1 - CbCr_bias) * Cb_B                           ), 4096 )
 A4 = alpha
 R4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp                            + (Cr2 - CbCr_bias) * Cr_R), 4096 )
 G4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_G + (Cr2 - CbCr_bias) * Cr_G), 4096 )
 B4 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp4 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_B                           ), 4096 )
 A5 = alpha
 R5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp                            + (Cr2 - CbCr_bias) * Cr_R), 4096 )
 G5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_G + (Cr2 - CbCr_bias) * Cr_G), 4096 )
 B5 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp5 - Yp_bias) * Yp + (Cb2 - CbCr_bias) * Cb_B                           ), 4096 )
 
 uint16_t ARGB[24];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 ARGB[8] = A2;
 ARGB[9] = R2;
 ARGB[10] = G2;
 ARGB[11] = B2;
 ARGB[12] = A3;
 ARGB[13] = R3;
 ARGB[14] = G3;
 ARGB[15] = B3;
 ARGB[16] = A4;
 ARGB[17] = R4;
 ARGB[18] = G4;
 ARGB[19] = B4;
 ARGB[20] = A5;
 ARGB[21] = R5;
 ARGB[22] = G5;
 ARGB[23] = B5;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel[8] = ARGB[permuteMap[0]+8];
 destPixel[9] = ARGB[permuteMap[1]+8];
 destPixel[10] = ARGB[permuteMap[2]+8];
 destPixel[11] = ARGB[permuteMap[3]+8];
 destPixel[12] = ARGB[permuteMap[0]+12];
 destPixel[13] = ARGB[permuteMap[1]+12];
 destPixel[14] = ARGB[permuteMap[2]+12];
 destPixel[15] = ARGB[permuteMap[3]+12];
 destPixel[16] = ARGB[permuteMap[0]+16];
 destPixel[17] = ARGB[permuteMap[1]+16];
 destPixel[18] = ARGB[permuteMap[2]+16];
 destPixel[19] = ARGB[permuteMap[3]+16];
 destPixel[20] = ARGB[permuteMap[0]+20];
 destPixel[21] = ARGB[permuteMap[1]+20];
 destPixel[22] = ARGB[permuteMap[2]+20];
 destPixel[23] = ARGB[permuteMap[3]+20];
 destPixel += 24;
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422CrYpCbYpCbYpCbYpCrYpCrYp10ToARGB16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const Pixel_16Q12 alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16Q12To422CrYpCbYpCbYpCbYpCrYpCrYp10
 
 @abstract Convert ARGB16Q12 to YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format.
 
 @param src
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 Any ordering of channels is supported as long as each channel appears only once.
 
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB16Q12 to YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 format
 
 
 6 ARGB16Q12 pixels are getting mapped into 6 packed YUV pixels.
 <pre> @textblock
 
 A0 R0 G0 B0  A1 R1 G1 B1  A2 R2 G2 B2  A3 R3 G3 B3  A4 R4 G4 B4  A5 R5 G5 B5  =>
 
 12 10-bit unsigned components are packed into 4 32-bit little-endian words.
 
 Word0
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cr0           10-bit Y0            10-bit Cb0
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word1
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y2            10-bit Cb1           10-bit Y1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word2
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Cb2           10-bit Y3            10-bit Cr1
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 
 Word3
 Decreasing Address order (32-bit little-endian)
 byte3           byte2            byte1            byte0
 10-bit Y5            10-bit Cr2           10-bit Y4
 X X 9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0  9 8 7 6 5 4 3 2 1 0
 @/textblock </pre>
 
 YUV 422CrYpCbYpCbYpCbYpCrYpCrYp10 can be used for 'v210' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16Q12 to 'v210' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 
 <pre> @textblock
 
 uint16_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A2 = srcPixel[permuteMap[0]];
 R2 = srcPixel[permuteMap[1]];
 G2 = srcPixel[permuteMap[2]];
 B2 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A3 = srcPixel[permuteMap[0]];
 R3 = srcPixel[permuteMap[1]];
 G3 = srcPixel[permuteMap[2]];
 B3 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A4 = srcPixel[permuteMap[0]];
 R4 = srcPixel[permuteMap[1]];
 G4 = srcPixel[permuteMap[2]];
 B4 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A5 = srcPixel[permuteMap[0]];
 R5 = srcPixel[permuteMap[1]];
 G5 = srcPixel[permuteMap[2]];
 B5 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 Yp2 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R2 * R_Yp      + G2 * G_Yp + B2 * B_Yp     )
 Yp3 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R3 * R_Yp      + G3 * G_Yp + B3 * B_Yp     )
 Cb1 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R2 * R_Cb      + G2 * G_Cb + B2 * B_Cb_R_Cr
 +   R3 * R_Cb      + G3 * G_Cb + B3 * B_Cb_R_Cr) / 2 )
 Cr1 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R2 * B_Cb_R_Cr + G2 * G_Cr + B2 * B_Cr
 +   R3 * B_Cb_R_Cr + G3 * G_Cr + B3 * B_Cr     ) / 2 )
 Yp4 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R4 * R_Yp      + G4 * G_Yp + B4 * B_Yp     )
 Yp5 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R5 * R_Yp      + G5 * G_Yp + B5 * B_Yp     )
 Cb2 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R4 * R_Cb      + G4 * G_Cb + B4 * B_Cb_R_Cr
 +   R5 * R_Cb      + G5 * G_Cb + B5 * B_Cb_R_Cr) / 2 )
 Cr2 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R4 * B_Cb_R_Cr + G4 * G_Cr + B4 * B_Cr
 +   R5 * B_Cb_R_Cr + G5 * G_Cr + B5 * B_Cr     ) / 2 )
 
 uint32_t *destPixel = dest.data;
 packv210AndStore(destPixel, Yp0, Yp1, Yp2, Yp3, Yp4, Yp5, Cb0, Cb1, Cb2, Cr0, Cr1, Cr2);
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16Q12To422CrYpCbYpCbYpCbYpCrYpCrYp10(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup 422CbYpCrYp16(bitdepth) ('v216')    */

/*!
 @function vImageConvert_422CbYpCrYp16ToARGB8888
 
 @abstract Convert YUV 422CbYpCrYp16 format to ARGB8888
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp16 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param alpha
 A value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 
 @discussion Convert YUV 422CbYpCrYp16 format to ARGB8888
 <pre> @textblock
 
 pixel0-1
 byte0 byte1   byte2 byte3   byte4 byte5   byte6 byte7
 LE-16-bit-Cb  LE-16-bit-Y0  LE-16-bit-Cr  LE-16-bit-Y1  =>  A0 R0 G0 B0  A1 R1 G1 B1
 
 (LE and left-justified 16-bit-per-component)
 @/textblock </pre>
 
 YUV 422CbYpCrYp16 can be used for 16-bit 'v216' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 16-bit 'v216' with ITU 601 video range, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint64_t *srcPixel = src.data;
 uint64_t pixel = *srcPixel;
 Yp0 = getYp0From16bitv216(pixel);
 Cb0 = getCb0From16bitv216(pixel);
 Yp1 = getYp1From16bitv216(pixel);
 Cr0 = getCr0From16bitv216(pixel);
 srcPixel += 1;
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 255 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 255 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 255 )
 
 uint8_t ARGB[8];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel += 8;
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock <pre>
 
 Results are guaranteed to be faithfully rounded.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422CbYpCrYp16ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint8_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888To422CbYpCrYp16
 
 @abstract Convert ARGB8888 to YUV 422CbYpCrYp16 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp16 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB8888 to YUV 422CbYpCrYp16 format
 <pre> @textblock
 
 pixel0-1
 byte0 byte1   byte2 byte3   byte4 byte5   byte6 byte7
 A0 R0 G0 B0  A1 R1 G1 B1  =>  LE-16-bit-Cb  LE-16-bit-Y0  LE-16-bit-Cr  LE-16-bit-Y1
 
 (LE and left-justified 16-bit-per-component)
 @/textblock </pre>
 
 YUV 422CbYpCrYp16 can be used for 16-bit 'v216' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB8888 to 16-bit 'v216' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint8_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R0 * R_Yp      + G0 * G_Yp + B0 * B_Yp     )
 Yp1 = ROUND_TO_NEAREST_INTEGER( Yp_bias   +   R1 * R_Yp      + G1 * G_Yp + B1 * B_Yp     )
 Cb0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * R_Cb      + G0 * G_Cb + B0 * B_Cb_R_Cr
 +   R1 * R_Cb      + G1 * G_Cb + B1 * B_Cb_R_Cr) / 2 )
 Cr0 = ROUND_TO_NEAREST_INTEGER( CbCr_bias + ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr
 +   R1 * B_Cb_R_Cr + G1 * G_Cr + B1 * B_Cr     ) / 2 )
 
 uint64_t *destPixel = dest.data;
 pack16bitv216AndStore(destPixel, Yp0, Yp1, Cb0, Cr0);
 destPixel += 1;
 
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 This function can work in place.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB8888To422CbYpCrYp16(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_422CbYpCrYp16ToARGB16U
 
 @abstract Convert YUV 422CbYpCrYp16 format to ARGB16U
 
 @param src
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp16 source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 
 @param info
 A pointer to vImage_YpCbCrToARGB which contains info coeffcient and preBias values.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 
 @param alpha
 A 16-bit value for alpha channel in dest.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 
 @discussion Convert YUV 422CbYpCrYp16 format to ARGB16U
 <pre> @textblock
 
 pixel0-1
 byte0 byte1   byte2 byte3   byte4 byte5   byte6 byte7
 LE-16-bit-Cb  LE-16-bit-Y0  LE-16-bit-Cr  LE-16-bit-Y1  =>  A0 R0 G0 B0  A1 R1 G1 B1
 
 (LE and left-justified 16-bit-per-component)
 @/textblock </pre>
 
 YUV 422CbYpCrYp16 can be used for 16-bit 'v216' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16U to 16-bit 'v216' with ITU 601 video range, then we need
 generate vImage_YpCbCrToARGB by vImageConvert_YpCbCrToARGB_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, Yp, Cr_R, Cb_G, Cr_G, and Cb_B are calculated and converted into the right
 format by vImageConvert_YpCbCrToARGB_GenerateConversion() inside of vImage_YpCbCrToARGB.
 
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint64_t *srcPixel = src.data;
 uint64_t pixel = *srcPixel;
 Yp0 = getYp0From16bitv216(pixel);
 Cb0 = getCb0From16bitv216(pixel);
 Yp1 = getYp1From16bitv216(pixel);
 Cr0 = getCr0From16bitv216(pixel);
 srcPixel += 1;
 
 A0 = alpha
 R0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 65535 )
 G0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 65535 )
 B0 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp0 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 65535 )
 A1 = alpha
 R1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp                            + (Cr0 - CbCr_bias) * Cr_R), 65535 )
 G1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_G + (Cr0 - CbCr_bias) * Cr_G), 65535 )
 B1 = CLAMP(0, ROUND_TO_NEAREST_INTEGER((Yp1 - Yp_bias) * Yp + (Cb0 - CbCr_bias) * Cb_B                           ), 65535 )
 
 uint16_t ARGB[8];
 ARGB[0] = A0;
 ARGB[1] = R0;
 ARGB[2] = G0;
 ARGB[3] = B0;
 ARGB[4] = A1;
 ARGB[5] = R1;
 ARGB[6] = G1;
 ARGB[7] = B1;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel[4] = ARGB[permuteMap[0]+4];
 destPixel[5] = ARGB[permuteMap[1]+4];
 destPixel[6] = ARGB[permuteMap[2]+4];
 destPixel[7] = ARGB[permuteMap[3]+4];
 destPixel += 8;
 
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_422CbYpCrYp16ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_YpCbCrToARGB *info, const uint8_t permuteMap[4], const uint16_t alpha, vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16UTo422CbYpCrYp16
 
 @abstract Convert ARGB16U to YUV 422CbYpCrYp16 format.
 
 @param src
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references YUV 422CbYpCrYp16 destination pixels.
 
 @param info
 A pointer to vImage_ARGBToYpCbCr which contains info coeffcient and postBias values.
 This is generated by vImageConvert_ARGBToYpCbCr_GenerateConversion().
 
 @param permuteMap
 Values that can be used to switch the channel order of src.
 permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 
 @param flags
 <pre> @textblock
 kvImageGetTempBufferSize    Returns 0. Does no work.
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 
 
 @discussion Convert ARGB16U to YUV 422CbYpCrYp16 format
 <pre> @textblock
 
 pixel0-1
 byte0 byte1   byte2 byte3   byte4 byte5   byte6 byte7
 A0 R0 G0 B0  A1 R1 G1 B1  =>  LE-16-bit-Cb  LE-16-bit-Y0  LE-16-bit-Cr  LE-16-bit-Y1
 
 (LE and left-justified 16-bit-per-component)
 @/textblock </pre>
 
 YUV 422CbYpCrYp16 can be used for 16-bit 'v216' that is defined in CVPixelBuffer.h.
 
 For example, if we want to use this function to convert ARGB16U to 16-bit 'v216' with ITU 601 video range, then we need
 generate vImage_ARGBToYpCbCr by vImageConvert_ARGBToYpCbCr_GenerateConversion() and call this function.
 
 Yp_bias, CbCr_bias, CbCr_bias, R_Yp, G_Yp, B_Yp, R_Cb, G_Cb, B_Cb_R_Cr, G_Cr and B_Cr are calculated and
 converted into the right format by vImageConvert_ARGBToYpCbCr_GenerateConversion() inside of vImage_ARGBToYpCbCr.
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint16_t *srcPixel = src.data;
 A0 = srcPixel[permuteMap[0]];
 R0 = srcPixel[permuteMap[1]];
 G0 = srcPixel[permuteMap[2]];
 B0 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 A1 = srcPixel[permuteMap[0]];
 R1 = srcPixel[permuteMap[1]];
 G1 = srcPixel[permuteMap[2]];
 B1 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 Yp0 = Yp_bias + ROUND_TO_NEAREST_INTEGER( R0 * R_Yp + G0 * G_Yp + B0 * B_Yp )
 Yp1 = Yp_bias + ROUND_TO_NEAREST_INTEGER( R1 * R_Yp + G1 * G_Yp + B1 * B_Yp )
 R0 += R1;  G0 += G1;   B0 += B1;
 Cb0 = CbCr_bias + ROUND_TO_NEAREST_INTEGER( ( R0 * R_Cb + G0 * G_Cb + B0 * B_Cb_R_Cr) / 2 )
 Cr0 = CbCr_bias + ROUND_TO_NEAREST_INTEGER( ( R0 * B_Cb_R_Cr + G0 * G_Cr + B0 * B_Cr) / 2 )
 
 uint64_t *destPixel = dest.data;
 pack16bitv216AndStore(destPixel, Yp0, Yp1, Cb0, Cr0);
 destPixel += 1;
 
 @/textblock </pre>
 
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 Chroma is sampled at center by default.
 
 Note: vImage doesn't do anything with the alpha here. It is just thrown away. The operation is therefore best suited for kCGImageAlphaNoneSkip<First/Last>
 images.  If it has alpha, you may wish to composite against an opaque background first, before the transparency information is lost.
 If it is premultiplied by alpha, you at minimum should unpremultiply it first, or composite it against an opaque background. See
 vImageUnpremultiplyData_ARGB16U() and vImageFlatten_ARGB16U().
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16UTo422CbYpCrYp16(const vImage_Buffer *src, const vImage_Buffer *dest, const vImage_ARGBToYpCbCr *info, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup RGBA1010102    */

/*!
 @function vImageConvert_RGBA1010102ToARGB8888
 
 @abstract Convert RGBA1010102 to ARGB8888 format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels. Source pixels must be at least 4 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels. Destination pixels may have any alignment.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 \p kvImageDoNotTile        Disables internal multithreading, if any.
 
 @discussion
 RGBA1010102 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 @endcode
 
 The per-pixel operation is:
 @code
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t A2  = pixel & 0x3;
 int32_t R10 = (pixel >> 22) & 0x3ff;
 int32_t G10 = (pixel >> 12) & 0x3ff;
 int32_t B10 = (pixel >>  2) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 A2  = (A2 * UCHAR_MAX + 1) / 3;
 R10 = ((R10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 G10 = ((G10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 B10 = ((B10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 
 uint8_t R8, G8, B8;
 R8 = CLAMP(0, R10, UCHAR_MAX);
 G8 = CLAMP(0, G10, UCHAR_MAX);
 B8 = CLAMP(0, B10, UCHAR_MAX);
 
 uint8_t ARGB[4];
 ARGB[0] = A2;
 ARGB[1] = R8;
 ARGB[2] = G8;
 ARGB[3] = B8;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @endcode
 
 @return kvImageNoError                     Is returned when there was no error.
 @return kvImageUnknownFlagsBit             Is returned when there is a unknown flag.
 @return kvImageRoiLargerThanInputBuffer    Is returned when src.width < dest.width || src.height < dest.height
 @return kvImageInvalidParameter            Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax.
 
 @note Results are guaranteed to be faithfully rounded.
 */
VIMAGE_PF vImage_Error vImageConvert_RGBA1010102ToARGB8888(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB8888ToRGBA1010102
 
 @abstract Convert ARGB8888 to RGBA1010102 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels. Source pixels may have any alignment.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.  Destination pixels must be at least 4 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of src.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 \p kvImageDoNotTile    Disables internal multithreading, if any.
 
 @discussion
 RGBA1010102 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 @endcode
 
 The per-pixel operation is:
 @code
 uint8_t *srcPixel = src.data;
 A8 = srcPixel[permuteMap[0]];
 R8 = srcPixel[permuteMap[1]];
 G8 = srcPixel[permuteMap[2]];
 B8 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 int32_t rounding = UCHAR_MAX >> 1;
 R10 = ((R8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 G10 = ((G8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 B10 = ((B8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 A10 = ((A10 * 3 + rounding) / UCHAR_MAX);
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = htonl((R10 << 22) | (G10 << 12) | (B10 << 2) | A10);
 destPixel += 1;
 @endcode
 
 @return kvImageNoError                  Is returned when there was no error.
 @return kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax.
 
 @note Results are guaranteed to be faithfully rounded.
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888ToRGBA1010102(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_RGBA1010102ToARGB16Q12
 
 @abstract Convert RGBA1010102 to ARGB16Q12 format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.  Samples must be at least 4 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved destination pixels.  Samples must be at least 2 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel is unique.  That is, ARRG is not an allowed order
 because R is repeated.
 
 @param flags
 <pre> @textblock
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 
 @discussion Convert RGBA1010102 to ARGB16Q12 format
 
 RGBA1010102 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 
 <pre> @textblock
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 @/textblock </pre>
 
 The per-pixel operation is:
 
 <pre> @textblock
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t A2  = pixel & 0x3;
 int32_t R10 = (pixel >> 22) & 0x3ff;
 int32_t G10 = (pixel >> 12) & 0x3ff;
 int32_t B10 = (pixel >>  2) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 int16_t R16, G16, B16;
 A2  = (A2 * 4096 + 1) / 3;
 R16 = ((R10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 G16 = ((G10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 B16 = ((B10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 
 R16 = CLAMP(INT16_MIN, R16, INT16_MAX);
 G16 = CLAMP(INT16_MIN, G16, INT16_MAX);
 B16 = CLAMP(INT16_MIN, B16, INT16_MAX);
 
 int16_t ARGB[4];
 ARGB[0] = A2;
 ARGB[1] = R16;
 ARGB[2] = G16;
 ARGB[3] = B16;
 
 int16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax.
 @/textblock </pre>
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_RGBA1010102ToARGB16Q12(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16Q12ToRGBA1010102
 
 @abstract Convert ARGB16Q12 to RGBA1010102 format.
 
 @param src
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved source pixels.   ARGB16Q12 pixels must be at least 2 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.  RGBA1010102 pixels must be at least 4 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for the range of 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for the range of 10-bit RGB pixel.
 
 @param RGB101010Max
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010Min
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 
 @param flags
 <pre> @textblock
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB16Q12 to RGBA1010102 format.
 
 RGBA1010102 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 
 <pre> @textblock
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 @/textblock </pre>
 
 RGB101010Max & RGB101010Min are available to specify the min / max of the representation.
 This will be used as clipping the results.
 
 <pre> @textblock
 RGB101010Max  = 1023;
 RGB101010Min  = 0;
 @/textblock </pre>
 
 This is needed because 16Q12 has a chance to be outside of [0.0, 1.0] range and we are converting those
 values into video-range. Then, there will be some numbers outside of 10-bit video-range and we want those
 values to be representable as much as possible.
 
 The per-pixel operation is:
 <pre> @textblock
 
 int16_t *srcPixel = src.data;
 A16 = srcPixel[permuteMap[0]];
 R16 = srcPixel[permuteMap[1]];
 G16 = srcPixel[permuteMap[2]];
 B16 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 R10 = CLAMP(RGB101010Min, ((R16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 G10 = CLAMP(RGB101010Min, ((G16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 B10 = CLAMP(RGB101010Min, ((B16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 A10 = CLAMP( 0, (A16 * 3 + 2048) >> 12), 3);
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = htonl((R10 << 22) | (G10 << 12) | (B10 << 2) | A10);
 destPixel += 1;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 kvImageInvalidParameter         Is returned when RGB101010Min > RGB101010Max || RGB101010RangeMin > RGB101010RangeMax
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16Q12ToRGBA1010102(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, int32_t RGB101010Min, int32_t RGB101010Max, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 @function vImageConvert_RGBA1010102ToARGB16U
 
 @abstract Convert RGBA1010102 to ARGB16U format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3.
 
 @param flags
 <pre> @textblock
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert RGBA1010102 to ARGB16U format
 
 RGBA1010102 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 
 <pre> @textblock
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 @/textblock </pre>
 
 The per-pixel operation is:
 <pre> @textblock
 
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t A2  = pixel & 0x3;
 int32_t R10 = (pixel >> 22) & 0x3ff;
 int32_t G10 = (pixel >> 12) & 0x3ff;
 int32_t B10 = (pixel >>  2) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 A2  = (A2 * USHRT_MAX + 1) / 3;
 R10 = ((R10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 G10 = ((G10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 B10 = ((B10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 
 uint16_t R16, G16, B16;
 R16 = CLAMP(0, R10, USHRT_MAX);
 G16 = CLAMP(0, G10, USHRT_MAX);
 B16 = CLAMP(0, B10, USHRT_MAX);
 
 uint16_t ARGB[4];
 ARGB[0] = A2;
 ARGB[1] = R16;
 ARGB[2] = G16;
 ARGB[3] = B16;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @/textblock </pre>
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax.
 @/textblock </pre>
 
 Note: Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_RGBA1010102ToARGB16U(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 @function vImageConvert_ARGB16UToRGBA1010102
 
 @abstract Convert ARGB16U to RGBA1010102 format.
 
 @param src
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock </pre>
 @discussion Convert ARGB16U to RGB101010 format.
 
 RGB101010 is almost the same format that is defined in CVPixelBuffer.h as 'kCVPixelFormatType_30RGB'
 except that this format uses the least significant 2 bits for alpha channel.
 
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMax & RGB101010RangeMin are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 
 RGB101010RangeMax  = 1023;
 RGB101010RangeMin  = 0;
 
 The per-pixel operation is:
 
 uint16_t *srcPixel = src.data;
 A16 = srcPixel[permuteMap[0]];
 R16 = srcPixel[permuteMap[1]];
 G16 = srcPixel[permuteMap[2]];
 B16 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 R10 = ((R16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 G10 = ((G16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 B10 = ((B16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 A10 = ((A16 * 3 + (USHRT_MAX >> 1)) / USHRT_MAX);
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = htonl((R10 << 22) | (G10 << 12) | (B10 << 2) | A10);
 destPixel += 1;
 
 @return
 <pre> @textblock
 kvImageNoError                  Is returned when there was no error.
 kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax.
 @/textblock </pre>
 
 Results are guaranteed to be faithfully rounded.
 
 
 */

VIMAGE_PF vImage_Error vImageConvert_ARGB16UToRGBA1010102(const vImage_Buffer *src, const vImage_Buffer *dest, int32_t RGB101010RangeMin, int32_t RGB101010RangeMax, const uint8_t permuteMap[4], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup RGB888 */

/*!
 @function vImagePermuteChannels_RGB888
 
 @abstract Reorder 3 color channels within the buffer according to the permute map.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit 3-channel interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit 3-channel interleaved destination pixels.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For exmaple, permuteMap[3] = {0, 1, 2} or NULL will produce the same dest pixels as the src.
 permuteMap[3] = {2, 1, 0} is the reverse ordered dest pixels from the dest.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, or 2.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion This function can be used to reorder 3 color channel buffer.
 
 The per-pixel operation is:
 
 @code
 uint8_t *srcRow = src.data;
 uint8_t *destRow = dest.data;
 
 R = srcRow[permuteMap[0]];
 G = srcRow[permuteMap[1]];
 B = srcRow[permuteMap[2]];
 srcRow += 3;
 
 destRow[0] = R;
 destRow[1] = G;
 destRow[2] = B;
 destRow += 3;
 @endcode
 
 @return kvImageNoError                  Is returned when there was no error.
 @return kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 
 @note Works in place.
 */

VIMAGE_PF vImage_Error vImagePermuteChannels_RGB888(const vImage_Buffer *src, const vImage_Buffer *dest, const uint8_t permuteMap[3], vImage_Flags flags) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 @functiongroup Format agnostic
 */

/*!
 @function vImageCopyBuffer
 
 @abstract Copy vImage buffer from src to dest.
 
 @param src
 A pointer to source vImage_Buffer.
 
 @param dest
 A pointer to destination vImage_Buffer.
 
 @param pixelSize
 Number of bytes for one pixel.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageGetTempBufferSize    Returns 0. Does no work.
 
 @return \p kvImageNoError                      Is returned when there was no error.
 @return \p kvImageRoiLargerThanInputBuffer     Is returned when src.width < dest.width || src.height < dest.height
 
 @seealso vImage_Buffer
 */

VIMAGE_PF vImage_Error vImageCopyBuffer(const vImage_Buffer *src, const vImage_Buffer *dest, size_t pixelSize, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!  @functiongroup XRGB2101010  */
/*!
 @function vImageConvert_XRGB2101010ToARGB8888
 
 @abstract Convert XRGB2101010 to ARGB8888 format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 XRGB2101010 pixels must be at least 4 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved destination pixels.
 ARGB8888 pixels may have any alignment.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3, as long as each channel appears only once.
 
 @param flags
 <pre> @textblock
 kvImageDoNotTile            Disables internal multithreading, if any.
 @/textblock <pre>
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 
 @code
 uint32_t *srcPixel = src.data;
 uint32_t pixel = srcPixel[0];
 srcPixel += 1;
 
 int32_t R10 = (pixel >> 20) & 0x3ff;
 int32_t G10 = (pixel >> 10) & 0x3ff;
 int32_t B10 = (pixel >>  0) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 R10 = ((R10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 G10 = ((G10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 B10 = ((B10 - RGB101010RangeMin) * UCHAR_MAX + (range10 >> 1)) / range10;
 
 uint8_t R8, G8, B8;
 R8 = CLAMP(0, R10, UCHAR_MAX);
 G8 = CLAMP(0, G10, UCHAR_MAX);
 B8 = CLAMP(0, B10, UCHAR_MAX);
 
 uint8_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R8;
 ARGB[2] = G8;
 ARGB[3] = B8;
 
 uint8_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB2101010ToARGB8888
 */
VIMAGE_PF vImage_Error vImageConvert_XRGB2101010ToARGB8888(const vImage_Buffer *src, Pixel_8 alpha,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,3)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB2101010ToARGB8888(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_ARGB8888ToXRGB2101010
 
 @abstract Convert ARGB8888 to XRGB2101010 format.
 
 @param src
 A pointer to vImage_Buffer that references 8-bit ARGB interleaved source pixels.
 ARGB8888 pixels may have any alignment.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.
 XRGB2101010 pixels must be at least 4 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of src.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB8888.
 permuteMap[4] = {3, 2, 1, 0} is BGRA8888.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 
 @code
 uint8_t *srcPixel = src.data;
 R8 = srcPixel[permuteMap[1]];
 G8 = srcPixel[permuteMap[2]];
 B8 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 int32_t rounding = UCHAR_MAX >> 1;
 R10 = ((R8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 G10 = ((G8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 B10 = ((B8 * range10 + rounding) / UCHAR_MAX) + RGB101010RangeMin;
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = (R10 << 20) | (G10 << 10) | (B10 << 0);
 destPixel += 1;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB8888ToARGB2101010
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB8888ToXRGB2101010(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB8888ToARGB2101010(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_XRGB2101010ToARGB16Q12
 
 @abstract Convert XRGB2101010 to ARGB16Q12 format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 XRGB2101010 pixels must be at least 4 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved destination pixels.
 ARGB16Q12 ixels must be at least 2 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 
 @code
 uint32_t *srcPixel = src.data;
 srcPixel += 1;
 
 int32_t R10 = (pixel >> 20) & 0x3ff;
 int32_t G10 = (pixel >> 10) & 0x3ff;
 int32_t B10 = (pixel >>  0) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 int16_t R16, G16, B16;
 R16 = ((R10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 G16 = ((G10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 B16 = ((B10 - RGB101010RangeMin) * 4096 + (range10 >> 1)) / range10;
 
 R16 = CLAMP(INT16_MIN, R16, INT16_MAX);
 G16 = CLAMP(INT16_MIN, G16, INT16_MAX);
 B16 = CLAMP(INT16_MIN, B16, INT16_MAX);
 
 int16_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R16;
 ARGB[2] = G16;
 ARGB[3] = B16;
 
 int16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB2101010ToARGB16Q12
 */
VIMAGE_PF vImage_Error vImageConvert_XRGB2101010ToARGB16Q12(const vImage_Buffer *src, Pixel_16Q12 alpha,
                                                            const vImage_Buffer *dest,
                                                            int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                            const uint8_t permuteMap[4],
                                                            vImage_Flags flags)
VIMAGE_NON_NULL(1,3)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB2101010ToARGB16Q12(const vImage_Buffer *src,
                                                            const vImage_Buffer *dest,
                                                            int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                            const uint8_t permuteMap[4],
                                                            vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_ARGB16Q12ToXRGB2101010
 
 @abstract Convert ARGB16Q12 to XRGB2101010 format.
 
 @param src
 A pointer to vImage_Buffer that references 16Q12 ARGB interleaved source pixels.
 ARGB16Q12 pixels must be at least 2 byte aligned.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.
 XRGB2101010 pixels must be at least 4 byte aligned.
 
 @param RGB101010RangeMax
 A maximum value for the range of 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for the range of 10-bit RGB pixel.
 
 @param RGB101010Max
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010Min
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16Q12.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16Q12.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 This is needed because 16Q12 has a chance to be outside of [0.0, 1.0] range and we are converting those
 values into video-range. In that case, there will be some numbers outside of 10-bit video-range and we
 want those values to be representable as much as possible.
 
 The per-pixel operation is:
 @code
 
 int16_t *srcPixel = src.data;
 R16 = srcPixel[permuteMap[1]];
 G16 = srcPixel[permuteMap[2]];
 B16 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 R10 = CLAMP(RGB101010Min, ((R16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 G10 = CLAMP(RGB101010Min, ((G16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 B10 = CLAMP(RGB101010Min, ((B16 * range10 + 2048) >> 12) + RGB101010RangeMin, RGB101010Max);
 A10 = CLAMP( 0, (A16 * 3 + 2048) >> 12), 3);
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = (R10 << 20) | (G10 << 10) | (B10 << 0);
 destPixel += 1;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB16Q12ToARGB2101010
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB16Q12ToXRGB2101010(const vImage_Buffer *src,
                                                            const vImage_Buffer *dest,
                                                            int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                            int32_t RGB101010Min, int32_t RGB101010Max,
                                                            const uint8_t permuteMap[4],
                                                            vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB16Q12ToARGB2101010(const vImage_Buffer *src,
                                                            const vImage_Buffer *dest,
                                                            int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                            int32_t RGB101010Min, int32_t RGB101010Max,
                                                            const uint8_t permuteMap[4],
                                                            vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_XRGB2101010ToARGB16U
 
 @abstract Convert XRGB2101010 to ARGB16U format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 @code
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t R10 = (pixel >> 20) & 0x3ff;
 int32_t G10 = (pixel >> 10) & 0x3ff;
 int32_t B10 = (pixel >>  0) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 R10 = ((R10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 G10 = ((G10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 B10 = ((B10 - RGB101010RangeMin) * USHRT_MAX + (range10 >> 1)) / range10;
 
 uint16_t R16, G16, B16;
 R16 = CLAMP(0, R10, USHRT_MAX);
 G16 = CLAMP(0, G10, USHRT_MAX);
 B16 = CLAMP(0, B10, USHRT_MAX);
 
 uint16_t ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = R16;
 ARGB[2] = G16;
 ARGB[3] = B16;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ARGB[permuteMap[0]];
 destPixel[1] = ARGB[permuteMap[1]];
 destPixel[2] = ARGB[permuteMap[2]];
 destPixel[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB2101010ToARGB16U
 */
VIMAGE_PF vImage_Error vImageConvert_XRGB2101010ToARGB16U(const vImage_Buffer *src, uint16_t alpha,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,3)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB2101010ToARGB16U(const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));

/*!
 @function vImageConvert_ARGB16UToXRGB2101010
 
 @abstract Convert ARGB16U to XRGB2101010 format.
 
 @param src
 A pointer to vImage_Buffer that references 16-bit ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16U.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16U.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 
 @discussion
 This format is 10-bit big endian 32-bit pixels.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 @code
 
 uint16_t *srcPixel = src.data;
 R16 = srcPixel[permuteMap[1]];
 G16 = srcPixel[permuteMap[2]];
 B16 = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 int32_t R10, G10, B10;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 R10 = ((R16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 G10 = ((G16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 B10 = ((B16 * range10 + (USHRT_MAX >> 1)) / USHRT_MAX) + RGB101010RangeMin;
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = (R10 << 20) | (G10 << 10) | (B10 << 0);
 destPixel += 1;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB16UToARGB2101010
 */
VIMAGE_PF vImage_Error vImageConvert_ARGB16UToXRGB2101010(const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB16UToARGB2101010(const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_XRGB2101010ToARGBFFFF
 
 @abstract Convert XRGB2101010 to ARGBFFFF format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 32-bit float ARGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGBFFFF.
 permuteMap[4] = {3, 2, 1, 0} is BGRAFFFF.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageDoNotClamp           Disables clamping floating point values to [0, 1].
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 @code
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t R10 = (pixel >> 20) & 0x3ff;
 int32_t G10 = (pixel >> 10) & 0x3ff;
 int32_t B10 = (pixel >>  0) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 float RF, GF, BF;
 RF = (R10 - RGB101010RangeMin) / (float)range10;
 GF = (G10 - RGB101010RangeMin) / (float)range10;
 BF = (B10 - RGB101010RangeMin) / (float)range10;
 
 if (!(flags & kvImageDoNotClamp)) {
 RF = CLAMP(RF, 0.0f, 1.0f);
 GF = CLAMP(GF, 0.0f, 1.0f);
 BF = CLAMP(BF, 0.0f, 1.0f);
 }
 
 float ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = RF;
 ARGB[2] = GF;
 ARGB[3] = BF;
 
 float *destPixel = dest.data;
 destRow[0] = ARGB[permuteMap[0]];
 destRow[1] = ARGB[permuteMap[1]];
 destRow[2] = ARGB[permuteMap[2]];
 destRow[3] = ARGB[permuteMap[3]];
 destPixel += 4;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB2101010ToARGBFFFF
 */
VIMAGE_PF vImage_Error vImageConvert_XRGB2101010ToARGBFFFF(const vImage_Buffer *src, Pixel_F alpha,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,3)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB2101010ToARGBFFFF(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));


/*!
 @function vImageConvert_ARGBFFFFToXRGB2101010
 
 @abstract Convert ARGBFFFF to XRGB2101010 format.
 
 @param src
 A pointer to vImage_Buffer that references 32-bit float ARGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 10-bit RGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of source.
 For exmaple, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGBFFFF.
 permuteMap[4] = {3, 2, 1, 0} is BGRAFFFF.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageDoNotClamp           Disables clamping floating point values to [0, 1].
 
 @discussion
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 @code
 float *srcPixel = src.data;
 float AF, RF, GF, BF;
 AF = srcPixel[permuteMap[0]];
 RF = srcPixel[permuteMap[1]];
 GF = srcPixel[permuteMap[2]];
 BF = srcPixel[permuteMap[3]];
 srcPixel += 4;
 
 if (!(flags & kvImageDoNotClamp)) {
 RF = CLAMP(RF, 0.0f, 1.0f);
 GF = CLAMP(GF, 0.0f, 1.0f);
 BF = CLAMP(BF, 0.0f, 1.0f);
 }
 
 int32_t A2, R10, G10, B10;
 int32_t range10  = RGB101010RangeMax - RGB101010RangeMin;
 A2  = (int)(AF * 3.0f + 0.5f);
 R10 = (int)(RF * range10 + 0.5f) + RGB101010RangeMin;
 G10 = (int)(GF * range10 + 0.5f) + RGB101010RangeMin;
 B10 = (int)(BF * range10 + 0.5f) + RGB101010RangeMin;
 
 uint32_t *destPixel = dest.data;
 destPixel[0] = (A2 << 30) | (R10 << 20) | (G10 << 10) | (B10 << 0);
 destPixel += 1;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is a unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGBFFFFToARGB2101010
 */
VIMAGE_PF vImage_Error vImageConvert_ARGBFFFFToXRGB2101010(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGBFFFFToARGB2101010(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                           const uint8_t permuteMap[4],
                                                           vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!
 @function vImageConvert_XRGB2101010ToARGB16F
 
 @abstract Convert XRGB2101010 to ARGB16F format.
 
 @param src
 A pointer to vImage_Buffer that references 10-bit RGB interleaved source pixels.
 
 @param dest
 A pointer to vImage_Buffer that references 16-bit float ARGB interleaved destination pixels.
 
 @param RGB101010RangeMax
 A maximum value for 10-bit RGB pixel.
 
 @param RGB101010RangeMin
 A minimum value for 10-bit RGB pixel.
 
 @param permuteMap
 Values that can be used to switch the channel order of dest.
 For example, permuteMap[4] = {0, 1, 2, 3} or NULL are ARGB16F.
 permuteMap[4] = {3, 2, 1, 0} is BGRA16F.
 Any order of permuteMap is allowed when each permuteMap value is 0, 1, 2, or 3,
 as long as each channel appears only once.
 
 @param flags
 \p kvImageDoNotTile            Disables internal multithreading, if any.
 \p kvImageDoNotClamp           Disables clamping floating point values to [0, 1].
 
 @discussion
 XRGB2101010 is the same format that is defined in CVPixelBuffer.h as
 'kCVPixelFormatType_30RGBLEPackedWideGamut' or 'w30r'.
 
 This format is 10-bit little endian 32-bit pixels. The 2 MSB are zero.
 
 RGB101010RangeMin & RGB101010RangeMax are available for non-full-range pixel values.
 For full-range pixel values, the user can set these as
 @code
 RGB101010RangeMin  = 0;
 RGB101010RangeMax  = 1023;
 @endcode
 
 The per-pixel operation is:
 @code
 uint32_t *srcPixel = src.data;
 uint32_t pixel = ntohl(srcPixel[0]);
 srcPixel += 1;
 
 int32_t R10 = (pixel >> 20) & 0x3ff;
 int32_t G10 = (pixel >> 10) & 0x3ff;
 int32_t B10 = (pixel >>  0) & 0x3ff;
 int32_t range10 = RGB101010RangeMax - RGB101010RangeMin;
 
 float RF, GF, BF;
 RF = (R10 - RGB101010RangeMin) / (float)range10;
 GF = (G10 - RGB101010RangeMin) / (float)range10;
 BF = (B10 - RGB101010RangeMin) / (float)range10;
 
 if (!(flags & kvImageDoNotClamp)) {
 RF = CLAMP(RF, 0.0f, 1.0f);
 GF = CLAMP(GF, 0.0f, 1.0f);
 BF = CLAMP(BF, 0.0f, 1.0f);
 }
 
 float ARGB[4];
 ARGB[0] = alpha;
 ARGB[1] = RF;
 ARGB[2] = GF;
 ARGB[3] = BF;
 
 uint16_t *destPixel = dest.data;
 destPixel[0] = ConvertFloatToHalf(ARGB[permA]);
 destPixel[1] = ConvertFloatToHalf(ARGB[permR]);
 destPixel[2] = ConvertFloatToHalf(ARGB[permG]);
 destPixel[3] = ConvertFloatToHalf(ARGB[permB]);
 destPixel += 4;
 @endcode
 
 @return \p kvImageNoError                  Is returned when there was no error.
 @return \p kvImageUnknownFlagsBit          Is returned when there is an unknown flag.
 @return \p kvImageRoiLargerThanInputBuffer Is returned when src.width < dest.width || src.height < dest.height.
 @return \p kvImageInvalidParameter         Is returned when RGB101010RangeMin is bigger than RGB101010RangeMax
 or when RGB101010RangeMin < 0 || RGB101010RangeMax > 1023.
 
 @note Results are guaranteed to be faithfully rounded.
 
 @seealso vImageConvert_ARGB2101010ToARGB16F
 */
VIMAGE_PF vImage_Error vImageConvert_XRGB2101010ToARGB16F(const vImage_Buffer *src, Pixel_F alpha,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,3)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageConvert_ARGB2101010ToARGB16F(const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          int32_t RGB101010RangeMin, int32_t RGB101010RangeMax,
                                                          const uint8_t permuteMap[4],
                                                          vImage_Flags flags)
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(9.3), watchos(3.0), tvos(10.0));

#ifdef __cplusplus
}
#endif

#endif


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/vImage_CVUtilities.h
/*!
* @header vImage_CVUtilities.h
*  vImage.framework
*
*  Created by Ian Ollmann on 12/5/13.
*
*  See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2013-2016 by Apple Inc. All rights reserved.
*
*  @discussion vImage_CVUtilities.h provides a suite of high level APIs to facilitate conversion between CVPixelBufferRef
*  formats and the set of formats describable by CoreGraphics types, including the core formats used by vImage for most
*  image filters.  The API should support nearly any CoreVideo format or Core Graphics format in a generic (format neutral)
*  manner.  Conversions by default are color corrected as necessary.  (The recipe for the correction is taken from ColorSync
*  but vImage does the heavy lifting, usually running from 3-156 times faster.)  High level interfaces are available to
*  read/write data directly to/from CVPixelBufferRefs to vImage_Buffers.  Lower level interfaces are provided to allow the
*  process to be broken apart a bit, to either respond to errors or eliminate redundant calculation. That is, while it is
*  expected that the high level interfaces above will work for most, there are two common situation where more work may be required:
*
*  Sometimes CVPixelBuffers are missing information attached to them that is needed to convert them to other formats. The vImageCVImageFormatRef
*  allows you to repair this problem prior to proceeding with conversions using the above high level interfaces or vImageConvert_AnyToAny.
*  The vImageCVImageFormatRef also provides additional control over how the conversion is done.
*
*  In addition, when the same conversion is done repeatedly, such as when converting multiple frames from the same movie, the high level interfaces
*  presented above may incur some unnecessary overhead because they are redundantly introspecting pixel format and creating/destroying the same
*  objects over and over. Breaking apart the conversion process into substeps allows you to recycle work from earlier conversions to save time.
*  Both vImageCVImageFormatRefs and vImageConverterRefs can be reused multiple times, by multiple threads concurrently, if needed.
*
*  In addition, please see the various RGB <-> CoreVideo basic conversions are available in vImage/Conversion.h.  These provide direct
*  access to the fast low level conversions available here. They are useful when you know exactly what formats you are working
*  with ahead of time and just want to do that with a minimum of fuss.
*
* @ignorefuncmacro VIMAGE_NON_NULL
*/

#ifndef vImage_CVUtilities_h
/*!
*  @define vImage_CVUtilities_h
*  @abstract Preprocessor symbol to make sure the header is only included once.
*  @discussion Set vImage_CVUtilities_h to 1 before including Accelerate headers to turn this header off.
*              You may wish to do that if the inclusion of CoreVideo/CVPixelBuffer.h is causing problems for your build.
*/
#define vImage_CVUtilities_h 1


#include <vImage/vImage_Utilities.h>
#include <vImage/Conversion.h>
#include <CoreVideo/CVPixelBuffer.h>  /* #define vImage_CVUtilities_h 1 before including Accelerate headers to turn this header off */

#ifdef __cplusplus
extern "C" {
#endif

/*
 *   CoreVideo interoperation:
 *     High Level interface
 */


/*!
 * @functiongroup vImage_Buffer Initialization
 */


/*!
 * @function vImageBuffer_InitWithCVPixelBuffer
 *
 * @abstract Initializes a vImage_Buffer to contain a representation of the CVPixelBufferRef provided.
 * @discussion It does the following:
 *
 *  <pre>
 *  @textblock
 *      o   Set buffer->height and buffer->width to match the size of the provided image.
 *
 *      o   set buffer->rowBytes for good performance                               (see kvImageDoNotAllocate flag below)
 *
 *      o   allocate a region of memory and assign a pointer to it to buffer->data  (see kvImageDoNotAllocate flag below)
 *
 *      o   convert the pixels contained in the image to the desired format and write to buffer->data.
 *  @/textblock
 *  </pre>
 *
 *
 * The entire image is converted. If you want to convert less, you can do so using vImageConvert_AnyToAny and a converter prepared with
 * vImageConverter_CreateForCVToCGImageFormat.
 *
 *      @param buffer           A pointer to a vImage_Buffer structure to be initialized. The height and width fields will be overwritten
 *                              with the size of the CVPixelBuffer. Please see the kvImageDoNotAllocate flag description below for
 *                              options about how the buffer->data and buffer->rowBytes field is handled.
 *
 *      @param desiredFormat    image format for the vImage_Buffer.
 *
 *      @param cvPixelBuffer    A CVPixelBufferRef for the image. It is not necessary to lock the CVPixelBuffer before calling this function.
 *
 *      @param cvImageFormat    An optional vImageCVImageFormatRef to specify the pixel format of the CVPixelBuffer.
 *
 *                                  If NULL, vImage attempts to discover this information automatically from the CVPixelBuffer. However, sometimes
 *                                  necessary color information in the CVPixelBuffer is missing, preventing conversion.  An error will be returned.
 *                                  See kvImageCVImageFormat return codes for this function for more information. To supply vImage with complete
 *                                  color information, provide a complete vImageCVImageFormatRef here.
 *
 *                                  If not NULL, the cvImageFormat is used instead of looking to the CVPixelBufferRef for color information. If the
 *                                  cvImageFormat is also incomplete, a kvImageCVImageFormat_ error code will be returned.
 *
 *      @param backgroundColor  In cases where the vImage_Buffer format specifies opaque alpha and the cvPixelBuffer is has non-opaque alpha, the
 *                              image will be composited against a background color to remove the alpha before writing to the vImage_Buffer. The
 *                              background color is given in the colorspace of the desiredFormat.
 *
 *
 *      @param flags    The following flags are understood by this function:
 *
 *          <pre>
 *          @textblock
 *          kvImageDoNotAllocate        Under normal operation, new memory is allocated to hold the image pixels and its address is written
 *                                      to buffer->data. You are responsible for freeing that data when you are done with it, using free().
 *                                      When the kvImageDoNotAllocate flag is set, the buffer->data pointer and buffer->rowBytes is used unmodified.
 *                                      This is intended to allow you to allocate the buffer yourself, or write directly into part of another image.
 *                                      Use CVPixelBufferGetHeight() and CVPixelBufferGetWidth() to find the size of the result buffer.
 *
 *          kvImageDoNotTile            Disable internal multithreading. This may be desired if you are extracting many
 *                                      such images in parallel, or are otherwise attempting to keep CPU utilization to
 *                                      a single core.
 *
 *          kvImageHighQualityResampling    For some CVPixelBuffer formats, the chroma channels are subsampled. This flag directs
 *                                          vImage to spend extra time where it can to give better image quality.
 *
 *          kvImagePrintDiagnosticsToConsole    In case of an error, print human readable error messages to the Apple System Logger (Console).
 *                                              This is useful for debugging, but probably should not be on for a shipping application.
 *          @/textblock
 *          </pre>
 *
 *  @return
 *          <pre>
 *          @textblock
 *      kvImageMemoryAllocationError            buffer->data was not able to be allocated.
 *
 *      kvImageBufferSizeMismatch               buffer and cvPixelBuffer are not the same height and width
 *
 *      kvImageCVImageFormat_ConversionMatrix   The conversion matrix is missing from the CVPixelBuffer / vImageCVImageFormatRef. See note below.
 *
 *      kvImageCVImageFormat_ChromaSiting       The chroma siting info is missing from the CVPixelBuffer / vImageCVImageFormatRef. See note below.
 *
 *      kvImageCVImageFormat_ColorSpace         The colorspace containing primaries and transfer function is missing from the CVPixelBuffer / vImageCVImageFormatRef.
 *
 *      kvImageInvalidParameter                 buffer is NULL
 *
 *      kvImageInvalidImageObject               cvPixelBuffer is NULL or can not be locked
 *
 *      kvImageInvalidImageFormat               desiredFormat is NULL or points to an illegal CG image format
 *
 *      Note: Some CVPixelBuffers have incompletely specified color information. This makes it impossible for vImage to do the conversion.
 *            When this happens, you will get one of the kvImageCVImageFormat_ errors above. To proceed, create a vImageCVImageFormatRef, add
 *            the missing information and pass as the cvImageFormat parameter. It is possible that more than one piece of information is missing.
 *            If the vImageCVImageFormatRef is missing information, then you will also get these errors.
 *          @/textblock
 *          </pre>
 *
 * Returned image notes:
 *
 *      vImage here conforms to CoreVideo practice of substituting gamma 1/1.961 for kCVImageBufferTransferFunction_ITU_R_709_2 and
 *      kCVImageBufferTransferFunction_SMPTE_240M_1995 instead of using the ITU-R BT.709-5 specified transfer function.  You may
 *      manually set the transfer function using vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction() and vImageCVImageFormat_SetColorSpace().
 *      vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction() does not make this substitution.
 */
VIMAGE_PF vImage_Error vImageBuffer_InitWithCVPixelBuffer( vImage_Buffer *buffer,
                                                          vImage_CGImageFormat *desiredFormat,
                                                          CVPixelBufferRef cvPixelBuffer,
                                                          vImageCVImageFormatRef cvImageFormat,
                                                          const CGFloat *backgroundColor,
                                                          vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 * @functiongroup vImageBuffer export
 */

/*!
 * @function vImageBuffer_CopyToCVPixelBuffer
 * @abstract Copies the contents of the vImage_Buffer to a CVPixelBufferRef.
 * @discussion If the format of the vImage_Buffer doesn't match the CVPixelBuffer format, the image will be converted to the CVPixelBuffer
 *              format as part of the copy.
 *
 *  The entire CVPixelBuffer is overwritten. If you want to copy less, you can do so using vImageConvert_AnyToAny and a converter prepared
 *  with vImageConverter_CreateForCFToCVImageFormat.
 *
 *  vImage here conforms to CoreVideo practice of substituting gamma 1/1.961 for kCVImageBufferTransferFunction_ITU_R_709_2 and
 *  kCVImageBufferTransferFunction_SMPTE_240M_1995 instead of using the ITU-R BT.709-5 specified transfer function.  You may
 *  manually set the transfer function using vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction() and vImageCVImageFormat_SetColorSpace().
 *  ImageCreateRGBColorSpaceWithPrimariesAndTransferFunction() does not make this substitution.
 *
 *
 *      @param buffer          A pointer to a vImage_Buffer containing the pixels to be copied (converted) to the CVPixelBuffer. May not be NULL.
 *
 *      @param bufferFormat    The format of buffer. May not be NULL.
 *
 *      @param cvPixelBuffer   The CVPixelBufferRef where the image will be written.  It should be a valid, preallocated CVPixelBufferRef
 *                      set to the desired image type (which need not match bufferFormat).  It is not necessary to lock the
 *                      CVPixelBuffer before calling this function. May not be NULL.
 *
 *      @param cvImageFormat   An optional vImageCVImageFormatRef to specify the pixel format of the CVPixelBuffer.
 *
 *                      If NULL, vImage attempts to discover this information automatically. However, sometimes necessary color information
 *                      in the CVPixelBuffer is missing, preventing conversion.  An error will be returned. See kvImageCVImageFormat return
 *                      codes for this function for more information. To supply vImage with complete color information, provide a complete
 *                      vImageCVImageFormatRef here.
 *
 *                      If not NULL, the cvImageFormat is used instead of looking to the CVPixelBufferRef for color information. If the
 *                      cvImageFormat is also incomplete, a kvImageCVImageFormat_ error code will be returned.
 *
 *                          CAUTION: In this case, it is your responsibility to make sure that the CVPixelBuffer has the right
 *                                   attachments for matrix, chroma siting and colorspace as necessary to be properly decoded.
 *                                   vImage does not set these things for you.
 *
 *      @param backgroundColor If bufferFormat->bitmapInfo encodes kCGImageAlphaPremultipliedLast, kCGImageAlphaPremultipliedFirst,
 *                      kCGImageAlphaLast or kCGImageAlphaFirst -- that is, has a real alpha channel -- and the CVPixelBuffer
 *                      does not (most CV pixel formats don't) then the image will be flattened against a solid color to remove
 *                      the alpha information. You can select which color that is here. The background color is a CGFloat[3]
 *                      (red, green, blue) in the RGB colorspace of the CVPixelBuffer. (YpCbCr images reference a RGB colorspace
 *                      through a matrix like ITU-709. That is the RGB colorspace we are talking about here.)
 *
 *                      This parameter may be NULL, indicating black.
 *
 *                      If you want to skip flattening, you can substitute in kCGImageAlphaNoneSkipFirst/Last for the encoding of
 *                      the input buffer. This may lead to undesired results in the case of premultiplied alpha however, when alpha
 *                      is not all either 1.0 or 0. In that case, unpremultiply it first as a separate pass. Unpremultiplication may
 *                      be more costly than just flattening it, but does not introduce regions of background color into the image.
 *
 *      @param flags           The following flags are understood by this function:
 *
 *          <pre>
 *          @textblock
 *          kvImageNoFlags                      Default operation.
 *
 *          kvImageDoNotTile                    Turn internal multithreading off. This may be helpful in cases where you already have
 *                                              many such operations going concurrently, and in cases where it is desirable to keep
 *                                              CPU utilization to a single core.
 *
 *          kvImageHighQualityResampling        For some CVPixelBuffer formats, the chroma channels are subsampled. This flag directs
 *                                              vImage to spend extra time where it can to give better image quality.
 *
 *          kvImagePrintDiagnosticsToConsole    In case of an error, print human readable error messages to the Apple System Logger (Console).
 *                                              This is useful for debugging, but probably should not be on for a shipping application.
 *          @/textblock
 *          </pre>
 *
 *      @return
 *          <pre>
 *          @textblock
 *          kvImageNoError                          Success
 *
 *          kvImageInvalidImageFormat               bufferFormat is NULL or encodes an invalid format
 *
 *          kvImageBufferSizeMismatch               buffer and cvPixelBuffer are not the same height and width
 *
 *          kvImageNullPointerArgument              buffer and cvPixelBuffer may not be NULL
 *
 *          kvImageCVImageFormat_ConversionMatrix   The conversion matrix is missing from the CVPixelBuffer / vImageCVImageFormatRef. See note below.
 *
 *          kvImageCVImageFormat_ChromaSiting       The chroma siting info is missing from the CVPixelBuffer / vImageCVImageFormatRef. See note below.
 *
 *          kvImageCVImageFormat_ColorSpace         The colorspace containing primaries and transfer function is missing from the CVPixelBuffer / vImageCVImageFormatRef.
 *          @/textblock
 *          </pre>
 *
 *          Note: Some CVPixelBuffers have incompletely specified color information. This makes it impossible for vImage to do the conversion.
 *            When this happens, you will get one of the kvImageCVImageFormat_ errors above. To proceed, create a vImageCVImageFormatRef, add
 *            the missing information and pass as the cvImageFormat parameter. It is possible that more than one piece of information is missing.
 *
 */
VIMAGE_PF vImage_Error vImageBuffer_CopyToCVPixelBuffer( const vImage_Buffer *buffer,
                                                        const vImage_CGImageFormat *bufferFormat,
                                                        CVPixelBufferRef cvPixelBuffer,
                                                        vImageCVImageFormatRef cvImageFormat,
                                                        const CGFloat *backgroundColor,
                                                        vImage_Flags flags )
VIMAGE_NON_NULL( 1, 2, 3) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/**************************
 *  Low level interfaces: *
 **************************/


/*!
 * @functiongroup vImageCVImageFormatRef methods
 */

/*!
 * @function vImageCVImageFormat_CreateWithCVPixelBuffer
 * @abstract Used to create a vImageCVImageFormatRef to describe the pixel format of an existing CVPixelBufferRef.
 *
 * @discussion If the CVPixelBufferRef has incomplete pixel format information, the vImageCVImageFormatRef will also be incomplete. Not all
 * missing fields ultimately will prove to be necessary, however.  If a function that consumes a vImageCVImageFormatRef returns a
 * vImageCVImageFormatError code, please add the missing information and try again. See "vImageCVImageFormatRef Accessors" below.
 *
 * @param buffer        The CBPixelBufferRef on which to base the vImageCVImageFormatRef
 *
 * @return
 *  On success, a non-NULL vImageCVImageFormatRef is returned. The vImageCVImageFormatRef has a retain count of 1. You are responsible
 *  for releasing it when you are done with it.
 */
VIMAGE_PF vImageCVImageFormatRef vImageCVImageFormat_CreateWithCVPixelBuffer( CVPixelBufferRef buffer )
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));



/*!
 * @function vImageCVImageFormat_Create
 * @abstract Create a vImageCVImageFormatRef (low level).
 * @discussion This function creates a vImageCVImageFormatRef from first principles. In most cases, vImageCVImageFormat_CreateWithCVPixelBuffer
 *              is easier, but if your video pipeline doesn't use CoreVideo, or you need absolute control then this is your alternative.
 *
 *              Other fields not given by function parameters like number of channels, channel names, and channel description are automatically
 *              configured using the imageFormatType. User data is set separately with vImageCVImageFormat_SetUserData.
 *
 *
 * @param imageFormatType       A CVPixelFormatType such as '2vuy'. See CVPixelBuffer.h for the complete list.
 *
 * @param  matrix               A vImage_ARGBToYpCbCrMatrix showing how to convert from RGB to the YpCbCr format. This may be NULL. However, it
 *                              is required for conversions involving YpCbCr images, so for YpCbCr images you will be eventually forced to set the matrix
 *                              using vImageCVImageFormat_CopyConversionMatrix before you can make a vImageConverterRef with this object.
 *                              There are some predefined conversion matrices in Conversion.h for Rec 601 and 709 formats.
 *
 * @param  cvImageBufferChromaLocation   See kCVImageBufferChromaLocationTopFieldKey in CVImageBuffer.h for a list of chroma locations.
 *                              kCVImageBufferChromaLocation_Center is typical.  This may be NULL. However, for YpCbCr formats with downsampled
 *                              chroma, you will be ultimately forced to set a chroma location using vImageCVImageFormat_SetChromaSiting, before
 *                              a vImageConverterRef can be made with this object.
 *
 * @param  baseColorspace       For RGB and monochrome images, this is the colorspace of the image.
 *
 *                              For YpCbCr images, this is the colorspace of the RGB image before it was converted to YpCbCr using the ARGB-to-YpCbCr
 *                              conversion matrix (see matrix parameter above). The colorspace is defined based on the YpCbCr format RGB primaries
 *                              and transfer function.
 *
 *                              This may be NULL. However, you will eventually be forced to set set a colorspace for all image types, before
 *                              a vImageConvertRef can be made with this object.
 *
 * @param  alphaIsOneHint       Typically this is 0. If your image format has an alpha channel, but you know the image is fully opaque,
 *                              or want it to be treated as opaque, you can set this to 1. This may allow for faster conversions to
 *                              opaque formats.
 *
 *
 * @return
 *   On success, a non-NULL vImageCVImageFormatRef will be returned, which encodes the information contained in the above parameters. The
 *   vImageCVImageFormatRef has a retain count of 1.  You must release it when you are done with it.
 *
 *   On failure, NULL is returned.
 */
VIMAGE_PF vImageCVImageFormatRef vImageCVImageFormat_Create( uint32_t imageFormatType,                        // see kCVPixelFormatType_ defined in enum in CVPixelBuffer.h
                                                            const vImage_ARGBToYpCbCrMatrix *matrix,         // See also predefined constants in Conversion.h for 601/709/etc.
                                                            CFStringRef cvImageBufferChromaLocation,         // e.g. kCVImageBufferChromaLocation_Center
                                                            CGColorSpaceRef baseColorspace,                  // e.g. CGColorSpaceCreateWithName(kCGColorSpaceSRGB)
                                                            int  alphaIsOneHint                              // Set to 1 if the image has an alpha channel, and all the values in there are opaque. 0 otherwise.
) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 *  @function vImageCVImageFormat_Copy
 *  @abstract Makes a copy of a vImageCVImageFormatRef.
 *  @discussion The new vImageCVFormatRef is different from the old one in that:
 *
 *      o       Its reference count is 1
 *
 *      o       The userData field and destructor callback are not copied, and are initialized to NULL.
 *
 *  Usually, it is preferable to simply retain a vImageCVImageFormatRef rather than copy it. You may wish to copy a vImageCVImageFormatRef if
 *  you want to modify an existing vImageCVImageFormatRef but can't because it is being read by another thread, or to replace another software layer's
 *  userData pointer with your own.  By convention, the new vImageCVImageFormatRef is considered to have been created by the software layer that called
 *  vImageCVImageFormat_Copy.
 *
 *  @param format  The vImageCVImageFormatRef to copy.
 *
 *  @return
 *  On success, a non-NULL vImageCVImageFormatRef is returned. Its reference count is 1. You are responsible for releasing it when you are done with it.
 *  On failure, this function returns NULL.
 */
VIMAGE_PF vImageCVImageFormatRef vImageCVImageFormat_Copy( vImageConstCVImageFormatRef format ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 * @function vImageCVImageFormat_Retain
 * @abstract Retains a vImageCVImageFormatRef
 * @discussion The vImageCVImageFormatRef follows standard retain/release semantics.
 *
 * vImageCVImageFormat_Retain causes the object's reference count to be incremented.
 *
 * vImageCVImageFormat_Release causes the object's reference count to be decremented. When the reference count reaches 0,
 * the userDataReleaseCallback (if any) is called, and the object is then destroyed. The userDataReleaseCallback can access
 * the vImageCVImageFormatRef, but can not prevent vImageCVImageFormatRef destruction. For this reason, the
 * userDataReleaseCallback should be careful who it hands off control to in case that software layer
 * attempts to retain the vImageCVImageFormatRef. This will result in undefined behavior.
 *
 * @param fmt   The vImageCVImageFormatRef to retain
 *
 * fmt may be NULL, in which case nothing occurs.
 */
VIMAGE_PF void vImageCVImageFormat_Retain( vImageCVImageFormatRef fmt ) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 * @function vImageCVImageFormat_Release
 * @abstract Releases a vImageCVImageFormatRef
 * @discussion The vImageCVImageFormatRef follows standard retain/release semantics.
 *
 * vImageCVImageFormat_Retain causes the object's reference count to be incremented.
 *
 * vImageCVImageFormat_Release causes the object's reference count to be decremented. When the reference count reaches 0,
 * the userDataReleaseCallback (if any) is called, and the object is then destroyed. The userDataReleaseCallback can access
 * the vImageCVImageFormatRef, but can not prevent vImageCVImageFormatRef destruction. For this reason, the
 * userDataReleaseCallback should be careful who it hands off control to in case that software layer
 * attempts to retain the vImageCVImageFormatRef. This will result in undefined behavior.
 *
 * @param fmt   The vImageCVImageFormatRef to release
 *
 * fmt may be NULL, in which case nothing occurs.
 */
VIMAGE_PF void vImageCVImageFormat_Release( vImageCVImageFormatRef fmt ) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @typedef vImageCVImageFormatError
 *  @abstract  Additional error codes for functions that use the vImageCVImageFormatRef
 *  @discussion It is possible for a vImageCVImageFormatRef to contain incomplete information. This frequently happens
 *              when it is created from a CVPixelBufferRef which itself has incomplete formatting information. It can
 *              also happen by design as the result of something like vImageCVImageFormat_SetColorSpace(fmt, NULL). When
 *              this occurs, the vImageCVImageFormatRef may not contain enough information to perform a requested conversion
 *              (e.g. vImageBuffer_InitWithCVPixelBuffer). In such cases, a vImageCVImageFormatError will be returned
 *              from the left hand side of the function to indicate which field is absent.
 *
 *  @constant   kvImageCVImageFormat_NoError    No error. The conversion was successfully completed.
 *
 *  @constant   kvImageCVImageFormat_ConversionMatrix The conversion matrix is absent and required. The conversion matrix
 *              provides the conversion from RGB to Y'CbCr.
 *
 *  @constant   kvImageCVImageFormat_ChromaSiting  The chroma siting information is absent.  Chroma siting indicates the position
 *              of chrominance information relative to luminance samples when chrominance is sub-sampled.
 *
 *  @constant   kvImageCVImageFormat_ColorSpace The colorspace of the image is missing. If Y'CbCr, this is the colorspace of the
 *              RGB image from which the Y'CbCr pixels were calculated. Otherwise, it is the colorspace of the pixels themselves.
 *              Most CVPixelBuffer formats only allow one or two colorspace models (e.g. kCGColorSpaceModelRGB)
 *
 *  @constant   kvImageCVImageFormat_VideoChannelDescription    The range and clipping information is missing. This is unlikely
 *              to occur, since the information is initialized automatically based on the imageFormatType (See vImageCVImageFormat_Create.)
 *
 *  @constant   kvImageCVImageFormat_AlphaIsOneHint    The alpha-is-one hint tells vImage that the alpha channel (if any) is opaque.
 *                                                      This hint may be used to avoid some computation to flatten the alpha channel
 *                                                      in some cases. Because it is a hint, it can not be missing.
 */

/* Additional error codes for functions that consume a vImageCVImageFormatRef */
typedef VIMAGE_CHOICE_ENUM( vImageCVImageFormatError, ssize_t )
{
    kvImageCVImageFormat_NoError                            VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = 0,
    kvImageCVImageFormat_ConversionMatrix                   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = -21600,
    kvImageCVImageFormat_ChromaSiting                       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = -21601,
    kvImageCVImageFormat_ColorSpace                         VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = -21602,
    kvImageCVImageFormat_VideoChannelDescription            VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = -21603,
    kvImageCVImageFormat_AlphaIsOneHint                     VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 )  = -21604,
};

/*!
 @typedef        vImageChannelDescription
 @abstract       A description of the range and clamp limits for a pixel format
 @discussion     The vImageChannelDescription is provided to allow for "video range" formats and detailed
 control overclamping on a per-channel basis. The min and max control clamping limits. Values
 outside the range [min, max] are clamped to be in that range.  The zero and full values give
 the normal range and bias for the format. They are the encodings for 0.0 and 1.0 respectively.
 (0.0 and 0.5 for Chroma.)
 
 @field          min     The minimum encoded value allowed. Values less than this encoding are clamped to this value.
 @field          zero    The encoding for the value 0.0.   For example, for 8-bit chroma data this would be 128. For
 8-bit full range Luminance, this is 0. 8-bit video range Luminance is 16.
 @field          full    The encoding for 1.0 (0.5 for chroma).
 @field          max     The maximum allowed encoding. Values greater than this are clamped to this value.
 
 @seealso        vImage_YpCbCrPixelRange
 @seealso        vImageCVImageFormatRef channel descriptions
 */

typedef struct vImageChannelDescription
{
    CGFloat           min;        /* e.g. Minimum allowed value for format. e.g. {16, 16, 16, 0} for {Y', Cb, Cr, A} 8-bit video range                                  */
    CGFloat           zero;       /* e.g. Encoded value for 0.0   e.g. {0, 128, 128, 0} for {Y', Cb, Cr, A} 8-bit video range                                           */
    CGFloat           full;       /* e.g. Encoded value for 1.0, (0.5 for Chroma).  e.g. {235, 240, 240, 255} for {Y', Cb, Cr, A} 8-bit video range, full range alpha   */
    CGFloat           max;        /* e.g. Maximum allowed value for format. e,g, {235, 240, 240, 255} to clamp to {Y', Cb, Cr, A} 8-bit video range, full range alpha   */
    /*                                             {0xff, 0xff, 0xff, 0xff} to clamp to full range                                        */
}vImageChannelDescription;

/*!
 *  @typedef    vImageMatrixType
 *  @abstract   An enumeration of RGB -> Y'CbCr conversion matrix types.
 *  @description    Currently, only one matrix type is available. Additional formats are reserved for future expansion.
 *
 *  @constant   kvImageMatrixType_ARGBToYpCbCrMatrix    A vImage_ARGBToYpCbCrMatrix
 */
typedef VIMAGE_CHOICE_ENUM( vImageMatrixType, uint32_t )
{
    /* No matrix required for this format. NULL will be returned. Attempts to set this matrix are ignored. */
    kvImageMatrixType_None                  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 ) = 0,
    
    /* A vImage_ARGBToYpCbCrMatrix */
    kvImageMatrixType_ARGBToYpCbCrMatrix    VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_10, __IPHONE_8_0 ) = 1,
    
    /* other values are reserved for future expansion. For example, BT.2020 would probably require a new matrix type for constant luminance. */
};


/*!
 *  @function vImageCVImageFormat_GetFormatCode
 *  @abstract Return the kCVPixelFormatType_ (4 character code) that encodes the pixel format.
 *  @discussion The kCVPixelFormatType_ of a CoreVideo pixel buffer is given by a four character code (4CC), such as '2vuy'. It describes the number of channels,
 *              channel packing order, bits per component (except in one case), and usually range information like whether it is full range or
 *              video range.
 *  @param format   The vImageCVImageFormatRef for which the 4 character code is desired.
 *  @return  A 4CC in host-endian format.
 *  @seealso //apple_ref/doc/constant_group/Pixel_Format_Types CoreVideo/CVPixelBuffer.h
 */
VIMAGE_PF uint32_t vImageCVImageFormat_GetFormatCode( vImageConstCVImageFormatRef format )  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetChannelCount
 *  @abstract Return the the number of color channels in the image, including alpha.
 *  @discussion The channels may be interleaved or planar. For RGBA, the result is 4. For 'yuvs' this is 3. This does not return
 *              the same results as vImageConverter_GetNumberOfSourceBuffers / vImageConverter_GetNumberOfSourceBuffers, which
 *              instead describe the number of vImage_Buffers to pass to vImageConvert_AnyToAny. Some vImage_Buffers contain
 *              multiple channels.
 *  @param format   The vImageCVImageFormatRef for which the number of channels is desired.
 *  @return  A uint32_t containing the number of channels
 */
VIMAGE_PF uint32_t vImageCVImageFormat_GetChannelCount( vImageConstCVImageFormatRef format)  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetChannelNames
 *  @abstract Get a const kvImageBufferTypeCode_EndOfList-terminated array indicating the names of the channels in the buffer.
 *  @discussion The array is owned by the vImageCvImageFormatRef and will cease to be valid when the object is destroyed.
 *              This function is not useful to discover the correct vImage_Buffer order for a call to vImageConvert_AnyToAny().
 *  @param format   The vImageCVImageFormatRef for which the channel names are desired.
 *  @return  A const pointer to an array of vImageBufferTypeCodes indicating the names of the channels in the image.
 *  @seealso vImageConverter_GetSourceBufferOrder
 *  @seealso vImageConverter_GetDestinationBufferOrder
 */
VIMAGE_PF const vImageBufferTypeCode *vImageCVImageFormat_GetChannelNames( vImageConstCVImageFormatRef format)  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetColorSpace
 *  @abstract Get the colorspace associated with the image.
 *  @discussion If the image format is a Y'CbCr image format, this is the RGB colorspace of the image after the inverse
 *              RGB->YpCbCr conversion matrix is applied. Otherwise, it is the colorspace of the pixels in the image.
 *  @param format   The vImageCVImageFormatRef for which the colorspace is desired.
 *  @return  The colorspace (if any) that is returned is referenced by the vImageCVImageFormatRef and will be released
 *           when that object is destroyed. This function may return NULL, indicating an absence of colorspace information.
 */
VIMAGE_PF CGColorSpaceRef vImageCVImageFormat_GetColorSpace( vImageConstCVImageFormatRef format) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 *  @function vImageCVImageFormat_SetColorSpace
 *  @abstract Set the colorspace associated with the image.
 *  @discussion If the image format is a Y'CbCr image format, this sets the RGB colorspace of the image before the
 *              RGB->YpCbCr conversion matrix was applied. Otherwise, it is the colorspace of the pixels in the image.
 *              A non-NULL colorspace must be present before a vImageCVImageFormatRef can be used to do a conversion.
 *  @param format       The vImageCVImageFormatRef for which the colorspace is to be set.
 *  @param colorspace   The new colorspace.  May be NULL, indicating missing colorspace information.
 *  @return  On Success, kvImageNoError. An error will be returned if the colorspace model doesn't match what is expected
 *           for the image format type. For example, a 'RGBA' image must be kCGColorSpaceModelRGB.  Y'CbCr images expect a
 *           RGB colorspace. The new colorspace will be retained and he old one will be released.
 *
 *           On failure, nothing occurs.
 */
VIMAGE_PF vImage_Error vImageCVImageFormat_SetColorSpace( vImageCVImageFormatRef format, CGColorSpaceRef colorspace ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetChromaSiting
 *  @abstract Get the chroma-siting for the image.
 *  @discussion When Y'CbCr images have subsampled chroma, the position of the chroma samples relative to the luminance samples needs to be
 *              specified. Chroma siting information is only needed for Y'CbCr images that are not 444.
 *  @param format       The vImageCVImageFormatRef for which the chroma siting information is desired.
 *  @return  Returns a CFStringRef that describes the positioning of the chroma samples. Eligible string return values are listed
 *           in CoreVideo/CVImageBuffer.h.   The result is NULL if the chroma siting information is missing.
 *  @seealso //apple_ref/c/data/kCVImageBufferChromaLocationTopFieldKey kCVImageBufferChromaLocationTopFieldKey
 */
VIMAGE_PF CFStringRef vImageCVImageFormat_GetChromaSiting( vImageConstCVImageFormatRef format ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_SetChromaSiting
 *  @abstract Set the chroma-siting for the image.
 *  @discussion When Y'CbCr images have subsampled chroma, the position of the chroma samples relative to the luminance samples needs to be
 *              specified. Chroma siting information is only needed for Y'CbCr images that are not 444. The new siting name will be retained.
 *              The old siting will be released. This function has no effect for image format types that do not require siting information.
 *  @param format       The vImageCVImageFormatRef for which the chroma siting information is desired.
 *  @param siting       The new siting information for the format. May be NULL.
 *  @return
 *      <pre>
 *      @textblock
 *          kvImageNoError                  Success
 *
 *          kvImageInvalidImageFormat       format is NULL
 *
 *          kvImageInvalidParameter         siting is not a recognized CFStringRef from the set of values appearing in CoreVideo/CVImageBuffer.h.
 *      @/textblock
 *      </pre>
 *  @seealso //apple_ref/c/data/kCVImageBufferChromaLocationTopFieldKey kCVImageBufferChromaLocationTopFieldKey
 */
VIMAGE_PF vImage_Error vImageCVImageFormat_SetChromaSiting( vImageCVImageFormatRef format, CFStringRef siting) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetConversionMatrix
 *  @abstract Get the RGB -> Y'CbCr conversion matrix for the image.
 *  @discussion  Y'CbCr images are defined in terms of a RGB image and a conversion matrix from that RGB format to Y'CbCr.
 *               The conversion frequently has the form:
 *
 *          <pre>
 *          @textblock
 *              Y' =  R_Yp * R  + G_Yp * G + B_Yp * B       Y' = [0, 1.0]
 *              Cb = k0 * (B - Y')                          Cb = [-0.5, 0.5]
 *              Cr = k1 * (R - Y')                          Cr = [-0.5, 0.5]
 *          @/textblock
 *          </pre>
 *
 *      That can be reformulated as a 3x3 matrix operation. The element names here correspond to the fields in the vImage_ARGBToYpCbCrMatrix type:
 *
 *          <pre>
 *          @textblock
 *              | Y' |   | R_Yp        G_Yp     B_Yp      |   | R |
 *              | Cb | = | R_Cb        G_Cb     B_Cb_R_Cr | * | G |
 *              | Cr |   | B_Cb_R_Cr   G_Cr     B_Cr      |   | B |
 *          @/textblock
 *          </pre>
 *
 *      Most Y'CbCr conversion matrices are of this form. However, some conversion matrices, such as that proposed to ITU-R BT.2020 for
 *      constant luminance, are more complicated.
 *
 *      It is possible for the matrix to be absent. Y'CbCr image types may not be converted without a conversion matrix.
 *
 *  @param format       The vImageCVImageFormatRef for which the matrix is desired
 *  @param outType      A pointer to a variable of type vImageMatrixType.
 *  @return  A pointer to a matrix will be returned from the left hand side of the function. The memory pointed to by outType will be
 *              overwritten with the type of the matrix returned.  The returned matrix may be NULL, indicating an absent matrix.
 *
 *          The matrix is owned by the vImageCvImageFormatRef and will cease to be valid when the vImageCvImageFormatRef is destroyed.
 *
 *  @seealso vImage_ARGBToYpCbCrMatrix
 *  @seealso vImageMatrixType
 */
VIMAGE_PF const void * vImageCVImageFormat_GetConversionMatrix( vImageConstCVImageFormatRef format, vImageMatrixType *outType)  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 *  @function vImageCVImageFormat_CopyConversionMatrix
 *  @abstract Set the RGB -> Y'CbCr conversion matrix for the image.
 *  @discussion  matrix is copied to the vImageCVImageFormatRef's internal matrix storage.
 *
 *  Y'CbCr images are defined in terms of a RGB image and a conversion matrix from that RGB format to Y'CbCr.
 *               The conversion frequently has the form:
 *
 *          <pre>
 *          @textblock
 *              Y' =  R_Yp * R  + G_Yp * G + B_Yp * B       Y' = [0, 1.0]
 *              Cb = k0 * (B - Y')                          Cb = [-0.5, 0.5]
 *              Cr = k1 * (R - Y')                          Cr = [-0.5, 0.5]
 *          @/textblock
 *          </pre>
 *
 *      That can be reformulated as a 3x3 matrix operation. The element names here correspond to the fields in the vImage_ARGBToYpCbCrMatrix type:
 *
 *          <pre>
 *          @textblock
 *              | Y' |   | R_Yp        G_Yp     B_Yp      |   | R |
 *              | Cb | = | R_Cb        G_Cb     B_Cb_R_Cr | * | G |
 *              | Cr |   | B_Cb_R_Cr   G_Cr     B_Cr      |   | B |
 *          @/textblock
 *          </pre>
 *
 *      Most Y'CbCr conversion matrices are of this form. However, some conversion matrices, such as that proposed to ITU-R BT.2020 for
 *      constant luminance, are more complicated.
 *
 *      It is possible for the matrix to be absent. Y'CbCr image types may not be converted without a conversion matrix.
 *
 *  @param format       The vImageCVImageFormatRef for which the matrix is desired
 *  @param matrix       The matrix data to be copied to the vImageCVImageFormatRef. If the matrix is a constant predefined by vImage,
 *                      the address shall be preserved, and returned unmodified by vImageCVImageFormat_GetConversionMatrix.
 *                      The matrix must have a matrix inverse.
 *  @param inType       The type of the matrix. The only type defined for OS X.10 and iOS 8.0 is kvImageMatrixType_ARGBToYpCbCrMatrix, which is a vImage_ARGBToYpCbCrMatrix.
 *
 *  @return
 *          <pre>
 *          @textblock
 *           kvImageNoError             Success.
 *
 *           kvImageInvalidParameter    The matrix type did not match that required for the image format.
 *
 *           kvImageInvalidParameter    The matrix is not invertible. (See console for log in this case.)
 *          @/textblock
 *          </pre>
 *
 *  @seealso vImage_ARGBToYpCbCrMatrix
 *  @seealso vImageMatrixType
 */
VIMAGE_PF vImage_Error vImageCVImageFormat_CopyConversionMatrix(vImageCVImageFormatRef format, const void *matrix, vImageMatrixType inType)  VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetAlphaHint
 *  @abstract Get the alpha-is-one hint from a vImageCVImageFormatRef
 *  @discussion  Some image formats have an alpha channel. Sometimes, the alpha channel for the entire image is known to be 1.0 (fully opaque).
 *               In some circumstances, that knowledge can be used to eliminate work from a conversion to make it faster, especially when converting
 *               to a format without an alpha channel.  If the alpha-is-one hint is non-zero, it indicates that the alpha channel is fully opaque.
 *
 *               Images that do not have an alpha channel will also return non-zero.
 *
 *               There are a few image formats that have room for an alpha channel (kCVPixelFormatType_16BE555, kCVPixelFormatType_16LE555,
 *               kCVPixelFormatType_30RGB) but which do not have an alpha channel. Setting the alpha-is-one hint to 0 does not add an alpha
 *               channel to these image types.
 *
 *               The alpha-is-one hint is a hint.  It can not be absent in a way that will prevent conversion. If it is not set or is zero, and
 *               the image format has alpha, then the alpha channel will be included in the calculation. If the result format has alpha, the alpha
 *               will propagate there. If the result format does not have alpha, the image will be flattened against the indicated background color
 *               for the conversion.
 *
 *  @param format       The vImageCVImageFormatRef for which the colorspace is to be set.
 *  @return  0  Alpha is not known to be opaque, or the hint has not been set.
 *
 *           non-zero  Alpha is known to be fully opaque, even if the values encoded for alpha in the image are not 1.0.
 */
VIMAGE_PF int vImageCVImageFormat_GetAlphaHint( vImageConstCVImageFormatRef format)  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_SetAlphaHint
 *  @abstract Set the alpha-is-one hint for a vImageCVImageFormatRef
 *  @discussion  Some image formats have an alpha channel. Sometimes, the alpha channel for the entire image is known to be 1.0 (fully opaque).
 *               In some circumstances, that knowledge can be used to eliminate work from a conversion to make it faster, especially when converting
 *               to a format without an alpha channel.  If the alpha-is-one hint is non-zero, it indicates that the alpha channel is fully opaque.
 *
 *               The alpha-is-one hint is a hint.  It can not be absent in a way that will prevent conversion. If it is not set or is zero, and
 *               the image format has alpha, then the alpha channel will be included in the calculation. If the result format has alpha, the alpha
 *               will propagate there. If the result format does not have alpha, the image will be flattened against the indicated background color
 *               for the conversion.
 *
 *               There are a few image formats that have room for a small alpha channel (kCVPixelFormatType_16BE555, kCVPixelFormatType_16LE555,
 *               kCVPixelFormatType_30RGB) but which do not have an alpha channel. Setting the alpha-is-one hint to 0 does not add an alpha
 *               channel to these image types.  If this behavior is desired, such image formats can generally be described using a vImage_CGImageFormat.
 *               If so, you can set the vImage_CGImageFormat.bitmap info to an appropriate CGImageAlphaInfo for the desired treatment for the alpha
 *               channel and convert using vImageConverter_CreateWithCGImageFormat() + vImageConvert_AnyToAny().
 *
 *  @param format       The vImageCVImageFormatRef for which the colorspace is to be set.
 *  @parma alphaIsOne   The new value for the alpha-is-one hint.
 *  @return  kvImageNoError             Success.
 *
 *           kvImageInvalidParameter    format is NULL
 */
VIMAGE_PF vImage_Error vImageCVImageFormat_SetAlphaHint( vImageCVImageFormatRef format, int alphaIsOne)  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetChannelDescription
 *  @abstract Get the channel description for a particular channel type
 *  @discussion  The channel description gives information about the range of values and clamping for a image color channel.
 *
 *  @param format       The vImageCVImageFormatRef that the channel description is for.
 *  @parma type         The type of the channel that you wish information about. Example: kvImageBufferTypeCode_Luminance
 *  @return  A const pointer to a vImageChannelDescription struct. The data in the structure may not be modified and belongs to the vImageCVImageFormatRef.
 *           It is destroyed when the vImageCVImageFormatRef is destroyed.
 *  @seealso vImageChannelDescription
 */
VIMAGE_PF const vImageChannelDescription *vImageCVImageFormat_GetChannelDescription( vImageConstCVImageFormatRef format, vImageBufferTypeCode type ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_CopyChannelDescription
 *  @abstract Set the channel description for a particular channel type
 *  @discussion  The channel description gives information about the range of values and clamping for a image color channel.
 *
 *  @param format       The vImageCVImageFormatRef that the channel description is for.
 *  @param desc         A pointer to a new vImageChannelDescription to use for the channel type.  The data is copied into the vImageCVImageFormatRef.
 *  @param type         The type of the channel that you wish to set information about. Example: kvImageBufferTypeCode_Luminance
 *  @return  kvImageNoError     Success
 *
 *           kvImageInvalidParameter    An invalid vImageBufferTypeCode, either out of range, or the channel type does not appear in the image format
 *  @seealso vImageChannelDescription
 */
VIMAGE_PF vImage_Error    vImageCVImageFormat_CopyChannelDescription( vImageCVImageFormatRef format, const vImageChannelDescription *desc, vImageBufferTypeCode type ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_GetUserData
 *  @abstract Get the user info pointer attached to the image format
 *  @discussion  There may be extra information that you wish to attach to a vImageCVImageFormatRef.  It might be a pthread_rwlock_t to help prevent
 *               concurrent access to the vImageCVImageFormatRef while it is being modified, or perhaps additional metadata about the image format
 *               that you may need later. It may even just a pointer to an object you wrote which wraps the vImageCVImageFormatRef.
 *
 *               The user data pointer is available for you to use to store a reference to this information. The token is opaque to vImage. vImage
 *               only returns it when asked via vImageCVImageFormat_GetUserData.  It can be set with vImageCVImageFormat_SetUserData.
 *
 *  @param format       The vImageCVImageFormatRef to get the userData from.
 *  @return  The address of the userData. It will be NULL if no userData has been set.
 *  @seealso vImageCVImageFormat_SetUserData
 */
VIMAGE_PF void *          vImageCVImageFormat_GetUserData( vImageConstCVImageFormatRef format)   VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageCVImageFormat_SetUserData
 *  @abstract  Sets the userData pointer and a userDataReleaseCallback function
 *  @seealso vImageCVImageFormat_SetUserData
 *  @discussion  The userDataReleaseCallback is called when the vImageCVImageFormatRef is destroyed. You may access the vImageCVImageFormatRef
 *               during the callback function. However vImageCVImageFormat_Retain() will not prevent the destruction of the object in that context.
 *               The userDataReleaseCallback will also be called on the previous user data in the event that vImageCVImageFormat_SetUserData
 *               is called to replace one set of user date with another.
 *
 *              CAUTION: vImage does not attempt to do anything smart when the old and new userData are actually the same or differ only by callback.
 *
 *              vImage does not attempt to free the user data when the vImageCVImageFormatRef is destroyed. If the userData needs to
 *              be freed/released/etc. at this time, then you should do so in your userDataReleaseCallback.
 *
 *              vImageCVImageFormat_SetUserData function is not atomic. vImageCVImageFormat_SetUserData is not safe to call reentrantly.
 *
 *              Since there can be only one userData attached to a vImageCVImageFormatRef, the userData field is reserved by convention
 *              for exclusive use by the app/framework/library that created the vImageCVImageFormatRef.  If you need to attach your own
 *              userData to a vImageCVImageFormatRef that you did not create, make a copy of it with vImageCVImageFormat_Copy.  The new
 *              copy will not have userData attached to it.
 *
 *  @param format       The vImageCVImageFormatRef to get the userData from.
 *  @param userData     The new userData pointer.
 *  @param userDataReleaseCallback  The callback that is called when the vImageCVImageFormatRef is destroyed, or
 *                                  when the userData is replaced with another one.
 *  @param callback_fmt             The vImageCVImageFormatRef that the userData is attached to.
 *  @param callback_userData        The userData field attached to callback_fmt.
 *  @return  kvImageNoError - Success
 *  @return  kvImageInvalidImageFormat  - Format is NULL
 *
 */
VIMAGE_PF vImage_Error    vImageCVImageFormat_SetUserData(vImageCVImageFormatRef format, void * userData, void (*userDataReleaseCallback)(vImageCVImageFormatRef callback_fmt, void *callback_userData) )   VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup  Low level colorspace initialization */

typedef struct vImageTransferFunction
{
    CGFloat c0, c1, c2, c3, gamma;          // R' = c0 * pow( c1 * R + c2, gamma ) + c3,    (R >= cutoff)
    CGFloat cutoff;                         // See immediately above and below.  For no linear region (no below segment), pass -INFINITY here.
    CGFloat c4, c5;                         // R' = c4 * R + c5                             (R < cutoff)
}vImageTransferFunction;

typedef struct vImageRGBPrimaries
{
    float red_x, green_x, blue_x, white_x;
    float red_y, green_y, blue_y, white_y;
}vImageRGBPrimaries;

/*!
 * @function vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction
 *
 * @abstract Create a RGB colorspace based on primitives typically found in Y'CbCr specifications
 *
 * @discussion This function may be used to create a CGColorSpaceRef to correspond with a given set of color
 * primaries and transfer function. This defines a RGB colorspace. (A Y'CbCr colorspace is defined as a RGB
 * colorspace and a conversion matrix from RGB to Y'CbCr.) The color primaries give the extent of a colorspace
 * in x,y,z space and the transfer function gives the transformation from linear color to non-linear color that
 * the pixels actually reside in.
 *
 *   <pre>
 *   @textblock
 *      Example:  ITU-R BT.709-5
 *
 *          const vImageTransferFunction f709 =
 *          {  // 1.2 transfer function
 *              .c0 = 1.099,
 *              .c1 = 1.0,
 *              .c2 = 0.0,
 *              .c3 = -0.099,
 *              .gamma = 0.45,
 *              .cutoff = 0.018,
 *              .c4 = 4.5,
 *              .c5 = 0
 *          };
 *
 *          const vImageRGBPrimaries p709 =
 *          {
 *              .red_x = .64,               // 1.3 red
 *              .green_x = .30,             // 1.3 green
 *              .blue_x = .15,              // 1.3 blue
 *              .white_x = 0.3127,          // 1.4 white
 *
 *              .red_y = .33,               // 1.3 red
 *              .green_y = .60,             // 1.3 green
 *              .blue_y = .06,              // 1.3 blue
 *              .white_y = 0.3290           // 1.4 white
 *          };
 *
 *          vImage_Error err = kvImageNoError;
 *          CGColorSpaceRef colorSpace = vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction( &p709, &f709, kvImageNoFlags, &err );
 *          @/textblock
 *          </pre>
 *
 *  Other methods to create a RGB colorspace:
 *
 *      You may find it easier to use CVImageBufferCreateColorSpaceFromAttachments or CVImageBufferGetColorSpace, in some cases.
 *      If there is enough color information attached to a CVPixelBuffer, you can also get one using vImageCVImageFormat_CreateWithCVPixelBuffer().
 *      There are also many ways to create a RGB CGColorSpace in CoreGraphics/CGColorSpace.h.
 *
 *   <pre>
 *   @textblock
 *      Note: This low level function does not conform to CoreVideo practice of automatically substituting gamma 1/1.961
 *      for kCVImageBufferTransferFunction_ITU_R_709_2 and kCVImageBufferTransferFunction_SMPTE_240M_1995 instead of using
 *      the ITU-R BT.709-5 specified transfer function. (vImageBuffer_InitWithCVPixelBuffer and vImageBuffer_CopyToCVPixelBuffer
 *      do.) If you would like that behavior, you can use the following transfer function:
 *
 *      const vImageTransferFunction f709_Apple =
 *      {
 *          .c0 = 1.0,
 *          .c1 = 1.0,
 *          .c2 = 0.0,
 *          .c3 = 0,
 *          .gamma = 1.0/1.961,
 *          .cutoff = -INFINITY,
 *          .c4 = 1,
 *          .c5 = 0
 *      };
 *    @/textblock
 *    </pre>
 *
 *
 *  @param  primaries   A set of x, y tristimulus values to defined the color primaries for the RGB colorspace. Here:
 *
 *          <pre>
 *          @textblock
 *                          x = X/(X+Y+Z),  y = Y/(X+Y+Z)
 *          @/textblock
 *          </pre>
 *
 *                      where X, Y, and Z are from CIEXYZ. z is derived automatically from x and y.
 *
 *  @param  tf          The transfer function to convert from linear RGB (using above primaries) to non-linear RGB.
 *                      The transfer function here is defined in the style of ITU-R BT.709 and is the inverse
 *                      operation of what appears in a ICC color profile.
 *
 *  @param  flags       Currently the only flag recognized here is  kvImagePrintDiagnosticsToConsole, which may be used to
 *                      debug the colorspace creation when it fails.
 *
 *  @param  error       May be NULL. If not NULL, a vImage_Error code is written to the memory pointed to by error to
 *                      indicate success or failure of the operation.
 *
 *
 *  @result On success, a non-NULL RGB CGColorSpaceRef will be returned.  The color space has a reference count of 1.
 *          You are responsible for releasing the colorspace when you are done with it to return the memory back
 *          to the system. If error is not NULL, kvImageNoError is written to *error.
 *
 *          On failure, NULL will be returned and one of the following errors is written to *error if error is non-NULL:
 *
 *          <pre>
 *          @textblock
 *          Errors:
 *
 *              kvImageInvalidParameter         tf->gamma = 0       (transfer function is not round-trippable)
 *              kvImageInvalidParameter         primaries define XYZ <-> RGB matrix which is not invertible
 *          @/textblock
 *          </pre>
 *
 *
 */
VIMAGE_PF CGColorSpaceRef vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction( const vImageRGBPrimaries *primaries,
                                                                                    const vImageTransferFunction *tf,
                                                                                    CGColorRenderingIntent intent,
                                                                                    vImage_Flags flags,
                                                                                    vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

typedef struct vImageWhitePoint
{
    float white_x;
    float white_y;
}vImageWhitePoint;

VIMAGE_PF CGColorSpaceRef vImageCreateMonochromeColorSpaceWithWhitePointAndTransferFunction( const vImageWhitePoint *whitePoint,
                                                                                            const vImageTransferFunction *tf,
                                                                                            CGColorRenderingIntent intent,
                                                                                            vImage_Flags flags,
                                                                                            vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @functiongroup  vImageConverterRef creation */

/*!
 *  @function vImageConverter_CreateForCGToCVImageFormat
 *
 *  @abstract Create a vImageConverterRef that converts a CoreGraphics formatted image to CoreVideo formatted image
 *
 *  @discussion  This creates a vImageConverterRef which may be used with vImageConvert_AnyToAny to convert a
 *              CoreGraphics formatted image, as described by a vImage_CGImageFormat to CV image data, the format of
 *              which is given by a vImageCVImageFormatRef.
 *
 *
 *  @param  srcFormat       The vImage_CGImageFormat that describes the pixel format associated with the source image buffers.
 *
 *  @param  destFormat      The vImageCVImageFormatRef that describes the pixel format associated with the destination buffers.
 *
 *  @param  backgroundColor In cases where the source format has an alpha channel and the destination does not (or is kCGImageAlphaNoneSkipFirst/Last)
 *                               the conversion will remove the alpha channel by flattening it against an opaque background color. The background color
 *                               is given as CGFloat[3] {red, green, blue} (sRGB).
 *
 *  @param  flags           The following flags are honored:
 *
 *          <pre>
 *          @textblock
 *              kvImagePrintDiagnosticsToConsole    cause extra information to be sent to Apple System Logger (Console) in case of failure
 *
 *              kvImageHighQualityResampling        In some cases, chroma may have to be up or downsampled as part of the conversion
 *                                                  When this flag bit is set, it instructs the converter to spend extra time to achieve better
 *                                                  image quality.
 *
 *              kvImageDoNotTile                    Disable multithreading in the conversion step when this converter is used with vImageConvert_AnyToAny.
 *          @/textblock
 *          </pre>
 *
 *
 *  @param  error           An optional pointer to a vImage_Error in which the returned error code is written.
 *                          Error be NULL, in which case no error value will be written.
 *
 *
 *  @result On success, a non-NULL vImageConverteRef will be returned, suitable for use with vImageConvert_AnyToAny(). If
 *          error is non-NULL, kvImageNoError will be written to *error, indicating success. You must release the
 *          vImageConverterRef when you are done with it, to return its resources to the system.  It has a reference count of 1.
 *
 *          On failure, a NULL vImageConverteRef will be returned. If error is non-NULL, an error code will be written to
 *          *error.  Some possible error values:
 *
 *          <pre>
 *          @textblock
 *          kvImageNoError                      Success. No error occurred. A non-NULL vImageConverterRef will be returned.
 *
 *          kvImageInternalError                Your usage was likely correct, but something appears to be very wrong inside
 *                                              vImage. Please file a bug, with a reproducible example of this failure. Please
 *                                              also try the kvImagePrintDiagnosticsToConsole flag for more information.
 *
 *          kvImageInvalidImageFormat           The vImage_CGImageFormat is invalid. kvImagePrintDiagnosticsToConsole may provide
 *                                              more information.
 *
 *          kvImageInvalidCVImageFormat         The vImageCVImageFormatRef is invalid. Probably, the vImageCVImageFormatRef is
 *                                              incomplete. This can happen when a vImageCVImageFormatRef is created from a
 *                                              CVPixelBufferRef and that itself has incomplete conversion information. Please
 *                                              see "vImageCVImageFormatRef Repair" above. kvImagePrintDiagnosticsToConsole
 *                                              may provide more information.
 *
 *          kvImageCVImageFormat_ConversionMatrix   The conversion matrix is missing from the vImageCVImageFormatRef. Please add one.
 *          kvImageCVImageFormat_ChromaSiting       The chroma siting info is missing from the vImageCVImageFormatRef. Please add.
 *          kvImageCVImageFormat_ColorSpace         The colorspace containing primaries and transfer function is missing from the vImageCVImageFormatRef.
 *          @/textblock
 *          </pre>
 *
 *  @seealso    vImageBuffer_InitForCopyToCVPixelBuffer When converting from CVPixelBuffer types with vImageConvert_AnyToAny, the CV format sometimes contains multiple data planes which are in turn represented by multiple vImage_Buffers.
 *  @seealso    vImageConverter_GetDestinationBufferOrder for manual ordering information
 */

VIMAGE_PF vImageConverterRef vImageConverter_CreateForCGToCVImageFormat( const vImage_CGImageFormat *srcFormat,
                                                                        vImageCVImageFormatRef destFormat,
                                                                        const CGFloat *backgroundColor,
                                                                        vImage_Flags flags,
                                                                        vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
/*!
 *  @function vImageConverter_CreateForCVToCGImageFormat
 *
 *  @abstract Create a vImageConverterRef that converts a CoreVideo formatted image to a CoreGraphics formatted image
 *
 *  @discussion  This creates a vImageConverterRef which may be used with vImageConvert_AnyToAny to do conversions of
 *              CV image data, as described by a vImageCVImageFormatRef to CoreGraphics formatted image data, as
 *              described by a vImage_CGImageFormat.
 *
 *
 *  @param  srcFormat       The vImageCVImageFormatRef that describes the pixel format associated with the source image buffers.
 *
 *  @param  destFormat      The vImage_CGImageFormat that describes the pixel format associated with the destination buffers.
 *
 *  @param  backgroundColor In cases where the source format has an alpha channel and the destination does not (or is kCGImageAlphaNoneSkipFirst/Last)
 *                               the conversion will remove the alpha channel by flattening it against an opaque background color. The background color
 *                               is given as CGFloat[3] {red, green, blue} (sRGB).
 *
 *  @param  flags           The following flags are honored:
 *
 *          <pre>
 *          @textblock
 *              kvImagePrintDiagnosticsToConsole    cause extra information to be sent to Apple System Logger (Console) in case of failure
 *
 *              kvImageHighQualityResampling        In some cases, chroma may have to be up or downsampled as part of the conversion
 *                                                  When this flag bit is set, it instructs the converter to spend extra time to achieve better
 *                                                  image quality.
 *
 *              kvImageDoNotTile                    Disable multithreading in the conversion step when this converter is used with vImageConvert_AnyToAny.
 *          @/textblock
 *          </pre>
 *
 *
 *  @param  error           An optional pointer to a vImage_Error in which the returned error code is written.
 *                          Error be NULL, in which case no error value will be written.
 *
 *
 *  @result On success, a non-NULL vImageConverteRef will be returned, suitable for use with vImageConvert_AnyToAny(). If
 *          error is non-NULL, kvImageNoError will be written to *error, indicating success. You must release the
 *          vImageConverterRef when you are done with it, to return its resources to the system.  It has a reference count of 1.
 *
 *          On failure, a NULL vImageConverteRef will be returned. If error is non-NULL, an error code will be written to
 *          *error.  Some possible error values:
 *
 *          <pre>
 *          @textblock
 *          kvImageNoError                      Success. No error occurred. A non-NULL vImageConverterRef will be returned.
 *
 *          kvImageInternalError                Your usage was likely correct, but something appears to be very wrong inside
 *                                              vImage. Please file a bug, with a reproducible example of this failure. Please
 *                                              also try the kvImagePrintDiagnosticsToConsole flag for more information.
 *
 *          kvImageInvalidImageFormat           The vImage_CGImageFormat is invalid. kvImagePrintDiagnosticsToConsole may provide
 *                                              more information.
 *
 *          kvImageInvalidCVImageFormat         The vImageCVImageFormatRef is invalid. Probably, the vImageCVImageFormatRef is
 *                                              incomplete. This can happen when a vImageCVImageFormatRef is created from a
 *                                              CVPixelBufferRef and that itself has incomplete conversion information. Please
 *                                              see "vImageCVImageFormatRef Repair" above. kvImagePrintDiagnosticsToConsole
 *                                              may provide more information.
 *
 *          kvImageCVImageFormat_ConversionMatrix   The conversion matrix is missing from the vImageCVImageFormatRef. Please add one.
 *          kvImageCVImageFormat_ChromaSiting       The chroma siting info is missing from the vImageCVImageFormatRef. Please add.
 *          kvImageCVImageFormat_ColorSpace         The colorspace containing primaries and transfer function is missing from the vImageCVImageFormatRef.
 *          @/textblock
 *          </pre>
 *
 *  @seealso    vImageBuffer_InitForCopyFromCVPixelBuffer When converting from CVPixelBuffer types with vImageConvert_AnyToAny, the CV format sometimes contains multiple data planes which are in turn represented by multiple vImage_Buffers.
 *  @seealso    vImageConverter_GetSourceBufferOrder for manual ordering information
 */
VIMAGE_PF vImageConverterRef vImageConverter_CreateForCVToCGImageFormat( vImageCVImageFormatRef srcFormat,
                                                                        const vImage_CGImageFormat *destFormat,
                                                                        const CGFloat *backgroundColor,
                                                                        vImage_Flags flags,
                                                                        vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*! @functiongroup  vImage_Buffer Initialization */


/*!
 * @function vImageBuffer_InitForCopyToCVPixelBuffer
 *
 * @abstract Initialize an array of vImage_Buffers in the right order to convert a image to a CV formatted image
 *
 * @discussion When converting to CVPixelBuffer types with vImageConvert_AnyToAny, the CV format sometimes contains
 * multiple data planes which are in turn represented by multiple vImage_Buffers. (These are passed in as an array
 * of vImage_Buffers to vImageConvert_AnyToAny().)  To make it easier to order the buffers correctly, we provide
 * vImageBuffer_InitForCopyToCVPixelBuffer, which initializes an array vImage_Buffer structs  in the order expected by
 * vImageConvert_AnyToAny. With appropriate flags, the conversion can be made to occur directly into the CVPixelBufferRef
 * backing store.
 *
 * You are responsible for updating any missing / incorrect color information in the pixelBuffer after writing to it.
 *
 * @param buffers   A pointer to an array of vImage_Buffer structs to be overwritten. The buffers will be initialized
 *                  in the correct order for use with vImageConvert_AnyToAny and the provided converter. On entry,
 *                  buffers must point to a valid region of memory of size no smaller than number_of_buffers *
 *                  sizeof(vImage_Buffer). The number_of_buffers is given by vImageConverter_GetNumberOfDestinationBuffers.
 *                  The buffers pointer may not be NULL.
 *
 * @param converter The converter that will be used to do the conversion.  May not be NULL.
 *
 * @param pixelBuffer   A locked (use CVPixelBufferLockBaseAddress) CVPixelBufferRef.
 *
 * @param  flags    kvImageNoAllocate must be used.  The following flags are allowed:
 *
 *          <pre>
 *          @textblock
 *              kvImageNoFlags                      Default operation.
 *
 *              kvImagePrintDiagnosticsToConsole    Print diagnostic messages to the console in the event an error occurs
 *
 *              kvImageNoAllocate                   Instructs the function to initialized the buffers to directly write to a
 *                                                  locked CVPixelBufferRef. You  may unlock the CVPixelBufferRef after
 *                                                  vImageConvert_AnyToAny has returned. Once the pixelBuffer is unlocked,
 *                                                  the vImage_Buffers initialized by this function are no longer valid and
 *                                                  must be reinitialized.
 *
 *          @/textblock
 *          </pre>
 *
 * @return  The following error codes may be returned:
 *
 *          <pre>
 *          @textblock
 *      kvImageNoError                  Success
 *
 *      kvImageNullPointerArgument      buffers is NULL.
 *      kvImageNullPointerArgument      converter is NULL.
 *
 *      kvImageInvalidParameter         pixelBuffer is not NULL but kvImageNoAllocate was not passed in flags. See pixelBuffer description above.
 *
 *      kvImageUnknownFlagsBit          An unknown / unhandled flags bit was set in flags.
 *
 *      kvImageInternalError            Something is very wrong inside vImage. This shouldn't happen. Please file a bug, along with a
 *                                      reproducible failure case.
 *          @/textblock
 *          </pre>
 *
 * @seealso vImageConverter_GetDestinationBufferOrder for another method to initialize the vImage_Buffers in the right order for vImageConvert_AnyToAny.
 *
 */
VIMAGE_PF vImage_Error vImageBuffer_InitForCopyToCVPixelBuffer( vImage_Buffer *buffers,
                                                               const vImageConverterRef converter,
                                                               const CVPixelBufferRef pixelBuffer,
                                                               vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 * @function vImageBuffer_InitForCopyFromCVPixelBuffer
 *
 * @abstract Initialize an array of vImage_Buffers in the right order to convert CV formatted image to another image format
 *
 * @discussion When converting from CVPixelBuffer types with vImageConvert_AnyToAny, the CV format sometimes contains
 * multiple data planes which are in turn represented by multiple vImage_Buffers. (These are passed in as an array
 * of vImage_Buffers to vImageConvert_AnyToAny().)  To make it easier to order the buffers correctly, we provide
 * vImageBuffer_InitForCopyFromCVPixelBuffer, which initializes an array vImage_Buffer structs  in the order expected by
 * vImageConvert_AnyToAny. With appropriate flags, the conversion can be made to occur directly from the CVPixelBufferRef
 * backing store.
 *
 * @param buffers   A pointer to an array of vImage_Buffer structs to be read. The buffers will be initialized
 *                  in the correct order for use with vImageConvert_AnyToAny and the provided converter. On entry,
 *                  buffers must point to a valid region of memory of size no smaller than number_of_buffers *
 *                  sizeof(vImage_Buffer). The number_of_buffers is given by vImageConverter_GetNumberOfSourceBuffers.
 *                  The buffers pointer may not be NULL.
 *
 * @param converter The converter that will be used to do the conversion.  May not be NULL.
 *
 * @param pixelBuffer   A locked (use CVPixelBufferLockBaseAddress) CVPixelBufferRef.
 *
 * @param  flags    kvImageNoAllocate must be used. The following flags are allowed:
 *
 *          <pre>
 *          @textblock
 *              kvImageNoFlags                      Default operation.
 *
 *              kvImagePrintDiagnosticsToConsole    Print diagnostic messages to the console in the event an error occurs
 *
 *              kvImageNoAllocate                   Instructs the function to initialized the buffers to directly read from a
 *                                                  locked CVPixelBufferRef. You  may unlock the CVPixelBufferRef after
 *                                                  vImageConvert_AnyToAny has returned. Once the pixelBuffer is unlocked,
 *                                                  the vImage_Buffers initialized by this function are no longer valid and
 *                                                  must be reinitialized.
 *
 *          @/textblock
 *          </pre>
 *
 * @return  The following error codes may be returned:
 *
 *          <pre>
 *          @textblock
 *      kvImageNoError                  Success
 *
 *      kvImageNullPointerArgument      buffers is NULL.
 *      kvImageNullPointerArgument      converter is NULL.
 *
 *      kvImageInvalidParameter         pixelBuffer is not NULL but kvImageNoAllocate was not passed in flags. See pixelBuffer description above.
 *
 *      kvImageUnknownFlagsBit          An unknown / unhandled flags bit was set in flags.
 *
 *      kvImageInternalError            Something is very wrong inside vImage. This shouldn't happen. Please file a bug, along with a
 *                                      reproducible failure case.
 *          @/textblock
 *          </pre>
 *
 * @seealso  vImageConverter_GetSourceBufferOrder vImageConverter_GetSourceBufferOrder for another method to initialize the vImage_Buffers in the right order for vImageConvert_AnyToAny.
 *
 */
VIMAGE_PF vImage_Error vImageBuffer_InitForCopyFromCVPixelBuffer( vImage_Buffer *buffers,
                                                                 const vImageConverterRef converter,
                                                                 const CVPixelBufferRef pixelBuffer,
                                                                 vImage_Flags flags )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));



#ifdef __cplusplus
}
#endif


#endif  /* vImage_vImage_CVUtilities_h */


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/vImage_Utilities.h
/*!
*  @header vImage_Utilities.h
*  vImage
*
*  Created by Ian Ollmann on 6/7/12.
*
*  See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2012-2016 by Apple Inc. All rights reserved.
*
*  @discussion These interfaces provide methods to help exchange data between CoreGraphics and vImage
*              and provides conversion routines to convert nearly any image format to nearly any
*              other image format.  They are intended to streamline vImage adoption, and make it possible
*              for your application's imaging pipeline to gracefully handle a wide diversity of image formats.
*              When conversions are necessary, they are vectorized and multithreaded to minimize cost in
*              time and energy.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*  @ignorefuncmacro VIMAGE_CHOICE_ENUM
*/

#ifndef vImage_Utilities_h
#define vImage_Utilities_h

#include <vImage/vImage_Types.h>
#include <CoreGraphics/CoreGraphics.h>  /* #define vImage_Utilities_h 1 before including Accelerate headers to turn this header off */

#ifdef __cplusplus
extern "C" {
#endif

#ifndef VIMAGE_PF
    #define VIMAGE_PF __attribute__ ((visibility ("default")))
#endif
    
/*!
 * @const kvImageDecodeArray_16Q12Format
 * @abstract Predefined decode array constant to use with 16Q12 formatted data
 * @discussion 16Q12 data is a signed 16-bit fixed point integer. The format is implicitly divided by 2**12
 *             to give a range of [-8,8)  (SHRT_MIN/4096,SHRT_MAX/4096). The type is present to allow
 *             8-bit content to be converted into other colorspaces and operated on without undue
 *             loss of precision or loss of color gamut due to clamping. This constant is "magic" in the
 *             sense that it is identified by address. Copying the values here will cause a CG format to
 *             be instead interpreted as a _unsigned_ 16 bit format.
 *
 *             16Q12 pixels do not follow CG image format conventions in two respects. The format is signed.
 *             The alpha channel is subject to the decode array transform too, meaning that 0 is transparent
 *             and 4096 opaque. Consequently, ALL buffers that use this format must be tagged with the
 *             kvImageDecodeArray_16Q12Format decode array.
 *
 */
extern VIMAGE_PF const CGFloat * kvImageDecodeArray_16Q12Format;

/*!
 * @struct vImage_CGImageFormat
 * @abstract A pixel format
 * @discussion A vImage_CGImageFormat describes the ordering of the color channels, how many there are,
 * the size and type of the data in the color channels and whether the data is premultiplied by alpha or not.
 * This format mirrors the image format descriptors used by CoreGraphics to create things like CGImageRef and
 * CGBitmapContextRef.
 *
 * This vImage_CGImageFormat:
 *
 *  <pre>@textblock
 *      vImage_CGImageFormat format = {
 *          .bitsPerComponent = 8,
 *          .bitsPerPixel = 32,
 *          .colorSpace = CGColorSpaceCreateDeviceRGB(),                                    // don't forget to release this!
 *          .bitmapInfo = kCGImageAlphaPremultipliedFirst | kCGBitmapByteOrder32Little,
 *          .version = 0,                                                                   // must be 0
 *          .decode = NULL,
 *          .renderingIntent = kCGRenderingIntentDefault
 *      };
 *  @/textblock</pre>
 *
 * codes for a little endian ARGB8888 pixel, or what is called in the rest of vImage, BGRA8888. Note: for 16-
 * and 32-bits per component formats (int16_t, uint16_t, half-float, float) most vImage image filters assume
 * the data is in host-endian format. (The APIs in this header do not.) Host-endian is little endian for Intel
 * and ARM, big endian for PowerPC. If the data is not in host-endian format, then you may use
 * vImagePermuteChannels_ARGB8888 or vImageByteSwap_Planar16U to swap the image data byte ordering.
 *
 * Some examples:
 *  <pre>@textblock
 *      ARGB8888     ->  {8, 32, NULL, alpha first, 0, NULL, kCGRenderingIntentDefault}     alpha first = { kCGImageAlphaFirst, kCGImageAlphaPremultipliedFirst, kCGImageAlphaNoneSkipFirst }
 *      RGBA8888     ->  {8, 32, NULL, alpha last,  0, NULL, kCGRenderingIntentDefault}     alpha last  = { kCGImageAlphaLast,  kCGImageAlphaPremultipliedLast,  kCGImageAlphaNoneSkipLast }
 *      BGRA8888     ->  {8, 32, NULL, alpha first | kCGBitmapByteOrder32Little, 0, NULL, kCGRenderingIntentDefault}
 *      RGB888       ->  {8, 24, NULL, kCGImageAlphaNone | kCGBitmapByteOrderDefault, 0, NULL, kCGRenderingIntentDefault}
 *      RGB565       ->  {5, 16, NULL, kCGImageAlphaNone | kCGBitmapByteOrder16Little, 0, NULL, kCGRenderingIntentDefault}
 *      ARGB1555     ->  {5, 16, NULL, alpha first | kCGBitmapByteOrder16Little, 0, NULL, kCGRenderingIntentDefault}
 *      RGBA16F      ->  {16, 64, NULL, alpha last | kCGBitmapFloatComponents | kCGBitmapByteOrder16Little, 0, NULL, kCGRenderingIntentDefault }
 *      CMYK8888     ->  {8, 32, CGColorSpaceCreateDeviceCMYK(), kCGImageAlphaNone, 0, NULL, kCGRenderingIntentDefault  }
 *      ARGBFFFF premultiplied    ->  {32, 128, NULL, kCGImageAlphaPremultipliedFirst | kCGBitmapFloatComponents | kCGBitmapByteOrder32Little, 0, NULL, kCGRenderingIntentDefault }
 *      ARGBFFFF not-premultiplied -> {32, 128, NULL, kCGImageAlphaFirst | kCGBitmapFloatComponents | kCGBitmapByteOrder32Little, 0, NULL, kCGRenderingIntentDefault }
 *      ARGBFFFF, alpha = 1 ->        {32, 128, NULL, kCGImageAlphaNoneSkipFirst | kCGBitmapFloatComponents | kCGBitmapByteOrder32Little, 0, NULL, kCGRenderingIntentDefault }
 *  @/textblock</pre>
 *
 *  Note that some of these formats, particularly RGB565 and 16F formats are supported by vImage but
 *  not necessarily CoreGraphics. They will be converted to a higher precision format as necessary by
 *  vImage in vImageCreateCGImageFromBuffer().
 *
 *  By C rules, uninitialized struct parameters are set to zero. The last three parameters are usually zero, so can usually be omitted.
 *
 *  <pre>@textblock
 *      vImage_CGImageFormat srgb888 = (vImage_CGImageFormat){
 *          .bitsPerComponent = 8,
 *          .bitsPerPixel = 24,
 *          .colorSpace = NULL,
 *          .bitmapInfo = kCGImageAlphaNone | kCGBitmapByteOrderDefault };
 *  @/textblock</pre>
 *
 * To understand how these various parameters relate to one another, we can look at the process of converting from
 * one vImage_CGImageFormat format to another:
 *
 *  1) transform endianness of src format given by bitmapInfo to host endian  (except 8 bitPerComponent content)
 *  2) remove decode array transformation, and up convert to a higher range format as necessary to preserve precision / range
 *  3) convert src colorspace to reference XYZ colorspace (may cause upconvert to preserve range / precision)
 *  4) convert XYZ to destination colorspace + rendering intent
 *  5) convert to destination precision (given by bitsPerComponent)
 *  6) deal with any alpha changes (given by bitmapInfo) or flattening that needs to occur
 *  7) Apply any channel reordering requested, if it didn't happen at an earlier step. (As indicated by src and dest bitmapInfo)
 *  8) Apply destination decode array
 *  9) Apply endianness transform given by dest bitmapInfo
 *
 * Clearly, for most common transformations not all steps need to occur and multiple steps can be collapsed into a compound operation.
 *
 *  @field  bitsPerComponent    The number of bits needed to represent one channel of data in one pixel. For ARGB8888, this would be 8. Expected values: {1, 2, 4, 5, 8, 10, 12, 16, 32}
 *  @field  bitsPerPixel        The number of bits needed to represent one pixel. For ARGB8888, this would be 32.
 *                              It is possible that bitsPerPixel > bitsPerComponent * number of components, but in practice this is rare.
 *                              The number of color components is given by the colorspace and the number of alpha components (0 or 1) is given by
 *                              by the bitmapInfo.
 *  @field  colorSpace          A description of how the pixel data in the image is positioned relative to a reference XYZ color space.
 *                                  See CoreGraphics/CGColorSpace.h.  Pass NULL as a shorthand for sRGB. The vImage_CGImageFormat is not
 *                                  capable of managing the memory held by the colorSpace. If you created the colorspace, you must
 *                                  be sure to release it before all references to it disappear from scope.
 *  @field  bitmapInfo          The CGBitmapInfo describing the color channels. See CoreGraphics/CGImage.h.
 *                                  ARGB8888 is kCGImageAlphaFirst | kCGBitmapByteOrderDefault
 *                                  BGRA8888 is kCGImageAlphaFirst | kCGBitmapByteOrder32Little
 *  @field  version             The struct is versioned for future expansion.  Pass 0 here.
 *  @field  decode              Prior to transformations caused by the colorspace, color channels are subject to a linear transformation.
 *                              This allows for a different range than the typical [0,1.0]. NULL indicates default behavior of [0,1.0]
 *                              range, and is what you should use if you don't understand this parameter. See description of CGImageCreate()
 *                              for a discussion of decode arrays. See also Decode Arrays section of Chapter 4.8 of the PDF specification.
 *                              The vImage_CGImageFormat is not capable of managing the memory held by the decode array. If you created a
 *                              decode array on the heap, you must be sure to release it before all references to it disappear from scope.
 *
 *  @field renderingIntent      See CGColorSpace.h. kCGRenderingIntentDefault is typical here. By convention, rendering intent changes that
 *                              are not accompanied by a colorspace change are ignored.
 */
typedef struct vImage_CGImageFormat
{
    uint32_t                bitsPerComponent;
    uint32_t                bitsPerPixel;
    CGColorSpaceRef         colorSpace;
    CGBitmapInfo            bitmapInfo;
    uint32_t                version;
    const CGFloat *         decode;
    CGColorRenderingIntent  renderingIntent;
}vImage_CGImageFormat;



/*!
 *  @functiongroup vImage_Buffer utilities
 *  @discussion Convenience methods for working with vImage_Buffers.
 */

/*!
 *  @function vImageBuffer_Init
 *  @abstract Convenience function to allocate a vImage_Buffer of desired size
 *  @discussion This function is a convenience method to help initialize a vImage_Buffer struct with a buffer sized
 *  and aligned for best performance. It will initialize the height, width and rowBytes fields, and allocate
 *  the pixel storage for you. You are responsible for releasing the memory pointed to by buf->data back to
 *  the system when you are done with it using free(). If no such allocation is desired, pass
 *  kvImageNoAllocate in the flags to cause buf->data to be set to NULL and the preferred alignment
 *  to be returned from the left hand side of the function.
 *
 *  Here is an example of typical usage:
 *
 *  <pre>@textblock
 *      vImage_Buffer buf;
 *      vImage_Error err = vImageBuffer_Init( &buf, height, width, 8 * sizeof(pixel), kvImageNoFlags);
 *      ...
 *  @/textblock</pre>
 *
 *  And typical usage using your own allocator (posix_memalign in this case):
 *
 *  <pre>@textblock
 *      vImage_Buffer buf;
 *      ssize_t alignment = vImageBuffer_Init( &buf, height, width, 8 * sizeof(pixel), kvImageNoAllocate);
 *      if( alignment >= 0 )  // <0 is an error
 *          error = posix_memalign( &buf.data, alignment, buf.height * buf.rowBytes );  // An allocator
 *  @/textblock</pre>
 *
 *      On return, buf is initialized to contain the provided height and width and best rowBytes for the image.
 *      buf->data will also be allocated by default. If kvImageNoAllocate is passed then allocation is skipped,
 *      buf->data is set to NULL, and the preferred alignment is returned out the left hand side of the
 *      function.
 *
 *  @param buf              A valid empty vImage_Buffer struct. On return, all fields will be initialized.
 *                          Please see behavior of kvImageNoAllocate in the flags parameter below.
 *
 *  @param height           The desired height of the image
 *  @param width            The desired width of the image
 *
 *  @param pixelBits        The number of bits in a pixel of image data. If the image is in a planar format
 *                          then this is the number of bits per color component. If pixelBits is not divisible
 *                          by 8, then vImage will pad the scanline out to a multiple of a byte so that
 *                          two scanlines can not share the same byte and all scanlines start at the start of
 *                          a byte.
 *
 *  @param flags            Must be from the following list:
 *
 *  <pre>@textblock
 *                          kvImageNoAllocate -- on return buf->data is initialized to NULL. A preferred
 *                              alignment suitable for use with posix_memalign is returned out the left hand
 *                              side of the function and buf->rowBytes will be set to the preferred rowBytes.
 *                              If the left hand side return is negative, it is an error code, not a size.
 *
 *                              If the kvImageNoAllocate flag is not passed, then on return buf->data will point
 *                              to a newly allocated buffer with preferred alignment and rowBytes. An appropriate
 *                              error code will be returned from the left hand side.
 *
 *                          kvImagePrintDiagnosticsToConsole -- directs the function to print diagnostic information
 *                              to the console in the event of failure.
 *  @/textblock</pre>
 *
 *  @return  One of the following error codes will be returned out the left hand side.
 *
 *  <pre>@textblock
 *      >0                              kvImageNoAllocate was passed. The value returned indicates the
 *                                      preferred alignment (in bytes) for buf->data. buf->data is NULL.
 *
 *      kvImageNoError                  Success
 *
 *      kvImageMemoryAllocationError    you requested that buf->data be allocated but the allocation failed
 *
 *      kvImageUnknownFlagsBit          flags was not from the list above
 *  @/textblock</pre>
 */

VIMAGE_PF vImage_Error  vImageBuffer_Init( vImage_Buffer *         buf,
                                          vImagePixelCount        height,
                                          vImagePixelCount        width,
                                          uint32_t                pixelBits,
                                          vImage_Flags            flags)
VIMAGE_NON_NULL(1)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*!
 *  @function vImageBuffer_GetSize
 *  @abstract Returns size of a vImage_Buffer as a CGSize.
 *  @discussion The CGSize / NSSize is rounded down to the nearest representable
 *  CGFloat that is less than or equal to the actual size of the image. In practice
 *  the conversion will always be exact, except for really, really big images. In
 *  that case, some part of the bottom or right edge might be truncated.
 *
 *  <pre>@textblock
 *  Rationale: If you attempt your own home-made conversion to CGSize / NSSize by
 *            ordinary C rules and the value rounds, it will round up half the time.
 *            This could lead to a crash later because the height or width
 *            will be reported to be larger than it really is and an ensuing image
 *            operation will attempt to touch scanlines that don't exist.
 *  @/textblock</pre>
 *
 *  @param buf  A pointer to a valid vImage_Buffer
 *
 *  @return  The largest CGSize that will fit in the buffer. In typical usage, this
 *           is equal to the size of the buffer.
 */
VIMAGE_PF CGSize vImageBuffer_GetSize( const vImage_Buffer *buf )
VIMAGE_NON_NULL(1)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));





/*!
 * @functiongroup vImage_CGImageFormat functions
 * @discussion    A vImage_CGImageFormat describes a pixel format compatible with CoreGraphics.
 *                Most access to the vImage_CGImageFormat struct is done directly through memberwise
 *                access, but a few tasks are complicated enough to deserve their own library function.
 */

/*!
 * @function vImageCGImageFormat_GetComponentCount
 * @abstract Calculate the number of channels (color + alpha) for a given image format
 * @discussion  The number of channels may not be safely calculated as bitsPerPixel / bitsPerComponent.
 *              Use this routine instead.
 * @param    format     A pointer to a valid vImage_CGImageFormat.  If format->colorspace is NULL,
 *                      the format is assumed to belong to the sRGB colorspace.
 *
 * @return   Returns the number of color + alpha channels in the image.
 */
VIMAGE_PF uint32_t vImageCGImageFormat_GetComponentCount( const vImage_CGImageFormat *format )
VIMAGE_NON_NULL(1)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 * @function vImageCGImageFormat_IsEqual
 * @abstract Test to see if two vImage_CGImageFormats are equivalent
 * @discussion Returns nonzero if two vImage_CGImageFormats are the same
 *              If either operand is NULL, the result is false.
 *              If vImage_CGImageFormat.colorSpace is NULL, sRGB is used.
 *
 * @param f1    A pointer to the first vImage_CGImageFormat
 * @param f2    A pointer to the second vImage_CGImageFormat
 *
 * @return  nonzero if two vImage_CGImageFormats are the same
 */
VIMAGE_PF Boolean vImageCGImageFormat_IsEqual( const vImage_CGImageFormat *f1,  const vImage_CGImageFormat *f2 )
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 *  @functiongroup vImage_Buffer CGImageRef interconversion
 *  @discussion High level routines to convert from vImage_Buffer to CGImageRef and back.
 */

/*!
 * @function vImageBuffer_InitWithCGImage
 * @abstract Initialize a vImage_Buffer struct with the contents of a CGImageRef
 * @discussion This function will initialize a vImage_Buffer struct with an image from a CGImageRef.
 * By default, a new region of memory to hold the image data will be allocated by this function.  You may
 * optionally allocate the memory region yourself by passing in the kvImageNoAllocate flag.
 *
 * You may release the CGImageRef and format->colorspace upon successful return of this function.
 *
 * You are responsible for returning the memory referenced by buf->data to the system using free() when you are done with it.
 * The CGImage may have other metadata associated with it, such as camera orientiation, which may require further
 * processing downstream.  vImage just does 1:1 pixel conversions from the raw image source.
 *
 *
 * To create a CGImageRef from a image file on disk:
 *
 *  <pre>@textblock
 *     CFURLRef path = ...;  // path to image
 *     CGImageSourceRef imageSource = CGImageSourceCreateWithURL( path, optionsDictionary ); // optionsDictionary may be NULL
 *     CGImageRef imageRef = CGImageSourceCreateImageAtIndex( imageSource, imageIndex, optionsDictionary );
 *  @/textblock</pre>
 *
 * Similarly, with a chunk of compressed image data in memory:
 *
 *  <pre>@textblock
 *     CFDataRef data = ...;
 *     CGImageSourceRef imageSource = CGImageSourceCreateWithData( path, optionsDictionary );
 *     CGImageRef imageRef = CGImageSourceCreateImageAtIndex( imageSource, imageIndex, optionsDictionary );
 *  @/textblock</pre>
 *
 * You can do similar things through AppKit / UIKit with [NSImage initWithContentsOfFile:],
 * [NSImage initWithContentsOfURL:], or [NSImage initWithData:],  and use [NSImage CGImageForProposedRect:context:hints:]
 * to get out a CGImageRef.
 *
 * @param buf   A pointer to a valid vImage_Buffer struct. The fields of the structure pointed to by buf will
 *              be updated to point to a vImage_Buffer representation of the CGImage. By default, a newly
 *              allocated piece of memory will be used to hold the image. You are responsible
 *              for releasing the memory pointed to by buf->data back to the system using free().
 *
 *                  If you want to allocate the buf->data and initialize rowBytes yourself, then you may pass
 *                  kvImageNoAllocate in the flags parameter. This will cause the buf->data and rowBytes values
 *                  passed into the function to be used directly without modification.  You may find vImageBuffer_Init,
 *                  vImageCGImageFormat_GetPixelBits, CGImageGetWidth and CGImageGetHeight helpful in sizing your buffer.
 *
 * @param format  A pointer to a valid vImage_CGImageFormat specifying the desired image format associated with the
 *                  output buf. If format->colorspace is NULL, sRGB will be used.
 *
 * @param backgroundColor If the CGImageRef encodes an alpha (or mask) and the output format does not have alpha then the
 *                  result will be flattened against a background color. See vImageConverter_CreateWithCGImageFormat
 *                  and functions like vImageFlatten_ARGB8888ToRGB888 for more on flattening. The background color here is
 *                  a series of values of range [0,1] interpreted according to the colorspace passed in format. Example: If
 *                  the format encodes for a AGBR 8-bit image (kCGImageAlphaLast, kCGBitmapByteOrder32Little), then this would be
 *                  {red, green, blue}, the canonical ordering for a RGB colorspace, as an array of three CGFloats.
 *                  If NULL is passed, an array full of zeros is used. The backgroundColor must have at least as many
 *                  CGFloats in it as the colorspace has color channels. See CGColorSpaceGetNumberOfComponents.
 *
 * @param image     A valid CGImageRef to be used as source data.
 *
 * @param flags     You may set the following flags:
 *
 *  <pre>@textblock
 *                  kvImageNoAllocate       the buf->data and buf->rowBytes values passed in are used without modification.
 *                                          This allows you to allocate your own buffer to hold the result. See buf description
 *                                          above.
 *
 *                  kvImagePrintDiagnosticsToConsole    In the event of a problem, print out some helpful debug messages.
 *
 *                  kvImageDoNoTile         It is possible that vImage will have to do an image format conversion from the
 *                                          image native format to the desired format. This will turn off multithreading for
 *                                          that step and any other vImage work that is multithreaded. Since any such conversions
 *                                          are likely happening outside your tiling engine, use of this flag here is
 *                                          probably counterproductive. In rare cases, it might be valuable as a method to
 *                                          leave unoccupied some cores for other tasks, if you have other multithreaded time
 *                                          sensitive tasks running. Likewise, if you are converting multiple images concurrently,
 *                                          it might be helpful to avoid oversubscribing the system.
 *  @/textblock </pre>
 *
 *
 *  @return If the call succeeds, kvImageNoError is returned and the memory region pointed to by buf will be initialized to
 *      describe a valid repesentation of the CGImageRef.
 *
 *      If the call fails, then one of the following error codes will be returned and buf->data will be set to NULL.
 *
 *  <pre>@textblock
 *          kvImageUnknownFlagsBit              flags must be kvImageNoFlags or kvImageNoAllocate
 *          kvImageMemoryAllocationError        Not enough memory to allocate buf->data
 *          kvImageInvalidParameter             format->bitmapInfo has unknown bits set
 *          kvImageInvalidParameter             format->version is not 0
 *          kvImageInvalidParameter             format->decode is not NULL
 *          kvImageInvalidParameter             format->bitsPerComponent is not in {0,1,2,4,5,8,16,32}
 *          kvImageInvalidImageFormat           format->renderingIntent is not a known value
 *          kvImageInvalidImageFormat           The format called for conversion to an input-only colorspace. Some color profiles
 *                                              (e.g. those arising from a scanner) are described as input only, because the device can
 *                                              not produce image output.
 *          kvImageNullPointerArgument          format may not be NULL
 *          kvImageNullPointerArgument          image may not be NULL
 *          kvImageInternalError                Something unexpected went wrong. Please file a bug.
 *  @/textblock </pre>
 *
 */

VIMAGE_PF vImage_Error vImageBuffer_InitWithCGImage(  vImage_Buffer        *  buf,
                                                    vImage_CGImageFormat *  format,
                                                    const CGFloat        *  backgroundColor,
                                                    CGImageRef              image,
                                                    vImage_Flags            flags  )
VIMAGE_NON_NULL(1,2,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*!
 * @function vImageCreateCGImageFromBuffer
 * @abstract Create a CGImageRef from a vImage_Buffer.
 * @discussion This function creates a CGImageRef using the image data in a vImage_Buffer. The CGImageRef has a retain count of 1.
 * By default, a copy of the image data is made. This allows the function to convert to a CG-friendly format as necessary
 * and allows you to continue to use the vImage_Buffer without causing problems for the CGImageRef.
 *
 *  <pre>@textblock
 * No copy mode
 * ------------
 * When the kvImageNoAllocate flag is passed, then "no-copy" operation is said to occur. Ownership of the memory region
 * pointed to by buf->data is transferred to the CGImageRef and it becomes private to and owned by that object and will
 * be used without modification. The memory region pointed to by buf->data will be destroyed when the CGImageRef is
 * destroyed. Caution: CGImageRefs are defined to be immutable once created. Behavior is undefined if you create
 * a CGImageRef then modify the pixels in its backing store.
 *
 * No-copy mode can be a little fussy about formats. When a format is rejected, kvImageInvalidImageFormat will be returned.
 * Formats that are likely to succeed are 8-bit unsigned, 16-bit unsigned and 32-bit floating-point. The image should be
 * kCGImageAlphaNone, kCGImageAlphaLast, kCGImageAlphaPremultipliedLast or kCGImageAlphaNoneSkipLast, and decode = NULL.
 * The image should be in host endian mode. This is kCGBitmapByteOrderDefault for 8-bit per component images, and or larger
 * types, the endianness is given by the endianness of the host system and the size is given by the size of the pixel
 * (bitsPerComponent < 8) or channel (bitsPerComponent > 8). For example RGB565 data should be kCGBitmapByteOrder16Little
 * and 32-bit floating point data should be kCGBitmapByteOrder32Little on little endian processors.
 *
 * It is recommended that if no-copy mode fails, that you try again without the kvImageNoAllocate flag. The call probably
 * will succeed.
 *  @/textblock </pre>
 *
 * CGImage Debugging Note:
 *      The format parameter describes the image data you pass in, but there is no requirement that this is the
 *      format that is actually used to represent the image data held by the CGImage. You should be able to get
 *      back the data in any format you like with vImageBuffer_InitWithCGImage. However, understand that if you
 *      request the data through another API like CGDataProviderCopyData(), it will be formatted as described by
 *      that API -- for CGDataProviderCopyData(), that would be as described by: CGImageGetBitmapInfo, CGImageGetDecode,
 *      CGImageGetRenderingIntent, CGImageGetColorSpace, CGImageGetBytesPerRow, CGImageGetBitsPerPixel,
 *      CGImageGetBitsPerComponent, etc.  Furthermore, the format that those APIs report is also not necessarily
 *      the format of the data held by the CGImageRef.  Common image data consumers like CoreAnimation and
 *      CoreGraphics have their format preferences and vImage caters to them in order to deliver good performance.
 *
 *      CGImageRefs can also be made from vImage_Buffers using CGImageCreate() and CGDataProviderCreateWithData().
 *
 *  @param buf             The vImage_Buffer from which to make the CGImageRef
 *
 *  @param format          The image format of the vImage_Buffer. format may not be NULL.  format->colorspace may be NULL,
 *                  in which case sRGB will be used.  The colorspace is retained as needed by the new CGImage.
 *
 *  @param callback        In no-copy mode, this callback is called to destroy the buf->data when the CGImageRef no longer needs it.
 *                  If NULL is passed for the callback, then free() will be used to destroy buf->data.  userData will be
 *                  passed to the callback function as the userData parameter and buf->data passed as the buf_data parameter.
 *
 *                  This parameter has no effect if kvImageNoAllocate is not in flags.
 *
 *                  The callback may be called at any time from any thread. It is possible for it to be called before
 *                  vImageCreateCGImageFromBuffer returns.
 *
 *                  The callback will not be called if the returned CGImageRef is NULL.
 *
 *  @param userData        The value to pass to the callbacks userData parameter. If callback is NULL or kvImageNoAllocate
 *                  is passed in flags, this value is ignored.
 *
 *  @param flags           The following flags are allowed:
 *  <pre>@textblock
 *                  kvImageNoAllocate                   Causes vImageCreateCGImageFromBuffer to run in no-copy mode.
 *                                                      Ownership of the memory pointed to by buf->data is transferred
 *                                                      to the CGImageRef.  You'll need to set up a callback and userData
 *                                                      to manage releasing the memory back to the system when the CGImage
 *                                                      is done with it.
 *
 *                  kvImagePrintDiagnosticsToConsole    In the event of a problem, print out some helpful debug messages.
 *
 *                  kvImageHighQualityResampling        Sometimes the system will ask for the image to be resamled to
 *                                                      a smaller size. If that happens vImageAffineWarp_<fmt>" will be
 *                                                      called. In that case, the value of this bit will be used to
 *                                                      determine whether Lanczos3 or Lanczos5 resampling is used.
 *
 *                  kvImageDoNotTile                    Disables multithreading in any conversions that need to be done.
 *                                                      Since it seems likely any such conversions will not be running in
 *                                                      the context of your tiling engine (if you wrote one) in this case,
 *                                                      this flag is probably counterproductive in this context. Conversions
 *                                                      can happen later, after this call returns, when the image is drawn.
 *  @/textblock </pre>
 *  @param error           if not NULL, points to a more informative error code to describe what went wrong on exit. May be NULL.
 *                  Testing the result against NULL is sufficient to detect success or failure. kvImagePrintDiagnosticsToConsole
 *                  is another way to get error information.
 *
 * @return  On success, the returned CGImageRef will be non-NULL. If error is not NULL, kvImageNoError will be written there.
 *      On failure, NULL will be returned, and if error is not NULL, a more informative error code will be written there.
 *
 *  <pre>@textblock
 *  Error Values:
 *      kvImageUnknownFlagsBit              flags was not from the list described in the flags parameter above
 *      kvImageMemoryAllocationError        Not enough memory to allocate the new CGImageRef
 *      kvImageInvalidParameter             format->bitmapInfo has unknown bits set
 *      kvImageInvalidParameter             format->bitsPerComponent is not in {5,8,16,32}
 *      kvImageInvalidImageFormat           format->renderingIntent is not a known value
 *      kvImageNullPointerArgument          format may not be NULL
 *      kvImageNullPointerArgument          buf may not be NULL
 *  @/textblock </pre>
 *
 */

VIMAGE_PF CGImageRef vImageCreateCGImageFromBuffer( const vImage_Buffer *buf,
                                                   const vImage_CGImageFormat *format,
                                                   void (*callback)(void *userData, void *buf_data),
                                                   void *userData,
                                                   vImage_Flags flags,
                                                   vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));



/*!
 * @functiongroup vImageConverterRef methods
 * @discussion A vImageConverterRef describes a particular pixel format to pixel format conversion. These functions
 *             create, retain, release and get information about a vImageConverterRef. vImageConvert_AnyToAny() uses
 *             a vImageConverterRef to do a image format conversion.
 */

/*!
 *  @function vImageConverter_Retain
 *  @abstract Retain a vImageConverterRef
 *  @discussion You should retain a vImageConverterRef when you receive it from elsewhere (that is, you did not
 *              create or copy it) and you want it to persist. If you retain a vImageConverterRef you are responsible
 *              for releasing it (see Memory Management Programming Guide for Core Foundation).
 *
 *  Like all of vImage, this interface is thread safe and may be called reentrantly.
 *
 *  @param converter  The vImageConverter to retain. If NULL, then nothing happens.
 */
VIMAGE_PF void vImageConverter_Retain( vImageConverterRef converter ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 *  @function vImageConverter_Release
 *  @abstract Release a vImageConverterRef
 *  @discussion If the retain count of a vImageConverterRef becomes zero, the memory allocated to the
 *              object is deallocated and the object is destroyed. If you create or explicitly
 *              retain (see the vImageConverter_Retain function) a vImageConverterRef, you are responsible for
 *              releasing it when you no longer need it (see Memory Management Programming Guide for Core Foundation).
 *
 *  Like all of vImage, this interface is thread safe and may be called reentrantly.
 *
 *  @param converter  The vImageConverter to release. If NULL, then nothing happens.
 */
VIMAGE_PF void vImageConverter_Release( vImageConverterRef converter ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 *  @function vImageConverter_CreateWithCGImageFormat
 *  @abstract Create a vImageConverterRef to convert from one vImage_CGImageFormat to another
 *  @discussion vImageConverter_CreateWithCGImageFormat creates a vImageConverter to convert between
 *              image formats describable with a vImage_CGImageFormat.  The vImageConverter is intended
 *              to be used (and reused, possibly reentrantly) with vImageConvert_AnyToAny() to convert
 *              images from one format to another.
 *
 *  <pre>@textblock
 *  Image Format Notes:
 *      CG Integer images map the range [0,1.0] (black to full color intensity) to the range [0, TYPE_MAX]. A 8-bit unsigned image
 *  has a range [0,255] and a 16-bit signed image has a range [0,32767]. Floating point images map [0,1.0] to [0,1.0]. Some formats,
 *  like floating-point and signed integers are capable of representing values outside of that range. Conversions involving these
 *  formats will occasionally produce results outside of [0,1]. If the conversion causes a value to go out of the representable range
 *  for the underlying type, it will be clamped to the nearest representable value. This can happen in normal operation during color
 *  correction. You can clamp floating-point results back into [0,1] using vImageClip_PlanarF, if you like.
 *
 *      Usually, colors must be direct mapped. Indexed color is supported format->bitsPerComponent and format->.bitsPerPixel are 1,2,4
 *  or 8, and equal (i.e. grayscale) only. Indexed color is not supported for the destination image. Images with indexed color must use
 *  (kCGImageAlphaNone | kCGBitmapByteOrderDefault) as the bitmapInfo. It is usually faster with indexed color spaces to use
 *  vImageConvert_AnyToAny to convert the color table rather than the image itself when possible, because the table is usually much
 *  smaller than the image. vImageConvert_AnyToAny can not do that automatically for you behind the scenes because it does not return
 *  a modified colorspace.
 *
 *  Device Colorspaces:
 *  Because vImage has no concept of a rendering context or destination graphics device, vImage maps device RGB, device grayscale
 *  and device CMYK to a virtual device which is not your display. The virtual device uses kColorSyncGenericGrayGamma22Profile,
 *  kColorSyncSRGBProfile and kColorSyncGenericCMYKProfile respectively. If you want the image to be converted to the right colorspace
 *  for your display device, you will need to pass in the CGColorSpaceRef for that device obtained from an API like CGDisplayCopyColorSpace().
 *
 *  Black Point Compensation:
 *  By default, BPC is off for this function. If you want black point compensation or other advanced ColorSync effects, construct your
 *  own ColorSyncTransformRef and pass the associated "code fragment" to vImageConverter_CreateWithColorSyncCodeFragment.
 *  See Apple Sample Code "Converting an Image with Black Point Compensation" https://developer.apple.com/library/mac/samplecode/convertImage/Introduction/Intro.html
 *  @/textblock </pre>
 *
 *  See also vImageConverter_CreateForCGToCVImageFormat and vImageConverter_CreateForCVToCGImageFormat for converters
 *  that can operate on CoreVideo formats.
 *
 *  @param srcFormat    A pointer to a populated vImage_CGImageFormat struct describing the image format
 *                      of the source image. If the CGColorSpaceRef is NULL, sRGB will be used
 *                      as the default value. The CGColorSpaceRef will be retained by this function. It
 *                      will be released when the vImageConverter is destroyed.
 *
 *  @param destFormat   A pointer to a populated vImage_CGImageFormat struct describing the image format
 *                      of the destination image. If the CGColorSpaceRef is NULL, sRGB will be
 *                      used as the default value. The CGColorSpaceRef will be retained by this function.
 *                      It will be released when the vImageConverter is destroyed.
 *
 *  @param  backgroundColor Points to an array of floats to be used as a background color if one is needed. The
 *                      backgroundColor range is assumed to be [0,1]. The channel ordering and number of color
 *                      channels must match the natural order of the destination colorSpace (e.g. RGB or CMYK).
 *                      The backgroundColor may be NULL if no background color is needed.
 *
 *                      A background color is used when the image is converted from an alpha-containing format
 *                      to an alpha-none format, in which case the alpha is removed by compositing against the
 *                      opaque background color pointed to by this parameter. If the image is instead converted
 *                      from one alpha containing format to another, then the image will be premultiplied or
 *                      unpremultiplied as necessary and no background color is necessary. (For unpremultiplication,
 *                      the result color value for pixels with alpha 0 is 0.)  Likewise, when converting between
 *                      alpha-none formats, a background color is not used. In the case of kCGImageAlphaNone ->
 *                      kCGImageAlphaNoneSkipFirst/Last, the vacant alpha channel is filled in with 1.0. If NULL
 *                      is passed here, then 0 will be used for the color channels.
 *
 *                      The vImageConverter will contain a copy of the data passed in this field.
 *
 *  @param flags        Any of the following flags are allowed:
 *
 *  <pre>@textblock
 *           kvImagePrintDiagnosticsToConsole    In the event of a problem, print out some helpful debug
 *                                               messages.
 *
 *           kvImageDoNotTile                    A converter created with this flag will operate as if
 *                                               kvImageDoNotTile was passed to vImageConvert_AnyToAny
 *                                               whether it was or not.
 *  @/textblock </pre>
 *
 *  @param error        May be NULL.  If not NULL, then a vImage_Error is returned at the address pointed to by error.
 *                      The vImage_Error will be less than 0 if an error condition occurred. Checking the vImageConverter
 *                      returned to make sure it is non-NULL is sufficient to verify success or failure of the function.
 *
 *                      The following error values can occur:
 *  <pre>@textblock
 *      kvImageNoError                      Success.
 *
 *      kvImageNullPointerArgument          srcFormat and/or destFormat is NULL.
 *
 *      kvImageUnknownFlagsBit              Currently only kvImagePrintDiagnosticsToConsole and kvImageDoNotTile are
 *                                          allowed. All other bits in the flags field must be 0.
 *
 *      kvImageInvalidParameter             backgroundColor is NULL and the conversion needed a backgroundColor
 *
 *      kvImageInvalidImageFormat           bitsPerComponent must be in {1,2,4,5,8,12,16,32}.
 *      kvImageInvalidImageFormat           The base colorspace must be grayscale for destination images using indexed color.
 *      kvImageInvalidImageFormat           The colorspace may be indexed color for images only if it is {1,2,4,8} bitsPerComponent
 *                                          and kCGImageAlphaNone.
 *      kvImageInvalidImageFormat           vImage_CGImageFormat.bitmapInfo & kCGBitmapAlphaInfoMask  does not encode a valid alpha
 *      kvImageInvalidImageFormat           floating point formats must be 16 or 32 bits per component. 16-bit floats are
 *                                          IEEE-754-2008 binary16 interchange format  (a.k.a. OpenEXR half float). 32-bit floats
 *                                          are the standard IEEE-754-2008 binary32 interchange format. (a.k.a float in C/C++/ObjC)
 *      kvImageInvalidImageFormat           format->renderingIntent is not a known value
 *      kvImageInvalidImageFormat           The conversion called for conversion to an input-only colorspace. Some color profiles
 *                                          (e.g. those arising from a scanner) are described as input only, because the device can
 *                                          not produce image output.
 *
 *      kvImageInternalError                The converter was unable to find a path from the source format to the destination format.
 *                                          This should not happen and indicates incorrect operation of the function. Please file a bug.
 *                                          The kvImagePrintDiagnosticsToConsole flag will provide additional diagnostic info.
 *  @/textblock </pre>
 *
 *          In cases where the error code is not sufficient to quickly determine the problem, the kvImagePrintDiagnosticsToConsole flag
 *          should provide additional diagnostic info.
 *
 *  @return  A vImageConverter object with reference count of 1 is created. If the call fails due to an error, then NULL
 *  will be returned. Use vImageConverter_Release to release your reference to it and allow the resources used
 *  by the converter to be returned to the system.
 *
 *  If error is not NULL, an error code will be written to that address on return.
 */

VIMAGE_PF vImageConverterRef vImageConverter_CreateWithCGImageFormat( const vImage_CGImageFormat *srcFormat,
                                                                     const vImage_CGImageFormat *destFormat,
                                                                     const CGFloat *backgroundColor,
                                                                     vImage_Flags flags,
                                                                     vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 * @function vImageConverter_CreateWithColorSyncCodeFragment
 * @abstract Create a vImageConverterRef substituting in a custom ColorSync transform for the one vImage usually generates for the color conversion steps.
 * @discussion vImageConverter_CreateWithColorSyncCodeFragment is like vImageConverter_CreateWithCGImageFormat, except that
 *  instead of creating its own colorspace transform for any colorspace conversions, it uses the one you pass in.
 *  This gives you greater control over the fine details of colorspace conversion, for exacting color fidelity.
 *  The colorspaces for source and destination images must refer to colorspaces that have the same number of channels
 *  as the codeFragment is designed to accept / produce.
 *
 *  See Apple Sample Code "Converting an Image with Black Point Compensation" https://developer.apple.com/library/mac/samplecode/convertImage/Introduction/Intro.html
 *  for an example of usage.
 *
 *  @param codeFragment A code fragment created with ColorSyncTransformCopyProperty( kColorSyncTransformFullConversionData,
 *                      kColorSyncTransformParametricConversionData or kColorSyncTransformSimplifiedConversionData)
 *                      May be NULL.  If NULL, no colorspace conversion / correction is done. In this case,
 *                      behavior is undefined if the colorspaces do not have the same channel order or have a
 *                      different number of channels or the colorspaces are not from the same family.
 *                      kColorSyncTransformFullConversionData is required for black point compensation.
 *                      CAUTION: vImageConverter_CreateWithColorSyncCodeFragment does not verify that the
 *                      codeFragment is actually appropriate for the srcFormat and destFormat provided. Nor
 *                      does it attempt to append additional color space transformation steps to make the
 *                      codeFragment appropriate to the images provided. If the colorspace of the srcFormat
 *                      and destFormat do not correspond to the ColorSyncProfileRefs used to create the
 *                      ColorSync transform in at least colorspace model, then the behavior is undefined.
 *                      See CGColorSpaceModel CoreGraphics/CGColorSpace.h
 *
 *  @param srcFormat    A pointer to a populated vImage_CGImageFormat struct describing the image format
 *                      of the source image. If the CGColorSpaceRef is NULL, sRGB will be used
 *                      as the default value. The CGColorSpaceRef will be retained by this function. It
 *                      will be released when the vImageConverter is destroyed.
 *
 *  @param destFormat   A pointer to a populated vImage_CGImageFormat struct describing the image format
 *                      of the destination image. If the CGColorSpaceRef is NULL, sRGB will be
 *                      used as the default value. The CGColorSpaceRef will be retained by this function.
 *                      It will be released when the vImageConverter is destroyed.
 *
 *  @param  backgroundColor Points to an array of floats to be used as a background color if one is needed. The
 *                      backgroundColor range is assumed to be [0,1]. The channel ordering and number of color
 *                      channels must match the natural order of the destination colorSpace (e.g. RGB or CMYK).
 *                      The backgroundColor may be NULL if no background color is needed.
 *
 *                      A background color is used when the image is converted from an alpha-containing format
 *                      to an alpha-none format, in which case the alpha is removed by compositing against the
 *                      opaque background color pointed to by this parameter. If the image is instead converted
 *                      from one alpha containing format to another, then the image will be premultiplied or
 *                      unpremultiplied as necessary and no background color is necessary. (For unpremultiplication,
 *                      the result color value for pixels with alpha 0 is 0.)  Likewise, when converting between
 *                      alpha-none formats, a background color is not use. In the case of kCGImageAlphaNone ->
 *                      kCGImageAlphaNoneSkipFirst/Last, the vacant alpha channel is filled in with 1.0. If NULL
 *                      is passed here, then 0 will be used for the color channels.
 *
 *                      The vImageConverter will contain a copy of the data passed in this field.
 *
 *  @param flags        Any of the following flags are allowed:
 *
 *  <pre>@textblock
 *           kvImagePrintDiagnosticsToConsole    In the event of a problem, print out some helpful debug
 *                                               messages.
 *
 *           kvImageDoNotTile                    A converter created with this flag will operate as if
 *                                               kvImageDoNotTile was passed to vImageConvert_AnyToAny
 *                                               whether it was or not.
 *  @/textblock </pre>
 *
 *  @param error        May be NULL.  If not NULL, then a vImage_Error is returned at the address pointed to by error.
 *                      The vImage_Error will be less than 0 if an error condition occurred. Checking the vImageConverter
 *                      returned to make sure it is non-NULL is sufficient to verify success or failure of the function.
 *
 *                      The following error values can occur:
 *  <pre>@textblock
 *      kvImageNoError                      Success.
 *
 *      kvImageNullPointerArgument          srcFormat and/or destFormat is NULL.
 *
 *      kvImageUnknownFlagsBit              Currently only kvImagePrintDiagnosticsToConsole and kvImageDoNotTile are
 *                                          allowed. All other bits in the flags field must be 0.
 *
 *      kvImageInvalidParameter             backgroundColor is NULL and the conversion needed a backgroundColor
 *
 *      kvImageInvalidImageFormat           bitsPerComponent must be in {1,2,4,5,8,12,16,32}.
 *      kvImageInvalidImageFormat           The base colorspace must be grayscale for destination images using indexed color.
 *      kvImageInvalidImageFormat           The colorspace may be indexed color for images only if it is {1,2,4,8} bitsPerComponent
 *                                          and kCGImageAlphaNone.
 *      kvImageInvalidImageFormat           vImage_CGImageFormat.bitmapInfo & kCGBitmapAlphaInfoMask  does not encode a valid alpha
 *      kvImageInvalidImageFormat           floating point formats must be 16 or 32 bits per component. 16-bit floats are
 *                                          IEEE-754-2008 binary16 interchange format  (a.k.a. OpenEXR half float). 32-bit floats
 *                                          are the standard IEEE-754-2008 binary32 interchange format. (a.k.a float in C/C++/ObjC)
 *      kvImageInvalidImageFormat           format->renderingIntent is not a known value
 *      kvImageInvalidImageFormat           The conversion called for conversion to an input-only colorspace. Some color profiles
 *                                          (e.g. those arising from a scanner) are described as input only, because the device can
 *                                          not produce image output.
 *
 *      kvImageInvalidImageFormat           codeFragment was found to be otherwise invalid / unusable
 *
 *      kvImageInternalError                The converter was unable to find a path from the source format to the destination format.
 *                                          This should not happen and indicates incorrect operation of the function. Please file a bug.
 *                                          The kvImagePrintDiagnosticsToConsole flag will provide additional diagnostic info.
 *  @/textblock </pre>
 *
 *          In cases where the error code is not sufficient to quickly determine the problem, the kvImagePrintDiagnosticsToConsole flag
 *          should provide additional diagnostic info.
 *
 *  @return  A vImageConverter object with reference count of 1 is created. If the call fails due to an error, then NULL
 *  will be returned. Use vImageConverter_Release to release your reference to it and allow the resources used
 *  by the converter to be returned to the system.
 *
 *  If error is not NULL, an error code will be written to that address on return.
 *
 */

VIMAGE_PF vImageConverterRef vImageConverter_CreateWithColorSyncCodeFragment( CFTypeRef codeFragment,
                                                                             const vImage_CGImageFormat *srcFormat,
                                                                             const vImage_CGImageFormat *destFormat,
                                                                             const CGFloat *backgroundColor,
                                                                             vImage_Flags flags,
                                                                             vImage_Error *error )
VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*!
 * @function  vImageConverter_MustOperateOutOfPlace
 * @abstract  Determine whether a converter is capable of operating in place.
 * @discussion  Some conversions will work if the src and destination image buffer
 *              scanlines start at the same address. Others will not. In such cases,
 *              you need to allocate additional storage to hold the destination buffer.
 *              This function returns kvImageOutOfPlaceOperationRequired if the conversion
 *              requires out of place operation.
 *
 *  <pre>@textblock
 *       In-place operation is considered to mean srcs[i].data = dests[i].data
 *       and srcs[i].rowBytes = dests[i].rowBytes. Other styles of partial buffer
 *       overlap produce undefined behavior.
 *  @/textblock </pre>
 *
 *  The list of source and destination buffers is optional. Results are as follows:
 *
 *  <pre>@textblock
 *      srcs = dests = NULL         kvImageNoError if any conversion with this converter is guaranteed to work in place,
 *                                  provided that srcs[i].data = dests[i].data and srcs[i].rowBytes = dests[i].rowBytes.
 *                                  If there exists at least one combination of height and width for which in place operation
 *                                  is not possible with this converter, then kvImageOutOfPlaceOperationRequired will be returned.
 *
 *      srcs != NULL, dests = NULL  kvImageNullPointerArgument
 *      srcs = NULL, dests != NULL  kvImageNullPointerArgument
 *
 *      srcs != NULL, dests != NULL kvImageNoError if the conversion will successfully operate in place for this particular
 *                                  combination of heights, widths and rowBytes. In this case, vImage does not check to see if the
 *                                  buffers overlap. It presumes that srcs[i].data = dests[i].data.  This is intended to allow
 *                                  you to defer allocation until later.  If in place operation will not work, then
 *                                  kvImageOutOfPlaceOperationRequired is returned.
 *  @/textblock </pre>
 *
 *  In no case during this function call does vImage examine the contents of the memory pointed to by srcs[i].data or dests[i].data.
 *
 *  @param converter           The converter to check
 *
 *  @param srcs                The list of source buffers you plan to use with vImageConvert_AnyToAny. May be NULL.
 *  @param dests               The list of destination buffers you plan to use with vImageConvert_AnyToAny. May be NULL.
 *
 *  @param flags               The flags you plan to pass to vImageConvert_AnyToAny.
 *
 *  <pre>@textblock
 *                              Note: in the case of kvImagePrintDiagnosticsToConsole, the flag means print
 *                              error information to the console for errors caught by vImageConverter_MustOperateOutOfPlace,
 *                              not vImageConvert_AnyToAny. At times, vImageConverter_MustOperateOutOfPlace may fail because
 *                              it detects an error condition that would cause vImageConvert_AnyToAny to fail.
 *  @/textblock </pre>
 *
 *  @return Error Codes:
 *  <pre>@textblock
 *      kvImageNoError                      In-place operation will work
 *      kvImageNullPointerArgument          The converter may not be NULL
 *      kvImageNullPointerArgument          srcs and dests must either both be NULL or neither must be NULL.
 *      kvImageInvalidParameter             The converter is invalid
 *      kvImageUnknownFlagsBit              An unknown / unsupported flag was used
 *      kvImageOutOfPlaceOperationRequired  vImageConvert_AnyToAny requires separate buffers be used for this operation
 *  @/textblock </pre>
 */
VIMAGE_PF vImage_Error vImageConverter_MustOperateOutOfPlace( const vImageConverterRef converter,
                                                             const vImage_Buffer *srcs,
                                                             const vImage_Buffer *dests,
                                                             vImage_Flags flags)
VIMAGE_NON_NULL(1)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*!
 * @function vImageConverter_GetNumberOfSourceBuffers
 * @abstract Get the number of source buffers consumed by the converter.
 * @discussion  All formats discribed by a vImage_CGImageFormat just consume one vImage_Buffer
 *              and produce one vImage_Buffer. There are no multi-plane vImage_CGImageFormats.
 *              However, some video formats (see vImage/vImage_CVUtilities) have planar
 *              data formats with data in more than one plane. For such conversions, it may be
 *              necessary to know how many input buffers are consumed by a converter.
 *
 *              For older operating systems, where these functions are not available,
 *              the number of source and destination buffers is always 1.
 *
 * @param converter The conversion for which you wish to know the number of source buffers
 *
 * @return On success, the number of source buffers is returned.  On failure, 0 is returned.
 */
VIMAGE_PF unsigned long vImageConverter_GetNumberOfSourceBuffers( const vImageConverterRef converter ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 * @function vImageConverter_GetNumberOfDestinationBuffers
 * @abstract Get the number of destination buffers written to by the converter.
 * @discussion  All formats discribed by a vImage_CGImageFormat just consume one vImage_Buffer
 *              and produce one vImage_Buffer. There are no multi-plane vImage_CGImageFormats.
 *              However, some video formats (see vImage/vImage_CVUtilities) have planar
 *              data formats with data in more than one plane. For such conversions, it may be
 *              necessary to know how many out buffers are overwritten by a converter.
 *
 *              For older operating systems, where these functions are not available,
 *              the number of source and destination buffers is always 1.
 *
 * @param converter The conversion for which you wish to know the number of result buffers
 *
 * @return On success, the number of result buffers is returned.  On failure, 0 is returned.
 */
VIMAGE_PF unsigned long vImageConverter_GetNumberOfDestinationBuffers( const vImageConverterRef converter ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 * @enum vImageBufferTypeCode
 *
 * @abstract Type codes for what is in a vImage_Buffer, such as red or luminance or chunky
 *
 * @discussion In rare circumstances, it may be necessary to introspect a vImageConverterRef that consumes
 *             or produces multiple image planes to find out which color channels go into which planes.
 *             See vImageConverter_GetSourceBufferOrder and vImageConverter_GetDestinationBufferOrder.
 *             (This is typically only necessary with video content, because no CG formatted buffers have
 *             multiple planes.)  The vImageBufferTypeCode encodes what kind of data goes into each channel.
 *
 * @constant kvImageBufferTypeCode_Alpha  The buffer contains the alpha channel / coverage component
 *
 * @constant kvImageBufferTypeCode_Indexed  The buffer contains data in an indexed colorspace. This is a
 *                      planar buffer that is used to index a lookup table of color values. The color
 *                      values in the table may belong to more than one color component.  Typically the
 *                      colorspace will have a color model of kCGColorSpaceModelIndexed and you will need
 *                      use CGColorSpaceGetBaseColorSpace to find out what to what color model the lookup
 *                      table maps.
 *
 * @constant kvImageBufferTypeCode_CVPixelBuffer_YCbCr  The buffer contains luminance, and both chroma channels
 *                      interleaved according to the vImageConstCVImageFormatRef image type.
 *
 * @constant kvImageBufferTypeCode_Luminance   The buffer contains only luminance data.
 *
 * @constant kvImageBufferTypeCode_Chroma       The buffer contains both chrominance channels, interleaved.
 *
 * @constant kvImageBufferTypeCode_Cb           The buffer contains the blue chrominance channel
 *
 * @constant kvImageBufferTypeCode_Cr           The buffer contains the red chrominance channel
 *
 * @constant kvImageBufferTypeCode_CGFormat     The buffer contains data describable as a vImage_CGImageFormat as
 *                                              a single (likely chunky) buffer
 *
 * @constant kvImageBufferTypeCode_Chunky       The buffer contains chunky data not describable as a vImage_CGImageFormat.
 *
 * @constant kvImageBufferTypeCode_RGB_Red      If the image has a RGB color model, the buffer contains the red channel.
 *
 * @constant kvImageBufferTypeCode_RGB_Green    If the image has a RGB color model, the buffer contains the green channel.
 *
 * @constant kvImageBufferTypeCode_RGB_Blue     If the image has a RGB color model, the buffer contains the blue channel.
 *
 * @constant kvImageBufferTypeCode_CMYK_Cyan    If the image has a CMYK color model, the buffer contains the cyan channel.
 *
 * @constant kvImageBufferTypeCode_CMYK_Magenta If the image has a CMYK color model, the buffer contains the magenta channel.
 *
 * @constant kvImageBufferTypeCode_CMYK_Yellow  If the image has a CMYK color model, the buffer contains the yellow channel.
 *
 * @constant kvImageBufferTypeCode_CMYK_Black   If the image has a CMYK color model, the buffer contains the black channel.
 *
 * @constant kvImageBufferTypeCode_XYZ_X        If the image has a XYZ color model, the buffer contains the X channel.
 *
 * @constant kvImageBufferTypeCode_XYZ_Y        If the image has a XYZ color model, the buffer contains the Y channel.
 *
 * @constant kvImageBufferTypeCode_XYZ_Z        If the image has a XYZ color model, the buffer contains the Z channel.
 *
 * @constant kvImageBufferTypeCode_LAB_L        If the image has a LAB color model, the buffer contains the L* channel.
 *
 * @constant kvImageBufferTypeCode_LAB_A        If the image has a LAB color model, the buffer contains the a* channel.
 *
 * @constant kvImageBufferTypeCode_LAB_B        If the image has a LAB color model, the buffer contains the b* channel.
 */
typedef VIMAGE_CHOICE_ENUM(vImageBufferTypeCode, uint32_t)
{
    kvImageBufferTypeCode_EndOfList VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = 0,
    
    /* planar formats -- each buffer contains a single color channel, arising from an image described by a colorspace */
    kvImageBufferTypeCode_ColorSpaceChannel1 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel2 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel3 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel4 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel5 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel6 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel7 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel8 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel9 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel10 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel11 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel12 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel13 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel14 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel15 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    kvImageBufferTypeCode_ColorSpaceChannel16 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    
    /* Coverage component */
    kvImageBufferTypeCode_Alpha VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    
    /* indexed color spaces */
    kvImageBufferTypeCode_Indexed VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    
    /* YUV formats.  */
    kvImageBufferTypeCode_CVPixelBuffer_YCbCr VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),          /* A YCbCr packed buffer formatted according to types in CVPixelBuffer.h. May be accompanied by an alpha channel */
    kvImageBufferTypeCode_Luminance VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                    /* A Luminance (Y) plane */
    kvImageBufferTypeCode_Chroma VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                       /* A two-channel chroma (CbCr) plane */
    kvImageBufferTypeCode_Cb VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                           /* A blue chroma (Cb) plane */
    kvImageBufferTypeCode_Cr VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                           /* A red chroma (Cr) plane */
    
    /* A interleaved (chunky) format with one or more channels, encodable as a vImage_CGImageFormat */
    kvImageBufferTypeCode_CGFormat VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                     /* always a singleton -- appearing as { kvImageBufferTypeCode_CGFormat, 0} */
    /* prior to OS X.10 and iOS 8.0, all vImageConvert_AnyToAny buffers have this type.*/
    
    kvImageBufferTypeCode_Chunky VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),                       /* always a singleton -- appearing as { kvImageBufferTypeCode_Chunky, 0} */
    /* buffer format is not encodable as vImage_CGImageFormat. Not YpCbCr. */
    
    /* must appear after last unique code */
    kvImageBufferTypeCode_UniqueFormatCount VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ),
    
    /* Convenience codes for better code readability */
    kvImageBufferTypeCode_Monochrome  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel1,
    
    
    kvImageBufferTypeCode_RGB_Red VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel1,
    kvImageBufferTypeCode_RGB_Green VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel2,
    kvImageBufferTypeCode_RGB_Blue VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel3,
    
    kvImageBufferTypeCode_CMYK_Cyan VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel1,
    kvImageBufferTypeCode_CMYK_Magenta VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel2,
    kvImageBufferTypeCode_CMYK_Yellow VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel3,
    kvImageBufferTypeCode_CMYK_Black VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel4,
    
    kvImageBufferTypeCode_XYZ_X VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel1,
    kvImageBufferTypeCode_XYZ_Y VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel2,
    kvImageBufferTypeCode_XYZ_Z VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel3,
    
    kvImageBufferTypeCode_LAB_L VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel1,
    kvImageBufferTypeCode_LAB_A VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel2,
    kvImageBufferTypeCode_LAB_B VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 ) = kvImageBufferTypeCode_ColorSpaceChannel3,
};

/*!
 *  @function vImageConverter_GetSourceBufferOrder
 *
 *  @abstract Get a list of vImage_Buffer channel names specifying the order of planes
 *
 *  @discussion These functions describe the identity of each buffer passed in the srcs parameters of vImageConvert_AnyToAny,
 *  to allow you to order the buffers correctly. It is provided for informational purposes, to help wire up image
 *  processing pipelines to vImage that are not supported through more direct means, CGImages, CVPixelBuffers, the alternative
 *  handling of which is described at the end of this comment.
 *
 *  Prior to OS X.10 and iOS 8.0, only CG Image formats are handled by vImageConvert_AnyToAny. Had these functions existed
 *  then, the result would always be {kvImageBufferTypeCode_CGFormat, kvImageBufferTypeCode_EndOfList}
 *
 *  <pre>@textblock
 *  Simplified Common Cases
 *  -----------------------
 *    CGImageRefs:
 *      CoreGraphics formats always come as a single buffer, with one or more channels. No buffer ordering is
 *      requred. The buffer order is always kvImageBufferTypeCode_CGFormat. Prior to OS X.10 and iOS 8.0, only
 *      converters to CG image formats are available, so where these functions are not available, the answer would
 *      have always been { kvImageBufferTypeCode_CGFormat, 0}. As a point of trivia, the ordering of the channels
 *      within a buffer is by convention as follows:
 *
 *              number of channels = number of channels in colorspace + (alpha != kCGImageAlphaNone)
 *              alpha is either first or last, given by the alpha component of the CGBitmapInfo
 *              The ordering of the non-alpha channels is given by the colorspace, e.g. {R,G,B} for a RGB image.
 *              For 8-bit images, the ordering of the channels may be reversed according to
 *                  kCGBitmapByteOrderLittleEndian32 or kCGBitmapByteOrderLittleEndian16, but
 *                  the pixel size must match the endian swap chunk size. This gives you access to formats
 *                  like BGRA8888. If the endian is default or big endian, then no swap occurs.
 *
 *    CVPixelBufferRefs:
 *      Though these APIs will work for this purpose, it is expected to be simpler to use vImageBuffer_InitForCopyToCVPixelBuffer
 *      or vImageBuffer_InitForCopyFromCVPixelBuffer to set up a vImage_Buffer array for srcs and dests. Pass kvImageDoNotAllocate
 *      to have it automatically alias a locked CVPixelBuffer. The conversion will then copy the data right into or out of the
 *      CVPixelBuffer without further copying or modification.  (It may still need to be copied out to the GPU, for example, however.)
 *
 *  @/textblock</pre>
 *
 *  @param converter       The conversion for which you wish to know the ordering of source or result buffers.
 *                      converter must be a valid vImageConverterRef.
 *
 *
 *  @return The function returns a kvImageBufferTypeCode_EndOfList terminated array of buffer type codes. The type codes
 *  indicate the order that the vImage_Buffers are passed in to vImageConvert_AnyToAny. The array is valid for the
 *  lifetime of the vImageConverterRef.  It belongs to the vImageConverterRef and should not be freed by you.
 */
VIMAGE_PF const vImageBufferTypeCode * vImageConverter_GetSourceBufferOrder( vImageConverterRef converter )  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*!
 *  @function vImageConverter_GetDestinationBufferOrder
 *
 *  @abstract Get a list of vImage_Buffer channel names specifying the order of planes
 *
 *  @discussion These functions describe the identity of each buffer passed in the dests parameters of vImageConvert_AnyToAny,
 *  to allow you to order the buffers correctly. It is provided for informational purposes, to help wire up image
 *  processing pipelines to vImage that are not supported through more direct means, CGImages, CVPixelBuffers, the alternative
 *  handling of which is described at the end of this comment.
 *
 *  Prior to OS X.10 and iOS 8.0, only CG Image formats are handled by vImageConvert_AnyToAny. Had these functions existed
 *  then, the result would always be {kvImageBufferTypeCode_CGFormat, kvImageBufferTypeCode_EndOfList}
 *
 *  <pre>@textblock
 *  Simplified Common Cases
 *  -----------------------
 *    CGImageRefs:
 *      CoreGraphics formats always come as a single buffer, with one or more channels. No buffer ordering is
 *      requred. The buffer order is always kvImageBufferTypeCode_CGFormat. Prior to OS X.10 and iOS 8.0, only
 *      converters to CG image formats are available, so where these functions are not available, the answer would
 *      have always been { kvImageBufferTypeCode_CGFormat, 0}. As a point of trivia, the ordering of the channels
 *      within a buffer is by convention as follows:
 *
 *              number of channels = number of channels in colorspace + (alpha != kCGImageAlphaNone)
 *              alpha is either first or last, given by the alpha component of the CGBitmapInfo
 *              The ordering of the non-alpha channels is given by the colorspace, e.g. {R,G,B} for a RGB image.
 *              For 8-bit images, the ordering of the channels may be reversed according to
 *                  kCGBitmapByteOrderLittleEndian32 or kCGBitmapByteOrderLittleEndian16, but
 *                  the pixel size must match the endian swap chunk size. This gives you access to formats
 *                  like BGRA8888. If the endian is default or big endian, then no swap occurs.
 *
 *    CVPixelBufferRefs:
 *      Though these APIs will work for this purpose, it is expected to be simpler to use vImageBuffer_InitForCopyToCVPixelBuffer
 *      or vImageBuffer_InitForCopyFromCVPixelBuffer to set up a vImage_Buffer array for srcs and dests. Pass kvImageDoNotAllocate
 *      to have it automatically alias a locked CVPixelBuffer. The conversion will then copy the data right into or out of the
 *      CVPixelBuffer without further copying or modification.  (It may still need to be copied out to the GPU, for example, however.)
 *
 *  @/textblock</pre>
 *
 *  @param converter       The conversion for which you wish to know the ordering of source or result buffers.
 *                      converter must be a valid vImageConverterRef.
 *
 *
 *  @return The function returns a kvImageBufferTypeCode_EndOfList terminated array of buffer type codes. The type codes
 *  indicate the order that the vImage_Buffers are passed in to vImageConvert_AnyToAny. The array is valid for the
 *  lifetime of the vImageConverterRef. It belongs to the vImageConverterRef and should not be freed by you.
 */
VIMAGE_PF const vImageBufferTypeCode * vImageConverter_GetDestinationBufferOrder( vImageConverterRef converter )  VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));


/*!
 *  @function vImageConvert_AnyToAny
 *
 *  @abstract Use a vImageConverterRef to convert the pixels in a vImage_Buffer to another format
 *
 *  @discussion With an appropriately configured vImageConverter, convert the image channels found in srcs
 *  to the image channels found in dests. Whenever possible, conversion passes are vectorized and multithreaded
 *  to reduce the time and energy cost of the function.
 *
 *  Please use vImageConverter_MustOperateOutOfPlace() to determine whether a particular conversion can operate in place.
 *  For an in-place conversion to work, it is required that srcs[i].data = dests[i].data and srcs[i].rowBytes = dests[i].rowBytes.
 *
 *  All scanlines must start at an at least byte aligned address. (Some formats have 1, 2, 4 or 12 bits per channel/pixel and
 *  conceivably might not start at a byte aligned address.) A single byte may not span multiple rows of data.
 *
 *  Some formats, particarly YUV 422 and 420 and those that have pixel size not evenly divisble by 8 bits, operate in chunks
 *  containing multiple pixels. For example, a Y'CbCr 422 chunk may have {Y0, Cb, Y1, Cr} in the chunk. The chunk contains two
 *  pixels, each with an independent Y (luminance) component, but shared chrominance.  Even though the chunk width is two,
 *  it is still possible for an image to have a width that is not divisible by two. This means that some part of the chunk on
 *  the rightmost edge of the scanline must refer to a non-existant pixel. When reading incomplete chunks, vImage will only
 *  touch the unused parts of the chunk when it knows it to be safe to do so. When writing incomplete chunks, vImage will
 *  copy the rightmost valid pixel color into the unused part of the chunk. Thus, on reading the entire chunk doesn't have to
 *  be there, but on writing, it does. Conventions on this are varied among chunk using imaging pipelines and this conservative
 *  approach should interoperate with most. However, some care must be exercised when writing to chunk based formats (not to be
 *  confused with chunky formats which merely have several channels interleaved) to make sure that the buffer is large enough
 *  to tolerate the write policy.  If you are tiling chunk based data, care must be taken not to run tile boundaries
 *  through the middle of a chunk.  Chunks are assumed to be indivisible.
 *
 *  @param converter  A valid vImageConverterRef indicating what conversion to do. The same vImageConverterRef
 *                  may be used concurrently from multiple threads. vImageConverterRefs may be created with
 *                  vImageConverter_CreateWithCGImageFormat, vImageConverter_CreateWithColorSyncCodeFragment,
 *                  vImageConverter_CreateForCGToCVImageFormat or vImageConverter_CreateForCVToCGImageFormat.
 *                  May not be NULL.
 *
 *  @param srcs     a pointer to an array of vImage_Buffer structs that describe the color planes that make
 *                  up the input image. Please see the description of the function that created the
 *                  vImageConverter for the ordering and number of input buffers. The ordering can also be
 *                  determined manually using vImageConverter_GetSourceBufferOrder.
 *
 *  @param dests    a pointer to an array of vImage_Buffer structs that describe the color planes that make
 *                  up the result image. Please see the description of the function that created the
 *                  vImageConverter for the ordering and number of output buffers. The ordering can also be
 *                  determined manually using vImageConverter_GetSourceBufferOrder. The destination buffer may
 *                  only alias the srcs buffers only if vImageConverter_MustOperateOutOfPlace() returns 0, and
 *                  only if the respective scanlines of the aliasing buffers start at the same address.
 *
 *  @param tempBuffer   May be NULL. If not NULL, the memory pointed to by tempBuffer will be used as scratch space
 *                  by the function. The size of the tempBuffer can be determined by passing kvImageGetTempBufferSize
 *                  to the in the flags parameter. See below. If NULL is passed here and a tempBuffer is needed '
 *                  (temp buffer size > 0) then the function will allocate one on the heap and free it before
 *                  returning. This may run more slowly, both because of the allocation cost and the cost of VM
 *                  faults to zero fill pages as they are used. NULL is the right option when the function is
 *                  used infrequently or convenience is valued.
 *
 *  @param flags    The following flags are allowed. Other flags will trigger an error.
 *
 *  <pre>@textblock
 *                  kvImagePrintDiagnosticsToConsole    In the event of a problem, print out some helpful debug messages.
 *
 *                  kvImageGetTempBufferSize            No image conversion work is done. The value returned out the
 *                                                      left hand side of the function is the an error code if it is
 *                                                      less than zero. Otherwise, it is the size of the tempBuffer
 *                                                      to be passed into the function. The size may be 0.
 *
 *                  kvImageDoNotTile                    Disables internal multithreading.  You may wish to pass this
 *                                                      flag if you are doing your own threading and think it will
 *                                                      conflict with vImage's attempts to do the same.
 *
 *                  kvImageNoFlags                      Default behavior.
 *  @/textblock </pre>
 *
 *  @return  The following error codes may be returned:
 *  <pre>@textblock
 *      kvImageNoError                      Success!
 *
 *      0                                   kvImageGetTempBufferSize was passed in flags, and no temp buffer is needed.
 *
 *      >0                                  kvImageGetTempBufferSize was passed in flags. The value indicates the size
 *                                          of the temp buffer needed.
 *
 *      kvImageMemoryAllocationError        NULL was passed in tempBuffer and vImage failed to allocate its own
 *                                              temp buffer
 *      kvImageBufferSizeMismatch           The source buffer(s) must be at least as large as the destination buffer(s)
 *                                           (src.height >= dest.height && src.width >= dest.width)
 *      kvImageUnknownFlagsBit              A flag was passed to the vImageConverter creation function which is
 *                                          unrecognized or not appropriate to this function
 *      kvImageNullPointerArgument          converter is NULL
 *      kvImageInvalidParameter             One of the buffers pointed to by srcs or dests has a NULL vImage_Buffer.data pointer
 *      kvImageUnknownFlagsBit              An unknown or unsupported flags bit was set.
 *      kvImageInvalidImageFormat           if a byte ordering is specified (e.g. kCGBitmapByteOrder16Little), the buffer.rowBytes
 *                                          must be multiple of 2 (kCGBitmapByteOrder16Little, kCGBitmapByteOrder16Big) or
 *                                          4 (kCGBitmapByteOrder32Little, kCGBitmapByteOrder32Big)
 *  @/textblock </pre>
 *
 */
VIMAGE_PF vImage_Error vImageConvert_AnyToAny( const vImageConverterRef converter,
                                              const vImage_Buffer * srcs,    /* an array of vImage_Buffer structs describing source data planes */
                                              const vImage_Buffer * dests,   /* an array of vImage_Buffer structs describing destination data planes */
                                              void  *tempBuffer,             /* may be NULL */
                                              vImage_Flags  flags
                                              )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  General vImage alignment requirements:
 *  ======================================
 *  All scanlines must be at least byte aligned
 *      A single byte of image data may not span multiple rows of the image.  For example, if the image is 1 bit per pixel monochrome, then
 *      8 pixels fit in a byte. If the width of such an image is not divisible by 8, then the potential exists that the first part of a byte might
 *      lie in one image row and the rest of the byte in the next. It would even be possible that one byte could span up to 8 scanlines if the
 *      image width is 1 pixel! These cases are not allowed. Implicit in the design of using a rowBytes instead of a rowBits describe the image
 *      is that each row must be exactly an integral number of bytes. When the width of the image is such that it would not naturally be so, the
 *      caller must pad out the remaining bits on the image row so that the next row starts on a byte boundary. This is done by setting the rowBytes
 *      to be at least (width * bitsPerPixel + 7) / 8 bytes. When planar images are used then this becomes rowBytes = (width * bitsPerComponent + 7) / 8.
 *
 *      In many cases, images that do not pad rows out to an integral number of bytes can still be processed by vImage by setting
 *      width = height * width and height = 1. However, operating on images with a single row may defeat internal multithreading, so performance
 *      may suffer. (Please use kvImageDoNotTile to defeat internal multithreading if that is your goal. That is its purpose.)  Cleverer schemes that
 *      redivide the image into a larger number of byte aligned rows may be devised.
 *
 *  Some image formats must have 2- or 4-byte aligned scanlines and may need to be host endian
 *      Images with components that are 16-bits in size (e.g. 16U and half float images) or pixels which are 16-bits in size and which do not
 *      have channel boundaries at byte boundaries (e.g. RGB565 and ARGB1555, but not GrayscaleAlpha88) must have vImage_Buffer.data 2-byte aligned and
 *      vImage_Buffer.rowBytes a multiple of 2. Likewise, 32-bits per component (e.g. float or a putative ARGB-2-10-10-10 format) must be 4 byte aligned
 *      and rowBytes a multiple of 4. Failure to conform to 2- and 4-byte alignment restrictions may result in undefined behavior -- at minimum poor
 *      performance and on some systems it can cause a program fault.
 *
 *      Except where otherwise explicitly allowed by the vImageConverter_Create function (e.g. vImage_CGImageFormat.bitmapInfo might be kCGBitmapByteOrder16Big
 *      even on little endian systems) these cases must also conform to the native endianness of the host system. For example, floating-point data on an Intel
 *      system is normally expected to be little-endian unless the converter provides a means to say that it is not. Please see the vImageConverter_Create
 *      function that you plan to use for additional restrictions that may apply.
 *
 *  Performance tips:
 *  =================
 *
 *  Alignment
 *  ---------
 *  Performance will typically improve with even greater alignment up to cacheline aligned image rows. vImageBuffer_Init() can be used to get vImage's
 *  estimation of what sort of row padding is generally likely to work best on the current machine for a given image and pixel size.
 *
 *  Memory reuse
 *  ------------
 *  It can also be quite helpful to reuse vImage_Buffers and operate in-place so as to avoid spending time in zero-fill faults zeroing the contents
 *  of newly allocated buffers. (These will show up in Instruments traces as time spent in kernel VM activity.) However, caching unused buffers for
 *  extended periods of time can contribute to degradation of overall system performance in low memory situations so recycling of buffers is usually
 *  best done when good temporal locality is expected. LibCache may be helpful for avoiding such problems when it is not known with certainty that the
 *  buffer will be reused immediately.
 *
 *  Tiling and multithreading
 *  -------------------------
 *  Most vImage functions, including vImageConvert_AnyToAny, will automatically split up work across multiple processors (when available) if there is
 *  enough image data to warrant it. It can take some time to wake up other processors or redirect their attention from their current task to a new one,
 *  so multithreading is not attempted if it seems likely the current processor can complete the work before the attention of the other cores can be
 *  redirected to work on the problem. With some exceptions, the work is typically divided at boundaries between image rows. (To be clear, tiles can be
 *  taller than just one row.) So, for some functions, if you break up the work to one scanline at a time, it will never multithread. Exactly which functions
 *  do that is subject to change. If you want to stop multithreading, please use kvImageDoNotTile.
 *
 *  The size of the tile chosen by the function varies by function. Some do little computation and so need the lowest possible load/store latencies to
 *  run at top speed. Others do more work per byte are consequently not so dependent on low memory access latencies, so may work equally well with tiles
 *  closer to the size of the L2 or L3 caches and so may use larger tiles. Some computationally intense functions may not care if the data is in cache
 *  at all. Each vImage function will tile its workload as it sees fit without regard to what other vImage functions may have done or plan to do.
 *
 *  While this scheme generally works well, you can often do much better with a little work especially in cases where multiple vImage functions are called
 *  back to back for a few reasons:
 *
 *      First of all, if the image is larger than the caches, then as new pixels are produced, they will cause earlier result pixels from the same image
 *  to be flushed to more remote levels of the cache or ultimately out to DRAM (or even disk in low memory situations!) to make room for the new results. When
 *  it is time to call the next vImage function on the result, you may find that even though you just worked on it, part of the image is not in the cache and
 *  so you have to pay some time to load it back in again when it as needed. This can happen over and over again with each new vImage filter.  A similar
 *  story may occur for smaller images as data is flushed from level 1 to level 2 to level 3 of the cache to make room for new data. It would be faster if
 *  you could somehow move on to the next vImage filter while a region of the image is still in the cache before it is evicted to make room for the rest
 *  of the image. We will get to the how a little later.
 *
 *      Second, since processor cores are usually assigned by vImage to work on parts of the image on a first-come first-served basis to keep per-call latency
 *  down, it is probably common that different cores will work on different parts of the image in each successive vImage call, even in cases where the tiling
 *  strategies between two back to back vImage calls are otherwise similar. That is, in the first vImage function call, the top left corner might be done by
 *  processor 1 and in the next vImage function call by processor 3. The next time you call the same code, it might be processor 2 and processor 5 that do
 *  the work on that region. Unfortunately, in many current processor architectures, the faster cache levels (level 1 and 2) are often not shared with most
 *  of the other cores and it can take more time for the other cores to get data from them. It would be faster if you could somehow convince the same core to
 *  operate on the same region of the image each time, because only then do we have some guarantee that the faster cache levels are doing us any good between
 *  back to back calls.
 *
 *      Third, just as it may take some time to get all the processors redirected to work on a particular task, you may also lose some time because
 *  they don't all finish at the same time. Some processors end up waiting for others to finish before the vImage call can return. Here one expects this
 *  time lost to be a tiny fraction of the overall time for large images, but for medium sized ones it may weigh against overall performance. (Smaller images
 *  may not even bother to multithread in such cases.) When you advance to the next back-to-back vImage call, some of the cores might have gone to sleep or
 *  moved on to unrelated work waiting for other cores to finish, and now you have to wait for them to get back on task again. You could reduce overall latency
 *  if you could somehow convince the next filter to start on regions that are done without having to wait for all of the regions of the image to be done,
 *  so that the cores stay busy and don't get distracted.
 *
 *      Fourth, doing back-to-back-to-back vImage calls probably means having a bunch of intermediate vImage_Buffers hanging around that take up memory.
 *  In cases where the intermediate computation has to be done in some higher precision, like floating-point, some of these can be quite big. It would be
 *  very helpful to somehow make those go away, or at least go on a diet.
 *
 *  So, how is this collection of problems fixed? Well, first of all, you don't /have/ to fix anything. Even with all of that, it should still work pretty
 *  well. If pretty well is not good enough, then the solution is to manage the threading yourself. Lets say you want to apply 4 vImage filters (A, B, C and
 *  D) to an image in a back-to-back fashion. First you break up the result image into a bunch of smaller chunks, called tiles. These should be small enough
 *  to fit conformably in the cache but not so small that we devote too much time doing setup tasks like parameter checking or initializing variables repeatedly.
 *  (You can figure out the right size later when it all works. Guess for now.) Starting from the result tiles, work backward. For each tile in the output of D,
 *  figure out which pixels from C are needed as input. This defines the set of tiles produced by C. In cases where the function needs to look at nearby pixels
 *  to calculate a result pixel (e.g. Convolution or various Morphology filters) then it is possible that the C-produced tiles may overlap a bit. The process is
 *  then repeated moving backward through B and A until we arrive at the starting image. We can now trace each result tile through a bunch of discrete image
 *  fragments back up to a region in the original image. Since we know that we have all the input pixels we need to calculate a result tile at each stage, we
 *  can apply filters A,B,C and D in series to a tile without needing to worry about what is happening in the other tiles. Thus, each tile can be operated on
 *  by a different thread, and multithreading becomes trivial.
 *
 *  Consider the implications. Since each tile of data is operated on by a single thread and threads usually do not gratuitously hop around from core to core
 *  (very much) the result from filter A on that tile should usually be in the right L1 or L2 cache when we call filter B to consume the result from A for that region,
 *  provided that we call filter B on that tile from that thread right away and don't run off and do other things. The same goes for filters C and D. So, you
 *  are now probably getting better-than-good-enough temporal cache locality.  Observe also that it is generally not required that the intermediate (A,B or
 *  C product) tiles be stored in memory that is contiguous with other tiles from the same intermediate image. They can be their own little chunks of memory
 *  stored somewhere else. Maybe you even put them on the stack -- which probably will avoid most zero fill faults and is quick to allocate. In fact, if the
 *  output tiles are of the same size, then as the thread finishes one tile, you can just reuse those small intermediate buffers that hold the intermediate
 *  tiles for the next tile. The intermediate image then never exists as a whole at any given time and so you never have to allocate storage for most of it.
 *  This gives you better-than-good-enough memory usage. Floating-point is starting to look a lot better! Next, because each tile is not waiting on its fellows,
 *  you can begin working on filter B as soon as the thread is done with filter A. There is no more waiting for filter A to be done on the entire image before
 *  B can be started. Threads go idle less. There is probably still bit of time lost to waiting for cores to spin up and some for the last thread to finish on
 *  exit, but it is amortized over four filters instead of each filter.
 *
 *  Recall that in standard opertaion, vImage will still be trying to multithread behind your back for all these little tiles. This will cause the threads to
 *  oversubscribe the number of cores. Work that was supposed to be done by one thread is now split up to many. Threads may hop around from core to core much
 *  more looking for a place to run, defeating cache locality. You may spend a lot of time waiting for cores to redirect to the next job with lots of little
 *  jobs. The Instruments system trace looks like a tangled knot of threads playing musical chairs for a limited number of cores. To stop this, pass the
 *  kvImageDoNotTile flag to make sure vImage is running only on the thread you assign to it.
 */


#ifdef __cplusplus
}
#endif

#endif  /* vImage_Utilities_h */

// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/vImage.h

/****************************************************************************************************************************************************************
 *                                                                                                                                                              *
 *  The main vImage documentation is available in the vImage Programming Guide:                                                                                 *
 *                                                                                                                                                              *
 *    OS X: https://developer.apple.com/library/mac/documentation/Performance/Conceptual/vImage/Introduction/Introduction.html                                  *
 *    iOS:  https://developer.apple.com/library/ios/documentation/Performance/Conceptual/vImage/Introduction/Introduction.html                                  *
 *                                                                                                                                                              *
 *  However, that resource, generously provided by ADC, is updated infrequently. In order to provide more timely documentation for newer vImage API, these      *
 *  headers contain additional headerdoc documentation in the comments. The vImage headerdoc documentation can be viewed in lovely HTML as follows:             *
 *                                                                                                                                                              *
 *    /usr/bin/headerdoc2html -o ~/vImage_docs \                                                                                                                *
 *            `xcrun --sdk {iphoneos|macosx} --show-sdk-path`/System/Library/Frameworks/Accelerate.framework/Frameworks/vImage.framework/Headers                *
 *        # Select the single platform for the SDK you are working with. vImage APIs generally stay in sync between platforms.                                  *
 *        # (or using system headers from command line tools: /System/Library/Frameworks/Accelerate.framework/Frameworks/vImage.framework/Headers)              *
 *    /usr/bin/gatherheaderdoc ~/vImage_docs                                                                                                                    *
 *    open ~/vImage_docs/masterTOC.html                                                                                                                         *
 *                                                                                                                                                              *
 *  For more on using headerdoc:                                                                                                                                *
 *    https://developer.apple.com/library/mac/documentation/DeveloperTools/Conceptual/HeaderDoc/usage/usage.html#//apple_ref/doc/uid/TP40001215-CH337-CDEBBJJA  *
 *                                                                                                                                                              *
 *  vImage headerdoc documentation is a work in progress, and currently only covers recent additions. Most functions not covered in headerdoc should be         *
 *  documented in the vImage Programming Guide. Please file a bug report if you encounter a function documented in neither resource.                            *
 *  http://bugreport.apple.com                                                                                                                                  *
 *                                                                                                                                                              *
 *  Option clicking a vImage symbol in Xcode will often bring up online help with information assembled from both resources.                                    *
 *                                                                                                                                                              *
 ****************************************************************************************************************************************************************/

#ifndef VIMAGE_H
#define VIMAGE_H

/*!
 *  @header vImage.h
 *  @copyright Copyright (c) 2002-2016 by Apple Inc. All rights reserved.
 *  @compilerflag  -framework Accelerate
 *  @charset utf-8
 *
 *  @discussion
 *
 *  <pre> @textblock
 *    Data Formats
 *    ------------
 *    Most functions are available in four flavors, one for each of the four standard data formats supported
 *    by vImage. These are:
 *
 *    8 bit planar integer data  -- "Planar8":
 *        The buffer contains a single color channel (e.g. red) as an array of packed unsigned chars.
 *
 *    32 bit planar floating point data -- "PlanarF":
 *        The buffer contains a single color channel (e.g. red) as an array of packed floats.
 *
 *    8 bit ARGB interleaved integer data -- "ARGB8888":
 *        The buffer contains four interleaved color channels in the order alpha, red, green, blue, in a
 *        packed array of unsigned chars. (This is a standard 32 bit ARGB color pixel, with 8 bits per channel).
 *
 *    32 bit ARGB interleaved floating point data -- "ARGBFFFF":
 *        The buffer contains four interleaved color channels in the order alpha, red, green, blue, in a
 *        packed array of floats.
 *
 *    Many functions are also available in Planar and ARGB versions of unsigned and signed 16-bit integer, 16S and
 *    16U respectively.
 *
 *    PERFORMANCE ADVISORY: Performance of the planar versions of these functions will in some cases
 *    be *MUCH* better than their interleaved counterparts. Most operations are done in register with planar
 *    data, even if the starting data format was an interleaved format. By using planar data formats,
 *    you avoid the de-interleave, re-interleave cost in each function call. In addition, where calculation
 *    on the alpha channel is not desired, the computational cost and cache usage for planar formats is
 *    in principle 3/4 the cost of interleaved formats for four channel data. Finally planar data is a natural
 *    form of tiling that works well when used with multiple serial filters. When applying multiple filters to
 *    a image, apply them all to one channel, then repeat the sequence for the next channel and so forth until
 *    all the color channels are calculated. This will help keep key data in the caches and in some cases may
 *    produce profound performance improvements.
 *
 *    The presumed value range for 8 bit color channels is 0 to 255, with 0 being black and 255 being full intensity color.
 *    For floating point data, 0.0f is black and 1.0f is full intensity color. Values outside this range are allowed for
 *    floating point data. Except where noted, FP calculations do not clip values outside the range 0.0f ... 1.0f. In certain
 *    cases (most notably geometry operations that involve resampling and colvolutions), it is possible that input data
 *    that is entirely withing the range 0.0f ... 1.0f will produce results that are slightly outside that range. A separate
 *    floating point clipping function is provided if clipping is required.
 *
 *    Other floating point ranges (e.g. 0.0f ... 255.0f) are likely to also work since most functions in vImage are linear.
 *    However, they are not extensively tested.
 *
 *    vImage_Buffers
 *    --------------
 *    Most vImage functions attempt to fill a destination buffer with pixels drawn from an input buffer using some
 *    transformation. In some cases, the operation would require that pixels outside the area covered by the input
 *    buffer be used to fill the output buffer. The input vImage_Buffer defines the absolute limit of the area of
 *    data that may be read by the function. The function will not stray outside of this area in its attempt to
 *    fill the destination with pixels. How a function copes with edge cases when data is unavailable is dependent
 *    on the function.
 *
 *        typedef struct vImage_Buffer
 *        {
 *            void                *data;		// Pointer to the top left pixel of the buffer.
 *            vImagePixelCount	  height;		// The height (in pixels) of the buffer
 *            vImagePixelCount    width;		// The width (in pixels) of the buffer
 *            size_t              rowBytes;	    // The number of bytes in a pixel row
 *        }vImage_Buffer;
 *
 *    Some functions may operate only on a rectangular subset of pixels in the input buffer. This is useful for tiling
 *    or for when image operations are only desired to be applied to part of an image. The top left corner of the subset
 *    is provided by a X and Y offset into the vImage_Buffer passed as the input buffer. The height and width of the input
 *    region of interest is in many cases given by the height and width of the destination buffer. The geometry operators
 *    use non-rectangular input regions of interest making it difficult to predict which pixels will be used. In this case,
 *    the offsets (if any) are the offsets into the input region of interest for the case where the scale factor is 1.0 and
 *    shear and translate are 0.0f.
 *
 *    Please be aware that many functions, especially those that require kernels or which do resampling, will read
 *    outside the input region of interest. Some extra care with multithreaded code may be required in those cases to make
 *    sure nobody is changing data outside the region of interest that is read by the function while the function is operating.
 *    At no time will a function read outside the vImage_Buffer within which the region of interest resides.
 *
 *    So as to make multithreaded tiled (stripmined) algorithms easy to write, vImage makes two guarantees:
 *
 *        (1) vImage will never write data outside of the destination vImage_Buffer. vImage will not try to cheat this
 *            guarantee by reading data outside the destination buffer and writing it back unmodified.
 *
 *        (2) vImage will never read data outside of the area passed to it as the input vImage_Buffer.
 *
 *    The state of the output vImage_Buffer while a vImage function is working on it is undefined. There may be times when
 *    a pixel in the ROI is neither the starting data or the finished result, but instead the result of some
 *    intermediate calculation. The calculation is complete when the function returns.
 *
 *    Note that although a vImage_Buffer struct may be marked const, this does not mean that the pixel data that
 *    it points to is also const. It is merely an indication that it is safe to reuse the struct between function
 *    calls. Except where noted, the pixel data in the input buffers will not be changed by the function. vImage
 *    does not go to extreme lengths to guarantee this however, so for example you may pass the same buffer as input
 *    and output to a function, in which case the input buffer will be changed.
 *    Caution: except where otherwise documented, most vImage functions do not work correctly in place.
 *
 *    Performance advisory: If the rowBytes of a vImage_Buffer is a integer power of 2, performance may be adversely
 *    affected on some functions. (This is a side effect of how some machines handle address arithmetic
 *    internally. It is not something we can solve in software, except by setting rowBytes differently. ) 
 *    In some cases, it may also be advantageous to pad rowbytes out to 16 bytes. Using vImageBuffer_Init() 
 *    in vImage/vImage_Utilities.h will help sidestep this issue. It will attempt tune rowBytes to benefit the 
 *    current architecture.
 *
 *    It is not required that you do set up your buffers exactly this way, however. To protect your investment in
 *    preexisting data structures, vImage is designed to work with any (natural) data alignment and any rowBytes.
 *    Floats must be 4 byte aligned. RowBytes must of course be greater than or equal to width * bytesPerPixel.
 *    BytesPerPixel values are as follows:
 *
 *        Buffer Data Format	Size
 *        ------------------	----
 *        planar uint8		    sizeof( Pixel_8)		// 1 byte
 *        planar float		    sizeof( Pixel_F )		// 4 bytes
 *        ARGB uint8			sizeof( Pixel_8888)		// 4 bytes
 *        ARGB float			sizeof( Pixel_FFFF )	// 16 bytes
 *
 *
 *
 *    Tiling / Strip Mining and Multithreading
 *    ----------------------------------------
 *    In general, the size of the data segment that you operate on is critical to the performance of vImage. In
 *    many cases, the number of pixels that may be processed per second can be up to an order of magnitude lower
 *    for large buffers than for small ones.  A common method to fix this is to operate on small chunks of the image
 *    at a time. Typically, you would apply all the filters one after another to one chunk, before going on to the next
 *    chunk. In this way, the data is much more likely to be in the caches when you need it. This motif is easily
 *    multithreaded in principle, since different processors can work on different chunks concurrently in many
 *    cases.
 *
 *    All vImage functions are thread safe and may be called reentrantly.
 *
 *    A quick test to do to see if tiling is good for you is to measure the number of pixels that you can process
 *    per second for very large (>4 MB uncompressed) and reasonably small (< 256 kB uncompressed) images. If the
 *    number of pixels you can process per unit time is much improved for smaller images over large ones then tiling
 *    is likely to be helpful.
 *
 *    It is important to whenever possible make sure the tile fits in the caches. In general, the data processed in
 *    a single tile (including input and output buffers) should be less than 256 kB, though in some cases there may
 *    be a performance advantage to even smaller buffers. We have found that for many, 16-32 kB is a better number.
 *    In many cases, you will only see performance improvement if the tile is significantly wider than it is tall,
 *    for example 16 rows tall and 1024 horizontal bytes of pixels wide has been observed to be twice as fast as
 *    64x128. Clearly having a tile wider than the image isn't very helpful.
 *
 *    A convenience method for making a vImage_Buffer that refers to a rectangular sub-region of another vImage_Buffer:
 *
 *      // Calculate a vImage_Buffer descriptor for a sub-rectangle within an image
 *      // This function does not attempt to ensure that the tile fits in the image.
 *      vImage_Buffer   MyMakeTileFromImage( const vImage_Buffer *image,
 *                                           unsigned long startColumn, // x coordinate of top left corner of tile
 *                                           unsigned long startRow,    // y coordinate of top left corner of tile
 *                                           unsigned long tileHeight,  // number of rows in tile
 *                                           unsigned long tileWidth,   // number of columns in tile
 *                                           size_t pixelBytes ){       // number of bytes in pixel
 *          return (vImage_Buffer){
 *              .data = (void*) ((char*) image->data + startColumn * pixelBytes + startRow * image->rowBytes),
 *              .height = tileHeight,
 *              .width = tileWidth,
 *              .rowBytes = image->rowBytes
 *          };
 *      }
 *
 *    CAUTION:  This will cause vImage to believe the edges of the image are at the edges of the tile. Usually that 
 *              doesn't affect the result, but will for convolultions, morphology operations and geometry operations
 *              --  things that take a edge mode flag.  For such routines, the start of the tile is declared using
 *              a srcOffsetToROI_X/Y  and the size of the tile is inferred from the destination image size.
 *
 *    Sometimes you may want to flip an image vertically. In vImage, this can be done cheaply by adjusting the pointer
 *    to point to the last scanline of the image and setting rowBytes negative, for either the source or destination
 *    image:
 *
 *      static inline vImage_Buffer MyFlipVertical( const vImage_Buffer *b ) __attribute__ ((always_inline,nodebug));
 *      static inline vImage_Buffer MyFlipVertical( const vImage_Buffer *b ){
 *          return (vImage_Buffer){ .data = (void*) ( (char*)b->data + b->rowBytes * (b->height-1)),
 *                                  .height = b->height,
 *                                  .width = b->width,
 *                                  .rowBytes = -b->rowBytes };
 *      }
 *
 *    CAUTION:  While vImage will put up with this sort of abuse, other frameworks may not. Make sure the data is
 *              right side up before passing it to anyone else.
 *
 *    Real Time Applications
 *    ----------------------
 *    vImage attempts to avoid doing things that will damage its suitability for use in real time applications.
 *    vImage will in general never take a lock, or do things that might involve taking a lock, such as allocating
 *    memory. Functions that may take a lock will be documented as such.  Some functions take temporary buffers
 *    as arguments. If you do not provide a temporary buffer, they may call malloc.
 *
 *    Unused Flag Bits
 *    ----------------
 *    Many of the bits in the vImage_Flags datatype are currently unused. Apple reserves all bits in the flags field
 *    for their exclusive use. You must set all unused bits to 0 in the flags field. If you do not do this, in the
 *    future some of these bits may become active and your application may start doing unexpected things when image
 *    processing.
 *
 *    Getting Data in and out of vImage
 *    ---------------------------------
 *    Since vImage will simply use your data in place in your objects, interfacing with your existing image pipeline
 *    should be a simple matter of intializing the vImage_Buffer struct fields to correspond to the location and
 *    and shape of your data.  
 * 
 *    When the objects are not yours and are opaque, then getting data in and out of those objects can be a little
 *    more work.  Most image frameworks have a method to import and export raw data. These are often simple to use
 *    but the data is frequently not in the format you wanted. In such cases, please see vImage/vImage_Utilities.h for
 *    vImageConvert_AnyToAny() which can convert nearly any image format to nearly any other one with full colorspace
 *    conversion, if desired, even on iOS. This should help you convert the output of one imaging library into 
 *    something useful for another one.  (It is used by ImageIO.framework for example, to convert the many different 
 *    image buffer formats produced by PNG, JPEG, GIF, TIFF, etc. to formats more favored by CoreGraphics, CoreAnimation 
 *    and CoreImage, for example.)  The conversions are vectorized and multithreaded to minimize time and energy 
 *    consumption.
 *
 *    If the opaque image object comes from CoreGraphics (CGImageRef) or CoreVideo (CVPixelBufferRef), then high level
 *    convenience routines are provided to import and export data from those sources as vImage_Buffers. See
 *    vImage/vImage_Utilities.h and vImage/vImage_CVUtilities.h respectively for these.  Since the conversions go through
 *    the same vImageConvert_AnyToAny core, it is possible to convert all the way from a YUV 420 format out to CMYK for
 *    a printer in a single function call, or anywhere inbetween, if you want. A few formats like 565 and half float not 
 *    usually part of CG are supported as a bonus.
 *
 *   @/textblock </pre>
 */


// Types, defines, flags and error codes
#include <vImage/vImage_Types.h>

// Alpha compositing, premultiplication, unpremultiplication
#include <vImage/Alpha.h>

// Sharpen, blur, edge detection, deconvolution
#include <vImage/Convolution.h>

// Hundreds of functions to convert from one image format to another.
#include <vImage/Conversion.h>

// Change the shape of an image by resizing it, vertical/horizontal reflect, rotation, shearing and affine warp.
#include <vImage/Geometry.h>

// Get a histogram. Make an image conform to a histogram.
#include <vImage/Histogram.h>

// Reshape structure elements in an image. Enlarge dark or light features. Close holes. Erode thin structures.
#include <vImage/Morphology.h>

// some limited support for PNG encoding
#include <vImage/BasicImageTypes.h>

// Gamma, polynomials, rationals, matrix multiplication (for hue saturation and brightness)
#include <vImage/Transform.h>

// The utility headers providing CG and CV interoperability pull in higher-level system headers that assume
// objective-c support that is not availble unless you are using clang as your compiler.
#if defined __clang__ && defined __has_include
# if __has_include(<CoreGraphics/CoreGraphics.h>)
// Convert any image format to nearly any other one. Exchange data with CGImageRefs
#  include <vImage/vImage_Utilities.h>
# endif

# if __has_include(<CoreVideo/CVPixelBuffer.h>)
// Import and export video frames from CVPixelBufferRefs
#  include <vImage/vImage_CVUtilities.h>
# endif
#endif /* defined __clang__ && defined __has_include */

#endif /* VIMAGE_H */
// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/vImage_Types.h
/*!
 *  @header vImage_Types.h
 *  vImage_Framework
 *
 *  See vImage/vImage.h for more on how to view the headerdoc documentation for types declared herein.
 *
 *  @copyright Copyright (c) 2002-2016 by Apple Inc. All rights reserved.
 *
 *  @discussion     Defines various types and constants common to vImage.
 */

#ifndef VIMAGE_TYPES_H
#define VIMAGE_TYPES_H

#ifdef __cplusplus
extern "C" {
#endif

#include <stdbool.h>
#include <stdint.h>
#include <stddef.h>
#include <unistd.h>
#include <os/availability.h>

/*!
 *  @define     __has_attribute
 *  @abstract   Compiler attribute detection
 *  @discussion A macro for testing whether your compiler supports a particular clang attribute.
 *  @noParse
 */

#ifndef __has_attribute          /* clang will define this. Other compilers maybe not. */
    #define __has_attribute(a)   0
#endif
    
/*!
 *  @define     VIMAGE_NON_NULL
 *  @abstract   Enumerates which parameters may not be NULL.
 *  @discussion Macro to use to decorate API for sanity checking. Many vImage pointer operands may not be NULL.
 *  @noParse
 *  @ignorefuncmacro VIMAGE_NON_NULL
 */
#if __has_attribute(nonnull)
#   define VIMAGE_NON_NULL(...)         __attribute__ ((nonnull(__VA_ARGS__)))
#else
#   define VIMAGE_NON_NULL(...)
#endif

/*!
 *  @define     __has_feature
 *  @abstract   Compiler feature detection
 *  @discussion A macro for testing whether your compiler supports a particular clang feature.
 *  @noParse
 */

#ifndef __has_feature           /* clang will define this. Other compilers maybe not. */
#   define __has_feature(f)     0
#endif
#ifndef __has_extension         /* clang will define this. Other compilers maybe not. */
#   define __has_extension(e)   0
#endif

/*!
 *  @define     CF_BRIDGED_TYPE
 *  @abstract   CoreFoundation definition detection and aliasing.
 *  @discussion A macro for annotating structs bridging to CoreFoundation types.
 *  @noParse
 */
#ifndef CF_BRIDGED_TYPE
#   if __has_feature(objc_bridge_id)
#       define CF_BRIDGED_TYPE(...) __attribute__((objc_bridge(__VA_ARGS__)))
#   else
#       define CF_BRIDGED_TYPE(...)
#   endif
#endif
    

    /* The C++'11 strongly typed enum feature turns out to be not exactly what we want because it precludes doing things like kvImageLeaveAlphaUnchanged | kvImageCopyInPlace */
    /* We are still exploring options to deliver something better than what we have today for enums. */
#if 0 /*(__cplusplus && __cplusplus >= 201103L && (__has_extension(cxx_strong_enums) || __has_feature(objc_fixed_enum))) || (!__cplusplus && __has_feature(objc_fixed_enum))*/
#   define VIMAGE_CHOICE_ENUM( _name, _type)        enum _name : _type _name; enum _name : _type
#   define VIMAGE_OPTIONS_ENUM(_name, _type)        enum _name : _type _name; enum _name : _type
#else
#   define VIMAGE_CHOICE_ENUM( _name, _type)        _type _name; enum
#   define VIMAGE_OPTIONS_ENUM( _name, _type)       _type _name; enum      
#endif

/*!
 *  @availabilitymacro VIMAGE_ENUM_AVAILABLE_STARTING
 */
    
#if __has_extension(enumerator_attributes)
#   ifdef __IPHONE_OS_VERSION_MIN_REQUIRED
#       define VIMAGE_ENUM_AVAILABLE_STARTING(_osx, _ios) __AVAILABILITY_INTERNAL##_ios
#   elif defined(__MAC_OS_X_VERSION_MIN_REQUIRED)
#       define VIMAGE_ENUM_AVAILABLE_STARTING(_osx, _ios) __AVAILABILITY_INTERNAL##_osx
#   else
#       define VIMAGE_ENUM_AVAILABLE_STARTING(_osx, _ios)
#   endif
#else
#   define VIMAGE_ENUM_AVAILABLE_STARTING( _a, _b )
#endif

/*
 *  @vImage function prototype prefix - Currently only set for visibility
 */
    
#ifndef VIMAGE_PF
#define VIMAGE_PF __attribute__ ((visibility ("default")))
#endif
    
/* Please see vImage.h and vImage documentation for the meaning of these types. */

/*!
    @typedef vImagePixelCount
    @discussion A number of pixels.  Typically, this is the height or width of an image.
 */

typedef unsigned long   vImagePixelCount;       /* Pedantic: A number of pixels. For LP64 (arm64/x86_64) this is a 64-bit quantity.  */

    
/*!
    @typedef vImage_Buffer
    @field  data        A pointer to the top left corner of the buffer contain image pixels.
    @field  height      The number of pixels in a column of the image. 
    @field  width       The number of visible pixels in a row of an image (excluding padding at the ends of rows)
    @field  rowBytes    The number of bytes from a pixel to the next pixel down in the same column.
    @discussion The vImage_Buffer describes a rectangular region within a regular array of pixels. It may describe
                the entire image, or just a sub rectangle of it.  The vImage_Buffer struct is not a complete description
                of an image. Other aspects like pixel format, color space, channel ordering, etc. are generally given
                by the names of functions that operate on the vImage_Buffer or by parameters passed to those functions.
                A vImage_Buffer may contain multiple color channels interleaved with one another, or a single color channel
                (or alpha) as a planar buffer.  vImage_Buffers are often initialized directly by you, by setting fields
                to appropriate values to point to image data you already own. Convenience methods are also available as
                vImageBuffer_Init, vImageBuffer_InitWithCGImage and vImageBuffer_InitWithCVPixelBuffer
 */
    

typedef struct vImage_Buffer
{
    void                *data;        /* Pointer to the top left pixel of the buffer.    */
    vImagePixelCount    height;       /* The height (in pixels) of the buffer        */
    vImagePixelCount    width;        /* The width (in pixels) of the buffer         */
    size_t              rowBytes;     /* The number of bytes in a pixel row, including any unused space between one row and the next. */
}vImage_Buffer;



/*!
    @typedef vImage_AffineTransform
    @field  a        top left cell in 3x2 transform matrix
    @field  b        top right cell in 3x2 transform matrix
    @field  c        middle left cell in 3x2 transform matrix
    @field  d        middle right cell in 3x2 transform matrix
    @field  tx       The x coordinate translation
    @field  ty       The y coordinate translation
    @discussion      This 3x2 matrix generally operates the same as the CGAffineTransform, except that the fields
                     are all float, not CGFloat. If you are looking for CGAffineTransform compatibility, it is 
                     recommended that you use vImage_CGAffineTransform and associated APIs instead.
 */
 
typedef struct vImage_AffineTransform
{
    float         a, b, c, d;
    float         tx, ty;
}vImage_AffineTransform;

/*!
  @typedef vImage_AffineTransform_Double
  @field  a        top left cell in 3x2 transform matrix
  @field  b        top right cell in 3x2 transform matrix
  @field  c        middle left cell in 3x2 transform matrix
  @field  d        middle right cell in 3x2 transform matrix
  @field  tx       The x coordinate translation
  @field  ty       The y coordinate translation
  @discussion      This 3x2 matrix generally operates the same as the CGAffineTransform, except that the fields
                    are all double precision, not CGFloat. If you are looking for CGAffineTransform compatibility, it is
                    recommended that you use vImage_CGAffineTransform and associated APIs instead.
*/

     
#ifndef VIMAGE_AFFINETRANSFORM_DOUBLE_IS_AVAILABLE
    #define VIMAGE_AFFINETRANSFORM_DOUBLE_IS_AVAILABLE      1     /* defined if vImage_AffineTransform_Double type is available. undefined otherwise */
    typedef struct vImage_AffineTransform_Double 
    {
        double         a, b, c, d;
        double         tx, ty;
    }vImage_AffineTransform_Double;
#endif

 /*!
  @typedef vImage_CGAffineTransform
  @field  a        top left cell in 3x2 transform matrix
  @field  b        top right cell in 3x2 transform matrix
  @field  c        middle left cell in 3x2 transform matrix
  @field  d        middle right cell in 3x2 transform matrix
  @field  tx       The x coordinate translation
  @field  ty       The y coordinate translation
  @discussion      This type mirrors the CGAffineTransform type, and may be used interchangeably with it.
  @seealso         CGAffineTransform utilities in CoreGraphics/CGAffineTransform.h
  */
#if VIMAGE_AFFINETRANSFORM_DOUBLE_IS_AVAILABLE       /* Interfaces that use this are only available on MacOS X.6 and later */
    #define VIMAGE_CGAFFINETRANSFORM_IS_AVAILABLE			1
    #if defined( __LP64__ )
        typedef    vImage_AffineTransform_Double    vImage_CGAffineTransform;
    #else
        typedef    vImage_AffineTransform           vImage_CGAffineTransform;    
    #endif
#endif

/*!
 @typedef   Pixel_8
 @abstract   An 8-bit per component unsigned planar pixel value.
*/
typedef uint8_t     Pixel_8;            /* 8 bit planar pixel value */

/*!
 @typedef   Pixel_F
 @abstract   A single precision floating-point planar pixel value.
 @discussion Typically, these have range [0,1] though other values are generally allowed.
*/
typedef float       Pixel_F;            /* floating point planar pixel value */

/*!
 @typedef   Pixel_88
 @abstract  A two channel, 8-bit per channel pixel.
 @discussion The channel order is generally given by the function that consumes the value.
 */
typedef uint8_t     Pixel_88[2];      /* CbCr interleaved (8 bit/channel) pixel value. uint8_t[2] = { Cb, Cr } */

/*!
 @typedef   Pixel_8888
 @abstract  A four channel, 8-bit per channel pixel.
 @discussion The channel order is generally given by the function that consumes the value.
 */
typedef uint8_t     Pixel_8888[4];      /* ARGB interleaved (8 bit/channel) pixel value. uint8_t[4] = { alpha, red, green, blue } */
 
/*!
 @typedef   Pixel_FFFF
 @abstract  A four channel, single precision floating-point per channel pixel. 
 @discussion The channel order is generally given by the function that consumes the value.
 */
typedef float       Pixel_FFFF[4];      /* ARGB interleaved (floating point) pixel value. float[4] = { alpha, red, green, blue } */
 
/*!
 @typedef   Pixel_16U
 @abstract  A 16-bit per channel unsigned pixel.  
 @discussion Typical range is [0,USHRT_MAX] meaning [0.0, 1.0], though most functions tolerate other ranges.
*/
typedef uint16_t    Pixel_16U;          /* 16 bit unsigned pixel */

/*!
 @typedef   Pixel_16S
 @abstract  A 16-bit per channel signed pixel.  
 @discussion Typical range is [SHRT_MIN,SHRT_MAX] meaning [-1.0, 1.0], though most functions tolerate other ranges.
*/
typedef int16_t     Pixel_16S;          /* 16 bit signed pixel */

/*!
 @typedef   Pixel_16Q12
 @abstract  A signed 16 bit fixed point number with 12 bits of fractional precision.
 @discussion Normal range is [-4096,4096] meaning [-1.0, 1.0]. Values in the range [-8.0, 8.0) are representable.
 */
typedef int16_t     Pixel_16Q12;		/* 16 bit signed pixel */

/*!
 @typedef   Pixel_16U16U
 @abstract  A two channel, 16-bit per channel pixel.
 @discussion The channel order is generally given by the function that consumes the value.
 */
typedef uint16_t     Pixel_16U16U[2];	/* CbCr interleaved (16 bit/channel) pixel value. uint16_t[2] = { Cb, Cr } */

/*!
 @typedef   Pixel_32U
 @abstract  Type used for XRGB2101010 format.
 @discussion Typical range for RGB channels is [0,1023] meaning [0.0, 1.0], though most functions tolerate other ranges.
*/
typedef uint32_t    Pixel_32U;          /* 32 bit unsigned pixel */

/*!
 @typedef   Pixel_ARGB_16U
 @abstract  A four channel, 16-bit unsigned per channel pixel.  
 @discussion The channel order is generally given by the function that consumes the value. It is not necessarily ARGB.
 */
typedef uint16_t    Pixel_ARGB_16U[4];  /* four-channel 16-bit unsigned pixel */

/*!
 @typedef   Pixel_ARGB_16S
 @abstract  A four channel, 16-bit signed per channel pixel.  
 @discussion The channel order is generally given by the function that consumes the value. It is not necessarily ARGB.
*/
typedef int16_t     Pixel_ARGB_16S[4];  /* four-channel 16-bit signed pixel */

/*!
 @typedef ResamplingFilter
 @abstract A ResamplingFilter is an opaque structure used by vImage to hold precalculated filter coefficients for a resampling filter,
             such as a Lanczos or Gaussian resampling filter. 
 @discussion It is created with vImageNewResamplingFilter or vImageNewResamplingFilterUsingBuffer and is consumed by various vertical 
             and horizontal shear functions in vImage/Geometry.h. When possible, for better performance, reuse ResamplingFilters over 
             multiple vImage calls, instead of allocating a new one each time.
*/
    
typedef void*       ResamplingFilter;   /* Used by certain Geometry functions.  */

    
/*!
 @typedef GammaFunction
 @discussion A GammaFunction is an opaque structure used by vImage to represent an approximation of a non-linear curve. It is created with
             vImageCreateGammaFunction, and destroyed with vImageDestroyGammaFunction. When possible, for better performance,
             reuse GammaFunctions over multiple vImage function calls, instead of creating a new one each time.
 */
    
typedef void*       GammaFunction;      /* Used by vImageGamma                  */

/* vImage Errors                                                                                */
/* ============                                                                                 */
/*   All return values < 0 indicate failure. In this case, the results in the destination       */
/*   buffer are undefined. The list of error codes may grow in the future.                      */
/*                                                                                              */
/*   Note: It is also possible for positive non-zero numbers to be returned out the LHS of a    */
/*   vImage function. This happens when the kvImageGetTempBufferSize bit is set in the flags.   */
/*   In this case, no work is done by the vImage function.  The value returned is the size of   */
/*   the temp buffer needed by the function.                                                    */
/*!
     @typedef   vImage_Error 
     @abstract  An error code returned by a vImage function.
     @discussion    All negative values are errors.
                    Positive return values are likely to be the result of kvImageGetTempBufferSize.
                    Zero indicates no error, or quasi-ambiguously a zero temp buffer size, if that
                    flag was passed. Please see the documentation for the function that returned the
                    error code for additional information about the error.
 
    @constant   kvImageNoError      Success.  If kvImageGetTempBufferSize is set in flags, then
                                    it indicates the temp buffer size is 0 and the function did
                                    nothing else.
    @constant   kvImageRoiLargerThanInputBuffer     The size or positioning of the result buffer 
                                    was such that pixels were needed in the source buffer, were
                                    found to be missing and could not be discovered using an edging
                                    process like kvImageEdgeExtend. Typically, this means that the
                                    source image was smaller than the destination image. 
    @constant   kvImageInvalidKernelSize    The size of a kernel was invalid. Typically, kernels have
                                    have an odd number of rows and columns. The kernel might also have
                                    been too large or of zero dimension. Typically only returned by
                                    convolutions and morphological operations.
 
    @constant   kvImageInvalidEdgeStyle   The edging style {kvImageBackgroundColorFill, kvImageCopyInPlace, kvImageEdgeExtend, kvImageTruncateKernel}
                                    was invalid. Typically, the edging style is either missing or the edging style 
                                    is not supported by this function.
 
    @constant   kvImageInvalidOffset_X   Some functions take an integer offset in the horizontal dimension. This indicates
                                    how far from the left edge of the vImage_Buffer we shoud consider the origin to be.
                                    The offset allows real pixel data to be used where an edging method like kvImageEdgeExtend
                                    might otherwise have to be used to generate missing pixels. An X offset is essential to
                                    obtaining correct results for tiled image processing when the tile is not at the left
                                    edge of the (whole) image and the function has a kernel or ResamplingFilter.
                                    Typically, this can happen when the offset is negative or larger than the source vImage_Buffer.width.
 
    @constant   kvImageInvalidOffset_Y   Some functions take an integer offset in the vertical dimension. This indicates
                                    how far from the top edge of the vImage_Buffer we shoud consider the origin to be.
                                    The offset allows real pixel data to be used where an edging method like kvImageEdgeExtend
                                    might otherwise have to be used to generate missing pixels. A Y offset is essential to
                                    obtaining correct results for tiled image processing when the tile is not at the top
                                    edge of the (whole) image and the function has a kernel or ResamplingFilter.
                                    Typically, this can happen when the offset is negative or larger than the source vImage_Buffer.height.
 
    @constant   kvImageMemoryAllocationError    vImage attempted to allocate memory and the allocator failed, returning NULL.
 
    @constant   kvImageNullPointerArgument      One or more arguments to the function are NULL, which are not allowed to be NULL.
                                    Typically, arguments that are not allowed to be NULL are listed in the VIMAGE_NON_NULL() attribute
                                    that follows the function declaration.
 
    @constant   kvImageInvalidParameter         A function parameter has an invalid value. This is the fallback error code when there is not
                                    an error code that more precisely describes the invalid parameter value such as kvImageNullPointerArgument,
                                    kvImageInvalidEdgeStyle, kvImageInvalidRowBytes, etc.
 
    @constant   kvImageBufferSizeMismatch       This most often occurs when a set of planar buffers are not the same size.  Most often
                                    this occurs when the destination image is planar and the destination planes are not the same size.
                                    It may also occur in rare cases when chunk sizes mismatch for some high level conversions.
 
    @constant   kvImageUnknownFlagsBit      A bit in the flags field is set, which the function does not understand or currently support.
                                    Please check flags enum availability info to make sure the flag is supported on the target OS revision.
 
    @constant   kvImageInternalError    A serious error occured inside vImage, which prevented vImage from continuing. This error is
                                    probably a problem with vImage itself, and not how it is being used. Please file a bug with a
                                    reproducible test case attached, if possible!
 
    @constant   kvImageInvalidRowBytes  The vImage_Buffer.rowBytes field is invalid. In some circumstances, rowBytes of 0 is not supported, 
                                    particularly with destination images or when vImage is asked to initialize a vImage_Buffer and the
                                    rowBytes is too small to hold a row of image data.
 
    @constant   kvImageInvalidImageFormat  Usually a vImage_CGImageFormat or vImageCVImageFormatRef contains an invalid format. It might be a
                                    NULL pointer, a description which is not allowed according to rules of CG or CV image format encodings.
 
    @constant   kvImageColorSyncIsAbsent    ColorSync.framework is completely missing. Certain operations will not work.
 
    @constant   kvImageOutOfPlaceOperationRequired   The source images and destination images may not alias the same image data. This will
                                    be returned by vImageCGConverter_MustOperateOutOfPlace(). However, there are many functions in vImage
                                    that do not work in place which do not check to see if the buffers overlap. Please inspect the documentation
                                    of each function for notes on in-place usage before using them that way.
 
    @constant   kvImageInvalidImageObject   An invalid CGImageRef or CVPixelBufferRef was passed to the function.  Typically, the object was NULL.
                                    A non-NULL invalid CGImageRef or CVPixelBufferRef will result in undefined behavior.
 
    @constant   kvImageInvalidCVImageFormat A vImageCVImageFormatRef contains an invalid format. It might be a NULL pointer, or an image format
                                    which is not allowed according to rules of CV image format encodings.
 
    @constant   kvImageUnsupportedConversion    Some lower level conversion APIs only support conversion among a sparse matrix of image formats.
 */
typedef VIMAGE_CHOICE_ENUM(vImage_Error, ssize_t)
{
    kvImageNoError                     VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    0,
    kvImageRoiLargerThanInputBuffer    VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21766,
    kvImageInvalidKernelSize           VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21767,
    kvImageInvalidEdgeStyle            VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21768,
    kvImageInvalidOffset_X             VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21769,
    kvImageInvalidOffset_Y             VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21770,
    kvImageMemoryAllocationError       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21771,
    kvImageNullPointerArgument         VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21772,
    kvImageInvalidParameter            VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21773,
    kvImageBufferSizeMismatch          VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21774,
    kvImageUnknownFlagsBit             VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21775,
    kvImageInternalError               VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21776,    /* Should never see this. File a bug! */
    kvImageInvalidRowBytes             VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21777,
    kvImageInvalidImageFormat          VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21778,
    kvImageColorSyncIsAbsent           VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21779,
    kvImageOutOfPlaceOperationRequired VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21780,
    kvImageInvalidImageObject          VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21781,
    kvImageInvalidCVImageFormat        VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21782,
    kvImageUnsupportedConversion       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21783,
    kvImageCoreVideoIsAbsent           VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) =    -21784,
};
    
/* vImage Flags                                                                                 */
/* ============                                                                                 */
/* The values here indicate bits in a vImage_Flags bit field.                                   */
/* Other bits are reserved for future use.                                                      */
/* Some flags are mutually exclusive. You can not have more                                     */
/* than one bit from this set set at the same time:                                             */
/* { kvImageCopyInPlace, kvImageBackgroundColorFill, kvImageEdgeExtend, kvImageTruncateKernel } */
/* all unused flags bits must be set to 0                                                       */
/* Not all flags are allowed by all functions.                                                  */
/*!
 @typedef   vImage_Flags
 @abstract      vImage_Flags is a 32-bit bitfield of options of general use to vImage functions.
 @discussion    Multiple bits may be set concurrently. kvImageUnknownFlagsBit may be returned by
                a function if a flag bit is set (1) but the function does not know what the flag
                bit means (e.g. your new code on an older version of vImage) or if the flag should
                cause a behavior that is unsupported by the function, for example, kvImageHighQualityResampling
                to a function that does not do image resampling.
 @constant  kvImageNoFlags   Use the default behavior. Internal multithreading is enabled. Debug 
                messages are generally not printed to the console. No edging method is specified. 
                Normal quality resampling methods are used (Lanczos3, probably). Do the function,
                instead of returning a temp buffer size. Allocate memory as needed.
 
 @constant  kvImageLeaveAlphaUnchanged   Some functions that operate on ARGB data in place allow
                you to operate on just the RGB components and leave the alpha channel unmodified.
                These are typically histogram and gamma functions.
 
 @constant  kvImageCopyInPlace  One of four edging modes. This one tells vImage to do nothing for
                destination pixels that need source pixels that are missing. The corresponding source
                pixel is copied to the destination image. This is only used for image filters that take a
                kernel, such as convolutions. It is not allowed for Morphology filters.
 
 @constant kvImageBackgroundColorFill  One of four edging modes. This one tells vImage to use the
                backgroundColor parameter of the function as the color of any missing pixels in
                a source image. Missing pixels occur when a filter needs to read off the edge of
                a source image. 
 
 @constant kvImageEdgeExtend    One of four edging modes.  This one tells vImage to use the nearest
                existing source image pixel when it needs source data but finds it needs a non-existant
                pixel off the edge of the provided source image. 
 
 @constant kvImageDoNotTile     Do not internally subdivide the image for processing on multiple CPUs
                or other compute devices. If this flag is set, the function will run single threaded
                on the current thread. Usually this flag may be expected to cause a significant increase
                in the execution time of a vImage function. However, if you are calling the function
                from a heavily multithreaded context (such as your own tiling engine) and CPU occupancy
                is high, this may lead to small performance improvements due to reduced CPU contention.
 
 @constant kvImageHighQualityResampling  Use a more expensive image resampling method than what is available
                by default. Typically this is Lanczos5. Note that as vImage resampling already leans towards
                quality over performance -- the GPU texture unit is your friend if you just want raw performance
                -- the additional quality from kvImageHighQualityResampling may be difficult to see in some images. 
                It is best suited to background rendering tasks.
 
 @constant kvImageTruncateKernel    One of four edging modes. This one tells vImage to use only the pixels it 
                has and reweight the kernel accordingly. The reweighting process can be expensive.  It can 
                also lead to difficulties if a contiguous sub-rectangle of the kernel sums to zero. In this
                case, the area of the kernel is zero and you have formally asked vImage to do division by zero,
                in which case vImage behavior is undefined.
 
 @constant kvImageGetTempBufferSize  Instead of performing the function requested, return (as a vImage_Error)
                the size of the temp buffer requried by the function for this set of parameters. The image pixels
                are not touched. If the function does not take a temp buffer, then 0 will be returned and the 
                function will do nothing. Some functions may return a 0 sized temp buffer for some sets of parameters 
                an not others. The size of the temporary buffer may change for different parameters, and for
                different OS revisions.
 
 @constant kvImagePrintDiagnosticsToConsole Some of the high level functions in vImage_Utilities.h and 
                vImage_CVUtilities.h have complex failure modes that could prove baffling with just a 
                error code return.  This flag instructions these functions to also print a human readable
                diagnostic message to the Apple System Logger when an error is encountered. The output 
                should be visible in Console.app.
 
 @constant kvImageNoAllocate    Some vImage functions may allocate memory, possibly returning it to you.
                This flag instructs the function to use the memory provided instead. For example, instead of
                overwriting vImage_Buffer.data with a newly allocated pointer to memory, use the memory
                pointed to by vImage_Buffer.data directly. In other cases, it may cause the function to
                assume ownership of a buffer, rather than allocating a copy. You are responsible for making
                sure the buffer that you allocate instead of vImage is large enough to hold the image. Most 
                vImage functions do not allocate memory and assume that vImage_Buffer.data is already allocated, 
                and in the case of source image buffers, contain valid pixel data.
 
 @constant kvImageHDRContent    The pixels described in the input image may contain high dymanic range content.
                HDR pixels may have value outside of [-2,2.0]. This flag is generally only applicable to 
                floating-point images. Most 8- and 16-bit pixels can not represent values outside [0,1] and
                functions that operate on 16Q12 formats are designed to operate over the full range of [-8,8).
                Most floating-point functions in vImage are linear in behavior and so work equally well 
                on any float.  Some non-linear functions like polynomials (or by extension colorspace conversion)
                are only valid over a limited range (typically [-2,2]) and will return incorrect answers 
                for values outside that range.  In addition, certain IIR or FFT algorithms in convolution may
                encounter precision issues with HDR content.  For these cases, if you know you have HDR content,
                pass kvImageHDRContent and a (typically slower) alternative method will be used for these
                sources.
 */
typedef VIMAGE_OPTIONS_ENUM(vImage_Flags, uint32_t)
{
    kvImageNoFlags                   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =    0,
    
     /* Operate on red, green and blue channels only. Alpha is copied from source 
        to destination. For Interleaved formats only. */
    kvImageLeaveAlphaUnchanged       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =    1,    
    
     /* Copy edge pixels. Convolution Only. */
    kvImageCopyInPlace               VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =    2,
    
    /* Use the background color for missing pixels. */
    kvImageBackgroundColorFill       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =    4,
    
    /* Use the nearest pixel for missing pixels. */
    kvImageEdgeExtend                VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =    8,
    
    /* Pass to turn off internal tiling and disable internal multithreading. Use this if 
       you want to do your own tiling, or to use the Min/Max filters in place. */
    kvImageDoNotTile                 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =   16,
    
    /* Use a higher quality, slower resampling filter for Geometry operations 
       (shear, scale, rotate, affine transform, etc.) */
    kvImageHighQualityResampling     VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 )     =   32,
    
     /* Use only the part of the kernel that overlaps the image. For integer kernels, 
        real_divisor = divisor * (sum of used kernel elements) / (sum of kernel elements). 
        This should preserve image brightness at the edges. Convolution only. */
    kvImageTruncateKernel            VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 )     =   64,
    
    /* The function will return the number of bytes required for the temp buffer. 
       If this value is negative, it is an error, per standard usage. */
    kvImageGetTempBufferSize         VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 )     =  128,
    
    /* Some functions such as vImageConverter_CreateWithCGImageFormat have so many possible error conditions 
       that developers may need more help than a simple error code to diagnose problems. When this 
       flag is set and an error is encountered, an informative error message will be logged to the Apple 
       System Logger (ASL).  The output should be visible in Console.app. */
    kvImagePrintDiagnosticsToConsole VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 )     =  256,
    
    /* Pass this flag to prevent vImage from allocating additional storage. */
    kvImageNoAllocate                VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_9, __IPHONE_7_0 )     =  512,

    /* Use methods that are HDR-aware, capable of providing correct results for input images with pixel values
       outside the otherwise limited (typically [-2,2]) range. This may be slower. */
    kvImageHDRContent                VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_11, __IPHONE_9_0 )    =  1024,

    /* Pass to disable clamping is some conversions to floating point formats. Use this if the input data
       may describe values outside [0,1] which should be preserved.. */
    kvImageDoNotClamp                 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_12, __IPHONE_9_3 )     =   2048
};
    
/*!
    @class vImageConverterRef
    @abstract   An opaque type which contains a decription of a conversion from one CoreGraphics image format to another.
 
    @discussion The vImageConverter is an opaque type which contains information needed to do a rapid conversion from
    one image type to another. Sometimes, it can take a significant amount of time to figure out how to convert
    from one format to another. It wouldn't be good to do that redundantly for a bunch of small images. The
    vImageConversionSetup allows us to set up the conversion once and reuse the information many times, to
    keep net latencies low.
 
    Note that creating a vImageConverter can at times take a while. While usually it is quick, it might have
    to do things like load other frameworks in the system (e.g. Colorsync) if they are not loaded already,
    or build a lookup table. It is a good idea to setup your conversions in advance and reuse the conversion
    objects.  The objects are thread safe. You can use the same object in multiple threads concurrently. They
    follow standard retain / release semantics and can be used as CFTypeRefs.
 
    @superclass CFTypeRef
 
    @seealso Please see vImage_Utilities.h for interfaces that operate on the vImageConverterRef
 */

typedef struct CF_BRIDGED_TYPE(id) vImageConverter * vImageConverterRef;

    
/*!
    @class vImageCVImageFormatRef
    @abstract   An opaque type which contains a decription of a conversion from a CoreGraphics image format to a CVPixelBuffer, or the reverse.
 
    @superclass CFTypeRef
    @discussion The vImageCVImageFormatRef describes how the image is encoded in a CVPixelBufferRef. vImage uses this information to construct converters that
    are capable of converting to and from this image encoding.  The format stores a description of the pixels in the image (planar/color representation/
    bit depth/number of channels, etc.) but not the image size, location of the base pointer or rowbytes. It is intended for the vImageCVImageFormatRef
    to be reused for other CVPixelBufferRefs of the same format, such as other frames from the same movie.
 
    vImageCVImageFormatRefs are capable of holding an incomplete encoding representation. You may be required to provide addition information such
    as colorspace and (YCbCr only) chroma siting or conversion matrix before the vImageCVImageFormatRef can be used for image conversion to other
    formats.
 
    The vImageCVImageFormatRef is a CFTypeRef. CFEqual does not test for equivalence of the userData field. You should use vImageCVImageFormat_Retain/Release
    when working with vImageCVImageFormatRef to manage ownership of the object.

        <pre>
        @textblock
        Thread Safety:

            The vImageCVImageFormatRef may be safely read from multiple threads concurrently.  However,
            it makes no attempt to keep its internal state coherent when multiple threads write to it,
            or when one thread writes to it while one or more threads are reading from it at the same
            time. This can be trivially handled by keeping the knowledge of the vImageCVImageFormatRef
            limited to a single thread while it is being created / configured and then treat it as
            immutable thereafter.  If necessary, you can also use a read/write lock to limit reentrant
            access.
        @/textblock
        </pre>
 
 
    Information tracked by vImageCVImageFormatRef:
 
 <pre>
 @textblock
    imageFormatType     A CVPixelFormatType such as '2vuy'. See CVPixelBuffer.h for the complete list.
 
    number_of_channels  How many  color + alpha channels are encoded in the image. An alpha channel is included in this count
                        if it takes up space in the image, even if its value is described always 1.0, for example by /Last
                        kCGImageAlphaNoneSkipFirst or kCVImageBufferAlphaChannelIsOpaque.  This field is automatically initialized
                        based on the imageFormatType (see above) and is never missing.
 
    channel_names       A list of vImageBufferTypeCodes corresponding to the channels in the image. Unlike what happens for
                        vImageConverterRefs, the type codes used here always encode a single color channel.  vImageConverterRefs
                        use the channel names to encode what is in each vImage_Buffer. Here it is used to describe each channel.
                        So, an ARGB buffer might be described as kvImageBufferTypeCode_CGFormat but will be described as
                        { kvImageBufferTypeCode_RGB_Red, kvImageBufferTypeCode_RGB_Green, kvImageBufferTypeCode_RGB_Blue,
                        kvImageBufferTypeCode_Alpha, kvImageBufferTypeCode_EndOfList }. The order of the channels in the list
                        may not match the order of the channels in the buffer.  This field is automatically initialized based on
                        the imageFormatType (see above) and is never missing.
 
   matrix               (YpCbCr only.)  A YpCbCr image has an associated 3x3 matrix that encodes how it was converted to YpCbCr from
                        a reference RGB colorspace (see colorspace below). The matrix is encoded as a NULL pointer when missing.
                        This field is ignored for non-YpCbCr formats.
 
 
   chroma_siting        Some YpCbCr formats store their chroma components as a smaller image than the luminance component.
                        This describes where the subsampled chroma samples are positioned relative to the luminance component.
                        This field is encoded as a NULL CFStringRef when missing. The field is ignored for RGB, monochrome,
                        indexed and 4:4:4 YpCbCr image formats.
 
   colorspace           For RGB, indexed and grayscale images, this is the colorspace that describes the image encoding.
                        For YpCbCr images, this is the colorspace of the RGB image that you get once the matrix (see above)
                        is unapplied. Thus, the colorspace encodes for the underlying primaries and transfer function of the
                        YpCbCr image. See also vImageCreateRGBColorSpaceWithPrimariesAndTransferFunction.  This field is required
                        for all image formats. A colorspace of NULL indicates a missing colorspace. (This is inconsistent with
                        the shorthand used in vImage_Utilities.h where NULL maps to sRGB.)  Since vImage has no concept of
                        a current graphics device, deviceRGB maps to sRGB and device gray maps to gray 2.2. If you wish to
                        ensure no color correction / conversion, you should match this colorspace with the one in the
                        vImage_CGImageFormat to / from which you are converting.
 
   channel_description  Some CVPixelBuffer formats do not use the entire representable range of the format to encode image data.
                        For example, a 'yuvs' "video range" buffer only uses the range [16,235] for luminance and [16,240] for
                        chroma. Values outside that range are considered to have value equal to the nearest in-range value. In
                        addition, we add additionional fields to leave open the possibility that some formats can encode information
                        outside of the traditional [0,1.0] range ([-1.0,1.0] for chroma) so reference values for the encoding for 0
                        and 1.0 are also described. (See vImageChannelDescription below.) The zero/one fields are analogous in function
                        to the decode arrays provided by CG. It is possible to use the channel description to create formats that are
                        not correctly understood by CoreVideo. These are provided to allow for interoperation with custom video formats.
                        The channel description is initialized automatically for known image format types (see imageFormatType above)
                        and probably only very rarely needs to be changed. It is never missing.
 
  alpha_is_one_hint     Some images are encoded with an alpha channel. However, you may have additional information that the image is
                        really completely opaque. The "alpha is one hint" tells vImage that the alpha channel is always 1.0 (opaque)
                        across the entire image. Setting the hint to 1 may allow vImage to avoid work and run faster in some cases.
                        There is no vImageCVImageFormatRef representation for premultiplied alpha, currently. Since it is a hint,
                        the hint may never be missing from a vImageCVImageFormatRef.  The hint is ignored for image formats that do
                        not contain an alpha channel.
 
  user_data             The vImageCVImageFormatRef has a userData field to allow you to easily reference your data starting from
                        a handle to the object. vImage does  not attempt to interact with the memory pointed to by the userData
                        pointer. It simply holds on to the pointer for you and will call a destructor callback function when the
                        vImageCVImageFormatRef to allow you to free that memory, and do any other post processing needed when the
                        vImageCVImageFormatRef is destroyed. Since there is only one userData pointer, by convention its use is
                        considered private to the application/library/framework that created the vImageCVImageFormatRef, including
                        cases where the creator does not set the userData field. If you need to attach your own data to a
                        vImageCVImageFormatRef created by someone else, you can make  a copy of it or wrap it with your own object.
 @/textblock
 </pre>
 
    @seealso Please see vImage_CVUtilities.h for interfaces that operate on the vImageCVImageFormatRef
 */
typedef struct CF_BRIDGED_TYPE(id) vImageCVImageFormat * vImageCVImageFormatRef;
typedef const struct CF_BRIDGED_TYPE(id) vImageCVImageFormat * vImageConstCVImageFormatRef;



/*!
    @typedef    vImageARGBType
    @abstract   An encoding of an image format type to be used with RGB <-> Y'CbCr conversions in vImage/Conversions.h
    @discussion These formats enumerate different vImage ARGB pixel formats.
 */
typedef enum
{
        kvImageARGB8888   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 )        = 0,   /* Any 8-bit four channel interleaved buffer [0,255]=[0,1.0]. Does not specify channel order. */
        kvImageARGB16U    VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 )        = 1,   /* Any 16-bit unsigned four channel interleaved buffer [0,65535]=[0,1.0]. Does not specify channel order. */
        kvImageARGB16Q12  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 )        = 2    /* Any 16-bit signed fixedpoint four channel interleaved buffer [0,4096]=[0,1.0]. Does not specify channel order.  */
}vImageARGBType;
    
/*!
    @typedef    vImageYpCbCrType
    @abstract   An encoding of an image format type to be used with RGB <-> Y'CbCr conversions in vImage/Conversions.h
    @discussion These formats enumerate different vImage/CoreVideo Y'CbCr pixel formats.
    @seealso    CVPixelBuffer OSTypes in CVPixelBuffer.h
 */
typedef enum
{
        kvImage422CbYpCrYp8                  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 0,   /* 2vuy        */
        kvImage422YpCbYpCr8                  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 1,   /* yuvs / yuvf */
        kvImage422CbYpCrYp8_AA8              VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 2,   /* a2vy        */
        kvImage420Yp8_Cb8_Cr8                VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 3,   /* y420 / f420 */
        kvImage420Yp8_CbCr8                  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 4,   /* 420v / 420f */
        kvImage444AYpCbCr8                   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 5,   /* r408 / y408 */
        kvImage444CrYpCb8                    VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 6,   /* v308        */
        kvImage444CbYpCrA8                   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 7,   /* v408        */
        kvImage444CrYpCb10                   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 8,   /* v410        */
        kvImage422CrYpCbYpCbYpCbYpCrYpCrYp10 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 9,   /* v210        */
        kvImage422CbYpCrYp16                 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 13,  /* v216 16-bit */
        kvImage444AYpCbCr16                  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_0, __IPHONE_8_0 ) = 14   /* y416        */
}vImageYpCbCrType;
    

/*!
 @struct    vImage_YpCbCrToARGBMatrix
 
 @field Yp      matrix[0][0].  Typically 1.
 @field Cr_R    matrix[0][2].
 @field Cb_G    matrix[1][1].
 @field Cr_G    matrix[1][2].
 @field Cb_B    matrix[2][1]
 
 @abstract   A 3x3 converson matrix for converting Y'CbCr signals to RGB
 @discussion The matrix is sparse. The 3x3 matrix is given by:
 
            <pre>
            @textblock
                    | R |   | Yp    0     Cr_R |   | Y' |
                    | G | = | Yp   Cb_G   Cr_G | * | Cb |
                    | B |   | Yp   Cb_B     0  |   | Cr |
            @/textblock
            </pre>
 
            Limits on the range of the fields in the matrix may apply.
 
 @seealso    vImage_ARGBToYpCbCrMatrix, which is the inverse matrix
 */

/* Input information into vImageConvert_YpCbCrToARGB_GenerateConversion() */
typedef struct vImage_YpCbCrToARGBMatrix
{
        float                      Yp;
        float                      Cr_R;
        float                      Cr_G;
        float                      Cb_G;
        float                      Cb_B;
}vImage_YpCbCrToARGBMatrix;
 
/*! @const      kvImage_YpCbCrToARGBMatrix_ITU_R_601_4
 @abstract   Y'CbCr->RGB conversion matrix for ITU-Recommendation BT.601-4 */
extern VIMAGE_PF const vImage_YpCbCrToARGBMatrix *kvImage_YpCbCrToARGBMatrix_ITU_R_601_4 API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @const      kvImage_YpCbCrToARGBMatrix_ITU_R_709_2
 @abstract   Y'CbCr->RGB conversion matrix for ITU-Recommendation BT.709-2 */
extern VIMAGE_PF const vImage_YpCbCrToARGBMatrix *kvImage_YpCbCrToARGBMatrix_ITU_R_709_2 API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

    
    
    
/*!
 @typedef    vImage_YpCbCrToARGB
 @abstract   An opaque representation of a 3x3 converson matrix for converting Y'CbCr signals to RGB.
             It is used to do the actual conversions. Please attempt to reuse these rather than making
             new ones each time.
 @discussion The representation also includes the range of the input and output pixels from the matrix and
             clamping information.
 
 @seealso    vImage_ARGBToYpCbCr, which is the inverse matrix
 */

typedef struct vImage_YpCbCrToARGB
{
        uint8_t __attribute__ ((__aligned__(16))) opaque[128];
}vImage_YpCbCrToARGB;
    
/*!
 @typedef    vImage_ARGBToYpCbCrMatrix
 @abstract   A 3x3 converson matrix for converting RGB signals to Y'CbCr
 @discussion The matrix has one repeating parameter. The 3x3 matrix is given by:
 
 @textblock
        | Y' |   | R_Yp        G_Yp  B_Yp      |   | R |
        | Cb | = | R_Cb        G_Cb  B_Cb_R_Cr | * | G |
        | Cr |   | B_Cb_R_Cr   G_Cr  B_Cr      |   | B |
 @/textblock
 
 Typically, these matrix coefficients come from a conversion of form:
 
 @textblock
    Y' = R_Yp * R + G_Yp * G + B_Yp * B         0 <= Y' <= 1
    Cb = k0 * (B - Y')                          -0.5 <= Cb <= 0.5
    Cr = k1 * (R - Y')                          -0.5 <= Cr <= 0.5
 @/textblock
 
 {R_Yp, G_Yp, B_Yp} are typically derived from the perceived brightness for red, green and blue.
 k0 and k1 are typically scaled so that Cb and Cr have the indicated range. Because of these
 relationships, {R_Yp, G_Yp, B_Yp} are usually positive, and B_Cb_R_Cr is usually 0.5.
 Limits on the range of the fields in the matrix may apply.
 @field R_Yp    matrix[0][0]
 @field G_Yp    matrix[0][1]
 @field B_Yp    matrix[0][2]
 @field R_Cb    matrix[1][0]
 @field G_Cb    matrix[1][1]
 @field B_Cb_R_Cr matrix[1][2] and matrix[2][0]
 @field G_Cr    matrix[2][1]
 @field B_Cr    matrix[2][2]
 @seealso    vImage_YpCbCrToARGBMatrix, which is the inverse matrix
 
 */
/* Input information into vImageConvert_ARGBToYpCbCr_GenerateConversion() */
typedef struct vImage_ARGBToYpCbCrMatrix
{
        float                      R_Yp;
        float                      G_Yp;
        float                      B_Yp;
        float                      R_Cb;
        float                      G_Cb;
        float                      B_Cb_R_Cr;
        float                      G_Cr;
        float                      B_Cr;
}vImage_ARGBToYpCbCrMatrix;

/*! @const      kvImage_ARGBToYpCbCrMatrix_ITU_R_601_4
 @abstract   RGB->Y'CbCr conversion matrix for ITU-Recommendation BT.601-4 */
extern VIMAGE_PF const vImage_ARGBToYpCbCrMatrix *kvImage_ARGBToYpCbCrMatrix_ITU_R_601_4 API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));

/*! @const      kvImage_ARGBToYpCbCrMatrix_ITU_R_709_2
 @abstract   RGB->Y'CbCr conversion matrix for ITU-Recommendation BT.709-2 */
extern VIMAGE_PF const vImage_ARGBToYpCbCrMatrix *kvImage_ARGBToYpCbCrMatrix_ITU_R_709_2 API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
    
/*!
 @typedef    vImage_ARGBToYpCbCr
 @abstract   An opaque representation of a 3x3 converson matrix for converting RGB signals to Y'CbCr.
 It is used to do the actual conversions. Please attempt to reuse these rather than making
 new ones each time.
 @discussion The representation also includes the range of the input and output pixels from the matrix and
 clamping information.
 
 @seealso    vImage_ARGBToYpCbCr, which is the inverse matrix
 */
typedef struct vImage_ARGBToYpCbCr
{
        uint8_t __attribute__ ((__aligned__(16))) opaque[128];
}vImage_ARGBToYpCbCr;
    
/*! 
 @typedef vImage_YpCbCrPixelRange
 @abstract  Range and clamping information for Y'CbCr pixel formats
 @discussion Y'CbCr formats frequently don't use the entire representable range available to 
             them to represent image data. While a "full range" video format does use the
             entire range, a "video range" format often leaves the extrema unused, except perhaps
             to represent values outside of the standard Y'=[0,1] CbCr = [-0.5, 0.5] range. 
             For example, a 8-bit video range format typically uses the range [16,235] for
             Y' and [16, 240] for Cb and Cr. 
 
 
             Some examples:
    
 @textblock
                (vImage_YpCbCrPixelRange){ 16, 128, 235, 240, 255, 0, 255, 1 }      // video range 8-bit, unclamped
                (vImage_YpCbCrPixelRange){ 16, 128, 235, 240, 235, 16, 240, 16 }    // video range 8-bit, clamped to video range
                (vImage_YpCbCrPixelRange){ 0, 128, 255, 255, 255, 1, 255, 0 }       // full range 8-bit, clamped to full range
 @/textblock
 
            The bias will be the prebias for YUV -> RGB and postbias for RGB -> YUV.
 
 @field Yp_bias The encoding for Y' = 0.0 for this video format (varies by bitdepth)
 @field CbCr_bias The encoding for {Cb,Cr} = 0.0 for this video format. This is usually the MIDDLE of the range of CbCr, not the low end. 
 @field YpRangeMax The encoding for Y' = 1.0 for this video format. For video range, this is typically less than the maximum representable value.
 @field CbCrRangeMax The encoding for {Cb,Cr} = 0.5 for this video format. This is usually near the high end of the encodable range (e.g. 0xf0), if not the maximum encodable value (e.g. 0xff)
 @field YpMax  The encoding for the maximum allowed Y' value. All values larger than this will be clamped to this value.
 @field YpMin   The encoding of the minimum allowed Y' value. All values less than this will be clamped to this value.
 @field CbCrMax The encoding of the maximum allowed {Cb, Cr} value. All chroma values greater than this value will be clamped to this value.
 @field CbCrMin The encoding of the minimum allowed {Cb, Cr} value. All chroma values less than this value will be clamped to this value.
 @seealso vImageChannelDescription
*/
typedef struct vImage_YpCbCrPixelRange
{
        int32_t                    Yp_bias;
        int32_t                    CbCr_bias;
        int32_t                    YpRangeMax;
        int32_t                    CbCrRangeMax;
        int32_t                    YpMax;
        int32_t                    YpMin;
        int32_t                    CbCrMax;
        int32_t                    CbCrMin;
}vImage_YpCbCrPixelRange;

    
#ifdef __cplusplus
}
#endif


#endif /* vImage_TYPES_H */
// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Transform.h
/*!
*  @header Transform.h
*  vImage Framework
*
*  See vImage/vImage.h for more on how to better view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2003-2016 by Apple Inc. All rights reserved.
*
*  @discussion   Transform.h defines a number of interfaces that do linear and nonlinear operations
*                to images.  Matrix multiply operations treat each pixel as a short vector and
*                multiply the vector by a matrix. Typically these are used to do colorspace conversion
*                and color twisting.  There are a series of gamma functions that apply a power function
*                to an image. The power operation is C2 symmetric around the origin such that negative values
*                are not NaN (unlike pow(-x, y)), but instead -pow(|x|, y) for negative x).
*                In addition a series of polynomial and rational evaluators are available. Many complex
*                functions can be approximated as a polynomial or rational and evaluated more
*                cheaply that way. Finally there are single and multi-dimensional interpolated lookup
*                tables, also commonly used in colorspace conversion.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*/

#ifndef VIMAGE_TRANSFORM_H
#define VIMAGE_TRANSFORM_H

#include <vImage/vImage_Types.h>

#ifdef __cplusplus
extern "C" {
#endif


/*!
 *  @functiongroup Image Matrix multiplication
 */

/*
 * vImageMatrixMultiply_Planar16S
 *
 * Transform M source planes to N destination planes by multiplying the
 * M x N transformation matrix by the source planes.  A pre-bias may
 * optionally be added to the source planes before the transformation.  A
 * post-bias may optionally be added to the resulting destination planes. As
 * a final step the destination planes are divided by a given divisor.
 *
 * if (pre_bias)
 *   { bA, bR, bG, ... } = { A + pre_bias[0], R + pre_bias[1],
 *                           G + pre_bias[2], ... }
 * else
 *   { bA, bR, bG, ... } = { A, R, G, ... }
 *
 *
 *                                            { a00  a01  a02  ... }
 * { A', R', G', ...} = { bA, bR, bG, ... } * { a10  a11  a12  ... }
 *                                            { a20  a21  a22  ... }
 *                                            { ...  ...  ...  ... }
 *
 * if (post_bias)
 *    { A', R', G', ... } += { post_bias[0], post_bias[1], post_bias[2], ... }
 * else // correct value for normal rounding: divisor/2
 *    { A', R', G', ... } += { divisor/2, divisor/2, divisor/2, ... }
 *
 * { A', R', G', ... } /= divisor
 *
 * where the values are:
 * { A', R', ... }    Resulting destination planes (dests).  For a concrete
 *                    example of the multiply step for one plane:
 *                    A' = bA * a00 + bR * a10 + bG * a20 + ... * ...
 *
 * { A, R, ... }    Source planes (srcs).
 *
 * { bA, bR, ... }    Pre-biased source planes. (for demonstration only, never
 *                    actually exists)
 *
 * a00, a01, ...    Elements in the transformation matrix (matrix).
 *
 * pre_bias         Pre-bias values corresponding to the source channels.
 *                    Value is zero when NULL.
 *
 * post_bias         Post-bias values corresponding to the destination channels.
 *                    When NULL, the correct value for normal rounding if used,
 *                    which is divisor/2.
 *
 * divisor            Divisor to normalize the destination planes.
 *
 * Operands:
 * ---------
 * srcs                A pointer to an array of vImage_Buffer pointers that
 *                    reference the source planes. This array must contain
 *                    src_planes number of vImage_Buffer pointers.
 *
 * dests            A pointer to an array of vImage_Buffer pointers that
 *                    reference where to write the destination planes. Only
 *                    the image data pointed to by each vImage_Buffer is modified
 *                    (i.e. dests[0]->data), everything else remains unchanged.
 *                    This array must contain dest_planes number of vImage_Buffer
 *                    pointers.
 *
 * src_planes        The number of source planes.  Must be less than 256.
 *
 * dest_planes        The number of destination planes.  Must be less than 256.
 *
 * matrix            The row major transformation matrix with dest_planes number
 *                    of columns and src_planes number of rows.  Be aware that if
 *                    any column of this matrix sums to a value larger than
 *                    +-65538 this function may silently overflow.
 *
 * divisor            Division by this value occurs as the last step, in effect
 *                    normalizing the output planes.
 *
 * pre_bias            An optional array of length src_planes consisting of int16_t
 *                    values. Each value will be added to the corresponding
 *                    source plane in srcs.  Pass NULL for no pre_bias.
 *
 * post_bias        An optional array of length dest_planes consisting of
 *                    int16_t values.  Each value will be added to the
 *                    corresponding destination planes in dests.  The post_bias
 *                    is added before any clipping, rounding or division.  Pass
 *                    NULL the correct value for for normal rounding (divisor/2).
 *
 * flags            The following flags are allowed:
 *
 *        kvImageDoNotTile            Turns off internal multithreading. You may
 *                                     wish to do this if you have your own
 *                                    multithreading scheme to avoid having the
 *                                    two interfere with one another.
 *
 * Return Value:
 * -------------
 * kvImageInvalidKernelSize            Either src_planes or dest_planes is 0.
 * kvImageBufferSizeMismatch        All buffers in dests must have the same
 *                                    width and height.
 * kvImageROILargerThanSourceBuffer The destination buffer size (width, or
 *                                    height) is larger than the source buffer.
 * kvImageUnknownFlagsBit            Unexpected flag was passed.
 * kvImageNoError                    Success!
 *
 *
 * Comments:
 * ---------
 * Be aware that 32-bit signed accumulators are used in this operation with no
 * overflow protection. To avoid the possibility of overflow, limit the sum
 * of any column in the transformation matrix to values less than +-65536.
 *
 * The 32-bit accumulated results out of 16-bit range (+-32767) will be subject
 * clipping before writing to the destination buffer.
 *
 * This routine will work in place provided all of the following are true:
 * src.data == dest.data
 * src.rowBytes == dest.rowBytes
 * kvImageDoNotTile is passed
 *
 *  Some matrix based color transforms, such as that obtained using kColorSyncConversionMatrix are defined differently.
 *
 *      ColorSync:      p' = M1 * p                  M1 = colorsync matrix,  p = input pixel as column vector, p' = output pixel as column vector
 *         vImage:      p'T = pT * M2                M2 = vImage matrix,T indicates transpose -- vImage pixels are row vectors
 *
 *  Given that (A*B)T = BT*AT, it can be shown that M2 = M1T. So, to use the alternative definition here, you need to transpose the matrix.
 *
 * Example:
 * --------
 * To convert RGB to YUV, one might use the following formula:
 *
 *          Y = ( (  66 * R + 129 * G +  25 * B + 128) >> 8) +  16
 *          U = ( ( -38 * R -  74 * G + 112 * B + 128) >> 8) + 128
 *          V = ( ( 112 * R -  94 * G -  18 * B + 128) >> 8) + 128
 *
 *      This translates to a matrix that looks like this:
 *
 *               66     -38     112
 *              129     -74     -94
 *               25     112     -18
 *
 *        Pass 256 as the divisor to handle the >> 8 operation.  The post_bias
 *        argument can handle the addition of { 16, 128, 128 }, however because
 *        this function performs the division last, this array must be scaled by
 *        the divisor, yielding a post_bias of:
 *            post_bias = { 16*divisor + divisor/2, 128*divisor + divisor/2,
 *                                    128*divisor + divisor/2}
 *            post bias = { 4224, 32896, 32896 }
 */

VIMAGE_PF vImage_Error vImageMatrixMultiply_Planar16S( const vImage_Buffer *srcs[],
                                                      const vImage_Buffer *dests[],
                                                      uint32_t              src_planes,
                                                      uint32_t              dest_planes,
                                                      const int16_t          matrix[],
                                                      int32_t              divisor,
                                                      const int16_t         *pre_bias,
                                                      const int32_t         *post_bias,
                                                      vImage_Flags           flags )
VIMAGE_NON_NULL(1,2,5)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 * vImageMatrixMultiply_*
 *
 *    Multiply the M channels in the src buffers (A,R,G...) by a NxM matrix (a##) to yield N channels in the dest buffer(s)
 *    For ARGB interleaved functions, both M and N must be 4. The pre_bias (ca, cr, cg...) is added to the M input channels
 *    before the matrix is applied. The post_bias (ka, kr, kg...)  is added afterward.
 *
 *                                                             { a00  a10  a20  ... }
 *        { A', R', G' ...} = { A + ca, R + cr, G + cg, ... } *  { a01  a11  a21  ... } + { ka, kr, kg, ....}
 *                                                             { a02  a12  a22  ... }
 *                                                             { ...  ...  ...  ... }
 *
 *        A', R', G', ... = result channels stored into dests
 *        A, R, G, ... = input channels from srcs
 *        ca, cr, cg, ... = pre_bias elements corresponding to the input src channels, or zero if pre_bias is NULL
 *        a00, a11, a12, ... = elmenets from the matrix[]
 *        ka, kr, kg, ... = post_bias elements corresponding to the destination dest channels, or zero if post_bias is NULL
 *
 *  For integer code, there is an additional division operation that happens at the end, in effect normalizing integer matrices.
 *  The post-bias is added before any clipping, rounding or division. If you pass NULL for the post-bias, the correct value for
 *  normal rounding will be used:
 *
 *      integer:            divisor/2
 *      floating point:     0.0f
 *
 *  Be aware that 32 bit signed accumulators are used for integer code. If the sum over any matrix column is larger
 *  than +- 2**23, then overflow may occur. Generally speaking this will not happen because the matrix elements are
 *  16 bit integers, so one would need more than 256 source buffers before it is possible to encounter trouble.
 *
 *  As an example, to convert RGB to YUV, one might use the following formula:
 *
 *          Y = ( (  66 * R + 129 * G +  25 * B + 128) >> 8) +  16
 *          U = ( ( -38 * R -  74 * G + 112 * B + 128) >> 8) + 128
 *          V = ( ( 112 * R -  94 * G -  18 * B + 128) >> 8) + 128
 *
 *  This translates to a matrix that looks like this:
 *
 *               66     -38     112
 *              129     -74     -94
 *               25     112     -18
 *
 *  There is also the >>8 operation and the extra terms to be dealt with. For integer data, you would use a divisor of
 *  256 to account for the >>8 operation. The divisor is applied last after all other multiplications and additions.
 *  (For floating point, there is no divisor. Just divide the whole matrix by the divisor if one is needed.) Use the
 *  post bias to handle the +128 and {+16, +128, +128} terms. Since the second set happen after the divisor in the
 *  formula above, but our post_bias is applied before the divide, you'll need to multiply those biases by the divisor.
 *  This will give a post_bias of:
 *
 *              {  16 * 256 + 128, 128 * 256 + 128, 128 * 256 + 128 } = { 4224, 32896, 32896 }
 *
 *  Finally, if there is an alpha component, such that you wish to convert ARGB to AYUV, leaving the alpha component
 *  unchanged then add another row and column:
 *
 *      matrix =    divisor      0       0       0
 *                      0        66     -38     112
 *                      0       129     -74     -94
 *                      0        25     112     -18
 *
 *      post_bias =     { divisor/2, 4224, 32896, 32896 }
 *      divisor =       256
 *
 *  Integer results out of range of 0...255 will be subject to saturated clipping before writing to the destination buffer.
 *
 *
 *    Programming Note:
 *        The pre-bias is provided as a convenience. The pre-bias can be converted to a post bias by
 *    multiplying it by the matrix, which is what we do behind the scenes. Pass NULL for the pre-bias if
 *    you don't want a prebias added in.
 *
 *    These functions work in place as long as overlapping buffers overlap exactly. Buffers that partially overlap
 *    will yield undefined results. Destination buffers will be stored to in the order they appear in the dests array.
 *    The buffers all need to be the same size. They can have different rowBytes.
 *
 *  In cases where the number of src and dest buffers match, these functions will work in place.
 *  The source and destination buffers may all be different sizes and have different sized rowbytes. The source buffers must
 *  be at least as large as the dest buffers. In cases where the source buffer is larger than a dest buffer, the portion of the
 *  source buffer that overlaps the destination buffer when their top left corners are aligned will be used.
 *
 *  Some matrix based color transforms, such as that obtained using kColorSyncConversionMatrix are defined differently.
 *
 *      ColorSync:      p' = M1 * p                  M1 = colorsync matrix,  p = input pixel as column vector, p' = output pixel as column vector
 *         vImage:      p'T = pT * M2                M2 = vImage matrix, T indicates transpose -- vImage pixels are row vectors
 *
 *  Given that (A*B)T = BT*AT, we find that M2 = M1T. So, to use the alternatively defined matrix here, you need to transpose the matrix
 *  before passing it to vImage.
 */

VIMAGE_PF vImage_Error vImageMatrixMultiply_Planar8(          const vImage_Buffer *srcs[],    //A set of src_planes as a const array of pointers to vImage_Buffer structs that reference vImage_Buffers.
                                                    const vImage_Buffer *dests[],    //A set of src_planes as a const array of pointers to vImage_Buffer structs that reference vImage_Buffers.
                                                    uint32_t        src_planes,
                                                    uint32_t        dest_planes,
                                                    const int16_t        matrix[],
                                                    int32_t             divisor,
                                                    const int16_t    *pre_bias,    //A packed array of src_plane int16_t values. NULL is okay
                                                    const int32_t    *post_bias,    //A packed array of dest_plane int32_t values. NULL is okay
                                                    vImage_Flags     flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error vImageMatrixMultiply_PlanarF(          const vImage_Buffer *srcs[],        //A set of src_planes as a const array of pointers to vImage_Buffer structs that reference vImage_Buffers.
                                                    const vImage_Buffer *dests[],       //A set of src_planes as a const array of pointers to vImage_Buffer structs that reference vImage_Buffers.
                                                    uint32_t        src_planes,
                                                    uint32_t        dest_planes,
                                                    const float        matrix[],
                                                    const float     *pre_bias,    //A packed array of float values. NULL is okay
                                                    const float     *post_bias,    //A packed array of float values. NULL is okay
                                                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2,5) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*
 * vImageMatrixMultiply_ARGB8888 will also work for other channel orders such as RGBA8888.  The ordering of terms in the matrix, pre_bias and
 * post_bias should be adjusted to compensate.
 */
VIMAGE_PF vImage_Error vImageMatrixMultiply_ARGB8888(         const vImage_Buffer *src,
                                                     const vImage_Buffer *dest,
                                                     const int16_t    matrix[4*4],
                                                     int32_t             divisor,
                                                     const int16_t    *pre_bias,    //Must be an array of 4 int16_t's. NULL is okay.
                                                     const int32_t     *post_bias,    //Must be an array of 4 int32_t's. NULL is okay.
                                                     vImage_Flags     flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*
 * vImageMatrixMultiply_ARGBFFFF will also work for other channel orders such as RGBAFFFF.  The ordering of terms in the matrix, pre_bias and
 * post_bias should be adjusted to compensate.
 */
VIMAGE_PF vImage_Error vImageMatrixMultiply_ARGBFFFF(         const vImage_Buffer *src,
                                                     const vImage_Buffer *dest,
                                                     const float        matrix[4*4],
                                                     const float        *pre_bias,    //Must be an array of 4 floats. NULL is okay.
                                                     const float        *post_bias,    //Must be an array of 4 floats. NULL is okay.
                                                     vImage_Flags     flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageMatrixMultiply_ARGB8888ToPlanar8
 * @abstract apply a 1d matrix to a four channel, 8-bit per component image and get a 1-channel 8-bit image as a result
 * @discussion vImageMatrixMultiply_ARGB8888ToPlanar8 is like vImageMatrixMultiply_ARGB8888, except that it produces
 *             only a single channel of output. It is intended to produce grayscale images from four channel content,
 *             but can be used for other purposes.
 *  <pre>@textblock
 *                  for each pixel[y][x] in image:
 *                      int32_t p = (pixel[y][x][0] + pre_bias[0]) * matrix[0]  +
 *                                  (pixel[y][x][1] + pre_bias[1]) * matrix[1]  +
 *                                  (pixel[y][x][2] + pre_bias[2]) * matrix[2]  +
 *                                  (pixel[y][x][3] + pre_bias[3]) * matrix[3];
 *                      result[y][x] = CLAMP( ( p + post_bias ) / divisor, 0, 0xff);
 *  @/textblock </pre>
 *             If you intend to just extract a single channel without modification (e.g. alpha), please see
 *             vImageExtractChannel_ARGB8888.  This function will work in place, provided that src->data = dest->data
 *             and src->rowBytes = dest->rowBytes.
 *
 *  @param      src         A four channel, 8-bit per component input buffer. It does not have to be ARGB.
 *
 *  @param      dest        A preallocated buffer to receive the 8-bit per component monochromatic result.
 *
 *  @param      matrix      The 1D matrix by which to multiply each pixel.
 *
 *  @param      divisor     Used to renormalize the image after scaling by the matrix. Typically this is the
 *                          sum over the matrix. If 0, 1 will be used. A faster implementation may be available
 *                          if the divisor is an integer power of 2.
 *
 *  @param      pre_bias    A set of values used to correct the input image so that 0 is encoded as 0.  For example,
 *                          if the input image is 444 AYCbCr video range, then {0, -16, -128, -128} could be used here.
 *                          If NULL, {0,0,0,0} will be used.
 *
 *  @param      post_bias   A value added to the sum at the end to provide both for rounding control and for
 *                          allowing for a bias to be encoded into the image format.  Typically, this is just
 *                          divisor/2 to allow for round to nearest behavior. However, other values may be appropriate
 *                          if the encoding for 0.0 is not 0.  For example, for video range luminance, you might
 *                          pass 16 * divisor + divisor/2.
 *
 *  @param      flags       The following flags are allowed:
 *      <pre> @textblock
 *          kvImageNoFlags                      Default operation
 *          kvImageDoNotTile                    Disable internal multithreading.
 *          kvImageGetTempBufferSize            return 0, do no work
 *          kvImagePrintDiagnosticsToConsole    Might print more helpful diagnostic info to the console in the event of an
 *                                              error.
 *      @/textblock</pre>
 *
 *  @return kvImageNoError                      Success.
 *  @return kvImageRoiLargerThanInputBuffer     dest->width and height must be less than or equal to the corresponding dimension of src.
 *  @return kvImageUnknownFlagsBit              A flag not from the above list of flags was passed in.
 *  @return If kvImageGetTempBufferSize was passed, 0 is returned and no work is done on the image.
 */
VIMAGE_PF vImage_Error vImageMatrixMultiply_ARGB8888ToPlanar8( const vImage_Buffer *src,
                                                              const vImage_Buffer *dest,
                                                              const int16_t       matrix[4],
                                                              int32_t             divisor,
                                                              const int16_t       pre_bias[4],
                                                              int32_t             post_bias,
                                                              vImage_Flags        flags )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));


/*!
 * @function vImageMatrixMultiply_ARGBFFFFToPlanarF
 * @abstract apply a 1d matrix to a four channel, float per component image and get a 1-channel float image as a result
 * @discussion vImageMatrixMultiply_ARGBFFFFToPlanarF is like vImageMatrixMultiply_ARGBFFFF, except that it produces
 *             only a single channel of output. It is intended to produce grayscale images from four channel content,
 *             but can be used for other purposes.
 *  <pre>@textblock
 for each pixel[y][x] in image:
 float p =   (pixel[y][x][0] + pre_bias[0]) * matrix[0]  +
 (pixel[y][x][1] + pre_bias[1]) * matrix[1]  +
 (pixel[y][x][2] + pre_bias[2]) * matrix[2]  +
 (pixel[y][x][3] + pre_bias[3]) * matrix[3];
 result[y][x] = p + post_bias;
 *  @/textblock </pre>
 *             vImage reserves the right to reorder computation from the above formulation to improve performance.
 *             If you intend to just extract a single channel without modification (e.g. alpha), please see
 *             vImageExtractChannel_ARGBFFFF. This function will work in place, provided that src->data = dest->data
 *             and src->rowBytes = dest->rowBytes.
 *
 *  @param      src         A four channel, floating-point input buffer. It does not have to be ARGB.
 *
 *  @param      dest        A preallocated buffer to receive the floating-point monochromatic result.
 *
 *  @param      matrix      The 1D matrix by which to multiply each pixel.
 *
 *  @param      pre_bias    A set of values used to correct the input image so that 0 is encoded as 0.
 *                          If NULL, {0,0,0,0} will be used.
 *
 *  @param      post_bias   A value added to the sum at the end to provide both for rounding control and for
 *                          allowing for a bias to be encoded into the image format.  Typically, this is just
 *                          zero.
 *
 *  @param      flags       The following flags are allowed:
 *      @textblock
 *          kvImageNoFlags                      Default operation
 *          kvImageDoNotTile                    Disable internal multithreading.
 *          kvImageGetTempBufferSize            return 0, do no work
 *          kvImagePrintDiagnosticsToConsole    Might print more helpful diagnostic info to the console in the event of an
 *                                              error.
 *      @/textblock
 *
 *  @return kvImageNoError                      Success.
 *  @return kvImageRoiLargerThanInputBuffer     dest->width and height must be less than or equal to the corresponding dimension of src.
 *  @return kvImageUnknownFlagsBit              A flag not from the above list of flags was passed in.
 *  @return If kvImageGetTempBufferSize was passed, 0 is returned and no work is done on the image.
 */
VIMAGE_PF vImage_Error vImageMatrixMultiply_ARGBFFFFToPlanarF( const vImage_Buffer *src,
                                                              const vImage_Buffer *dest,
                                                              const float         matrix[4],
                                                              const float         pre_bias[4],
                                                              float               post_bias,
                                                              vImage_Flags        flags )
VIMAGE_NON_NULL(1,2,3)
API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));


/*
 * The gamma calculation is at the simplest level:
 *
 *      if( value < 0)
 *          sign = -1.0f;
 *      else
 *          sign = 1.0f;
 *
 *      result = pow( fabs( value ), gamma ) * sign;
 *
 *  i.e. negative values are treated as if they are positive, and the sign restored at the end.
 *
 * This provides for symmetric gamma curves about 0, and also solves the problem of NaN results from
 * pow( negative number, non-integer).
 *
 * The results are available in two varieties, full and half precision. The full precision version covers
 * all pixel values and all exponents and delivers a result within a few ULPs of the IEEE-754 correct powf().
 *
 * The half precision variant provides a precision of 1/4096:
 *
 *          fabs((correct result - result provided)) < 1/4096
 *
 * Half-precision is intended to be used with data that will ultimately be converted to 8-bit integer data.
 * As such, the faster half precision variants only work for floating point pixel values in the range 0.0 ... 1.0.
 * Out of range pixel values will clamp appropriately to 0.0 or 1.0 before the calculation is performed.
 *
 * In addition, there are restrictions on range on the exponent. In general, for best performance, it should be near 1.0,
 * though a faster path exists out to +-12. If the exponent is outside the prescribed range the code will return a full
 * precision gamma instead. (In MacOS X.4.3 and earlier, all exponents fall back on the full precision version. )
 *
 * Half precision calculations that conform to these restrictions are likely to be much faster
 * than the full precision gamma. Use kvImageGamma_UseGammaValue and kvImageGamma_UseGammaValue_half_precision
 * to control whether you get a full or half precision result. kvImageGamma_UseGammaValue will never use a
 * half-precision calculation.
 *
 * There are some additional gamma types for specific gamma curves. These are precise to 1/4096, share the same
 * restrictions as above for the half precision gamma, but execute in less time. (These fixed exponent gamma curves
 * are present in MacOS X.4, unless otherwise noted below.)
 *
 * vImageGamma_PlanarF and vImageGamma_PlanarFToPlanar8 will work in place, provided that the following are true:
 *      src->data == dest->data
 *      src->rowBytes >= dest->rowBytes
 *      if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 *
 * vImageGamma_Planar8ToPlanarF will NOT work in place.
 *
 *  For the func
 *
 * These functions will also work for multichannel data, such as RGBAFFFF buffers by adjusting the width of the buffer to
 * reflect the additional channels. Note that this will cause the alpha channel if there is one to become gamma corrected.
 */

/*
 * constants for use in the gamma_type
 */
enum
{
    kvImageGamma_UseGammaValue                 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   0,          /* __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_UseGammaValue_half_precision  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   1,          /* __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_5_over_9_half_precision       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   2,          /* gamma = 5/9. (Gamma 1/1.8) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_9_over_5_half_precision       VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   3,          /* gamma = 9/5. (Gamma 1.8)  __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_5_over_11_half_precision      VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   4,          /* gamma = 5/11. (Gamma 1/2.2) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_11_over_5_half_precision      VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   5,          /* gamma = 11/5. (Gamma 2.2) On exit, gamma = 5/11. __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_sRGB_forward_half_precision   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   6,          /* gamma = sRGB standard 2.2. (like 2.2 but offset a bit and with a linear segment: x<0.03928?x/12.92:pow((x+0.055)/1.055,2.4) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_sRGB_reverse_half_precision   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   7,          /* gamma = sRGB standard 1/2.2. (like 2.2 but offset a bit and with a linear segment: x<0.00304?12.92*x:1.055pow(x,1/2.4)-0.055) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_11_over_9_half_precision      VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   8,          /* gamma = 11/9 (Gamma (11/5)/(9/5)) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_9_over_11_half_precision      VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =   9,          /* gamma = 9/11 (Gamma (9/5)/(11/5)) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_BT709_forward_half_precision  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =     10,            /* gamma = ITU-R BT.709 standard (like sRGB above but without the 1.125 viewing gamma for computer graphics: x<0.081?x/4.5:pow((x+0.099)/1.099, 1/0.45) ) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
    kvImageGamma_BT709_reverse_half_precision  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) =  11            /* gamma = ITU-R BT.709 standard *reverse* (like sRGB 1/2.2 above but without the 1.125 viewing gamma for computer graphics: x<0.018?4.5*x:1.099*pow(x,0.45)-0.099) __OSX_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) */
};

VIMAGE_PF GammaFunction   vImageCreateGammaFunction(          float           gamma,
                                                    int             gamma_type,
                                                    vImage_Flags    flags )             API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF void            vImageDestroyGammaFunction( GammaFunction f )                           API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/* There is a 8 bit lookup table in Conversion.h, if you are looking for a 8bit to 8bit gamma function. */
VIMAGE_PF vImage_Error    vImageGamma_Planar8toPlanarF(       const vImage_Buffer *src,
                                                       const vImage_Buffer *dest,
                                                       const GammaFunction gamma,
                                                       vImage_Flags        flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error    vImageGamma_PlanarFtoPlanar8(       const vImage_Buffer *src,
                                                       const vImage_Buffer *dest,
                                                       const GammaFunction gamma,
                                                       vImage_Flags        flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error    vImageGamma_PlanarF(                const vImage_Buffer *src,
                                              const vImage_Buffer *dest,
                                              const GammaFunction gamma,
                                              vImage_Flags        flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/* vImagePiecewiseGamma_Planar8
 * vImagePiecewiseGamma_Planar8toPlanar16Q12
 * vImagePiecewiseGamma_Planar8toPlanarF
 * vImagePiecewiseGamma_Planar16Q12
 * vImagePiecewiseGamma_Planar16Q12toPlanar8
 * vImagePiecewiseGamma_PlanarF
 * vImagePiecewiseGamma_PlanarFtoPlanar8
 *
 * The piecewise gamma calculation combines a linear and an exponential (gamma)
 * curve on two regions of the input interval, separated by a user-supplied
 * boundary value.  When the input is greater or equal to the boundary value,
 * the gamma curve is used to generate the output. Otherwise, the linear curve
 * is used.
 *
 * The operation can be described as follows:
 *
 *  For each source pixel value x:
 *    if x < cutoff:
 *      r = linearCoeffs[0]*x + linearCoeffs[1]
 *    else:
 *      t = exponentialCoeffs[0]*x + exponentialCoeffs[1]
 *      r = pow(t, gamma) + exponentialCoeffs[2]
 *    output pixel value = r
 *
 * If the source format is Planar8, we multiply by 1/255.0 to get the "input"
 * value used in the expression above; if the destination format is Planar8,
 * we clamp the output value to [0, 1.0], multiply by 255.0 and round to
 * nearest to get the value stored to the destination buffer.
 *
 * If the source format is Planar16Q12, we multiply by 1/4096.0 to get the
 * input value; we clamp to [-8, 8), multiply by 4096.0, and round to nearest
 * to get the stored to the destination buffer.
 *
 * Regardless of the input or output pixel type, the parameters describing the
 * piecewise gamma function are 32-bit floats, with the single exception of the
 * boundary parameter.
 *
 *  Operands:
 *  ---------
 *      src                 A pointer to a vImage_Buffer that references the source pixels
 *
 *      dest                A pointer to a vImage_Buffer that references the destination pixels
 *
 *      exponentialCoeffs   An array of three floating point coefficients for the gamma curve
 *
 *      gamma               The exponent of a power function for calculating gamma correction
 *
 *      linearCoeffs        An array of two floating point coefficients for the linear curve
 *
 *      boundary            The boundary value for switching from linear to gamma curve
 *
 *      flags               The following flags are allowed:
 *
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    Does no work and returns zero.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageNullPointerArgument      src, dest, exponentialCoeffs, or linearCoeffs pointer is NULL.
 *      kvImageBufferSizeMismatch       The destination buffer size (width or
 *                                      height) is larger than the source buffer.
 *      kvImageUnknownFlagsBit          An unknown or invalid flag was passed. See flags above.
 *
 *  The variants of this routine that have source and dest pixel types of the
 *  same size (Planar8, Planar16Q12, and PlanarF) operate in place so long as
 *  the source and dest image scanlines overlap exactly.  The other variants
 *  (Planar8toPlanar16Q12, Planar8toPlanarF, etc) do not support in-place
 *  operation.
 */

VIMAGE_PF vImage_Error vImagePiecewiseGamma_Planar8(const vImage_Buffer *src,
                                                    const vImage_Buffer *dest,
                                                    const float         exponentialCoeffs[3],
                                                    const float         gamma,
                                                    const float         linearCoeffs[2],
                                                    const Pixel_8       boundary,
                                                    vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_Planar8toPlanar16Q12(const vImage_Buffer *src,
                                                                 const vImage_Buffer *dest,
                                                                 const float         exponentialCoeffs[3],
                                                                 const float         gamma,
                                                                 const float         linearCoeffs[2],
                                                                 const Pixel_8       boundary,
                                                                 vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_Planar16Q12(const vImage_Buffer *src,
                                                        const vImage_Buffer *dest,
                                                        const float         exponentialCoeffs[3],
                                                        const float         gamma,
                                                        const float         linearCoeffs[2],
                                                        const Pixel_16S     boundary,
                                                        vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_Planar16Q12toPlanar8(const vImage_Buffer *src,
                                                                 const vImage_Buffer *dest,
                                                                 const float         exponentialCoeffs[3],
                                                                 const float         gamma,
                                                                 const float         linearCoeffs[2],
                                                                 const Pixel_16S     boundary,
                                                                 vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_Planar8toPlanarF(const vImage_Buffer *src,
                                                             const vImage_Buffer *dest,
                                                             const float         exponentialCoeffs[3],
                                                             const float         gamma,
                                                             const float         linearCoeffs[2],
                                                             const Pixel_8       boundary,
                                                             vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_PlanarF(const vImage_Buffer *src,
                                                    const vImage_Buffer *dest,
                                                    const float         exponentialCoeffs[3],
                                                    const float         gamma,
                                                    const float         linearCoeffs[2],
                                                    const float         boundary,
                                                    vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

VIMAGE_PF vImage_Error vImagePiecewiseGamma_PlanarFtoPlanar8(const vImage_Buffer *src,
                                                             const vImage_Buffer *dest,
                                                             const float         exponentialCoeffs[3],
                                                             const float         gamma,
                                                             const float         linearCoeffs[2],
                                                             const float         boundary,
                                                             vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/* vImageSymmetricPiecewiseGamma_Planar16Q12
 * vImageSymmetricPiecewiseGamma_PlanarF
 *
 * The symmetric piecewise gamma calculation combines a linear and an exponential
 * (gamma) curve on two regions of the input interval, separated by a user-supplied
 * boundary value. When the input magnitude is greater or equal to the boundary
 * value, the gamma curve is used to generate the output. Otherwise, the linear
 * curve is used. The result is clamped to [0, FMT_MAX] and assigned the sign
 * of the input value. This creates a curve which is symmetric about the origin,
 * with discontinuities flattened to the x-axis.
 *
 * The operation can be described as follows:
 *
 *  For each source pixel value x:
 *    y = fabsf(x)
 *    if y < cutoff:
 *      t = linearCoeffs[0]*y + linearCoeffs[1]
 *    else:
 *      s = exponentialCoeffs[0]*y + exponentialCoeffs[1]
 *      t = pow(t, gamma) + exponentialCoeffs[2]
 *    r = MAX(t, 0) * copysignf(1.0f, x)
 *    output pixel value = r
 *
 * If the source format is Planar16Q12, we multiply by 1/4096.0 to get the
 * input value; we clamp to [-8, 8), multiply by 4096.0, and round to nearest
 * to get the stored to the destination buffer.
 *
 * Regardless of the input or output pixel type, the parameters describing the
 * piecewise gamma function are 32-bit floats, with the single exception of the
 * boundary parameter.
 *
 *  Operands:
 *  ---------
 *      src                 A pointer to a vImage_Buffer that references the source pixels
 *
 *      dest                A pointer to a vImage_Buffer that references the destination pixels
 *
 *      exponentialCoeffs   An array of three floating point coefficients for the gamma curve
 *
 *      gamma               The exponent of a power function for calculating gamma correction
 *
 *      linearCoeffs        An array of two floating point coefficients for the linear curve
 *
 *      boundary            The boundary value for switching from linear to gamma curve
 *
 *      flags               The following flags are allowed:
 *
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    Does no work and returns zero.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageNullPointerArgument      src, dest, exponentialCoeffs, or linearCoeffs pointer is NULL.
 *      kvImageBufferSizeMismatch       The destination buffer size (width or
 *                                      height) is larger than the source buffer.
 *      kvImageUnknownFlagsBit          An unknown or invalid flag was passed. See flags above.
 *
 *  All variants of this routine operate in place so long as the source and
 *  dest image scanlines overlap exactly.
 */
VIMAGE_PF vImage_Error vImageSymmetricPiecewiseGamma_Planar16Q12(const vImage_Buffer *src,
                                                                 const vImage_Buffer *dest,
                                                                 const float         exponentialCoeffs[3],
                                                                 const float         gamma,
                                                                 const float         linearCoeffs[2],
                                                                 const Pixel_16S     boundary,
                                                                 vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));
VIMAGE_PF vImage_Error vImageSymmetricPiecewiseGamma_PlanarF(const vImage_Buffer *src,
                                                             const vImage_Buffer *dest,
                                                             const float         exponentialCoeffs[3],
                                                             const float         gamma,
                                                             const float         linearCoeffs[2],
                                                             const float         boundary,
                                                             vImage_Flags        flags) VIMAGE_NON_NULL(1,2,3,5)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*
 *  vImagePiecewisePolynomial*
 *
 *  Apply one or more polynomials to the input image to give the output image.
 *
 *  vImagePiecewisePolynomial_Planar8toPlanarF uses 8 bit data on input and produces floating point data.
 *  vImagePiecewisePolynomial_PlanarFtoPlanar8 uses floating point data on input and produces 8-bit data with saturated clamping at 0 and 255 to prevent modulo overflow.
 *  vImagePiecewisePolynomial_PlanarF uses floating point data on both input and output.
 *  vImageTableLookUp_Planar8 uses 8 bit data and produces 8 bit data. In certain cases, the matrix multiply function may also be appropriate.
 *
 *  No other arithmetic is done. If you wish a /255 or *255 operation to be done as part of the calculation, you must incorporate that into your polynomial.
 *
 *  The arrangement of the polynomials is defined as follows:
 *      Let there be N polynomials that each cover part of the single precision floating point range that are arranged in order of area of influence from -Infinity to Infinity.
 *      The ith polynomial shall operate on the set of input pixel values that fall in the range:
 *
 *          boundary[i] <= pixel_value < boundary[i+1].
 *
 *     for which:
 *
 *          boundary[0] = smallest value fit by the polynomial. Input pixels smaller than this will be clamped to this value before the calculation is done. Use -Inf for no lower limit.
 *          boundary[N] = largest value fit by the polynomial. Input pixels larger than this will be clamped to this value before the calculation is done. Use +Inf for no upper limit.
 *          boundary[1....N-1] = the boundaries separating the input ranges covered by the various polynomials provided (see below)
 *
 *     NaNs will return NaNs. The last polynomial also operates on Inf.  N must be an integer power of 2.
 *     Values found in the destination array are undefined until after the function returns.
 *       The behavior is undefined if boundaries are NaN.
 *
 *  These functions will also work for multichannel data, such as RGBAFFFF buffers by adjusting the width of the buffer to
 *  reflect the additional channels. Note that this will cause the alpha channel, if there is one, to become modified like the other channels.
 *  These will work in place, provided that the following are true:
 *      src->data == dest->data
 *      src->rowBytes >= dest->rowBytes
 *      if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 
 *
 *  The input parameters are as follows:
 *
 *      src = a pointer to a vImage_Buffer containing the input data for the function
 *      dest = a pointer to a vImage_Buffer structure that describes where to write the results
 *
 *      coefficients = a packed array of pointers to packed arrays of (order+1) polynomial coefficients ( i.e. coefficients[ N ][ order + 1 ] ).
 *                      The polynomials must appear in order from least to greatest sorted by area of influence.
 *                      The polynomials must all be of the same order.
 *                      The polynomial coefficients are sorted from 0th order term to highest order term
 *
 *      boundaries = a packed array of (N+1) floating point values that mark the dividing line between one polynomial and the next. These must be sorted from most negative to most positive.
 *                        Input pixel values less than boundaries[0] will be clamped to be equal to boundaries[0] before the calculation is done
 *                        Input pixel values greater than boundaries[N] will be clamped to be equal to boundaries[N] before the calculation is done
 *
 *      order = the number of coefficients minus one used for each polynomial -- all the polynomials must be of the same order
 *                  A polynomial with _two_ coefficients (y = c0 + c1 * x) is a _first_ order polynomial. Pass 1 for a first order polynomial. Pass 2 for a second order polynomial, etc.
 *
 *      log2segments = log2(N)
 *
 *      flags = no flags are currently honored. You must pass zero here.
 *
 *      vImagePiecewisePolynomial_PlanarF will work in place.
 *      vImagePiecewisePolynomial_Planar8toPlanarF will work in place.
 *      vImagePiecewisePolynomial_PlanarFtoPlanar8 will NOT work in place.
 *
 *  Performance advisory:
 *      It costs much more to resolve additional polynomials than to work with higher order polynomials.
 *      For performance, you are typically better off with one 9th order polynomial that spans the range you are
 *      interested in than many first order polynomials that cover the area in a piecewise fashion.
 *      Vector code execution time is roughly proportional to:
 *
 *              time = (base cost to touch all the data) + polynomial order + 4 * log2segments
 *
 *      The vector code for an unsplit 13th order polynomial should be about as fast as vImageLookupTable_Planar8toPlanarF()
 *          on a G4.
 *
 *      With data not in cache, the time may be significantly different. For sufficiently small polynomials, the
 *      cost may be a fixed cost, dependent only on how much data is touched, and not on polynomial order.
 *
 *        This performance behavior is provided to help developers evaluate speed tradeoffs. It is not a guarantee.
 *        It is subject to change in future operating system revisions, and may be different on different hardware
 *        within the same or different operating system revisions.
 *
 *      Vector code is not invoked for log2segments > 3.
 *
 *  Accuracy advisory:
 *      Single precision floating point arithmetic is used. While some polynomials may fit a desired curve
 *      within prescribed error limits when using infinite precision math, limited floating point precision
 *      may in practice cause significant error to accumulate for some sets of polynomial coefficients. It is
 *      recommended that you test all reasonable floating point pixel values to make sure that they do indeed
 *      give results that conform to prescribed error limits.
 *
 *
 *  Usage Example:
 *  --------------
 *      Lets say you want to mimic the sRGB gamma curve using vImagePiecewisePolynomial_PlanarF. The sRGB gamma curve is defined as follows:
 *
 *              if( {R,G,B} < 0.00304 )
 *                  result = 12.92 * {R,G,B}
 *              else
 *                  result = -0.055 + 1.055 * Pow( {R,G,B}, 2.4 )
 *
 *      Because the power function isn't a polynomial and we need a polynomial, we will approximate it with a second order polynomial:
 *
 *              if( {R,G,B} < 0.00304 )
 *                  result = 12.92 * {R,G,B}
 *              else
 *                  result = c0 + c1 * {R,G,B} + c2 * {R,G,B}^2
 *
 *      (Finding the best values for c0, c1, c2 to approximate -0.055 + 1.055 * Pow( {R,G,B}, 2.4 ) over the range [0.00304, 1.0] is
 *          left as an exercise for the reader.)
 *
 *      We have two polynomials -- one for the region below 0.00304 and one for the region above, so N = 2.
 *      The highest order polynomial is a second order polynomial, so order = 2.
 *
 *              const int N = 2;        // two polynomials
 *              const int order = 2;    // the polynomials are second order (have three terms, including zero terms)
 *
 *      The two polynomials are:
 *
 *              float linearPart[ order + 1 ] = { 0, 12.92, 0 };    // result = 0 + 12.92 * {R,G,B} + 0 * {R,G,B} * {R,G,B}
 *              float nonLinearPart[ order + 1 ] = { c0, c1, c2 };  // result = c0 + c1 * {R,G,B} + c2 * {R,G,B} * {R,G,B}
 *
 *      Here we assemble the rest of the information:
 *
 *              float *coefficients[ N ] = { linearPart, nonLinearPart };   // sorted in order of area of influence from least to greatest. ( x < 0.00304, x >= 0.00304 )
 *              float boundaries[ N+1 ] = { 0.0f, 0.00304, 1.0f };      // sorted in order from least to greatest. 0.0f and 1.0f define the range over which the polynomials are valid. 0.00304 is the single value separating the two polynomials
 *              int log2segments = 1;   // log2(N)
 *              int flags = 0;          // no flags
 *
 *              VIMAGE_PF vImage_Error error = vImagePiecewisePolynomial_PlanarF( &mySourceBuffer, &myDestinationBuffer, coefficients, boundaries, order, log2segments, flags );
 */
VIMAGE_PF vImage_Error    vImagePiecewisePolynomial_PlanarF(  const vImage_Buffer *src,       //floating point data
                                                            const vImage_Buffer *dest,      //floating point data
                                                            const float         **coefficients,
                                                            const float         *boundaries,
                                                            uint32_t            order,
                                                            uint32_t            log2segments,
                                                            vImage_Flags        flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));



VIMAGE_PF vImage_Error    vImagePiecewisePolynomial_Planar8toPlanarF( const vImage_Buffer *src,        //8-bit data
                                                                     const vImage_Buffer *dest,       //floating point data
                                                                     const float         **coefficients,
                                                                     const float         *boundaries,
                                                                     uint32_t            order,
                                                                     uint32_t            log2segments,
                                                                     vImage_Flags        flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error    vImagePiecewisePolynomial_PlanarFtoPlanar8( const vImage_Buffer *src,       //floating point data
                                                                     const vImage_Buffer *dest,      //8-bit data
                                                                     const float         **coefficients,
                                                                     const float         *boundaries,    /*The 0th and Nth terms in the boundaries array are typically 0.0f and 255.0f respectively. Other values may incur additional computational cost. */
                                                                     uint32_t            order,
                                                                     uint32_t            log2segments,
                                                                     vImage_Flags        flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*
 *  vImageSymmetricPiecewisePolynomial*
 *
 *  Apply one or more polynomials to the input image to give the output image.  The polynomial p(x) is provided as a
 *  series of coefficients.  e.g.:
 *
 *      p(x,i) = coefficient[i][0] + (coefficient[i][1] + (coefficient[i][2] + coefficient[i][3]*x)*x)*x;    (assuming a 3rd order polynomial)
 *
 *  This is just like vImagePiecewisePolynomial_<fmt>.  However, when it is applied, a modified polynomial p'(x) is actually used:
 *
 *      p'(x) = p(fabsf(x)) * copysignf( 1.0f, x)
 *
 *  This makes the polynomial C2 symmetric about the origin.  That is, the negative domain looks like the positive domain, rotated 180 degrees about the origin.
 *
 *  vImageSymmetricPiecewisePolynomial_PlanarF floating point data on both input and output.
 *
 *  The arrangement of the polynomials is defined as follows:
 *      Let there be N polynomials that each cover part of the single precision floating point range that are arranged in order of area of influence from -Infinity to Infinity.
 *      The ith polynomial shall operate on the set of input pixel values that fall in the range:
 *
 *          boundary[i] <= pixel_value < boundary[i+1].
 *
 *     for which:
 *
 *          boundary[0] = smallest value fit by the polynomial. Input pixels smaller than this will be clamped to this value before the calculation is done. Use -Inf for no lower limit.
 *          boundary[N] = largest value fit by the polynomial. Input pixels larger than this will be clamped to this value before the calculation is done. Use +Inf for no upper limit.
 *          boundary[1....N-1] = the boundaries separating the input ranges covered by the various polynomials provided (see below)
 *
 *     NaNs will return NaNs. The last polynomial also operates on Inf.  N must be an integer power of 2.
 *     Values found in the destination array are undefined until after the function returns.
 *       The behavior is undefined if boundaries are NaN.
 *
 *  These functions will also work for multichannel data, such as RGBAFFFF buffers by adjusting the width of the buffer to
 *  reflect the additional channels. Note that this will cause the alpha channel, if there is one, to become modified like the other channels.
 *  These will work in place, provided that the following are true:
 *      src->data == dest->data
 *      src->rowBytes >= dest->rowBytes
 *      if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 
 *
 *  The input parameters are as follows:
 *
 *      src = a pointer to a vImage_Buffer containing the input data for the function
 *      dest = a pointer to a vImage_Buffer structure that describes where to write the results
 *
 *      coefficients = a packed array of pointers to packed arrays of (order+1) polynomial coefficients ( i.e. coefficients[ N ][ order + 1 ] ).
 *                      The polynomials must appear in order from least to greatest sorted by area of influence.
 *                      The polynomials must all be of the same order.
 *                      The polynomial coefficients are sorted from 0th order term to highest order term
 *
 *      boundaries = a packed array of (N+1) floating point values that mark the dividing line between one polynomial and the next. These must be sorted from most negative to most positive.
 *                        Input pixel values less than boundaries[0] will be clamped to be equal to boundaries[0] before the calculation is done
 *                        Input pixel values greater than boundaries[N] will be clamped to be equal to boundaries[N] before the calculation is done
 *
 *      order = the number of coefficients minus one used for each polynomial -- all the polynomials must be of the same order
 *                  A polynomial with _two_ coefficients (y = c0 + c1 * x) is a _first_ order polynomial. Pass 1 for a first order polynomial. Pass 2 for a second order polynomial, etc.
 *
 *      log2segments = log2(N)
 *
 *      flags = no flags are currently honored. You must pass zero here.
 *
 *      vImagePiecewisePolynomial_PlanarF will work in place.
 *      vImagePiecewisePolynomial_Planar8toPlanarF will work in place.
 *      vImagePiecewisePolynomial_PlanarFtoPlanar8 will NOT work in place.
 *
 *  Performance advisory:
 *      It costs much more to resolve additional polynomials than to work with higher order polynomials.
 *      For performance, you are typically better off with one 9th order polynomial that spans the range you are
 *      interested in than many first order polynomials that cover the area in a piecewise fashion.
 *      Vector code execution time is roughly proportional to:
 *
 *              time = (base cost to touch all the data) + polynomial order + 4 * log2segments
 *
 *      The vector code for an unsplit 13th order polynomial should be about as fast as vImageLookupTable_Planar8toPlanarF()
 *          on a G4.
 *
 *      With data not in cache, the time may be significantly different. For sufficiently small polynomials, the
 *      cost may be a fixed cost, dependent only on how much data is touched, and not on polynomial order.
 *
 *        This performance behavior is provided to help developers evaluate speed tradeoffs. It is not a guarantee.
 *        It is subject to change in future operating system revisions, and may be different on different hardware
 *        within the same or different operating system revisions.
 *
 *      Vector code is not invoked for log2segments > 3.
 *
 *  Accuracy advisory:
 *      Single precision floating point arithmetic is used. While some polynomials may fit a desired curve
 *      within prescribed error limits when using infinite precision math, limited floating point precision
 *      may in practice cause significant error to accumulate for some sets of polynomial coefficients. It is
 *      recommended that you test all reasonable floating point pixel values to make sure that they do indeed
 *      give results that conform to prescribed error limits.
 *
 *
 *  Usage Example:
 *  --------------
 *      Lets say you want to mimic the sRGB gamma curve using vImagePiecewisePolynomial_PlanarF. The sRGB gamma curve is defined as follows:
 *
 *              if( {R,G,B} < 0.00304 )
 *                  result = 12.92 * {R,G,B}
 *              else
 *                  result = -0.055 + 1.055 * Pow( {R,G,B}, 2.4 )
 *
 *      Because the power function isn't a polynomial and we need a polynomial, we will approximate it with a second order polynomial:
 *
 *              if( {R,G,B} < 0.00304 )
 *                  result = 12.92 * {R,G,B}
 *              else
 *                  result = c0 + c1 * {R,G,B} + c2 * {R,G,B}^2
 *
 *      (Finding the best values for c0, c1, c2 to approximate -0.055 + 1.055 * Pow( {R,G,B}, 2.4 ) over the range [0.00304, 1.0] is
 *          left as an exercise for the reader.)
 *
 *      We have two polynomials -- one for the region below 0.00304 and one for the region above, so N = 2.
 *      The highest order polynomial is a second order polynomial, so order = 2.
 *
 *              const int N = 2;        // two polynomials
 *              const int order = 2;    // the polynomials are second order (have three terms, including zero terms)
 *
 *      The two polynomials are:
 *
 *              float linearPart[ order + 1 ] = { 0, 12.92, 0 };    // result = 0 + 12.92 * {R,G,B} + 0 * {R,G,B} * {R,G,B}
 *              float nonLinearPart[ order + 1 ] = { c0, c1, c2 };  // result = c0 + c1 * {R,G,B} + c2 * {R,G,B} * {R,G,B}
 *
 *      Here we assemble the rest of the information:
 *
 *              float *coefficients[ N ] = { linearPart, nonLinearPart };   // sorted in order of area of influence from least to greatest. ( x < 0.00304, x >= 0.00304 )
 *              float boundaries[ N+1 ] = { 0.0f, 0.00304, 1.0f };      // sorted in order from least to greatest. 0.0f and 1.0f define the range over which the polynomials are valid. 0.00304 is the single value separating the two polynomials
 *              int log2segments = 1;   // log2(N)
 *              int flags = 0;          // no flags
 *
 *              VIMAGE_PF vImage_Error error = vImagePiecewisePolynomial_PlanarF( &mySourceBuffer, &myDestinationBuffer, coefficients, boundaries, order, log2segments, flags );
 */
VIMAGE_PF vImage_Error    vImageSymmetricPiecewisePolynomial_PlanarF( const vImage_Buffer *src,   //floating point data
                                                                     const vImage_Buffer *dest,  //floating point data
                                                                     const float         **coefficients,
                                                                     const float         *boundaries,
                                                                     uint32_t            order,
                                                                     uint32_t            log2segments,
                                                                     vImage_Flags        flags )
VIMAGE_NON_NULL(1,2,3,4)
API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));


/*
 *      vImagePiecewiseRational_PlanarF is similar to vImagePiecewisePolynomial_PlanarF
 *      Except that it evaluates a piecewise rational expression in the form of:
 *
 *                      c0 + c1*x + c2*x^2 + c3*x^3...
 *          result = -----------------------------------
 *                      d0 + d1*x + d2*x^2 + d3*x^3...
 *
 *      The function is behaves nearly exactly like vImagePiecewisePolynomial_PlanarF, except that
 *      there are now two polynomials, for top and bottom of the divide shown above. Each
 *      polynomial has its own set of coefficients and its own polynomial order. The two
 *      polynomials share the same set of segment boundaries. If the polynomials are split then
 *      all the top polynomials must be of the same order, and all the bottom polynomials must
 *      be of the same order. However, regardless of whether the polynomial is split or not,
 *      the top polynomials do not need to be the same order as the bottom polynomials.
 *
 *      This function does not deliver IEEE-754 correct division. The divide does not round per
 *      the IEEE-754 current rounding mode. It incurs up to 2 ulps of error. Edge cases involving
 *      denormals, infinities, NaNs and division by zero return undefined results. (They will not
 *      crash, but NaN is a likely result in such cases.) Denormals can be rescued on AltiVec enabled
 *      machines by turning off the Non-Java bit in the VSCR, at the expense of taking a many-thousand
 *      cycle kernel exception every time a denormal number is encountered. Since you can predict ahead
 *      of time whether a given set of bounded polynomials is going to encounter these conditions,
 *      this problem should be avoidable by wise choice of polynomials. Developers who require IEEE-754
 *      correct results should call the polynomial evaluator above twice and do the division themselves.
 *
 *      These functions will also work for multichannel data, such as RGBAFFFF buffers by adjusting the
 *      width of the buffer to reflect the additional channels. Note that this will cause the alpha channel,
 *      if there is one, to become modified like the other channels.
 *
 *      These will work in place, provided that the following are true:
 *          src->data == dest->data
 *          src->rowBytes >= dest->rowBytes
 *          if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 
 *      Performance Advisory:
 *        Approximate cost of evaluating a rational (in the same units as polynomial above) is:
 *
 *              time = (base cost to touch all the data)
 *                          + top polynomial order
 *                          + bottom polynomial order
 *                          + 4
 *                          + 4 * log2segments
 *
 *        With data not in cache, the time may be significantly different. For sufficiently small polynomials, the
 *        cost may be a fixed cost, dependent only on how much data is touched, and not on polynomial order.
 *
 *        This performance behavior is provided to help developers evaluate speed tradeoffs. It is not a guaranteed.
 *        It is subject to change in future operating system revisions, and may be different on different hardware
 *        within the same or different operating system revisions.
 *
 */
VIMAGE_PF vImage_Error    vImagePiecewiseRational_PlanarF(  const vImage_Buffer *src,         //floating point data
                                                          const vImage_Buffer *dest,       //floating point data
                                                          const float         **topCoefficients,
                                                          const float         **bottomCoefficients,
                                                          const float         *boundaries,
                                                          uint32_t            topOrder,
                                                          uint32_t            bottomOrder,
                                                          uint32_t            log2segments,
                                                          vImage_Flags        flags ) VIMAGE_NON_NULL(1,2,3,4,5) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));

/*  A simple lookup table:
 *
 *          Pixel_16U table[256];
 *          Pixel_16U result_pixel = table[input_8_bit_pixel];
 *
 *  The input is a buffer of 8-bit pixels. The output is a buffer of 16-bit pixels.
 *
 *    Note: It is okay to use this to convert Planar8 data to other 16 bit types as well, such as 16S or 16F,
 *  or ARGB1555 or RGB565 pixels.  Simply use an array of the appropriate type for the lookup table.
 *
 *  This function can also work for multichannel data by scaling the width of the image to compensate for the
 *  additional channels. All channels will use the same table.
 *
 *  vImageLookupTable_Planar8ToPlanar16 will not work in place.
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanar16(const vImage_Buffer *src,
                                                           const vImage_Buffer *dest,
                                                           const Pixel_16U      table[256],
                                                           vImage_Flags         flags)
VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageLookupTable_Planar8toPlanar24
 *
 *  Use a lookup table to remap 0..255 values in the source image to a
 *  different set of 3 channel 8-bit unsigned integer values in the destination.
 *
 *          uint32_t table[256];       // 32-bits per pixel & 8-bit per channel
 *          uint8_t *t_ptr = (uint8_t*)(table + srcRow[i]);
 *          uint8_t r = t_ptr[1];
 *          uint8_t g = t_ptr[2];
 *          uint8_t b = t_ptr[3];
 *          dstRow[3*i+0] = r;
 *          dstRow[3*i+1] = g;
 *          dstRow[3*i+2] = b;
 *
 *      src         A pointer to a vImage_Buffer that references 8-bit source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references 3 channel 8-bit destination pixels
 *
 *      table       A pointer to the lookup table. 256 elements / 32-bits per entry / 4 8-bit values per entry.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanar24( const vImage_Buffer *src, const vImage_Buffer *dest,  const uint32_t table[256], vImage_Flags flags )  VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 *  vImageLookupTable_Planar8toPlanar48
 *
 *  Use a lookup table to remap 0..255 values in the source image to a
 *  different set of 3 channel 16-bit unsigned integer values in the destination.
 *
 *          uint64_t table[256];       // 64-bits per pixel & 16-bit per channel
 *          uint16_t *t_ptr = (uint16_t*)(table + srcRow[i]);
 *          uint16_t r = t_ptr[1];
 *          uint16_t g = t_ptr[2];
 *          uint16_t b = t_ptr[3];
 *          dstRow[3*i+0] = r;
 *          dstRow[3*i+1] = g;
 *          dstRow[3*i+2] = b;
 *
 *      src         A pointer to a vImage_Buffer that references 8-bit source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references 3 channel 16-bit destination pixels
 *
 *      table       A pointer to the lookup table. 256 elements / 64-bits per entry / 4 16-bit values per entry.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanar48( const vImage_Buffer *src, const vImage_Buffer *dest,  const uint64_t table[256], vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 *  vImageLookupTable_Planar8toPlanar96
 *
 *  Use a lookup table to remap 0..255 values in the source image to a
 *  different set of 3 channel 32-bit unsigned integer values in the destination.
 *
 *          uint32_t table[256*4];       // 128-bits per pixel & 32-bit per channel
 *          uint32_t *t_ptr = (uint32_t*)(table + 4 * srcRow[i]);
 *          uint32_t r = t_ptr[1];
 *          uint32_t g = t_ptr[2];
 *          uint32_t b = t_ptr[3];
 *          dstRow[3*i+0] = r;
 *          dstRow[3*i+1] = g;
 *          dstRow[3*i+2] = b;
 *
 *      src         A pointer to a vImage_Buffer that references 8-bit source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references 3 channel 32-bit destination pixels.
 *
 *      table       A pointer to the lookup table. 256 elements / 128-bits per entry / 4 32-bit values per entry.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *  dest.data must be 4-byte aligned at least.
 *
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanar96( const vImage_Buffer *src, const vImage_Buffer *dest,  const Pixel_FFFF table[256], vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));


/*
 *  vImageLookupTable_Planar8toPlanar128
 *
 *  Use a lookup table to remap 0..255 values in the source image to a
 *  different set of 4 channel 32-bit unsigned integer values in the destination.
 *
 *          uint32_t table[256*4];       // 128-bits per pixel & 32-bit per channel
 *          uint32_t *t_ptr = (uint32_t*)(table + 4 * srcRow[i]);
 *          uint32_t a = t_ptr[0];
 *          uint32_t r = t_ptr[1];
 *          uint32_t g = t_ptr[2];
 *          uint32_t b = t_ptr[3];
 *          dstRow[3*i+0] = a;
 *          dstRow[3*i+1] = r;
 *          dstRow[3*i+2] = g;
 *          dstRow[3*i+3] = b;
 *
 *      src         A pointer to a vImage_Buffer that references 8-bit source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references 4 channel 32-bit destination pixels.
 *
 *      table       A pointer to the lookup table. 256 elements / 128-bits per entry / 4 32-bit values per entry.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *  dest.data must be 4-byte aligned at least.
 *
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanar128( const vImage_Buffer *src, const vImage_Buffer *dest,  const Pixel_FFFF table[256], vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  A simple lookup table:
 *
 *          float table[256];
 *          float result_pixel = table[ input_8_bit_pixel ];
 *
 *  The input is a buffer of 8-bit pixels. The output is a buffer of floating point pixels.
 *
 *    Note: It is okay to use this to convert Planar8 data to other 32 bit types as well, such as a ARGB8888 pixel.
 *            Simply pass a Pixel_8888[256] array instead of a Pixel_F[256] array.
 *
 *  This function can also work for multichannel data by scaling the width of the image to compensate for the
 *  additional channels. All channels will use the same table.
 *
 *  vImageLookupTable_Planar8ToPlanarF will not work in place.
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar8toPlanarF(const vImage_Buffer *src,
                                                          const vImage_Buffer *dest,
                                                          const Pixel_F        table[256],
                                                          vImage_Flags         flags)
VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*
 *  A simple lookup table with floating point inputs:
 *
 *          uint8_t table[4096];
 *          uint32_t index =  (uint32_t) MIN( MAX( input_float_pixel * 4096.0f, 0.0f), 4095.0f);
 *          uint8_t result_pixel = table[ index ];
 *
 *  The input is a buffer of floating-point pixels. The output is a buffer of 8-bit pixels.
 *  This function can also work for multichannel data by scaling the width of the image to compensate for the
 *  additional channels. All channels will use the same table.
 *
 *  vImageLookupTable_PlanarFToPlanar8 will work in place, provided that the following are true:
 *      src->data == dest->data
 *      src->rowBytes >= dest->rowBytes
 *      if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 */
VIMAGE_PF vImage_Error
vImageLookupTable_PlanarFtoPlanar8(
                                   const vImage_Buffer *src,          /* floating point pixels */
                                   const vImage_Buffer *dest,         /* 8-bit pixels */
                                   const Pixel_8       table[4096],
                                   vImage_Flags        flags )
VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*
 *  vImageLookupTable_8to64U
 *
 *  Use a lookup table to remap 0..255 values in the source image to a
 *  different set of 64-bit unsigned integer values in the destination.
 *
 *          uint64_t table[256];
 *          uint64_t result_pixel = table[ input_8_bit_pixel ];
 *
 *      src         A pointer to a vImage_Buffer that references the source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references the destination pixels
 *
 *      table       A pointer to the lookup table. The table should be an array with 256 elements.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *
 */
VIMAGE_PF vImage_Error
vImageLookupTable_8to64U(
                         const vImage_Buffer *src,
                         const vImage_Buffer *dest,
                         const uint64_t      LUT[256],
                         vImage_Flags flags)
VIMAGE_NON_NULL(1,2,3)   API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageLookupTable_Planar16
 *
 *  Use a lookup table to remap 0..0xffff values in the source image to a
 *  different set of 16-bit unsigned integer values in the destination.
 *
 *          Pixel_16U table[0x10000];
 *          Pixel_16U result_pixel = table[ input_16_bit_pixel ];
 *
 *      src         A pointer to a vImage_Buffer that references the source pixels
 *
 *      dest        A pointer to a vImage_Buffer that references the destination pixels
 *
 *      table       A pointer to the lookup table. The table should be an array with 65536 elements.
 *
 *      flags       The following flags are allowed:
 *
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *
 *  Return Value:
 *  -------------
 *      kvImageNoError                  Success!
 *      kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *      kvImageNullPointerArgument      src, dest or table pointer is NULL.
 *
 *  This routine will not work in place.
 *
 */

VIMAGE_PF vImage_Error vImageLookupTable_Planar16(const vImage_Buffer *src, const vImage_Buffer *dest, const Pixel_16U table[0x10000], vImage_Flags flags) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageInterpolatedLookupTable_PlanarF
 *
 *  Results are calculated as follows:
 *
 *    float clippedPixel = MAX( MIN( src_pixel, maxFloat ), minFloat );    //clip src_pixel to be in range
 *    float fIndex = (float) (tableEntries - 1) * (clippedPixel - minFloat ) / (maxFloat - minFloat);
 *    float fract = fIndex - floor( fIndex );
 *    unsigned long index =  fIndex;
 *    float result = table[ index ] * ( 1.0f - fract ) + table[ index + 1] * fract;
 *
 *  In English, this translates to a lookup table that contains tableEntries values, that span
 *  the input range of minFloat...maxFloat, inclusive, in an evenly spaced fashion.
 *
 *                      tableEntries
 *               <--------------------->
 *      |
 *    r |          ****
 *    e |        *      *              *
 *    s |                 *          *
 *    u |                  *       *
 *    l |                    *****
 *    t +--------+---------------------+--------
 *            minFloat              maxFloat
 *
 *    IMPORTANT: For correct operation, the table must be allocated to contain tableEntries+1 entries.
 *                If the table is too small or the value of table[ tableEntries ] is infinite or NaN,
 *                behavior is undefined.
 *
 *  The function will work in place, provided that the following are true:
 *      src->data == dest->data
 *      src->rowBytes >= dest->rowBytes
 *      if( src->rowBytes > dest->rowBytes ) kvImageDoNotTile must be passed in the flags parameter
 *
 *  This function may be used on multichannel images by scaling the width by the number of channels.
 *  The same table will be used for all the channels, including alpha, if there is an alpha channel.
 */
VIMAGE_PF vImage_Error    vImageInterpolatedLookupTable_PlanarF(  const vImage_Buffer *src,
                                                                const vImage_Buffer *dest,
                                                                const Pixel_F       *table,
                                                                vImagePixelCount    tableEntries,
                                                                float               maxFloat,
                                                                float               minFloat,
                                                                vImage_Flags        flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


/*
 *  vImageMultidimensionalTable_Create
 *
 *  Creates a vImage_MultidimensionalTable for use with vImageMultiDimensionalInterpolatedLookupTable_<fmt>(). The
 *  input data is a contiguous array of N dimensional samples which define the lookup table grid points.
 *  The samples have range [0,65535] interpreted as [0, 1.0f]. (Samples have an implict divide by 65535
 *  in them, like the Planar16U format.)
 *
 *  The vImage_MultidimensionalTable is not a CFType.
 *
 *  Example:
 *      Suppose the multidimensional table is intended to convert RGB (3 channel) data to CMYK (4 channel).
 *      In each of the {R, G, B} dimensions, we have 17 samples. The table is then comprised of
 *      17x17x17=4913 uint16_t[4]'s, each of which contains {C,M,Y,K} for that position. The dimensions
 *      are iterated in standard C order (row major). In this case, they would be:
 *
 *      {CMYK for  R0G0B0, CMYK for R0G0B1,  CMYK for  R0G0B2, ...  CMYK for  R0G0B16,
 *       CMYK for  R0G1B0, CMYK for R0G1B1,  CMYK for  R0G1B2, ...  CMYK for  R0G1B16,
 *       ...
 *       CMYK for R0G16B0, CMYK for R0G16B1, CMYK for R0G16B2, ...  CMYK for R0G16B16,
 *       CMYK for  R1G0B0, CMYK for R1G0B1,  CMYK for R1G0B2,  ...  CMYK for R1G0B16,
 *       ...
 *       CMYK for R1G16B0, CMYK for R1G16B1, CMYK for R1G16B2,  ...  CMYK for R1G16B16,
 *       ...
 *       CMYK for R16G16B0, CMYK for R16G16B1, CMYK for R16G16B2,  ...  CMYK for R16G16B16 }
 *
 *  Parameters:
 *      tableData                   A non-NULL pointer to the data used to build the table
 *      numSrcChannels              The number of channels in an input pixel
 *      numDestChannels             The number of channels in an output pixel
 *      table_entries_per_dimension An array containing the number of table entries for each dimension in numSrcChannels
 *                                      The entries are in the same order as the channels in the src pixel.
 *      hint                        An indication of how the table would be used.  Pass either kvImageMDTableHint_16Q12
 *                                      or kvImageMDTableHint_Float or both. If only one is passed, we will save memory
 *                                      and time skipping work to set up the table for unused formats.
 *      flags                        The following flags are allowed:
 *
 *                kvImageDoNotTile    Turns off internal multithreading. You may
 *                                     wish to do this if you have your own
 *                                    multithreading scheme to avoid having the
 *                                    two interfere with one another.
 *      err                         A pointer to a vImage_Error. If err != NULL, on return, the memory pointed to by error
 *                                      will be overwritten with an appropriate error or kvImageNoError. It is sufficient to
 *                                      simply check the LHS return value against NULL to determine whether the function succeeeded.
 *
 *  Return value:
 *      On return a valid vImage_MultidimensionalTable for use with vImageMultiDimensionalInterpolatedLookupTable_<fmt>
 *      if err is not NULL, the memory it points to will be overwritten with an appropriate error code. The
 *      vImage_MultidimensionalTable will contain a copy of the data provided in tableData. It is safe to deallocate
 *      the input tableData immediately after this call returns.
 *
 *
 *
 *  Errors:
 *      kvImageNoError                Success
 *      kvImageInvalidParameter       hint must be kvImageMDTableHint_16Q12 or kvImageMDTableHint_Float or both
 *        kvImageInvalidParameter       numSrcChannels and numDestChannels must be non-zero.
 *      kvImageNullPointerArgument    tableData and table_entries_per_dimension must not be NULL.
 *      kvImageUnknownFlagsBit        An illegal or unknown flag was passed to the function.
 *      kvImageMemoryAllocationError  Can not allocate memory for buffer.
 *
 *  Use vImageMultidimensionalTable_Release to destroy the vImage_MultidimensionalTable when you are done with it.
 *  The vImage_MultidimensionalTable may be reused. The table is immutable, thread-safe and may be used by many
 *  vImageMultiDimensionalInterpolatedLookupTable_<fmt> calls concurrently so long as care is taken that it is not
 *  freed while it is being used. To support multithreaded concurrent access, the table follows standard retain/release
 *  semantics. On creation the table has a retain count of 1. When the reference count drops to 0, the table is destroyed.
 *
 */

typedef struct vImage_MultidimensionalTableData * vImage_MultidimensionalTable;

/* Hints to describe use of vImage_MultidimensionalTableData look up table. */
typedef enum
{
    kvImageMDTableHint_16Q12 VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) = 1,
    kvImageMDTableHint_Float VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) = 2,
}vImageMDTableUsageHint;

VIMAGE_PF vImage_MultidimensionalTable vImageMultidimensionalTable_Create( const uint16_t *tableData,
                                                                          uint32_t numSrcChannels,
                                                                          uint32_t numDestChannels,
                                                                          const uint8_t table_entries_per_dimension[],   /* uint8_t[numSrcChannels] */
                                                                          vImageMDTableUsageHint hint,
                                                                          vImage_Flags flags,
                                                                          vImage_Error *err )
VIMAGE_NON_NULL(1,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageMultidimensionalTable_Retain
 *  vImageMultidimensionalTable_Release
 *
 *  A vImage_MultidimensionalTable follows Retain/Release semantics. On creation, the table has  a retain count of 1.
 *  If you call vImageMultidimensionalTable_Retain on it, the retain count is incremented. If you call
 *  vImageMultidimensionalTable_Release on it, the retain count is decremented. When the retain count reaches 0,
 *  the object is destroyed. If any vImage function is called on an object whose reference count has already reached
 *  0, behavior is undefined.
 *
 *  Parameters:
 *      table       A pointer to vImage_MultidimensionalTableData. If NULL, then nothing happens.
 *
 *  Errors:
 *      kvImageNoError                  Success
 */
VIMAGE_PF vImage_Error vImageMultidimensionalTable_Retain( vImage_MultidimensionalTable table ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error vImageMultidimensionalTable_Release( vImage_MultidimensionalTable table ) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageMultiDimensionalLookupTable_PlanarF
 *
 *  vImageMultiDimensionalLookupTable_<fmt> uses the input color channel values (treated as a N-dimensional coordinate)
 *  to index a N-dimensional lookup table to find a new color value, possibly in a new color space. The number of color channels
 *  in the destination image does not need to match the number of color channels in the source image. The table is created using
 *  vImageMultidimensionalTable_Create.
 *
 *  In each dimension, the table must be of size (2**K[dimension])+1. The first entry corresponds to 0 and the last to 1.0. The rest
 *  correspond to steps at 2**-K[dimension]. (K[] is the table_entries_per_dimension[] array in vImageMultidimensionalTable_Create.)
 *  Thus a 17x17x17 3D lookup table defines the vertices that delineate a grid of 16x16x16 cubes, just as a ruler marked in 16ths of an
 *  inch has 17 marks between 0 and 1 inch, inclusive. (SI units are usually awkwardly subdivided by powers of 10, rather than powers of 2
 *  so we avoid them in this example. :-)    The most significant portion of the color channel values are used to index the multidimensional
 *  grid. Usually there will be some fractional precision left over.  That is used to do a linear interpolation between nearby gridpoints
 *  to find the value at that position. If the vImage_InterpolationMethod is kvImageFullInterpolation, then all 2**N (for N dimensions)
 *  nearby gridpoints are considered. If the vImage_InterpolationMethod is kvImageHalfInterpolation, then the {0,0,0...}, {1,1,1...} verticies
 *  along the gray axis are considered along with N-1 nearest other vertices.
 *
 *
 *  Parameters:
 *
 *      srcs                    An array of vImage_Buffers that reference the source image planes. The number of
 *                              such buffers is given by the numSrcChannels parameter passed to vImageMultidimensionalTable_Create.
 *
 *      dests                   An array of vImage_Buffers that reference the destination image planes. The number of
 *                              such buffers is given by the numDestChannels parameter passed to vImageMultidimensionalTable_Create.
 *
 *      tempBuffer              May be NULL. If non-NULL, this is a pointer to a region of memory that vImage can use as a
 *                              scratch space for storing temporary data. The minimum size of the scratch space is obtained
 *                              by calling the function with identical parameters and the kvImageGetTempBufferSize flag.
 *                              If vImageMultiDimensionalInterpolatedLookupTable_<fmt> can be called from multiple threads
 *                              simultaneously, the temp buffer must be different (or NULL) for each thread.
 *
 *      table                   A valid table created by vImageMultidimensionalTable_Create
 *
 *      method                  either kvImageFullInterpolation or kvImageHalfInterpolation. See description above.
 *
 *      flags                   Must be kvImageNoFlags or one or more from this list:
 *
 *                                  kvImageDoNotTile            turn off internal multithreading.
 *                                  kvImageGetTempBufferSize    Return the size of temp buffer. It may return 0. Do nothing.
 *
 *  Error Values:
 *        >= 0                        Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *      kvImageNoError              Success
 *      kvImageNullPointerArgument  srcs, dests and table may not be NULL
 *      kvImageUnknownFlagsBit      An illegal or unknown flag was passed to the function.
 *      kvImageInvalidParameter     An unknown or illegal interpolation method was indicated
 *      kvImageInvalidParameter     The interpolation method doesn't have a table for the requested data type.
 *                                      This happened because an incorrect vImageMDTableUsageHint was passed to
 *                                      vImageMultidimensionalTable_Create
 *      kvImageBufferSizeMismatch   Within srcs, all vImage_Buffers must have the same height and width.
 *                                    Similarly, within dests, all vImage_Buffers must have the same height and width.
 *                                    Otherwise this error is returned.
 *      kvImageBufferSizeMismatch   The src height and width must be greater than or equal to the destination height
 *                                    and width.
 *
 */

/* interpolation method for vImageMultiDimensionalInterpolatedLookupTable_<fmt>*/
typedef enum
{
    kvImageNoInterpolation   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) = 0,             /* nearest neighbor. Fast but probably causes banding and will certainly quantize the histogram. */
    kvImageFullInterpolation VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) = 1,           /* full linear interpolation */
    kvImageHalfInterpolation VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_4, __IPHONE_5_0 ) = 2            /* partial linear interpolation between vertices on gray axis and N-1 nearest vertices */
}vImage_InterpolationMethod;

VIMAGE_PF vImage_Error vImageMultiDimensionalInterpolatedLookupTable_PlanarF( const vImage_Buffer srcs[],
                                                                             const vImage_Buffer dests[],
                                                                             void *tempBuffer,
                                                                             vImage_MultidimensionalTable table,
                                                                             vImage_InterpolationMethod method,
                                                                             vImage_Flags flags )
VIMAGE_NON_NULL(1,2,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

/*
 *  vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12
 *
 *  vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12 is like vImageMultiDimensionalInterpolatedLookupTable_PlanarF
 *  except that it operates on signed fixed-point data. The 16Q12 format is a signed 16-bit fixed-point value with 12 fractional
 *  bits, 3 non-fractional bits and one sign bit. It can represent values in the range (-8,8). However the table always describes
 *  a region between [0,1] in each dimension. Values outside this range are clamped to the nearest in-range value by Manhattan
 *  distance before indexing the table. See vImageConvert_Planar8to16Q12 and vImageConvert_16Q12toPlanar8 for more on the format.
 */
VIMAGE_PF vImage_Error vImageMultiDimensionalInterpolatedLookupTable_Planar16Q12( const vImage_Buffer srcs[],
                                                                                 const vImage_Buffer dests[],
                                                                                 void *tempBuffer,
                                                                                 vImage_MultidimensionalTable table,
                                                                                 vImage_InterpolationMethod method,
                                                                                 vImage_Flags flags )
VIMAGE_NON_NULL(1,2,4)
API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));




#ifdef __cplusplus
}
#endif

#endif /* vImage_TRANSFORM_H */


// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Histogram.h
/*!
*  @header Histogram.h
*  vImage_Framework
*
*  See vImage/vImage.h for more on how to view the headerdoc documentation for functions declared herein.
*
*  @copyright Copyright (c) 2003-2016 by Apple Inc. All rights reserved.
*
*  @discussion Exports interfaces for collecting histograms of images, and causing images to conform to histograms.
*
*  @ignorefuncmacro VIMAGE_NON_NULL
*/

#ifndef VIMAGE_HISTOGRAM_H
#define VIMAGE_HISTOGRAM_H

#ifdef __cplusplus
extern "C" {
#endif

#include <vImage/vImage_Types.h>

/*!
 * @functiongroup Histogram Calculation
 * @discussion  These functions build a histogram of the frequency of occurence of
 *              each channel value in the provided image.
 */

/*!
 * @function vImageHistogramCalculation_Planar8
 *
 * @abstract Calculates a histogram for a Planar8 image.
 *
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each pixel, do the following:
 *          histogram[src[x]]++;
 * @/textblock</pre>
 *
 * This routine will not work in place
 *
 * @param   src            A pointer to a vImage_Buffer that references the source pixels
 *
 * @param   histogram      A pointer to a histogram. On return, this array will
 *                   contain the histogram for the source image.
 *                   The histogram will be an array with 256 elements.
 *
 * @param   flags          The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageNullPointerArgument      src or histogram pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramCalculation_Planar8(
                                   const vImage_Buffer *src,
                                   vImagePixelCount *histogram,
                                   vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageHistogramCalculation_PlanarF
 *
 * @abstract Calculates a histogram for a PlanarF image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each pixel, do the following:
 *          val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *          i = clip val between 0 and histogram_entries-1
 *          histogram[i]++;
 * @/textblock</pre>
 *
 * This routine will not work in place
 *
 * @param   src         A pointer to a vImage_Buffer that references the source pixels
 *
 * @param   histogram   A pointer to a histogram. On return, this array will
 *                      contain the histogram for the source image.
 *                      The histogram will have histogram_entries elements.
 *
 * @param   histogram_entries The number of histogram entries, or bins.
 *                      The histogram will be an array with histogram_entries elements.
 *
 * @param   minVal      A minimum pixel value. Any pixel value less than this will be
 *                      clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the first histogram entry. src is not modified.
 *
 * @param   maxVal      A maximum pixel value. Any pixel value greater than this will
 *                      be clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the last histogram entry.  src is not modified.
 *
 * @param   flags       The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 * @/textblock</pre>
 *
 * @return  The following error codes may occur:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or histogram pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramCalculation_PlanarF(
                                   const vImage_Buffer *src,
                                   vImagePixelCount *histogram,
                                   unsigned int histogram_entries,
                                   Pixel_F minVal,
                                   Pixel_F maxVal,
                                   vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageHistogramCalculation_ARGB8888
 *
 * @abstract Calculates histograms for each channel of an ARGB8888 image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *
 *      For each pixel and channel, do the following:
 *          val = src[x];
 *          histogram[ch][val[ch]]++;
 *
 * @/textblock</pre>
 *
 * This routine will not work in place
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the histogram will match the channel order of the image.
 *
 * @param   src         A pointer to a vImage_Buffer that references the source pixels
 *
 * @param   histogram   A pointer to a histograms, one each for alpha, red, green, and blue (in that order).
 *                      On return, this array will contain the four histograms for the corresponding channels.
 *                      Each of the four histograms will be an array with 256 elements.
 *                      This function does not allocate storage for the histograms. You must allocate
 *                      storage for each of the four histograms, create an array and populate it with
 *                      pointers to the histograms before calling the function.
 *
 * @param   flags       The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Do not calculate the histogram for the alpha channel
 * @/textblock</pre>
 *
 * @return  The following error codes may occur
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageNullPointerArgument      src or histogram pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramCalculation_ARGB8888(
                                    const vImage_Buffer *src,
                                    vImagePixelCount *histogram[4],
                                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageHistogramCalculation_ARGBFFFF
 *
 * @abstract Calculates histograms for each channel of an ARGBFFFF image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each pixel, do the following:
 *          val = src[x];
 *          For each channel do:
 *              i = histogram_entries * (val[ch] - minVal) / (maxVal - minVal);
 *              i = clip i between 0 and histogram_entries-1
 *              histogram[ch][i]++;
 * @/textblock</pre>
 *
 * This routine will not work in place
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the histogram will match the channel order of the image.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param histogram     A pointer to an array of four histograms,
 *                      one each for alpha, red, green, and blue (in that order).
 *                      On return, this array will contain the four histograms for the corresponding channels.
 *                      Each of the four histograms will be an array with histogram_entries elements.
 *                      This function does not allocate storage for the histograms. You must allocate
 *                      storage for each of the four histograms, create an array and populate it with
 *                      pointers to the histograms before calling the function.
 *
 * @param histogram_entries The number of histogram entries, or bins. Each of the four
 *                      calculated histograms will be an array with histogram_entries elements.
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *                      This minimum value is applied to each of the four channels separately.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *                      This maximum value is applied to each of the four channels separately.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Do not calculate the histogram for the alpha channel
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or any of the histogram pointers is NULL.
 * @/textblock</pre>
 *
 */
VIMAGE_PF vImage_Error
vImageHistogramCalculation_ARGBFFFF(
                                    const vImage_Buffer *src,
                                    vImagePixelCount *histogram[4],
                                    unsigned int histogram_entries,
                                    Pixel_F minVal,
                                    Pixel_F maxVal,
                                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 * @functiongroup Histogram Equalization
 * @discussion These functions attempt to change an image such that its histogram
 *              contains an even distribution of values.
 */

/*!
 * @function vImageEqualization_Planar8
 *
 * @abstract Equalizes the histogram of a Planar8 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image;
 *      Calculate normalized sum of histogram;
 *      For each pixel, do the following:
 *          dest[x] = equalized_histogram[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src       A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest      A pointer to a vImage buffer that references the destination pixels
 *
 * @param flags     The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEqualization_Planar8(
                           const vImage_Buffer *src,
                           const vImage_Buffer *dest,
                           vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageEqualization_PlanarF
 *
 * @abstract Equalizes the histogram of a PlanarF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image:
 *          For each pixel, do the following:
 *              val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *              i = clip val between 0 and histogram_entries-1
 *              histogram[i]++;
 *      Calculate normalized sum of histogram;
 *      For each pixel, do the following:
 *          val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *          i = clip val between 0 and histogram_entries-1
 *          dest[x] = equalized_histogram[i];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param histogram_entries The number of histogram entries, or bins.
 *                      The histogram will be an array with histogram_entries elements.
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be
 *                      clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the first histogram entry.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will
 *                      be clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the last histogram entry.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 * @/textblock</pre>
 *
 * @return The following values may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or histogram pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEqualization_PlanarF(
                           const vImage_Buffer *src,
                           const vImage_Buffer *dest,
                           void *tempBuffer,
                           unsigned int histogram_entries,
                           Pixel_F minVal,
                           Pixel_F maxVal,
                           vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageEqualization_ARGB8888
 *
 * @abstract Equalizes the histogram of an ARGB8888 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Calculates histograms for each channel of the ARGB8888 source image;
 *      Calculate normalized sum of each histogram;
 *      For each pixel do the following:
 *          val = src[x];
 *          For each channel:
 *              val[ch] = equalized_histogram[ch][val[ch]];
 *              dest[x] = val;
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *
 * @param src       A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest      A pointer to a vImage buffer that references the destination pixels
 *
 * @param flags     The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following error codes may be returned
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEqualization_ARGB8888(
                            const vImage_Buffer *src,
                            const vImage_Buffer *dest,
                            vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageEqualization_ARGBFFFF
 *
 * @abstract Equalizes the histogram of an ARGBFFFF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image:
 *          For each pixel, do the following:
 *              val = src[x];
 *              For each channel do:
 *                  i = histogram_entries * (val[ch] - minVal) / (maxVal - minVal);
 *                  i = clip i between 0 and histogram_entries-1
 *                  histogram[ch][i]++;
 *
 *      Calculate normalized sum of each histogram;
 *
 *      For each pixel, do the following:
 *          val = src[x];
 *          For each channel do:
 *              i = histogram_entries * (val[ch] - minVal) / (maxVal - minVal);
 *              i = clip i between 0 and histogram_entries-1
 *              val[ch] = equalized_histogram[ch][i];
 *          dest[x] = val;
 * @/textblock</pre>
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *                      This minimum value is applied to each of the four channels separately.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *                      This maximum value is applied to each of the four channels separately.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 *
 *          kvImageLeaveAlphaUnchanged    Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following error codes may be returned
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 */
VIMAGE_PF vImage_Error
vImageEqualization_ARGBFFFF(
                            const vImage_Buffer *src,
                            const vImage_Buffer *dest,
                            void *tempBuffer,
                            unsigned int histogram_entries,
                            Pixel_F minVal,
                            Pixel_F maxVal,
                            vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 * @functiongroup Histogram Specification
 * @discussion These functions cause the source image pixel distribution to conform to the
 *             desired histogram.
 */

/*!
 * @function vImageHistogramSpecification_Planar8
 *
 * @abstract Performs a histogram specification operation on a Planar8 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image;
 *      Calculate normalized sum of the input histogram and the desired_histogram
 *      Generate the inverse transform
 *      Transform final image using inv_hist as a LUT. For each pixel, do the following:
 *          dest[x] = inv_histogram[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src               A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest              A pointer to a vImage buffer that references the destination pixels
 *
 * @param desired_histogram A pointer to the desired histogram for the transformed image.
 *                          The histogram should be an array with 256 elements.
 *
 * @param flags             The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 * @/textblock</pre>
 *
 * @return  The following error codes may occur:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src, dest or desired_histogram pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramSpecification_Planar8(
                                     const vImage_Buffer *src,
                                     const vImage_Buffer *dest,
                                     const vImagePixelCount *desired_histogram,
                                     vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageHistogramSpecification_PlanarF
 *
 * @abstract Performs a histogram specification operation on a PlanarF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image:
 *          For each pixel, do the following:
 *              val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *              i = clip val between 0 and histogram_entries-1
 *              histogram[i]++;
 *      Calculate normalized sum of the input histogram and the desired_histogram
 *      Generate the inverse transform
 *      Transform final image using inverse transform as a LUT. For each pixel, do the following:
 *          val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *          i = clip val between 0 and histogram_entries-1
 *          dest[x] = inv_histogram[i];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param desired_histogram A pointer the desired histogram for the transformed image.
 *                      The histogram should be an array with histogram_entries elements.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be
 *                      clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the first histogram entry.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will
 *                      be clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the last histogram entry.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src, dest or desired_histogram pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramSpecification_PlanarF(
                                     const vImage_Buffer *src,
                                     const vImage_Buffer *dest,
                                     void *tempBuffer,
                                     const vImagePixelCount *desired_histogram,
                                     unsigned int histogram_entries,
                                     Pixel_F minVal,
                                     Pixel_F maxVal,
                                     vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 * @function vImageHistogramSpecification_ARGB8888
 *
 * @abstract Performs a histogram specification operation on an ARGB8888 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each channel:
 *          Compute the histogram of the input image;
 *          Calculate normalized sum of the input histogram and the desired_histogram
 *          Generate the inverse transform
 *          Transform final image using inv_hist as a LUT. For each pixel, do the following:
 *              dest[x] = inv_histogram[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the desired_histogram must match the channel order of the image.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param desired_histogram A pointer to an array of four histograms, one each for
 *                      alpha, red, green, and blue (in that order).
 *                      These are the desired histograms for the transformed image.
 *                      Each histogram should be an array with 256 elements.
 *                      This function does not allocate storage for the histograms. You must allocate
 *                      storage for each of the four histograms, create an array and populate it with
 *                      pointers to the histograms before calling the function. The contents of the
 *                      histograms are generally obtained from vImageHistogramCalculation_<fmt> from
 *                      another image, but need not be so.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following values may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src, dest or any of the desired_histogram pointers is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramSpecification_ARGB8888(
                                      const vImage_Buffer *src,
                                      const vImage_Buffer *dest,
                                      const vImagePixelCount *desired_histogram[4],
                                      vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 * @function vImageHistogramSpecification_ARGBFFFF
 *
 * @abstract Performs a histogram specification operation on an ARGBFFFF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each channel do:
 *          Compute the histogram of the input image:
 *              For each pixel, do the following:
 *                  val = histogram_entries * (src[x] - minVal) / (maxVal - minVal);
 *                  i = clip val between 0 and histogram_entries-1
 *                  histogram[ch][i]++;
 *          Calculate normalized sum of the input histogram and the desired_histogram
 *          Generate the inverse transform
 *          Transform final image using inverse transform as a LUT. For each pixel, do the following:
 *              val = histogram_entries * (src[ch][x] - minVal) / (maxVal - minVal);
 *              i = clip val between 0 and histogram_entries-1
 *              dest[ch][x] = inv_histogram[ch][i];
 * @/textblock </pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the desired_histogram must match the channel order of the image.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param desired_histogram A pointer to an array of four histograms, one each for
 *                      alpha, red, green, and blue (in that order).
 *                      These are the desired histograms for the transformed image.
 *                      Each histogram should be an array with histogram_entries elements..
 *                      This function does not allocate storage for the histograms. You must allocate
 *                      storage for each of the four histograms, create an array and populate it with
 *                      pointers to the histograms before calling the function. The contents of the
 *                      histograms are generally obtained from vImageHistogramCalculation_<fmt> from
 *                      another image, but need not be so.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *                      This minimum value is applied to each of the four channels separately.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *                      This maximum value is applied to each of the four channels separately.
 *
 * @param flags The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return  The following error codes may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src, dest or any of desired_histogram pointers is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageHistogramSpecification_ARGBFFFF(
                                      const vImage_Buffer *src,
                                      const vImage_Buffer *dest,
                                      void *tempBuffer,
                                      const vImagePixelCount *desired_histogram[4],
                                      unsigned int histogram_entries,
                                      Pixel_F minVal,
                                      Pixel_F maxVal,
                                      vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @functiongroup Contrast Stretch
 * @discussion Some images do not make enough use of the light and dark ends of the spectrum.
 *             vImageContrastStretch functions widen the pixel value distribution to use a wider
 *             range of values.
 */

/*!
 * @function vImageContrastStretch_Planar8
 *
 * @abstract Stretches the contrast of a Planar8 source image.
 * @discussion This function performs the following operation:
 *   <pre>@textblock
 *      Compute the histogram of the input image;
 *      Generate LookUp table based on the histogram
 *      Transform final image using the LUT. For each pixel, do the following:
 *          dest[x] = LUT[src[x]];
 *   @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * vImageEndsInContrastStretch_Planar8 or vImageTableLookUp_Planar8 may be used instead when more control
 * over the stretch is desired.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param flags         The following flags are allowed:
 *   <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 *   @/textblock</pre>
 *
 * @return The following error codes may be returned:
 *   <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageContrastStretch_Planar8(
                              const vImage_Buffer *src,
                              const vImage_Buffer *dest,
                              vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageContrastStretch_PlanarF
 *
 * @abstract Stretches the contrast of a PlanarF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Find minimum and maximum value of the input image: loVal, hiVal;
 *      scale_factor = ( maxVal - minVal ) / (float) ( hiVal - loVal );
 *      Transfer the image. For each pixel do:
 *          dest[x] = (src[x] - loVal) * scale_factor + minVal;
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * vImageEndsInContrastStretch_PlanarF or vImageInterpolatedLookupTable_PlanarF may be used instead when more control
 * over the stretch is desired.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param histogram_entries The number of histogram entries, or bins to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be
 *                      clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the first histogram entry.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will
 *                      be clipped to this value (for the purposes of histogram calculation),
 *                      and assigned to the last histogram entry.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageContrastStretch_PlanarF(
                              const vImage_Buffer *src,
                              const vImage_Buffer *dest,
                              void *tempBuffer,
                              unsigned int histogram_entries,
                              Pixel_F minVal,
                              Pixel_F maxVal,
                              vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageContrastStretch_ARGB8888
 * @abstract Stretches the contrast of an ARGB8888 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each channel:
 *          Compute the histogram of the input image;
 *          Generate LookUp table based on the histogram
 *          Transform final image using the LUT. For each pixel, do the following:
 *              dest[x] = LUT[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *
 * vImageEndsInContrastStretch_ARGB8888 or vImageTableLookUp_ARGB8888 may be used instead when more control
 * over the stretch is desired.
 *
 * @param src       A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest      A pointer to a vImage buffer that references the destination pixels
 *
 * @param flags     The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageContrastStretch_ARGB8888(
                               const vImage_Buffer *src,
                               const vImage_Buffer *dest,
                               vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageContrastStretch_ARGBFFFF
 *
 * @abstract Stretches the contrast of  an ARGBFFFF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *        For each channel:
 *            Find minimum and maximum value of the input image: loVal, hiVal;
 *            scale_factor = ( maxVal - minVal ) / (float) ( hiVal - loVal );
 *            Transfer the image. For each pixel do:
 *                dest[x] = (src[x] - loVal) * scale_factor + minVal;
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *                      This minimum value is applied to each of the four channels separately.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *                      This maximum value is applied to each of the four channels separately.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal or histogram_entries is 0
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 * @/textblock<pre>
 */
VIMAGE_PF vImage_Error
vImageContrastStretch_ARGBFFFF(
                               const vImage_Buffer *src,
                               const vImage_Buffer *dest,
                               void *tempBuffer,
                               unsigned int histogram_entries,
                               Pixel_F minVal,
                               Pixel_F maxVal,
                               vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 *  @functiongroup Ends-In Contrast Stretch
 *  @discussion Readjust the image pixel value distribution to use only some of the range of available intensities.
 */

/*!
 * @function vImageEndsInContrastStretch_Planar8
 *
 * @abstract Performs an ends-in contrast stretch operation on a Planar8 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      Compute the histogram of the input image;
 *      Generate LookUp table based on the histogram and percentage parameters
 *      Transform final image using the LookUp table. For each pixel, do the following:
 *          dest[x] = LUT[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param percent_low   A percentage value.
 *                      The number of pixels that map to the lowest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param percent_high  A percentage value.
 *                      The number of pixels that map to the highest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile         Turns off internal multithreading. You may
 *                                   wish to do this if you have your own
 *                                   multithreading scheme to avoid having the
 *                                   two interfere with one another.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   kvImageInvalidParameter         percent_low + percent_high is greater than 100.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEndsInContrastStretch_Planar8(
                                    const vImage_Buffer *src,
                                    const vImage_Buffer *dest,
                                    unsigned int percent_low,
                                    unsigned int percent_high,
                                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageEndsInContrastStretch_PlanarF
 *
 * @abstract Performs an ends-in contrast stretch operation on a PlanarF source image.
 * @discussion
 * <pre>@textblock
 *      Compute the histogram of the input image;
 *      Generate LookUp table based on the histogram and percentage parameters
 *      Transform final image using the LookUp table. For each pixel, do the following:
 *          val = histogram_entries * (src[x] - minVal) / ( maxVal - minVal );
 *          i = clip val between 0 and histogram_entries-1
 *          dest[x] = LUT[i];
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param percent_low   A percentage value.
 *                      The number of pixels that map to the lowest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param percent_high  A percentage value.
 *                      The number of pixels that map to the highest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value, the low end of the histogram.
 *                      Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *
 * @param maxVal        A maximum pixel value, the high end of the histogram.
 *                      Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *
 * @param    flags      The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal, histogram_entries is 0 or
 *                                   percent_low + percent_high is greater than 100.
 *   kvImageNullPointerArgument      src or dest pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock </pre>
 */
VIMAGE_PF vImage_Error
vImageEndsInContrastStretch_PlanarF(
                                    const vImage_Buffer *src,
                                    const vImage_Buffer *dest,
                                    void *tempBuffer,
                                    unsigned int percent_low,
                                    unsigned int percent_high,
                                    unsigned int histogram_entries,
                                    Pixel_F minVal,
                                    Pixel_F maxVal,
                                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*!
 * @function vImageEndsInContrastStretch_ARGB8888
 *
 * @abstract Performs an ends-in contrast stretch operation on an ARGB8888 source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each channel:
 *          Compute the histogram of the input image;
 *          Generate LookUp table based on the histogram and percentage parameters
 *          Transform final image using the LookUp table. For each pixel, do the following:
 *              dest[x] = LUT[src[x]];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the percent_low and percent_high parameters match the order of the channels.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param percent_low   An array of four percentage values, one each for alpha, red, green, and blue.
 *                      The number of pixels that map to the lowest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param percent_high  An array of four percentage values, one each for alpha, red, green, and blue.
 *                      The number of pixels that map to the highest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 *
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   kvImageNoError                  Success!
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match
 *   kvImageNullPointerArgument      src, dest, percent_low or percent_high pointer is NULL.
 *   kvImageInvalidParameter         Some of percent_low[i]+percent_high[i] is greater than 100.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEndsInContrastStretch_ARGB8888(
                                     const vImage_Buffer *src,
                                     const vImage_Buffer *dest,
                                     const unsigned int percent_low[4],
                                     const unsigned int percent_high[4],
                                     vImage_Flags flags ) VIMAGE_NON_NULL(1,2,3,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


/*!
 * @function vImageEndsInContrastStretch_ARGBFFFF
 *
 * @abstract Performs a histogram specification operation on an ARGBFFFF source image.
 * @discussion This function performs the following operation:
 * <pre>@textblock
 *      For each channel do:
 *          Compute the histogram of the input image;
 *          Generate LookUp table based on the histogram and percentage parameters
 *          Transform final image using the LookUp table. For each pixel, do the following:
 *              val = histogram_entries * (src[x] - minVal) / ( maxVal - minVal );
 *              i = clip val between 0 and histogram_entries-1
 *              dest[x] = LUT[i];
 * @/textblock</pre>
 *
 * This routine will work in place provided that src.data == dest.data and src.rowBytes >= dest.rowBytes.
 *    if src.rowBytes > dest.rowBytes and src.data == dest.data, the function will only work if you pass
 *    kvImageDoNotTile
 *
 * All four channel histogram functions (i.e. those that support ARGB8888 or ARGBFFFF images)
 *    work equally well on four channel images with other channel orderings such as RGBA8888 or BGRAFFFF.
 *    The ordering of the percent_low and percent_high parameters match the order of the channels.
 *
 * @param src           A pointer to a vImage_Buffer that references the source pixels
 *
 * @param dest          A pointer to a vImage buffer that references the destination pixels
 *
 * @param tempBuffer    A pointer to a temporary buffer. If you pass NULL, the function allocates the buffer,
 *                      then deallocates it before returning. Alternatively, you can allocate the buffer yourself,
 *                      in which case you are responsible for deallocating it when you is no longer need it.
 *                      If you want to allocate the buffer yourself, see the documentation for information
 *                      on how to determine the minimum size that you must allocate.
 *
 * @param percent_low   An array of four percentage values, one each for alpha, red, green, and blue.
 *                      The number of pixels that map to the lowest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param percent_high  An array of four percentage values, one each for alpha, red, green, and blue.
 *                      The number of pixels that map to the highest end of the histogram of the
 *                      transformed image should represent this percentage of the total pixels.
 *
 * @param histogram_entries The number of histogram entries, or bins, to be used in histograms for this operation
 *
 * @param minVal        A minimum pixel value. Any pixel value less than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the first histogram entry.
 *                      This minimum value is applied to each of the four channels separately.
 *
 * @param maxVal        A maximum pixel value. Any pixel value greater than this will be clipped to this value
 *                      (for the purposes of histogram calculation), and assigned to the last histogram entry.
 *                      This maximum value is applied to each of the four channels separately.
 *
 * @param flags         The following flags are allowed:
 * <pre>@textblock
 *          kvImageDoNotTile            Turns off internal multithreading. You may
 *                                      wish to do this if you have your own
 *                                      multithreading scheme to avoid having the
 *                                      two interfere with one another.
 *
 *          kvImageGetTempBufferSize    If this is passed, then the size of a temp
 *                                      buffer is returned from the left hand side
 *                                      of the function and no other work is done.
 *                                      An error may still be returned in this case.
 *                                      All vImage errors are < 0.  0 may also be
 *                                      returned, indicating that the temp buffer
 *                                      size is 0 for this set of parameters on
 *                                      this device with this operating system at
 *                                      this time of day.
 *
 *          kvImageLeaveAlphaUnchanged  Copy the alpha channel to the destination image unchanged
 * @/textblock</pre>
 * @return The following error codes may be returned:
 * <pre>@textblock
 *   >= 0                            Minimum temp buffer size, if kvImageGetTempBufferSize was specified.
 *   kvImageNoError                  Success!
 *   kvImageInvalidParameter         maxVal is less than minVal, histogram_entries is 0 or
 *                                   some of percent_low[i]+percent_high[i] is greater than 100.
 *   kvImageNullPointerArgument      src, dest, percent_low or percent_high pointer is NULL.
 *   kvImageBufferSizeMismatch       Sizes of the src and dest images do not match.
 *   kvImageMemoryAllocationError    Can not allocate memory for the buffer.
 * @/textblock</pre>
 */
VIMAGE_PF vImage_Error
vImageEndsInContrastStretch_ARGBFFFF(
                                     const vImage_Buffer *src,
                                     const vImage_Buffer *dest,
                                     void *tempBuffer,
                                     const unsigned int percent_low[4],
                                     const unsigned int percent_high[4],
                                     unsigned int histogram_entries,
                                     Pixel_F minVal,
                                     Pixel_F maxVal,
                                     vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4,5) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));


#ifdef __cplusplus
}
#endif

#endif

// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/BasicImageTypes.h
/*
*  BasicImageTypes.h
*  vImage Framework
*
*  Copyright (c) 2004-2016 by Apple Inc. All rights reserved.
*
*/

#ifndef VIMAGE_BASIC_IMAGE_TYPES_H
#define VIMAGE_BASIC_IMAGE_TYPES_H

#include <vImage/vImage_Types.h>

#ifdef __cplusplus
extern "C" {
#endif


/*
 *  PNG + Tiff filter
 *
 *  difference Style described in PNG filter definition: http://www.w3.org/TR/PNG-Filters.html
 *  Implements PNG decompression filtering for filter method 0 of the PNG standard, section 9.2
 *
 *  Per PNG specifications, pixels that are outside of the src buffer are presumed to have value
 *  0, for filter types that specify that pixels other than the immediate source pixel be considered.
 *
 *  Due to the way some of the filter modes operate, this filter is always applied in place.
 */

enum
{
    
    kvImage_PNG_FILTER_VALUE_NONE  = 0,
    kvImage_PNG_FILTER_VALUE_SUB   = 1,
    kvImage_PNG_FILTER_VALUE_UP    = 2,
    kvImage_PNG_FILTER_VALUE_AVG   = 3,
    kvImage_PNG_FILTER_VALUE_PAETH = 4
};

VIMAGE_PF vImage_Error  vImagePNGDecompressionFilter( const vImage_Buffer *buffer,
                                                     vImagePixelCount    startScanline,
                                                     vImagePixelCount    scanlineCount,
                                                     uint32_t            bitsPerPixel,
                                                     uint32_t            filterMethodNumber, /* must be 0 */
                                                     uint32_t            filterType,         /* For filter method 0, value 0-4 inclusive */
                                                     vImage_Flags        flags)  API_AVAILABLE(macos(10.4), ios(5.0), watchos(1.0), tvos(5.0));


#ifdef __cplusplus
}
#endif

#endif

// ==========  Accelerate.framework/Frameworks/vImage.framework/Headers/Geometry.h
/*
*  Geometry.h
*  vImage_Framework
*
*  Copyright (c) 2002-2016 by Apple Inc. All rights reserved.
*
*/


/*
*
*    This suite of functions allows the caller to rotate, resize and distort images. Resampling
*    is currently done with a Lanczos kernel.
*
*    The 90 degree rotate function requires that the corresponding destination axes share the
*    same even/odd parity for size as the source buffer. Otherwise, the image may become uncentered
*    by a half a pixel. For example if the input buffer has a width of 3 and a height of 4, and you rotate
*    90 degrees, the destination buffer should have a even width (4 is even) and a odd height (3 is odd).
*    For a 180 degree rotate, the width would have to be odd and the height even.
*
*    Set the vImageEdgeExtend flag if you wish to use the edge pixels of the image as the color of regions
*    outside the input vImage_Buffer. In general, this is not a good idea, however if you are just resizing,
*    it will prevent the background color from bleeding into the edges of the image.
*
*    The kvImageLeaveAlphaUnchanged flag is ignored for this set of functions, since most of these
*    functions involve a high degree of resampling. It is not clear in most cases what the
*    appropriate "unchanged" value would be to use.
*
*/


#ifndef VIMAGE_GEOMETRY_H
#define VIMAGE_GEOMETRY_H

#include <vImage/vImage_Types.h>

#ifdef __cplusplus
extern "C" {
#endif

//Rotation constants for use with Rotate_90_*
enum
{
    kRotate0DegreesClockwise          VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 0,
    kRotate90DegreesClockwise         VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 3,
    kRotate180DegreesClockwise        VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 2,
    kRotate270DegreesClockwise        VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 1,
    
    kRotate0DegreesCounterClockwise   VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 0,
    kRotate90DegreesCounterClockwise  VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 1,
    kRotate180DegreesCounterClockwise VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 2,
    kRotate270DegreesCounterClockwise VIMAGE_ENUM_AVAILABLE_STARTING( __MAC_10_3, __IPHONE_5_0 ) = 3
};


/*
 * High Level Geometry Functions
 * ----------------------------
 * vImage provides a set of high level geometry functions to do simple geometric transforms on an image.
 * These are:
 *
 *    Rotate -- rotate the input image around a center point by any amount and write to a destination buffer.
 *    Scale  -- resize the input image to the size of the destination buffer
 *    Affine Transform -- Apply an affine transform to an image
 *
 * When calling the Rotate and Affine Transform functions, it is possible that some part of the output
 * image may come from regions of the input image that are outside the original input image. In these cases,
 * the "revealed" pixels will be of the color provided in the backgroundColor parameter passed to the function,
 * unless kvImageEdgeExtend is passed, in which case it will be taken from the nearest edge pixel.
 *
 * The Scaling function may need to read outside the edges of the input buffer to service the resampling kernel.
 * In this case the values of the edge pixels a replicated outward infinitely. This allows us to avoid having
 * the background color bleed into the edges of the image. If you wish to rescale part of an image and have the rest
 * of the image considered for this operation, use the Affine Transform for scaling.
 *
 * Temporary Buffers
 * -----------------
 * The high level geometry functions cannot operate in place and cannot operate simply within the confines of the
 * source and destination buffers. This is because they employ multipass algorithms that need to save
 * intermediate pixel values between passes. The destination buffer may not be large enough to hold that
 * information or later passes may not operate correctly in place, meaning that additional storage is
 * required. Temporary storage is required by the higher level Geometry operations: Rotate, Scale,
 * and Affine Transform.
 *
 * Even though temporary storage is required by these functions, you may pass NULL for the pointer to
 * a temporary buffer. This may cause vImage to call malloc to allocate a buffer for you. It will
 * be destroyed before the function returns to prevent a memory leak. This is a perfectly sensible thing
 * to do when the function is not called often and possibly blocking on a lock for a short period of time
 * is not a problem. (Malloc may block on a lock.)
 *
 * If you plan to call the function frequently over a relatively short period of time, or you need real time
 * performance guarantees (which make locks a problem) then you should preallocate your own temporary
 * working buffer for vImage to use and reuse that. This will avoid the per-call malloc overhead, helping
 * to guarantee top performance. In addition, it avoids a problem caused by the fact that large
 * (i.e. image sized) buffers allocated by malloc are usually allocated by the OS in a paged-out state.
 * (They are not on disk. They do not physically exist until you touch the page, at which point they are
 * allocated and zero filled on demand.) This means that as we touch each new 4k page we take an interrupt to zero fill
 * the page, which would be nice to avoid. If you call the vImage function rarely, this is not worth worrying
 * about, since whatever buffer you preallocate is likely to be paged out to disk by the VM system anyway.
 * It is just on the 2nd and later calls in quick succession that this is a performance win, since we can
 * avoid the paging activity.
 *
 * You may allocate the temporary buffer anywhere you like -- heap, global, stack, etc.
 * Be aware that the stack size limit is user configurable and in many cases may not be large enough.
 * The buffer should not be shared reentrantly between different threads in a multithreaded app, unless you
 * protect it with a mutual exclusion device such as a lock/mutex/etc. Otherwise two vImage functions may
 * overwrite each other and produce garbled image data. No information is kept in the temporary buffer
 * between function calls.
 *
 *  All four channel geometry functions (i.e. those that support ARGB8888, ARGB16U, ARGB16S or ARGBFFFF images) work equally
 *  well on four channel images with other channel orderings such as RGBA or BGRA.
 */

/*
 * vImageRotate_<fmt>
 * =================
 * vImageRotate_<fmt> is a convenience function to provide facile rotation of images about their center point. The operation can also be done with vImageWarp_<fmt>, or
 * by using appropriate low level vImageHorizontal/VerticalShear_<fmt> interfaces. vImageWarp_<fmt> may be appropriate if you wish to rotate around a non-center point in
 * the image. vImageHorizontal/VerticalShear_<fmt> will provide the greatest detail in control, since it allows for alternative sampling methods, and also the opportunity to
 * control how tiling is done. This might allow for better cache utilization in cases where a format conversion (or other fast filter) is required either before or after
 * the scaling operation and you wish to incorporate it into your tiling design.
 *
 * To avoid artifacts in high frequency regions of the image, the data should be non-premultiplied, or at minimum have the same alpha over the entire image. For integer
 * formats with constant alpha < PIXEL_MAX, it is possible for result color values to be greater than alpha.  vImageClipToAlpha_<fmt> can be used to correct that problem.
 * Some other functions like vImageUnpremultiplyData_<fmt> will correct the problem as part of their operation, if they appear later in your image pipeline.
 * Otherwise, integer formats are clamped in the range [0,255] and can not experience modulo overflow problems. For floating-point formats, it is always possible to
 * produce out-of-gamut or greater than alpha results, most often in high-frequency regions of the image. Out-of-gamut results are often resolved when the floating-point
 * format is converted to an integer format in a later vImage call -- all conversions to integer format are clamped to the representable range. However, color values
 * greater than alpha will persist if the alpha is less than fully opaque, and can be fixed by vImageClipToAlpha_<fmt> as with the integer formats.
 * vImageUnpremultiplyData_<fmt> and vImagePremultiplyData_<fmt> are much, much faster than vImageRotate_<fmt> and typically only add a few percent to the overall cost
 * of the filter.
 *
 * vImageRotate_<fmt>() does not work in place
 * The ARGB8888, ARGB16U, ARGB16S and ARGBFFFF functions work equally well on other channel orderings of 4-channel images, such as RGBA or BGRA.
 *
 * Acceptable flags are kvImageEdgeExtend, kvImageBackgroundColorFill, kvImageDoNotTile, kvImageHighQualityResampling, kvImageNoFlags. If no edging mode is passed,
 * the edging mode is undefined. You may not pass both kvImageEdgeExtend and kvImageBackgroundColorFill. kvImageEdgeExtend will leave odd stripes in areas for which
 * no corresponding source image pixel exists, corresponding to the value of the nearest edge pixel, for some value of "nearest". kvImageBackgroundColorFill is most
 * commonly used.
 */

VIMAGE_PF vImage_Error    vImageRotate_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageRotate_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageRotate_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, float angleInRadians, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 * vImageScale_<fmt>
 * =================
 * vImageScale_<fmt> is a convenience function to provide facile resizing of images.
 * The operation can also be done with vImageWarp_<fmt>, or by using appropriate
 * low level vImageHorizontal/VerticalShear_<fmt> interfaces. vImageWarp_<fmt>
 * may be appropriate if you wish to use an edging mode other than kvImageEdgeExtend.
 * vImageHorizontal/VerticalShear_<fmt> will provide the greatest detail in control,
 * since it allows for alternative sampling methods, and also the opportunity to
 * control how tiling is done. This might allow for better cache utilization in
 * cases where a format conversion (or other fast filter) is required either before
 * or after the scaling operation and you wish to incorporate it into your tiling design.
 *
 * To avoid artifacts in high frequency regions of the image, the data should be
 * non-premultiplied, or at minimum have the same alpha over the entire image.
 * For integer formats with constant alpha < PIXEL_MAX, it is possible for result
 * color values to be greater than alpha.
 * vImageClipToAlpha_<fmt> can be used to correct that problem.
 * Some other functions like vImageUnpremultiplyData_<fmt> will correct the problem
 * as part of their operation, if they appear later in your image pipeline.
 * Otherwise, integer formats are clamped in the range [PIXEL_MIN,PIXEL_MAX] and can not
 * experience modulo overflow problems. For floating-point formats, it is always
 * possible to produce out-of-gamut or greater than alpha results, most often in
 * high-frequency regions of the image. Out-of-gamut results are often resolved
 * when the floating-point format is converted to an integer format in a later vImage
 * call -- all conversions to integer format are clamped to the representable range.
 * However, color values greater than alpha will persist if the alpha is less than
 * fully opaque, and can be fixed by vImageClipToAlpha_<fmt> as with the integer formats.
 * vImageUnpremultiplyData_<fmt> and vImagePremultiplyData_<fmt> are much, much faster
 * than vImageScale_<fmt> and typically only add a few percent to the overall cost
 * of the filter.
 *
 * vImageScale_<fmt>() does not work in place
 * The ARGB8888, ARGB16U, ARGB16S and ARGBFFFF functions work equally well on
 * other channel orderings of 4-channel images, such as RGBA or BGRA.
 *
 * Acceptable flags are kvImageEdgeExtend, kvImageDoNotTile,
 * kvImageHighQualityResampling, kvImageNoFlags.
 * If no edging mode is passed, kvImageEdgeExtend is used.
 * Developers using vImageScale_<fmt> on MacOS X.3 should pass kvImageEdgeExtend
 * in the flags field to avoid ringing artifacts at the edges of images
 */
VIMAGE_PF vImage_Error    vImageScale_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageScale_Planar16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageScale_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageScale_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageScale_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageScale_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageScale_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageScale_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, vImage_Flags flags ) VIMAGE_NON_NULL(1,2)    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error
vImageScale_CbCr8(
                  const vImage_Buffer *src,
                  const vImage_Buffer *dest,
                  void *tempBuffer,
                  vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageScale_CbCr16U(
                    const vImage_Buffer *src,
                    const vImage_Buffer *dest,
                    void *tempBuffer,
                    vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageScale_XRGB2101010W(
                         const vImage_Buffer *src,
                         const vImage_Buffer *dest,
                         void *tempBuffer,
                         vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*
 * vImageAffineWarp_<fmt>
 * ======================
 * vImageAffineWarp_<fmt> is a convenience function to provide facile affine transformation of images. The operation can also be done by using appropriate low level
 * vImageHorizontal/VerticalShear_<fmt> interfaces. vImageHorizontal/VerticalShear_<fmt> will provide the greatest detail in control, since it allows for alternative
 * sampling methods, and also the opportunity to control how tiling is done, possibly allowing for better cache utilization in cases where a format conversion (or
 * other fast filter) is required either before or after the scaling operation and you wish to incorporate it into your tiling design.
 *
 * To avoid artifacts in high frequency regions of the image, the data should be non-premultiplied, or at minimum have the same alpha over the entire image. For integer
 * formats with constant alpha < 255, it is possible for result color values to be greater than alpha.  vImageClipToAlpha_<fmt> can be used to correct that problem.
 * Some other functions like vImageUnpremultiplyData_<fmt> will correct the problem as part of their operation, if they appear later in your image pipeline.
 * Otherwise, integer formats are clamped in the range [PIXEL_MIN, PIXEL_MAX] and can not experience modulo overflow problems. For floating-point formats, it is always possible to
 * produce out-of-gamut or greater than alpha results, most often in high-frequency regions of the image. Out-of-gamut results are often resolved when the floating-point
 * format is converted to an integer format in a later vImage call -- all conversions to integer format are clamped to the representable range. However, color values
 * greater than alpha will persist if the alpha is less than fully opaque, and can be fixed by vImageClipToAlpha_<fmt> as with the integer formats.
 * vImageUnpremultiplyData_<fmt> and vImagePremultiplyData_<fmt> are much, much faster than vImageAffineWarp_<fmt> and typically only add a few percent to the overall cost
 * of the filter.
 *
 * For the Affine Transform function, the coordinate space places the origin at the bottom left corner of the image. Positive movement in the X and Y direction moves you
 * right and up. Both source and destination images are assumed to place their bottom left hand corner at the origin.
 *
 * vImageAffineWarp_<fmt>() does not work in place
 * The ARGB8888, ARGB16U, ARGB16S and ARGBFFFF functions work equally well on other channel orderings of 4-channel images, such as RGBA or BGRA.
 *
 * Acceptable flags are kvImageEdgeExtend, kvImageBackgroundColorFill, kvImageDoNotTile, kvImageHighQualityResampling, kvImageNoFlags. If no edging mode is passed, the edging
 * mode is undefined. You may not pass both kvImageEdgeExtend and kvImageBackgroundColorFill. kvImageEdgeExtend will leave odd stripes in areas for which no corresponding source
 * image pixel exists, corresponding to the value of the nearest edge pixel, for some value of "nearest". kvImageBackgroundColorFill is most commonly used, except when the result
 * will be cropped to cover only regions present in the original image and no background color leakage into the edges of the result image are desired.
 *
 * Versions of the API that use alternative formulations of the affine transform matrix follow immediately afterward.
 */
VIMAGE_PF vImage_Error    vImageAffineWarp_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageAffineWarp_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageAffineWarp_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageAffineWarp_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarp_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarp_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform *transform, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

#if VIMAGE_AFFINETRANSFORM_DOUBLE_IS_AVAILABLE
/* A single precision transformation matrix is often not enough. This one uses double precision. */
VIMAGE_PF vImage_Error    vImageAffineWarpD_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpD_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpD_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpD_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarpD_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarpD_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_AffineTransform_Double *transform, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#endif

#if VIMAGE_CGAFFINETRANSFORM_IS_AVAILABLE
/* Convenience Interfaces for working directly with CGAffineTransform, which changes size depending on whether we are LP64 or not. */
VIMAGE_PF vImage_Error    vImageAffineWarpCG_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpCG_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpCG_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageAffineWarpCG_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarpCG_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageAffineWarpCG_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, void *tempBuffer, const vImage_CGAffineTransform *transform, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
#endif



/*
 *    Low Level Geometry Functions
 *    ----------------------------
 *
 *    vImage also provides a series of low level geometry functions that do simple, often 1-D transforms on images.
 *    They are:
 *
 *        Reflect -- reflect an image across a  mirror plane at the center of the image in the x or y direction
 *        Shear --  shear and rescale an image in the x or y direction
 *        Rotate90 -- rotate an image by 0, 90, 180 or 270 degrees
 *
 *    The Reflect functions simply reflect images horizontally or vertically. Horizontal reflection inverts the image
 *    left to right as if seen from behind. Vertical reflection causes the image to appear upside down.
 *
 *  Acceptable flags are kvImageDoNotTile, kvImageNoFlags.
 *
 *  These functions do not work in place.
 *
 *  All four channel geometry functions (i.e. those that support ARGB8888, ARGB16U, ARGB16S or ARGBFFFF images) work equally well on four channel images
 *  with other channel orderings such as RGBA or BGRA.
 */

VIMAGE_PF vImage_Error    vImageHorizontalReflect_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalReflect_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error    vImageVerticalReflect_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalReflect_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 *     The Rotate90 function does simple 0, 90, 180 or 270 degree rotation according to the value of a rotation constant
 *    passed to it. The images are rotated about their center. If the source and destination sizes do not match, parts
 *    of the image may be cut off. Other parts may have no image, in which case the background color will be drawn there.
 *
 *    CAUTION: 90 and 270 degree rotation does not rotate about the true center of the image if the height of the source image
 *    is an odd number of pixels and the width of the destination image an even number of pixels, or vice versa. This is also
 *    true of the source width and destination height. In this case, you should use the High level rotate function for 90 or
 *    270 rotates so that the resampling can be done to shift the image a half pixel for proper centering. It may be somewhat
 *    faster to widen the destination image by 1 and use the low level shearing functions to resample the image at a a half pixel offset.
 *    For 0 and 180 degree rotates, if the source and destination buffers are a different size, the two heights must have matching
 *    even/oddness and the two widths must have matching even/oddness. Otherwise the image will be rotated and shifted a half pixel
 *    away from the center.
 *
 *    rotationConstant:    0 -- rotate 0 degrees (simply copy the data from src to dest)
 *                1 -- rotate 90 degrees counterclockwise
 *                2 -- rotate 180 degress
 *                3 -- rotate 270 degrees counterclockwise
 *
 *    backColor:    The color of the background. This color will be copied to any place where pixels are revealed because
 *            the input image does not completely cover the output image.
 *
 *  Acceptable flags are kvImageDoNotTile, kvImageNoFlags.
 *
 *  These functions do not work in place.
 *
 *  All four channel geometry functions (i.e. those that support ARGB8888, ARGB16U, ARGB16S or ARGBFFFF images) work equally well on
 *  four channel images with other channel orderings such as RGBA or BGRA.
 */

VIMAGE_PF vImage_Error    vImageRotate90_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate90_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageRotate90_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate90_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageRotate90_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageRotate90_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageRotate90_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, uint8_t rotationConstant, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2,4) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 *    The Shearing functions use resampling to rescale a image and offset it to
 *  fractional pixel offsets. The functions actually shear, resize in one
 *  dimension and translate. All of it is done with fractional pixel precision.
 *  The shear slope is set using the shearSlope parameter. They are intended to
 *  be identical to the off diagonal components of the AffineWarp matrix for the
 *  same shear (i.e. at 1,0 or 0,1). The xTranslate or yTranslate variable may
 *  be used to adjust the position of the destination image in the x and y
 *  directions. Scaling (making the image larger or smaller in one dimension)
 *  is done by adjusting the resampling kernel.
 *
 *  All four channel geometry functions (i.e. those that support ARGB8888,
 *    ARGB16U, ARGB16S or ARGBFFFF images) work equally well on four channel
 *    images with other channel orderings such as RGBA or BGRA.
 *
 *  These functions do not work in place.
 *
 *  Acceptable flags are kvImageEdgeExtend, kvImageBackgroundColor,
 *  kvImageDoNotTile, kvImageNoFlags.
 *  Only one of kvImageEdgeExtend or kvImageBackgroundColor may be used.
 *  If none is used then the edging mode is undefined and the results may be
 *  unpredictable.
 *
 *  The ResamplingFilter is created using vImageNewResamplingFilter or
 *  vImageNewResamplingFilterForFunctionUsingBuffer. The latter gives more
 *  precise control over where the memory is allocated and which filter function
 *  is used, at the expense of having to define / setup the same.
 */

VIMAGE_PF vImage_Error    vImageHorizontalShear_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_Planar16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalShear_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float xTranslate, float shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF vImage_Error    vImageVerticalShear_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_Planar16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, Pixel_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_Planar16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, Pixel_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.10), ios(8.0), watchos(1.0), tvos(8.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter,  const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter,  const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter,  const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalShear_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, float yTranslate, float shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/* Versions of shear functions that take coordinates in double precision */
VIMAGE_PF vImage_Error    vImageHorizontalShearD_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageHorizontalShearD_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageHorizontalShearD_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageHorizontalShearD_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalShearD_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageHorizontalShearD_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double xTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));

VIMAGE_PF vImage_Error    vImageVerticalShearD_Planar8( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_8 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageVerticalShearD_PlanarF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, Pixel_F backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageVerticalShearD_ARGB8888( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter,  const Pixel_8888 backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));
VIMAGE_PF vImage_Error    vImageVerticalShearD_ARGB16U( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter,  const Pixel_ARGB_16U backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalShearD_ARGB16S( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter,  const Pixel_ARGB_16S backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));
VIMAGE_PF vImage_Error    vImageVerticalShearD_ARGBFFFF( const vImage_Buffer *src, const vImage_Buffer *dest, vImagePixelCount srcOffsetToROI_X, vImagePixelCount srcOffsetToROI_Y, double yTranslate, double shearSlope, ResamplingFilter filter, const Pixel_FFFF backColor, vImage_Flags flags ) VIMAGE_NON_NULL(1,2) API_AVAILABLE(macos(10.8), ios(6.0), watchos(1.0), tvos(6.0));

VIMAGE_PF vImage_Error
vImageHorizontalShear_CbCr8(
                            const vImage_Buffer *src,
                            const vImage_Buffer *dest,
                            vImagePixelCount srcOffsetToROI_X,
                            vImagePixelCount srcOffsetToROI_Y,
                            float xTranslate,
                            float shearSlope,
                            ResamplingFilter filter,
                            const Pixel_88 backColor,
                            vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageHorizontalShear_CbCr16U(
                              const vImage_Buffer *src,
                              const vImage_Buffer *dest,
                              vImagePixelCount srcOffsetToROI_X,
                              vImagePixelCount srcOffsetToROI_Y,
                              float xTranslate,
                              float shearSlope,
                              ResamplingFilter filter,
                              const Pixel_16U16U backColor,
                              vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));


VIMAGE_PF vImage_Error
vImageVerticalShear_CbCr8(
                          const vImage_Buffer *src,
                          const vImage_Buffer *dest,
                          vImagePixelCount srcOffsetToROI_X,
                          vImagePixelCount srcOffsetToROI_Y,
                          float yTranslate,
                          float shearSlope,
                          ResamplingFilter filter,
                          const Pixel_88 backColor,
                          vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageVerticalShear_CbCr16U(
                            const vImage_Buffer *src,
                            const vImage_Buffer *dest,
                            vImagePixelCount srcOffsetToROI_X,
                            vImagePixelCount srcOffsetToROI_Y,
                            float yTranslate,
                            float shearSlope,
                            ResamplingFilter filter,
                            const Pixel_16U16U backColor,
                            vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageHorizontalShear_XRGB2101010W(
                                   const vImage_Buffer *src,
                                   const vImage_Buffer *dest,
                                   vImagePixelCount srcOffsetToROI_X,
                                   vImagePixelCount srcOffsetToROI_Y,
                                   float xTranslate,
                                   float shearSlope,
                                   ResamplingFilter filter,
                                   const Pixel_32U backColor,
                                   vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

VIMAGE_PF vImage_Error
vImageVerticalShear_XRGB2101010W(
                                 const vImage_Buffer *src,
                                 const vImage_Buffer *dest,
                                 vImagePixelCount srcOffsetToROI_X,
                                 vImagePixelCount srcOffsetToROI_Y,
                                 float yTranslate,
                                 float shearSlope,
                                 ResamplingFilter filter,
                                 const Pixel_32U backColor,
                                 vImage_Flags flags ) VIMAGE_NON_NULL(1,2)
API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));


/*
 *    The shearing functions use a resampling filter for their work. Before you call these functions you must
 *    create a new filter. The filter must be remade for different scaling factors, but may be reused in different
 *    function calls if the scaling factor is the same.
 *
 *     For general purpose work, use NewResamplingFilter() to create a resampling filter. It currently uses
 *    either Lanczos3 or Lanczos5 filter, depending on whether the kvImageHighQualityResampling bit is set in the flags field.
 *    What filter is used is subject to change.
 *
 *        ResamplingFilter kernel = NewResamplingFilter( theScale, kvImageHighQualityResampling );
 *
 *    Use DestroyResamplingFilter() when you are done with the filter to return the memory it uses to the heap.
 *
 *    The scale parameter sets the level of rescaling to be done. A value of 1.0f leaves the image at its
 *    original size. 2.0f will magnify in one direction to make the image twice as wide / tall. 0.5f will
 *    make it half as wide / tall. Any float is allowed, including negative values, which will have the effect
 *    of flipping the image along that axis. Scaling will happen relative to the left or bottom edge of the image.
 *
 *    Set the kvImageEdgeExtend bit in the flags field if you would like the image buffer edges extended infinitely
 *    rather than use the backColor value. This is useful when you do not wish a background color to bleed into
 *    the edges of your image. This is generally only useful when the shear angle is 0.
 *
 */

VIMAGE_PF ResamplingFilter    vImageNewResamplingFilter( float scale, vImage_Flags flags )    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));
VIMAGE_PF void vImageDestroyResamplingFilter( ResamplingFilter filter )    API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 *    vImageNewResamplingFilter and vImageDestroyResamplingFilter are merely convenience functions to make
 *    the common case of the default resampling filter into a heap allocated buffer easier for you.
 *
 *    If you would like to use your own special purpose resampling filter or need to restrict your use of malloc,
 *    you may instead call NewResamplingFilterForFunctionUsingBuffer(), which creates a filter for a specific
 *    resampling filter function that you provide in the form of a simple y = f(x) function, and writes it into a buffer
 *    that you provide. This function writes the kernel values into a preallocated kernel buffer that you provide. The
 *    kernel buffer should be at least the size of the kernel data, which is given by vImageGetResamplingKernelSize.
 *
 */

VIMAGE_PF vImage_Error    vImageNewResamplingFilterForFunctionUsingBuffer( ResamplingFilter filter,
                                                                          float scale,
                                                                          void (*kernelFunc)( const float *xArray, float *yArray, unsigned long count, void *userData ),
                                                                          float kernelWidth,
                                                                          void *userData,
                                                                          vImage_Flags flags ) VIMAGE_NON_NULL(1) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

VIMAGE_PF size_t vImageGetResamplingFilterSize(  float scale,
                                               void (*kernelFunc)( const float *xArray, float *yArray, unsigned long count, void *userData ),
                                               float kernelWidth,
                                               vImage_Flags flags ) API_AVAILABLE(macos(10.3), ios(5.0), watchos(1.0), tvos(5.0));

/*
 *    The scale parameter is the amount to enlarge or reduce the output image. It is the same meaning
 *    as for the generic resampling kernel.
 *
 *    The kernelFunc defines the shape of the resampling kernel. Here is a sample kernelFunc that does
 *    linear interpolation:
 *
 *        void MyLinearInterpolationFilterFunc( const float *xArray, float *yArray, int count, void *userData )
 *        {
 *            int i;
 *            float sum = 0.0f;
 *
 *            //Calculate kernel values
 *            for( i = 0; i < count; i++ )
 *            {
 *                float unscaledResult = 1.0f - fabs( xArray[i] );    //LERP
 *                yArray[i] = unscaledResult;
 *                sum += unscaledResult;
 *            }
 *
 *            //Make sure the kernel values sum to 1.0. You can use some other value here.
 *            //Values other than 1.0 will cause the image to lighten or darken.
 *            sum = 1.0f / sum;
 *            for( i = 0; i < count; i++ )
 *                yArray[i] *= sum;
 *
 *        }
 *
 *    This specifies the tent line shape centered around 0.0f that defines a linear interpolation kernel.
 *    The array of values you will be asked to provide are the entire resampling window for a single pixel.
 *    The x values are pixel positions relative to the result, and the y values are where you should put
 *    the kernel values for those positions. The x values will range from -kernelWidth to kernelWidth.
 *
 *    The array format is used for efficiency, and also to give you the opportunity to make sure the kernel
 *    adds up to 1.0 or some other desired value. Making sure that the kernel values add up to 1.0 will
 *    prevent interference patterns from emerging in the image. Since the sampling kernel function itself is
 *    sampled during the process of pairing kernel values against sampled pixel data, the sampled kernel data
 *    does not always integrate to the same value that the parent function does, so usually we must normalize
 *    it to prevent interference/moiree patterns from to emerging in the image as the pixels in the input buffer
 *    drift in and out of phase with those in the output buffer. Sums less than 1.0f will cause the image to
 *    darken. Gain values greater than 1.0 may cause significant clipping for integer pixel data. Kernel values
 *    are clipped at approximately +- 2.0 for all integer data. If your filter function values exceed +- 2.0, then
 *    you will need to use the floating point data format to avoid overflow.
 *
 *    If you pass NULL for the kernelFunc, vImage will use its default resampling kernel. You can control
 *    whether the high quality resampling kernel is used or not by setting the kvImageHighQualityResampling
 *    bit in the flags field. This allows you to use the default resampling kernel in your own buffer. If you do
 *    not pass NULL for the kernelFunc, the kernelFunc and your userData parameter must be valid for the lifetime
 *    of the ResamplingKernel.
 *
 *    The kernelWidth parameter controls the limiting values of x, when creating the new kernel. A kernelWidth
 *    of 1.0 would allow input samples between -1.0 and 1.0 pixels away from the destination pixel to influence
 *    the result. For Lanczos3, we would use a kernelWidth of 3.0f, because the Lanczos3 function is defined
 *    over the range -3.0f to 3.0f. If your function is only defined over part of the range -x...x, then have
 *    your function return zero for the rest of the function. The computational cost of the shearing functions
 *    is proportional to the kernelWidth.
 *
 *    The userData field is passed back unchanged to your kernelFunc when it is called.
 *
 *    The only flag honored by this function in this release is kvImageHighQualityResampling, which is only
 *    used if NULL is passed for the kernelFunc to decide whether to use the normal or high quality default
 *    resampling filter. Except in this case, you should in general pass kvImageNoFlags in the flags field
 *     to avoid conflicting with future implementations where flags may be used.
 *
 *    Do not use DestroyResamplingKernel() to free the memory used by kernels created with NewResamplingKernelForFunction().
 *    Since you are responsible for allocating that buffer, you are also responsible for destroying it, using the matching
 *    free function.
 *
 *    Do not call vImageNewResamplingKernelForFunctionUsingBuffer() on a buffer created with vImageNewResamplingKernel().
 *    The buffer sizes may be different. You must use vImageGetResamplingKernelSize() to find out the new buffer size,
 *    allocate a buffer yourself using something like malloc(), then call vImageNewResamplingKernelForFunctionUsingBuffer().
 */


/*
 *  vImageGetSamplingFilterExtent
 *
 *  returns the maximum sampling radius for the resampling filter.  This is the maximum distance from any pixel
 *  that the filter will look either horizontally or vertically, depending on whether vImageHorizontalShear or
 *  vImageVerticalShear is used.  It is analogous to kernelWidth in vImageNewResamplingFilterForFunctionUsingBuffer,
 *  but might be slightly larger to allow for extra slop when dealing with sub-pixel coordinates during resampling.
 *
 *      filter      a valid ResamplingFilter
 *      flags       the flags you intend to pass to vImage{Horizontal/Vertical}Shear_<fmt>.
 */
VIMAGE_PF vImagePixelCount vImageGetResamplingFilterExtent( ResamplingFilter filter, vImage_Flags flags )  VIMAGE_NON_NULL(1)  API_AVAILABLE(macos(10.9), ios(7.0), watchos(1.0), tvos(7.0));

#ifdef __cplusplus
}
#endif

#endif


// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vecLibTypes.h
/*
     File:       vecLib/vecLibTypes.h
 
     Contains:   Master include for vecLib framework
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 2000-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
     Various types are defined here to use with vecLib.  For more generally
     useful vector/SIMD types, please consider the headers in /usr/include/simd.
*/
#ifndef __VECLIBTYPES__
#define __VECLIBTYPES__

#include <os/availability.h>

#if PRAGMA_ONCE
#pragma once
#endif


#pragma options align=power

#if defined(__ppc__) || defined(__ppc64__)


/*
    The goal of the following preprocessor statements is to define the
    preprocessor symbol _AltiVecPIMLanguageExtensionsAreEnabled if and only if
    the AltiVec high-level language programming interface is enabled.  We
    designed the statements to accommodate some non-Apple compilers but cannot
    assure behavior with non-Apple products.

    Apple GCC version 3.3 and earlier versions defined the preprocessor symbol
    __VEC__ if and only if the AltiVec programming interface were enabled.
    Later versions define __VEC__ when the AltiVec instruction set is enabled
    (and thus available for the compiler's assembly code generation) even if
    the programming interface is not enabled (and thus not available at the
    source code level).  This occurs, for example, when -maltivec is specified
    but neither -faltivec is specified nor <altivec.h> is included.

    Due to this change, code that formerly used __VEC__ to select source code
    with or without use of the AltiVec programming interface should now use
    _AltiVecPIMLanguageExtensionsAreEnabled.

    For more information about the interface, see AltiVec Technology
    Programming Interface Manual (ALTIVECPIM/D 6/1999 Rev. 0, published by
    Motorola Inc. [now Freescale, Inc.]).
 */

#if			defined _ALTIVEC_H \
		||	(defined __APPLE_CC__ && __APPLE_ALTIVEC__) \
		|| 	(!defined __GNUC__ && defined __VEC__)
	#define _AltiVecPIMLanguageExtensionsAreEnabled
#endif

#if defined _AltiVecPIMLanguageExtensionsAreEnabled
	typedef __vector unsigned char            vUInt8;
	typedef __vector signed char              vSInt8;
	typedef __vector unsigned short           vUInt16;
	typedef __vector signed short             vSInt16;
	typedef __vector unsigned int             vUInt32;
	typedef __vector signed int               vSInt32;
	typedef __vector float                    vFloat;
	typedef __vector bool int                 vBool32;

    /* 
     * GCC allows us to create a vDouble type, even on AltiVec which has no double precision vector 
     * instructions (apart from boolean operations which are type agnostic). You can use standard 
     * operators: +-* etc. with the vDouble type. GCC will use scalar code on PowerPC to do the work. 
     * The type is provided for developers interested in writing shared Intel-PowerPC code. 
     */
    #if defined(__GNUC__) && ! defined( __XLC__ )
        #if defined(__GNUC_MINOR__) && (((__GNUC__ == 3) && (__GNUC_MINOR__ <= 3)) || (__GNUC__ < 3))
        #else
            #define __VECLIBTYPES_VDOUBLE__ 1
            typedef double vDouble         __attribute__ ((__vector_size__ (16)));
        #endif
    #endif

#endif	/* defined _AltiVecPIMLanguageExtensionsAreEnabled */

#elif defined(__i386__) || defined(__x86_64__)
#include <immintrin.h>
#ifdef __SSE__
#if defined(__GNUC__)
typedef float                   vFloat          __attribute__ ((__vector_size__ (16)));
#else /* not __GNUC__ */
typedef __m128                          vFloat;
#endif /* __GNUC__ */
#endif  /* defined(__SSE__) */

#ifdef __SSE2__
    #define __VECLIBTYPES_VDOUBLE__ 1
    
    #if defined(__GNUC__)
        #if defined(__GNUC_MINOR__) && (((__GNUC__ == 3) && (__GNUC_MINOR__ <= 3)) || (__GNUC__ < 3))
            typedef __m128i vUInt8;
            typedef __m128i vSInt8;
            typedef __m128i vUInt16;
            typedef __m128i vSInt16;
            typedef __m128i vUInt32;
            typedef __m128i vSInt32;
            typedef __m128i vBool32;
            typedef __m128i vUInt64;
            typedef __m128i vSInt64;
            typedef __m128d vDouble;
        #else /* gcc-3.5 or later */
            typedef unsigned char           vUInt8          __attribute__ ((__vector_size__ (16)));
            typedef char                    vSInt8          __attribute__ ((__vector_size__ (16)));
            typedef unsigned short          vUInt16         __attribute__ ((__vector_size__ (16)));
            typedef short                   vSInt16         __attribute__ ((__vector_size__ (16)));
            typedef unsigned int            vUInt32         __attribute__ ((__vector_size__ (16)));
            typedef int                     vSInt32         __attribute__ ((__vector_size__ (16)));
            typedef unsigned int            vBool32         __attribute__ ((__vector_size__ (16)));
            typedef unsigned long long      vUInt64         __attribute__ ((__vector_size__ (16)));
            typedef long long               vSInt64         __attribute__ ((__vector_size__ (16)));
            typedef double                  vDouble         __attribute__ ((__vector_size__ (16)));
        #endif /* __GNUC__ <= 3.3 */
    #else /* not __GNUC__ */
        typedef __m128i                         vUInt8;
        typedef __m128i                         vSInt8;
        typedef __m128i                         vUInt16;
        typedef __m128i                         vSInt16;
        typedef __m128i                         vUInt32;
        typedef __m128i                         vSInt32;
        typedef __m128i                         vBool32;
        typedef __m128i                         vUInt64;
        typedef __m128i                         vSInt64;
        typedef __m128d                         vDouble;
    #endif /* __GNUC__ */
#endif  /* defined(__SSE2__) */

#elif defined __arm__ && defined __ARM_NEON__

	#if !defined ARM_NEON_GCC_COMPATIBILITY  

		#define ARM_NEON_GCC_COMPATIBILITY

		#if \
			defined __ARM_NEON_H && \
			defined __GNUC__ && \
			! defined __clang__ && \
			! defined SQUELCH_VECLIB_WARNINGS_ABOUT_BROKEN_NEON_TYPES

			/*	GCC decided to make neon vector types using something other
				than basic C types as the underlying element by default. This
				prevents the below types from being used with the functions in
				arm_neon.h.  GCC did put in a workaround, however.  Define
				ARM_NEON_GCC_COMPATIBILITY before including arm_neon.h and then
				everything will start working as designed. You are getting this
				warning  because some other header included arm_neon.h without
				defining ARM_NEON_GCC_COMPATIBILITY before we arrived here. 

				#define SQUELCH_VECLIB_WARNINGS_ABOUT_BROKEN_NEON_TYPES to silence this warning
			*/
			#warning "arm_neon.h was included without #define ARM_NEON_GCC_COMPATIBILITY.  Vector types defined in vecLibTypes.h, such as vUInt8, might not work with NEON intrinsics."

		#endif

	#endif	/* !defined ARM_NEON_GCC_COMPATIBILITY */

	#include <arm_neon.h>
	typedef unsigned char  vUInt8  __attribute__((__vector_size__(16), __aligned__(16)));
	typedef signed char    vSInt8  __attribute__((__vector_size__(16), __aligned__(16)));
	typedef unsigned short vUInt16 __attribute__((__vector_size__(16), __aligned__(16)));
	typedef signed short   vSInt16 __attribute__((__vector_size__(16), __aligned__(16)));
	typedef unsigned int   vUInt32 __attribute__((__vector_size__(16), __aligned__(16)));
	typedef signed int     vSInt32 __attribute__((__vector_size__(16), __aligned__(16)));
	typedef float          vFloat  __attribute__((__vector_size__(16), __aligned__(16)));
	typedef double         vDouble __attribute__((__vector_size__(16), __aligned__(16)));
	typedef unsigned int   vBool32 __attribute__((__vector_size__(16), __aligned__(16)));

#else

	typedef unsigned char       vUInt8  __attribute__((__vector_size__(16)));
	typedef signed char         vSInt8  __attribute__((__vector_size__(16)));
	typedef unsigned short      vUInt16 __attribute__((__vector_size__(16)));
	typedef signed short        vSInt16 __attribute__((__vector_size__(16)));
	typedef unsigned int        vUInt32 __attribute__((__vector_size__(16)));
	typedef signed int          vSInt32 __attribute__((__vector_size__(16)));
    typedef long long           vSInt64 __attribute__((__vector_size__(16)));
    typedef unsigned long long  vUInt64 __attribute__((__vector_size__(16)));
	typedef float               vFloat  __attribute__((__vector_size__(16)));
	typedef double              vDouble __attribute__((__vector_size__(16)));
	typedef unsigned int        vBool32 __attribute__((__vector_size__(16)));

#endif


#pragma options align=reset


#endif /* __VECLIBTYPES__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vDSP_translate.h
/*
     File:       vecLib/vDSP_translate.h
 
     Contains:   Master include for vecLib framework
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 2000-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
*/
#ifndef __VDSP_TRANSLATE__
#define __VDSP_TRANSLATE__


#include <os/availability.h>

#if PRAGMA_ONCE
#pragma once
#endif

#if defined __MAC_OS_X_VERSION_MIN_REQUIRED && __MAC_OS_X_VERSION_MIN_REQUIRED < __MAC_10_11

/*	When compiling for an OS X target earlier than 10.11, the legacy translates
	are defined.
*/
#define vDSP_create_fftsetup create_fftsetup
#define vDSP_destroy_fftsetup destroy_fftsetup
#define vDSP_ctoz ctoz
#define vDSP_ztoc ztoc
#define vDSP_fft_zip fft_zip
#define vDSP_fft_zipt fft_zipt
#define vDSP_fft_zop fft_zop
#define vDSP_fft_zopt fft_zopt
#define vDSP_fft_zrip fft_zrip
#define vDSP_fft_zript fft_zript
#define vDSP_fft_zrop fft_zrop
#define vDSP_fft_zropt fft_zropt
#define vDSP_fft2d_zip fft2d_zip
#define vDSP_fft2d_zipt fft2d_zipt
#define vDSP_fft2d_zop fft2d_zop
#define vDSP_fft2d_zopt fft2d_zopt
#define vDSP_fft2d_zrip fft2d_zrip
#define vDSP_fft2d_zript fft2d_zript
#define vDSP_fft2d_zrop fft2d_zrop
#define vDSP_fft2d_zropt fft2d_zropt
#define vDSP_fft3_zop fft3_zop
#define vDSP_fft5_zop fft5_zop
#define vDSP_fft_cip fft_cip
#define vDSP_fft_cipt fft_cipt
#define vDSP_fft_cop fft_cop
#define vDSP_fft_copt fft_copt
#define vDSP_fftm_zop fftm_zop
#define vDSP_fftm_zopt fftm_zopt
#define vDSP_fftm_zip fftm_zip
#define vDSP_fftm_zipt fftm_zipt
#define vDSP_fftm_zrop fftm_zrop
#define vDSP_fftm_zropt fftm_zropt
#define vDSP_fftm_zrip fftm_zrip
#define vDSP_fftm_zript fftm_zript
#define vDSP_f3x3 f3x3
#define vDSP_f5x5 f5x5
#define vDSP_conv conv
#define vDSP_dotpr dotpr
#define vDSP_imgfir imgfir
#define vDSP_mtrans mtrans
#define vDSP_mmul mmul
#define vDSP_vadd vadd
#define vDSP_vsub vsub
#define vDSP_vmul vmul
#define vDSP_vsmul vsmul
#define vDSP_vam vam
#define vDSP_vsq vsq
#define vDSP_vssq vssq
#define vDSP_zvadd zvadd
#define vDSP_zvsub zvsub
#define vDSP_zdotpr zdotpr
#define vDSP_zconv zconv
#define vDSP_zvcma zvcma
#define vDSP_zvmul zvmul
#define vDSP_zidotpr zidotpr
#define vDSP_zmma zmma
#define vDSP_zmms zmms
#define vDSP_zmsm zmsm
#define vDSP_zmmul zmmul
#define vDSP_zrvadd zrvadd
#define vDSP_zrvmul zrvmul
#define vDSP_zrvsub zrvsub
#define vDSP_zrdotpr zrdotpr
#define vDSP_fft_zipD fft_zipD
#define vDSP_fft_ziptD fft_ziptD
#define vDSP_fft_zopD fft_zopD
#define vDSP_fft_zoptD fft_zoptD
#define vDSP_fft_zripD fft_zripD
#define vDSP_fft_zriptD fft_zriptD
#define vDSP_fft_zropD fft_zropD
#define vDSP_fft_zroptD fft_zroptD
#define vDSP_fft2d_zipD fft2d_zipD
#define vDSP_fft2d_ziptD fft2d_ziptD
#define vDSP_fft2d_zopD fft2d_zopD
#define vDSP_fft2d_zoptD fft2d_zoptD
#define vDSP_fft2d_zripD fft2d_zripD
#define vDSP_fft2d_zriptD fft2d_zriptD
#define vDSP_fft2d_zropD fft2d_zropD
#define vDSP_fft2d_zroptD fft2d_zroptD
#define vDSP_fftm_zipD fftm_zipD
#define vDSP_fftm_ziptD fftm_ziptD
#define vDSP_fftm_zopD fftm_zopD
#define vDSP_fftm_zoptD fftm_zoptD
#define vDSP_fftm_zripD fftm_zripD
#define vDSP_fftm_zriptD fftm_zriptD
#define vDSP_fftm_zropD fftm_zropD
#define vDSP_fftm_zroptD fftm_zroptD
#define vDSP_fft3_zopD fft3_zopD
#define vDSP_fft5_zopD fft5_zopD
#define vDSP_ctozD ctozD
#define vDSP_ztocD ztocD
#define vDSP_vsmulD vsmulD
#define vDSP_create_fftsetupD create_fftsetupD
#define vDSP_destroy_fftsetupD destroy_fftsetupD
#define vDSP_f3x3D f3x3D
#define vDSP_f5x5D f5x5D
#define vDSP_convD convD
#define vDSP_dotprD dotprD
#define vDSP_imgfirD imgfirD
#define vDSP_mtransD mtransD
#define vDSP_mmulD mmulD
#define vDSP_vaddD vaddD
#define vDSP_vsubD vsubD
#define vDSP_vmulD vmulD
#define vDSP_vamD vamD
#define vDSP_vsqD vsqD
#define vDSP_vssqD vssqD
#define vDSP_zvaddD zvaddD
#define vDSP_zvsubD zvsubD
#define vDSP_zdotprD zdotprD
#define vDSP_zconvD zconvD
#define vDSP_zvcmaD zvcmaD
#define vDSP_zvmulD zvmulD
#define vDSP_zidotprD zidotprD
#define vDSP_zmmaD zmmaD
#define vDSP_zmmsD zmmsD
#define vDSP_zmsmD zmsmD
#define vDSP_zmmulD zmmulD
#define vDSP_zrvaddD zrvaddD
#define vDSP_zrvmulD zrvmulD
#define vDSP_zrvsubD zrvsubD
#define vDSP_zrdotprD zrdotprD

#else	//	#if __MAC_OS_X_VERSION_MIN_REQUIRED < __MAC_10_11

/*	When compiling for OS X 10.11 or later, the legacy translates are not
	defined, and the old names are marked deprecated.  To do this, we define
	a symbol telling vDSP.h to make such declarations.
*/
#define	vDSP_DeprecateTranslations


#endif	//	#if __MAC_OS_X_VERSION_MIN_REQUIRED < __MAC_10_11


#endif /* __VDSP_TRANSLATE__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/BNNS/bnns.h
// Basic Neural Network Subroutines (BNNS)

#ifndef __BNNS_HEADER__
#define __BNNS_HEADER__

#include <stddef.h>
#include <stdint.h>

// Availability
#if __has_include( <Availability.h> )
#include <Availability.h>
#else
#define __API_AVAILABLE(...)
#endif

#ifdef __cplusplus
extern "C" {
#endif

// Nullability
#if __has_feature(assume_nonnull)
  _Pragma("clang assume_nonnull begin")
#else
#define _Null_unspecified
#define _Nullable
#define _Nonnull
#endif

/*

  Logging

  When a parameter is invalid or an internal error occurs, an error message will be logged.
  Some combinations of parameters may not be supported. In that case, an info message will be logged.

*/

/*!

@abstract Type for user-provided memory allocation function

@discussion Should conform to posix_memalign(), and must be compatible with the memory deallocation function.

@param memptr Pointer to a <tt>(void *)</tt> receiving the address of the allocated memory.
@param alignment Requested alignment, must be a power of 2, and at least <tt>sizeof(void *)</tt>.
@param size Number of bytes to allocate.

@return 0 on success, or an error value on failure.

*/
typedef int (*BNNSAlloc)(void * _Nullable * _Nullable memptr, size_t alignment, size_t size);

/*!

@abstract Type for user-provided memory deallocation function

@discussion Should conform to free(), and must be compatible with the memory allocation function.

@param ptr Address of the block to release.

*/
typedef void (*BNNSFree)(void * _Null_unspecified ptr);

#pragma mark - Constants

/*!
 
@abstract Storage data type

@constant BNNSDataTypeFloatBit
Common bit to floating point types, this constant is not a valid type

@constant BNNSDataTypeFloat16
16-bit half precision floating point

@constant BNNSDataTypeFloat32
32-bit single precision floating point

@constant BNNSDataTypeIntBit
Common bit to signed integer types, this constant is not a valid type

@constant BNNSDataTypeInt8
8-bit signed integer

@constant BNNSDataTypeInt16
16-bit signed integer

@constant BNNSDataTypeIn32
32-bit signed integer

@constant BNNSDataTypeUIntBit
Common bit to unsigned integer types, this constant is not a valid type (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSDataTypeUInt8
8-bit unsigned integer (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSDataTypeUInt16
16-bit unsigned integer (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSDataTypeUIn32
32-bit unsigned integer (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSDataTypeIndexedBit
Common bit to indexed floating point types, this constant is not a valid type

@constant BNNSDataTypeIndexed8
8-bit unsigned indices into a floating point conversion table (256 values)

*/
typedef enum {

  BNNSDataTypeFloatBit            = 0x10000,
  BNNSDataTypeFloat16             = BNNSDataTypeFloatBit | 16,
  BNNSDataTypeFloat32             = BNNSDataTypeFloatBit | 32,

  BNNSDataTypeIntBit              = 0x20000,
  BNNSDataTypeInt8                = BNNSDataTypeIntBit | 8,
  BNNSDataTypeInt16               = BNNSDataTypeIntBit | 16,
  BNNSDataTypeInt32               = BNNSDataTypeIntBit | 32,

  // The UInt fields are available in macOS 10.13, iOS 11, tvOS 11, watchOS 4
  BNNSDataTypeUIntBit             = 0x40000,
  BNNSDataTypeUInt8               = BNNSDataTypeUIntBit | 8,
  BNNSDataTypeUInt16              = BNNSDataTypeUIntBit | 16,
  BNNSDataTypeUInt32              = BNNSDataTypeUIntBit | 32,

  BNNSDataTypeIndexedBit          = 0x80000,
  BNNSDataTypeIndexed8            = BNNSDataTypeIndexedBit | 8,

} BNNSDataType;

/*!

@abstract Pooling layer function

@discussion
In the definitions below, the input sample is <tt>X<sub>i</sub></tt> and has <tt>N</tt> elements.

@constant BNNSPoolingFunctionMax
max(X<sub>i</sub>)

@constant BNNSPoolingFunctionAverage
&sum;<sub>i</sub> X<sub>i</sub> / N

*/
typedef enum {

  BNNSPoolingFunctionMax          = 0,
  BNNSPoolingFunctionAverage      = 1,

} BNNSPoolingFunction;

/*!

@abstract Activation layer function

@constant BNNSActivationFunctionIdentity
x

@constant BNNSActivationFunctionRectifiedLinear
0 if x<0, and x if x>=0

@constant BNNSActivationFunctionLeakyRectifiedLinear
alpha*x if x<0, and x if x>=0

@constant BNNSActivationFunctionSigmoid
sigmoid(x)

@constant BNNSActivationFunctionTanh
tanh(x)

@constant BNNSActivationFunctionScaledTanh
alpha*tanh(x*beta)

@constant BNNSActivationFunctionAbs
abs(x) (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSActivationFunctionLinear
alpha*x (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSActivationFunctionClamp
min(max(x, alpha), beta) (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSActivationFunctionIntegerLinearSaturate
Saturate<output_type>((iscale * x + ioffset) >> ishift)
This is an arithmetic shift, preserving sign. (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSActivationFunctionIntegerLinearSaturatePerChannel
Saturate<output_type>((iscale_per_channel[channel] * x + ioffset_per_channel[channel]) >> ishift_per_channel[channel]) for each channel
This is an arithmetic shift, preserving sign. (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@constant BNNSActivationFunctionSoftmax
softmax(x)_i = exp(x_i) / ( sum_i exp(x_i) )
(macOS 10.13, iOS 11, tvOS 11, watchOS 4)
 
*/
typedef enum {

  BNNSActivationFunctionIdentity                        =  0,
  BNNSActivationFunctionRectifiedLinear                 =  1,
  BNNSActivationFunctionLeakyRectifiedLinear            =  2,
  BNNSActivationFunctionSigmoid                         =  3,
  BNNSActivationFunctionTanh                            =  4,
  BNNSActivationFunctionScaledTanh                      =  5,
  BNNSActivationFunctionAbs                             =  6,

  // The following fields are available in macOS 10.13, iOS 11, tvOS 11, watchOS 4

  BNNSActivationFunctionLinear                          =  7,
  BNNSActivationFunctionClamp                           =  8,
  BNNSActivationFunctionIntegerLinearSaturate           =  9,
  BNNSActivationFunctionIntegerLinearSaturatePerChannel = 10,
  BNNSActivationFunctionSoftmax                         = 11,

} BNNSActivationFunction;

/*!

@abstract Filter creation flags

@constant BNNSFlagsUseClientPtr

Instructs the filter to keep the pointers provided by the client at creation time (weights, bias), and work directly from this data. In that
case, the client must ensure these pointers remain valid through the entire lifetime of the filter.

If not set, the filter creation function must allocate buffers, and keep an internal copy of the data. In that case, the client doesn't have
to keep the pointers valid after the filter is created.

*/
typedef enum {

  BNNSFlagsUseClientPtr                         = 0x0001,

} BNNSFlags;

#pragma mark - Data formats

/*!

@abstract Image stack descriptor

@discussion
An image stack is a sequence of images with the same width and height. Each image in the sequence is called a channel.
For example, a RGB image will be stored as three separate channels. A pixel has only one scalar value, stored using the type
described by <tt>data_type</tt>.

Pixel <tt>P(c,x,y)</tt> at position <tt>(x,y)</tt> in channel <tt>c</tt> is stored in
<tt>data[x + row_stride * y + image_stride * c]</tt>, with
<tt>x=0..width-1</tt>,
<tt>y=0..height-1</tt>,
<tt>c=0..channels-1</tt>. <tt>row_stride &geq; width</tt>, <tt>image_stride &geq; row_stride * height</tt>.

Int<n> types are converted to floating point using float Y = DATA_SCALE * (float)X + DATA_BIAS, and back to integer using Int<n> X = convert_and_saturate(Y / DATA_SCALE - DATA_BIAS)

@field width Image width
@field height Image height
@field channels Number of images in stack
@field row_stride Increment (in values) between image rows
@field image_stride Increment (in values) between image channels
@field data_type Storage data type for image values. INDEXED data types are not allowed here.
@field data_scale Conversion scale for image values, used for INT,UINT data types only
@field data_bias Conversion bias for image values, used for INT,UINT data types only

*/
typedef struct {

  size_t width;
  size_t height;
  size_t channels;
  size_t row_stride;
  size_t image_stride;
  
  BNNSDataType data_type;
  float        data_scale;
  float        data_bias;

} BNNSImageStackDescriptor;

/*!

@abstract Vector format descriptor

@discussion
Represents a vector of dimension <tt>size</tt>.
Each vector element is a scalar value, stored using the type specified in <tt>data_type</tt>.

Component <tt>V(i)</tt> at index <tt>i</tt> is stored in <tt>data[i]</tt>, with <tt>i=0..size-1</tt>.

Int<n> types are converted to floating point using float Y = DATA_SCALE * (float)X + DATA_BIAS, and back to integer using Int<n> X = convert_and_saturate(Y / DATA_SCALE - DATA_BIAS)

@field size Vector dimension
@field data_type Storage data type for vector values. INDEXED data types are not allowed here.
@field data_scale Conversion scale for vector values, used for INT,UINT data types only
@field data_bias Conversion bias for vector values, used for INT,UINT data types only

*/
typedef struct {

  size_t size;
  
  BNNSDataType data_type;
  float        data_scale;
  float        data_bias;

} BNNSVectorDescriptor;

#pragma mark - Layer parameters

/*!

@abstract Common layer parameters

@discussion Int<n> types are converted to floating point using float Y = DATA_SCALE * (float)X + DATA_BIAS, and back to integer using Int<n> X = convert_and_saturate(Y / DATA_SCALE - DATA_BIAS)

@field data Pointer to layer values (weights, bias), layout and size are specific to each layer
@field data_type Storage data type for the values stored in <tt>data</tt>
@field data_scale Conversion scale for values, used for INT data types only, ignored for INDEXED and FLOAT data types
@field data_bias Conversion bias for values, used for INT data types only, ignored for INDEXED and FLOAT data types
@field data_table Conversion table (256 values) for indexed floating point data, used for INDEXED data types only

*/
typedef struct {

  const void * _Nullable  data;
  BNNSDataType            data_type;
  float                   data_scale;
  float                   data_bias;
  const float * _Nullable data_table;

} BNNSLayerData;

/*!

@abstract Common activation function parameters

@field function Activation function
@field alpha Parameter to the activation function
@field beta Parameter to the activation function

@field iscale Scale for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)
@field ioffset Offset for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)
@field ishift Shift for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

@field iscale_per_channel Scale per channel for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)
@field ioffset_per_channel Offset per channel for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)
@field ishift_per_channel Shift per channel for integer functions (macOS 10.13, iOS 11, tvOS 11, watchOS 4)

*/
typedef struct {

  BNNSActivationFunction function;
  float alpha;
  float beta;

  // The following fields are available in macOS 10.13, iOS 11, tvOS 11, watchOS 4

  int32_t iscale;
  int32_t ioffset;
  int32_t ishift;

  const int32_t * _Nullable iscale_per_channel;
  const int32_t * _Nullable ioffset_per_channel;
  const int32_t * _Nullable ishift_per_channel;

} BNNSActivation;

/*!

@abstract Convolution parameters

@discussion
The convolution product Output = Weights &times; Input is defined as follows.  Pixel <tt>Output(o,x,y)</tt> of the output image stack receives:
<br><tt>Output(o,x,y) = &sum;<sub>i,kx,ky</sub> Weight(o,i,kx,ky) * Input(i,x_stride * x + kx,y_stride * y + ky)</tt> with
<tt>kx=0..k_width-1</tt>, <tt>ky=0..k_height-1</tt>,
<tt>i=0..in_channels-1</tt>, <tt>o=0..out_channels-1</tt>,
<tt>x=0..out_width-1</tt>, <tt>y=0..out_height-1</tt>.

After the convolution is applied, the output is updated with bias and/or activation function as follows:
<br><tt>Output(o,x,y) = ActivationFunction( Bias(o) + Output(o,x,y) )</tt>.

Dimensions must satisfy:
<br><tt>in_width + 2 * x_padding = x_stride * ( out_width - 1 ) + k_width</tt>, and <tt>in_height + 2 * y_padding = y_stride * ( out_height - 1 ) + k_height</tt>.
<br>A common use case is <tt>x_stride=y_stride=1</tt>, and <tt>x_padding=y_padding=0</tt>. In that case, <tt>in_width = out_width + k_width - 1</tt>, and <tt>in_height = out_height + k_height - 1</tt>.

Padding is a border of 0 values virtually added to the input image.

Coefficient <tt>Weight(o,i,kx,ky)</tt> for output image <tt>o=0..out_channels-1</tt>, input image <tt>i=0..in_channels-1</tt>, and kernel point (kx,ky) is
stored in <tt>weights[kx + k_width * (ky + k_height * (i + in_channels * o))]</tt>, where
the convolution kernel dimensions are <tt>k_width,k_height</tt>.

@field x_stride X increment in the input image
@field y_stride Y increment in the input image
@field x_padding X padding, virtual 0 values added to the left and right of each channel of the input stack
@field y_padding Y padding, virtual 0 values added to the top and bottom of each channel of the input stack
@field k_width Width of the convolution kernel
@field k_height Height of the convolution kernel
@field in_channels Number of input channels
@field out_channels Number of output channels

@field weights Convolution weights, <tt>k_width * k_height * in_channels * out_channels</tt> values, with the layout described in the discussion
@field bias Layer bias, <tt>out_channels</tt> values, one for each output channel
@field activation Layer activation function

*/
typedef struct {

  size_t x_stride;
  size_t y_stride;
  size_t x_padding;
  size_t y_padding;
  size_t k_width;
  size_t k_height;
  size_t in_channels;
  size_t out_channels;

  BNNSLayerData weights;
  BNNSLayerData bias;
  BNNSActivation activation;
  
} BNNSConvolutionLayerParameters;

/*!

@abstract Fully connected layer parameters

@discussion
The output of a fully connected layer is the result of a matrix-vector product.
The output vector is defined by <tt>Output(o) = &sum;<sub>i</sub> Weight(o,i) * Input(i)</tt> for <tt>i=0..in_size-1</tt>, <tt>o=0..out_size-1</tt>.

Coefficient <tt>Weight(o,i)</tt> is stored in <tt>weights[i + o * in_size]</tt>.

After the matrix product, the output is updated with bias and/or activation function as follows:
<br><tt>Output(o) = ActivationFunction( Bias(o) + Output(o) )</tt>.

@field in_size Size of input vector
@field out_size Size of output vector

@field weights Matrix coefficients, <tt>in_size * out_size</tt> values, with the layout described in the discussion
@field bias Layer bias, <tt>out_size</tt> values, one for each output component
@field activation Layer activation function

*/
typedef struct {

  size_t in_size;
  size_t out_size;

  BNNSLayerData weights;
  BNNSLayerData bias;
  BNNSActivation activation;
  
} BNNSFullyConnectedLayerParameters;

/*!

@abstract Pooling layer parameters

@discussion
The pooling is defined as follows.  Pixel <tt>Output(o,x,y)</tt> of the output image stack receives:
<br><tt>Output(o,x,y) = PoolingFunction( Input(o,x_stride * x + kx,y_stride * y + ky) )</tt> with <tt>kx=0..k_width-1</tt>, <tt>ky=0..k_height-1</tt>,
<tt>o=0..out_channels-1</tt>, <tt>x=0..out_width-1</tt>, <tt>y=0..out_height-1</tt>.

After the pooling is applied, the output is updated with bias and/or activation function as follows:
<br><tt>Output(o,x,y) = ActivationFunction( Bias(o) + Output(o,x,y) )</tt>.

Dimensions must satisfy:
<br><tt>in_width + 2 * x_padding >= x_stride * (out_width - 1) + 1</tt>,
<br><tt>iin_height + 2 * y_padding >= p->y_stride * (o->height - 1) + 1</tt>.

Padding is a border of 0 values virtually added to the input image.

@field x_stride X increment in the input image
@field y_stride Y increment in the input image
@field x_padding X padding, virtual 0 values added to the left and right of each channel of the input stack
@field y_padding Y padding, virtual 0 values added to the top and bottom of each channel of the input stack
@field k_width Width of the pooling kernel
@field k_height Height of the pooling kernel
@field in_channels Number of input channels
@field out_channels Number of output channels

@field pooling_function Selects the pooling function to apply to each sample
@field bias Layer bias, <tt>out_channels</tt> values
@field activation Layer activation function

*/
typedef struct {

  size_t x_stride;
  size_t y_stride;
  size_t x_padding;
  size_t y_padding;
  size_t k_width;
  size_t k_height;
  size_t in_channels;
  size_t out_channels;
  
  BNNSPoolingFunction pooling_function;
  BNNSLayerData bias;
  BNNSActivation activation;

} BNNSPoolingLayerParameters;

#pragma mark - Filter parameters

/*!

@abstract Common filter parameters

@field flags
A logical OR of zero or more values from BNNSFlags.

@field n_threads
If 0, use the best number of threads for the current machine.
Otherwise, specifies the number of worker threads to execute.

@field alloc_memory
If not NULL, will be called to allocate memory. Otherwise, posix_memalign() will be called.
Must be compatible with the free_memory function.

@field free_memory
If not NULL, will be called to deallocate memory. Otherwise, free() will be called.
Must be compatible with the alloc_memory function.

*/
typedef struct {

  uint32_t flags;
  size_t n_threads;
  BNNSAlloc _Nullable alloc_memory;
  BNNSFree _Nullable free_memory;

} BNNSFilterParameters;

#pragma mark - Filter creation

/*!

@abstract Filter object

*/
typedef void * _Nullable BNNSFilter;

/*!

@abstract Create a convolution layer filter

@discussion
Creates a filter applying the convolution described in <tt>layer_params</tt>.
Some combinations of the parameters may not be supported, in which case the call will fail.

@param in_desc Input image stack descriptor
@param out_desc Output image stack descriptor
@param layer_params Layer parameters and weights
@param filter_params Filter runtime parameters, may be NULL for default parameters

@return A new non-NULL filter on success, and NULL on failure.

*/
BNNSFilter BNNSFilterCreateConvolutionLayer(const BNNSImageStackDescriptor * in_desc,
                                            const BNNSImageStackDescriptor * out_desc,
                                            const BNNSConvolutionLayerParameters * layer_params,
                                            const BNNSFilterParameters * _Nullable filter_params)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!

@abstract Create a fully connected layer filter

@discussion
Creates a filter applying the fully connected layer described in <tt>layer_params</tt>.
Some combinations of the parameters may not be supported, in which case the call will fail.

@param in_desc Input vector descriptor
@param out_desc Output vector descriptor
@param layer_params Layer parameters and weights
@param filter_params Filter runtime parameters, may be NULL for default parameters

@return A new non-NULL filter on success, and NULL on failure.

*/
BNNSFilter BNNSFilterCreateFullyConnectedLayer(const BNNSVectorDescriptor * in_desc,
                                               const BNNSVectorDescriptor * out_desc,
                                               const BNNSFullyConnectedLayerParameters * layer_params,
                                               const BNNSFilterParameters * _Nullable filter_params)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!

@abstract Create a pooling layer filter

@discussion
Creates a filter applying the pooling layer described in <tt>layer_params</tt>

Some combinations of the parameters may not be supported, in which case the call will fail.

@param in_desc Input image stack descriptor
@param out_desc Output image stack descriptor
@param layer_params Layer parameters and weights
@param filter_params Filter runtime parameters, may be NULL for default parameters

@return A new non-NULL filter on success, and NULL on failure.

*/
BNNSFilter BNNSFilterCreatePoolingLayer(const BNNSImageStackDescriptor * in_desc,
                                        const BNNSImageStackDescriptor * out_desc,
                                        const BNNSPoolingLayerParameters * layer_params,
                                        const BNNSFilterParameters * _Nullable filter_params)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!

@abstract Create a vector conversion/activation layer filter

@discussion
Creates a filter applying the given activation function and conversions to vectors. Input and output vectors must have the same size.

@param in_desc Input vector descriptor
@param out_desc Output vector descriptor
@param activation Activation function to apply and its parameters
@param filter_params Filter runtime parameters, may be NULL for default parameters

@return A new non-NULL filter on success, and NULL on failure.

*/
BNNSFilter BNNSFilterCreateVectorActivationLayer(const BNNSVectorDescriptor * in_desc,
                                                 const BNNSVectorDescriptor * out_desc,
                                                 const BNNSActivation * activation,
                                                 const BNNSFilterParameters * _Nullable filter_params)
__API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!

@abstract Apply a filter

@param filter Filter to apply
@param in Pointer to the input data
@param out Pointer to the output data

@return 0 on success, and -1 on failure.
*/
int BNNSFilterApply(BNNSFilter filter,const void * in,void * out)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!

@abstract Apply a filter to a several pairs of (input, output) data

@discussion
The filter is applied for each of the <tt>batch_size</tt> inputs, and produces <tt>batch_size</tt> outputs.
<tt>in</tt> (resp. <tt>out</tt>) is expected to point to <tt>batch_size</tt> times the input (resp. output) object size defined when the filter was created.

@param filter Filter to apply
@param batch_size Number of (input, output) pairs to process
@param in Pointer to the input data
@param in_stride Increment (in values) between inputs
@param out Pointer to the output data
@param out_stride Increment (in values) between outputs

@return 0 on success, and -1 on failure.
*/

int BNNSFilterApplyBatch(BNNSFilter filter,size_t batch_size,const void * in,size_t in_stride,void * out,size_t out_stride)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!

@abstract Destroy filter

@discussion Releases all resources allocated for this filter.

@param filter Filter to destroy
*/
void BNNSFilterDestroy(BNNSFilter filter)
__API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

// Availability
#if !__has_include( <Availability.h> )
#undef __API_AVAILABLE
#endif

// Nullability
#if __has_feature(assume_nonnull)
  _Pragma("clang assume_nonnull end")
#else
#undef _Nullable
#undef _Null_unspecified
#undef _Nonnull
#endif

#ifdef __cplusplus
}
#endif

#endif // __BNNS_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vfp.h
/*  vfp.h (from vecLib-728.0)
 *  Copyright (c) 1999-2019 by Apple Inc. All rights reserved.
 *
 *  Overview:
 *  vfp.h provides math library operations for SIMD vectors.  These functions
 *  are intended for use as replacements for calls to the system math library
 *  in hand-vectorized code.
 *
 *  If you are not writing vector code, but are looking for high-performance
 *  math library operations, consult vForce.h instead, which provides math
 *  library operations on arrays of floating-point data.
 *
 *  Compatibility:
 *  These routines operate on SIMD vectors, and are compatible with the types
 *  declared in the headers for both SSE (Intel) and NEON (ARM) intrinsics.
 *
 *  Bugs:
 *  For bug reports or feature requests use
 *  http://developer.apple.com/bugreporter/
 */

#ifndef __VFP__
#define __VFP__
#if defined __SSE2__ || defined __ARM_NEON__

#include "vecLibTypes.h"
#include <stdint.h>

#include <os/availability.h>

#ifdef __cplusplus
extern "C" {
#endif

/*  Rounding Functions
 *
 *  Each lane of the result vector contains the value in the corresponding
 *  lane of the input vector rounded to an integral value in the specified
 *  direction:
 *
 *     Function         Rounding Direction
 *     --------         ------------------------
 *     vceilf           toward +infinity
 *     vfloorf          toward -infinity
 *     vtruncf          toward zero
 *     vnintf           to nearest, ties to even
 *
 *  When SSE4.1 code generation is enabled on Intel architectures, single-
 *  instruction implementations of these operations are inlined instead of
 *  making an external function call.                                         */

#if defined __SSE4_1__ && !defined __clang_tapi__
#include <immintrin.h>
#define __VFP_INLINE_ATTR__ __attribute__((__always_inline__, __nodebug__))
static __inline__ vFloat __VFP_INLINE_ATTR__  vceilf(vFloat __vfp_a) { return _mm_ceil_ps(__vfp_a); }
static __inline__ vFloat __VFP_INLINE_ATTR__ vfloorf(vFloat __vfp_a) { return _mm_floor_ps(__vfp_a); }
static __inline__ vFloat __VFP_INLINE_ATTR__ vtruncf(vFloat __vfp_a) { return _mm_round_ps(__vfp_a, _MM_FROUND_TRUNC); }
static __inline__ vFloat __VFP_INLINE_ATTR__  vnintf(vFloat __vfp_a) { return _mm_round_ps(__vfp_a, _MM_FROUND_NINT); }
#else
extern vFloat  vceilf(vFloat) API_AVAILABLE(macos(10.5), ios(6.0));
extern vFloat vfloorf(vFloat) API_AVAILABLE(macos(10.5), ios(6.0));
extern vFloat vtruncf(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat  vnintf(vFloat) API_AVAILABLE(macos(10.5), ios(6.0));
/*  The legacy name vintf is not available on iOS, and is deprecated on macOS.  Use vtruncf instead.      */
extern vFloat   vintf(vFloat) API_DEPRECATED_WITH_REPLACEMENT("vtruncf", macos(10.5, 10.14)) API_UNAVAILABLE(ios, tvos, watchos);
#endif


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#else
    #define __nullable
    #define __nonnull
#endif


/*  Exponential and Logarithmic Functions
 *
 *  Each lane of the result contains the result of the specified operation
 *  applied to the corresponding lane of the input vector:
 *
 *      Function        Lanewise Operation
 *      --------        ----------------------------
 *      vexpf           base-e exponential function.
 *      vexp2f          base-two exponential function.
 *      vexpm1f         e**x - 1, computed in such a way as to be more
 *                      accurate than calling vexpf and then subtracting 1
 *                      when the argument is close to zero.
 *      vlogf           natural logarithm.
 *      vlog2f          base-two logarithm.
 *      vlog10f         base-ten logarithm.
 *      vlog1pf         natural logarithm of (1+x), computed in such a way as
 *                      to be more accurate than adding 1 and calling vlogf
 *                      when the argument is close to zero.                   */
 
extern vFloat   vexpf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vexp2f(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat vexpm1f(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vlogf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vlog2f(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat vlog10f(vFloat) API_AVAILABLE(macos(10.5), ios(6.0));
extern vFloat vlog1pf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
 
/*  Scaling Functions
 *
 *  These functions can be used to efficiently rescale floating-point
 *  computations when necessary:
 *
 *      Function        Lanewise Operation
 *      --------        ----------------------------
 *      vlogbf          extracts the exponent of its argument as a signed
 *                      integral value.  Subnormal arguments are treated as
 *                      though they were first normalized.  Thus:
 *                          1 <= x * 2**(-logbf(x)) < 2
 *      vscalbf         efficiently computes x * 2**n, where x is the first
 *                      argument and n is the second.                         */

extern vFloat  vlogbf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat vscalbf(vFloat, vSInt32) API_AVAILABLE(macos(10.0), ios(6.0));

/*  Power Functions
 *  
 *  vpowf raises the first argument to the power specified by the second
 *  argument, and returns the result.  Edge cases are as specified for the
 *  pow( ) function in the math library.  vipowf also raises the first
 *  argument to the power specified by the second argument, but the second
 *  argument to vipowf is an integer, not a floating-point number.            */

extern vFloat  vpowf(vFloat, vFloat)  API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat vipowf(vFloat, vSInt32) API_AVAILABLE(macos(10.0), ios(6.0));

/*  Trigonometric and Hyperbolic Functions
 *
 *  These functions compute lanewise trigonometric and hyperbolic functions
 *  and their inverses.  All inputs to the trigonometric functions, and 
 *  results from their inverses, are interpreted as angles measured in radians.
 *
 *      Function        Result
 *      --------        ----------------------------
 *      vsinf           sine of the argument.
 *      vcosf           cosine of the argument.
 *      vsincosf        returns the cosine of the first argument, and stores
 *                      the sine of the first argument to the destination
 *                      specified by the second argument.  This address must
 *                      be a valid pointer and must be 16-byte aligned.
 *      vtanf           tangent of the argument.
 *
 *      vsinpif         sine of the argument multiplied by pi.
 *      vcospif         cosine of the argument multiplied by pi.
 *      vtanpif         tangent of the argument multiplied by pi.
 *
 *      vasinf          arcsine of the argument, in the range [-pi/2, pi/2].
 *      vacosf          arccosine of the argument, in the range [0, pi].
 *      vatanf          arctangent of the argument, in the range [-pi/2, pi/2].
 *      vatan2f         arctangent of the first argument divided by the
 *                      second argument, using the sign of both arguments to
 *                      determine in which quadrant the result lies.  The
 *                      result is in the range [-pi, pi], and is the signed
 *                      angle from the positive x axis to the point
 *                          (second argument, first argument).
 *
 *      vsinhf          hyperbolic sine of the argument.
 *      vcoshf          hyperbolic cosine of the argument.
 *      vtanhf          hyperbolic tangent of the argument.
 *
 *      vasinhf         inverse hyperbolic sine of the argument.
 *      vacoshf         inverse hyperbolic cosine of the argument.
 *      vatanhf         inverse hyperbolic tangent of the argument.           */

extern vFloat    vsinf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat    vcosf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat vsincosf(vFloat, vFloat *) API_AVAILABLE(macos(10.5), ios(6.0));
extern vFloat    vtanf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vsinpif(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat  vcospif(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat  vtanpif(vFloat) API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat   vasinf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vacosf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vatanf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vatan2f(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vsinhf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vcoshf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vtanhf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vasinhf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vacoshf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vatanhf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));

/*  Arithmetic Functions
 *
 *  vrecf, vsqrtf, and vrsqrtf provide lane-wise reciprocal, square-root, and
 *  reciprocal square-root operations, respectively.  Each lane in the result
 *  of vdivf contains the corresponding lane of the first argument divided by
 *  the corresponding lane of the second argument.                            */

extern vFloat   vrecf(vFloat) API_AVAILABLE(macos(10.5), ios(6.0));
extern vFloat  vsqrtf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat vrsqrtf(vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat   vdivf(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));

/*  Remainder Functions
 *
 *  These functions compute various forms of the remainder from division of the
 *  first argument by the second argument.  If we call the first argument x and
 *  the second argument y, then the behavior of these functions is as follows:
 *
 *  vfmodf returns the value r = x - qy, where q is an integer such that r has
 *  the same sign as x and satisfies |r| < |y|, if y is not zero.
 *
 *  vremainderf performs the remainder operation defined in the IEEE-754
 *  standard.  It returns the value r = x - qy, where q is the integer value
 *  closest to the exact value of x/y.  If there are two integers closest to
 *  x/y, then the one which is even is used.  Thus, |r| <= |y|/2.
 *
 *  vremquof returns the same remainder as vremainderf, and also stores the
 *  7 low-order bits of q to the address pointed to by the third argument.
 *  This must be a valid pointer, and must have 16-byte alignment.            */

extern vFloat      vfmodf(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat vremainderf(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat    vremquof(vFloat, vFloat, vUInt32 *) API_AVAILABLE(macos(10.0), ios(6.0));

/*  Floating-point Utility Functions
 *
 *  These functions provide vector versions of common utility operations
 *  for working with floating-point data:
 *
 *      Function        Lanewise Operation
 *      --------        ----------------------------
 *      vfabsf          absolute value
 *      vcopysignf      returns a floating-point value with the magnitude of
 *                      the first operand and the sign of the second operand.
 *      vsignbitf       non-zero if and only if the signbit of the argument is
 *                      set.  (Note that this applies to NaNs, zeros, and 
 *                      infinities as well, and so is not the same as x < 0.)
 *      vnextafterf     returns the floating-point value adjacent to the
 *                      first operand in the direction of the second operand.
 *      vclassifyf      returns the value of the FP_xxxx macro (defined in
 *                      <math.h>) corresponding to the "class" of the argument.
 *                      e.g. if the argument is infinity, the result is
 *                      FP_INFINITE; if the argument is zero, the result is
 *                      FP_ZERO.  Consult <math.h> for further details.       */

extern vFloat       vfabsf(vFloat)         API_AVAILABLE(macos(10.9), ios(6.0));
extern vFloat   vcopysignf(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vUInt32   vsignbitf(vFloat)         API_AVAILABLE(macos(10.0), ios(6.0));
extern vFloat  vnextafterf(vFloat, vFloat) API_AVAILABLE(macos(10.0), ios(6.0));
extern vUInt32  vclassifyf(vFloat)         API_AVAILABLE(macos(10.0), ios(6.0));
/*  The legacy name vfabf is not available on iOS.  Use vfabsf instead.       */
extern vFloat        vfabf(vFloat)         API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);

/*  Generalized Table Lookup
 *
 *  This function provides a gather operation (table lookup).  Each lane of the
 *  result vector contains the value found in a table at index specified by
 *  the corresponding lane of the first argument.  The table base address is
 *  specified by the second argument.  Note please that the indices are
 *  *signed* 32-bit integers.                                                 */

extern vUInt32 vtablelookup(vSInt32, uint32_t *) API_AVAILABLE(macos(10.0), ios(6.0));

#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif

#ifdef __cplusplus
}
#endif
#endif /* defined __SSE2__ || defined __ARM_NEON__ */
#endif /* __VFP__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vecLib.h
/*
     File:       vecLib/vecLib.h
 
     Contains:   Master include for vecLib framework
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 2000-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
*/
#ifndef __VECLIB__
#define __VECLIB__


#ifndef __VECLIBTYPES__
#include <vecLib/vecLibTypes.h>
#endif

#ifndef __VBASICOPS__
#include <vecLib/vBasicOps.h>
#endif

#ifndef __VBIGNUM__
#include <vecLib/vBigNum.h>
#endif

#ifndef __VECTOROPS__
#include <vecLib/vectorOps.h>
#endif

#ifndef __VFP__
#include <vecLib/vfp.h>
#endif

#ifndef __VDSP__
#include <vecLib/vDSP.h>
#endif

#if defined __ppc__ || defined __i386__
#ifndef __VDSP_TRANSLATE__
#include <vecLib/vDSP_translate.h>
#endif
#endif

#ifndef CBLAS_H	
#include <vecLib/cblas.h>
#endif

#ifndef __CLAPACK_H
#include <vecLib/clapack.h>
#endif

#ifndef __LINEAR_ALGEBRA_PUBLIC_HEADER__
#include <vecLib/LinearAlgebra/LinearAlgebra.h>
#endif

#ifndef __SPARSE_HEADER__
#include <vecLib/Sparse/Sparse.h>
#include <vecLib/Sparse/Solve.h>
#endif

#ifndef __QUADRATURE_PUBLIC_HEADER__
#include <vecLib/Quadrature/Quadrature.h>
#endif // __QUADRATURE_PUBLIC_HEADER__

#ifndef __BNNS_HEADER__
#include <vecLib/BNNS/bnns.h>
#endif // __BNNS_HEADER__

#ifndef __VFORCE_H
#include <vecLib/vForce.h>
#endif

#endif /* __VECLIB__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vBigNum.h
/*
     File:       vecLib/vBigNum.h
 
     Contains:   Algebraic and logical operations on large operands.
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 1999-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
*/
#ifndef __VBIGNUM__
#define __VBIGNUM__

#include <stdint.h>
/*
#ifndef __VECLIBTYPES__
#include <vecLib/vecLibTypes.h>
#endif

*/
#include "vecLibTypes.h"

#include <os/availability.h>

#if PRAGMA_ONCE
#pragma once
#endif

#ifdef __cplusplus
extern "C" {
#endif


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#else
    #define __nullable
    #define __nonnull
#endif


#pragma options align=power

#if defined(__ppc__) || defined(__ppc64__) || defined(__i386__) || defined(__x86_64__)
/************************************************************************************
*                                                                                   *
*  This library provides a set of subroutines for basic algebraic and some logical  *
*  operations performed on operands with the following sizes:                       *
*                                                                                   *
*            128 - bits                                                             *
*            256 - bits                                                             *
*            512 - bits                                                             *
*           1024 - bits                                                             *
*                                                                                   *
*  Following basic and algebraic operations are included:                           *
*                                                                                   *
*            Addition                                                               *
*            Subtraction                                                            *
*            Multiplication                                                         *
*            Division                                                               *
*            Mod                                                                    *
*            Shift Right                                                            *
*            Shift Right Arithmatic                                                 *
*            Shift Left                                                             *
*            Rotate Right                                                           *
*            Rotate Left                                                            *
*                                                                                   *
*                                                                                   *
************************************************************************************/
/***********************************************************************************
*   Following abbreviations are used in the names of functions in this library:    *
*                                                                                  *
*      v            Vector                                                         *
*      U            Unsigned                                                       *
*      S            Signed                                                         *
*      128          128  - bit                                                     *
*      256          256  - bit                                                     *
*      512          512  - bit                                                     *
*      1024         1024 - bit                                                     *
*      Add          Addition, modular arithmetic                                   *
*      AddS         Addition with Saturation                                       *
*      Sub          Subtraction, modular arithmetic                                *
*      SubS         Subtraction with Saturation                                    *
*      Multiply     Multiplication                                                 *
*      Divide       Division                                                       *
*      Half         Half (multiplication, width of result is the same as width of  *
*                      operands)                                                   *                         
*      Full         Full (multiplication, width of result is twice width of each   *
*                      operand)                                                    *
*                                                                                  *
*      Mod          Modular operation                                              *
*      Neg          Negate a number                                                *
*      A            Algebraic                                                      *
*      LL           Logical Left                                                   *
*      LR           Logical Right                                                  *
*      Shift        Shift                                                          *
*      Rotate       Rotation                                                       *
*                                                                                  *
***********************************************************************************/
/************************************************************************************
*                                                                                   *
*  A few explanations for the choices made in naming, passing arguments, and        *
*  various functions.                                                               *
*                                                                                   *
*      1) Names for the functions are made compatible with the names used in the    *
*      vBasicOps library. The format of the names are the same and include a        *
*      designation to show a vector operation, then a symbol for the type of data   *
*      (signed or unsigned), followed by the size of operands, then the operation   *
*      performed.                                                                   *
*                                                                                   *
*      2) Note that the logical and arithmetic shiftLeft operation are the same.    *
*                                                                                   *
*      3) Rotate operation is performed on unsigned and signed numbers.             *
*                                                                                   *
************************************************************************************/

/************************************************************************************
*                                                                                   *
*  Following are a set of structures for vector data types and scalar data types    *
*                                                                                   *
************************************************************************************/
#if defined _AltiVecPIMLanguageExtensionsAreEnabled
union vU128 {
  vUInt32             v;
  struct {
    vUInt32             v1;
  }                       vs;
  struct {
    uint32_t            MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            LSW;
  }                       s;
};
typedef union vU128                     vU128;
union vS128 {
  vUInt32             v;
  struct {
    vUInt32             v1;
  }                       vs;
  struct {
    int32_t             MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            LSW;
  }                       s;
};
typedef union vS128                     vS128;
union vU256 {
  vUInt32             v[2];
  struct {
    vUInt32             v1;
    vUInt32             v2;
  }                       vs;
  struct {
    uint32_t            MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            LSW;
  }                       s;
};
typedef union vU256                     vU256;
union vS256 {
  vUInt32             v[2];
  struct {
    vUInt32             v1;
    vUInt32             v2;
  }                       vs;
  struct {
    int32_t             MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            LSW;
  }                       s;
};
typedef union vS256                     vS256;
union vU512 {
  vUInt32             v[4];
  struct {
    vUInt32             v1;
    vUInt32             v2;
    vUInt32             v3;
    vUInt32             v4;
  }                       vs;
  struct {
    uint32_t            MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            d8;
    uint32_t            d9;
    uint32_t            d10;
    uint32_t            d11;
    uint32_t            d12;
    uint32_t            d13;
    uint32_t            d14;
    uint32_t            d15;
    uint32_t            LSW;
  }                       s;
};
typedef union vU512                     vU512;
union vS512 {
  vUInt32             v[4];
  struct {
    vUInt32             v1;
    vUInt32             v2;
    vUInt32             v3;
    vUInt32             v4;
  }                       vs;
  struct {
    int32_t             MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            d8;
    uint32_t            d9;
    uint32_t            d10;
    uint32_t            d11;
    uint32_t            d12;
    uint32_t            d13;
    uint32_t            d14;
    uint32_t            d15;
    uint32_t            LSW;
  }                       s;
};
typedef union vS512                     vS512;
union vU1024 {
  vUInt32             v[8];
  struct {
    vUInt32             v1;
    vUInt32             v2;
    vUInt32             v3;
    vUInt32             v4;
    vUInt32             v5;
    vUInt32             v6;
    vUInt32             v7;
    vUInt32             v8;
  }                       vs;
  struct {
    uint32_t            MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            d8;
    uint32_t            d9;
    uint32_t            d10;
    uint32_t            d11;
    uint32_t            d12;
    uint32_t            d13;
    uint32_t            d14;
    uint32_t            d15;
    uint32_t            d16;
    uint32_t            d17;
    uint32_t            d18;
    uint32_t            d19;
    uint32_t            d20;
    uint32_t            d21;
    uint32_t            d22;
    uint32_t            d23;
    uint32_t            d24;
    uint32_t            d25;
    uint32_t            d26;
    uint32_t            d27;
    uint32_t            d28;
    uint32_t            d29;
    uint32_t            d30;
    uint32_t            d31;
    uint32_t            LSW;
  }                       s;
};
typedef union vU1024                    vU1024;
union vS1024 {
  vUInt32             v[8];
  struct {
    vUInt32             v1;
    vUInt32             v2;
    vUInt32             v3;
    vUInt32             v4;
    vUInt32             v5;
    vUInt32             v6;
    vUInt32             v7;
    vUInt32             v8;
  }                       vs;
  struct {
    int32_t             MSW;
    uint32_t            d2;
    uint32_t            d3;
    uint32_t            d4;
    uint32_t            d5;
    uint32_t            d6;
    uint32_t            d7;
    uint32_t            d8;
    uint32_t            d9;
    uint32_t            d10;
    uint32_t            d11;
    uint32_t            d12;
    uint32_t            d13;
    uint32_t            d14;
    uint32_t            d15;
    uint32_t            d16;
    uint32_t            d17;
    uint32_t            d18;
    uint32_t            d19;
    uint32_t            d20;
    uint32_t            d21;
    uint32_t            d22;
    uint32_t            d23;
    uint32_t            d24;
    uint32_t            d25;
    uint32_t            d26;
    uint32_t            d27;
    uint32_t            d28;
    uint32_t            d29;
    uint32_t            d30;
    uint32_t            d31;
    uint32_t            LSW;
  }                       s;
};
typedef union vS1024                    vS1024;
#elif (defined(__i386__) || defined(__x86_64__)) && defined(__SSE2__)
union vU128 {
  vUInt32             v;
  struct {
    vUInt32             v1;
  }                       vs;
  struct {
    uint32_t            LSW;                  /*MSW;*/
    uint32_t            d3;                   /*d2;*/
    uint32_t            d2;                   /*d3;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vU128                     vU128;
union vS128 {
  vUInt32             v;
  struct {
    vUInt32             v1;
  }                       vs;
  struct {
    int32_t             LSW;                  /*MSW;*/
    uint32_t            d3;                   /*d2;*/
    uint32_t            d2;                   /*d3;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vS128                     vS128;
union vU256 {
  vUInt32             v[2];
  struct {
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    uint32_t            LSW;                  /*MSW;*/
    uint32_t            d7;                   /*d2;*/
    uint32_t            d6;                   /*d3;*/
    uint32_t            d5;                   /*d4;*/
    uint32_t            d4;                   /*d5;*/
    uint32_t            d3;                   /*d6;*/
    uint32_t            d2;                   /*d7;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vU256                     vU256;
union vS256 {
  vUInt32             v[2];
  struct {
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    int32_t             LSW;                  /*MSW;*/
    uint32_t            d7;                   /*d2;*/
    uint32_t            d6;                   /*d3;*/
    uint32_t            d5;                   /*d4;*/
    uint32_t            d4;                   /*d5;*/
    uint32_t            d3;                   /*d6;*/
    uint32_t            d2;                   /*d7;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vS256                     vS256;
union vU512 {
  vUInt32             v[4];
  struct {
    vUInt32             v4;
    vUInt32             v3;
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    uint32_t            LSW;                  /*MSB;*/
    uint32_t            d15;                  /*d2;*/
    uint32_t            d14;                  /*d3;*/
    uint32_t            d13;                  /*d4;*/
    uint32_t            d12;                  /*d5;*/
    uint32_t            d11;                  /*d6;*/
    uint32_t            d10;                  /*d7;*/
    uint32_t            d9;                   /*d8;*/
    uint32_t            d8;                   /*d9;*/
    uint32_t            d7;                   /*d10;*/
    uint32_t            d6;                   /*d11;*/
    uint32_t            d5;                   /*d12;*/
    uint32_t            d4;                   /*d13;*/
    uint32_t            d3;                   /*d14;*/
    uint32_t            d2;                   /*d15;*/
    uint32_t            MSW;                  /*LSB;*/
  }                       s;
};
typedef union vU512                     vU512;
union vS512 {
  vUInt32             v[4];
  struct {
    vUInt32             v4;
    vUInt32             v3;
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    int32_t             LSW;                  /*MSW;*/
    uint32_t            d15;                  /*d2;*/
    uint32_t            d14;                  /*d3;*/
    uint32_t            d13;                  /*d4;*/
    uint32_t            d12;                  /*d5;*/
    uint32_t            d11;                  /*d6;*/
    uint32_t            d10;                  /*d7;*/
    uint32_t            d9;                   /*d8;*/
    uint32_t            d8;                   /*d9;*/
    uint32_t            d7;                   /*d10;*/
    uint32_t            d6;                   /*d11;*/
    uint32_t            d5;                   /*d12;*/
    uint32_t            d4;                   /*d13;*/
    uint32_t            d3;                   /*d14;*/
    uint32_t            d2;                   /*d15;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vS512                     vS512;
union vU1024 {
  vUInt32             v[8];
  struct {
    vUInt32             v8;
    vUInt32             v7;
    vUInt32             v6;
    vUInt32             v5;
    vUInt32             v4;
    vUInt32             v3;
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    uint32_t            LSW;                  /*MSW;*/
    uint32_t            d31;                  /*d2;*/
    uint32_t            d30;                  /*d3;*/
    uint32_t            d29;                  /*d4;*/
    uint32_t            d28;                  /*d5;*/
    uint32_t            d27;                  /*d6;*/
    uint32_t            d26;                  /*d7;*/
    uint32_t            d25;                  /*d8;*/
    uint32_t            d24;                  /*d9;*/
    uint32_t            d23;                  /*d10;*/
    uint32_t            d22;                  /*d11;*/
    uint32_t            d21;                  /*d12;*/
    uint32_t            d20;                  /*d13;*/
    uint32_t            d19;                  /*d14;*/
    uint32_t            d18;                  /*d15;*/
    uint32_t            d17;                  /*d16;*/
    uint32_t            d16;                  /*d17;*/
    uint32_t            d15;                  /*d18;*/
    uint32_t            d14;                  /*d19;*/
    uint32_t            d13;                  /*d20;*/
    uint32_t            d12;                  /*d21;*/
    uint32_t            d11;                  /*d22;*/
    uint32_t            d10;                  /*d23;*/
    uint32_t            d9;                   /*d24;*/
    uint32_t            d8;                   /*d25;*/
    uint32_t            d7;                   /*d26;*/
    uint32_t            d6;                   /*d27;*/
    uint32_t            d5;                   /*d28;*/
    uint32_t            d4;                   /*d29;*/
    uint32_t            d3;                   /*d30;*/
    uint32_t            d2;                   /*d31;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vU1024                    vU1024;
union vS1024 {
  vUInt32             v[8];
  struct {
    vUInt32             v8;
    vUInt32             v7;
    vUInt32             v6;
    vUInt32             v5;
    vUInt32             v4;
    vUInt32             v3;
    vUInt32             v2;
    vUInt32             v1;
  }                       vs;
  struct {
    int32_t             LSW;                  /*MSW;*/
    uint32_t            d31;                  /*d2;*/
    uint32_t            d30;                  /*d3;*/
    uint32_t            d29;                  /*d4;*/
    uint32_t            d28;                  /*d5;*/
    uint32_t            d27;                  /*d6;*/
    uint32_t            d26;                  /*d7;*/
    uint32_t            d25;                  /*d8;*/
    uint32_t            d24;                  /*d9;*/
    uint32_t            d23;                  /*d10;*/
    uint32_t            d22;                  /*d11;*/
    uint32_t            d21;                  /*d12;*/
    uint32_t            d20;                  /*d13;*/
    uint32_t            d19;                  /*d14;*/
    uint32_t            d18;                  /*d15;*/
    uint32_t            d17;                  /*d16;*/
    uint32_t            d16;                  /*d17;*/
    uint32_t            d15;                  /*d18;*/
    uint32_t            d14;                  /*d19;*/
    uint32_t            d13;                  /*d20;*/
    uint32_t            d12;                  /*d21;*/
    uint32_t            d11;                  /*d22;*/
    uint32_t            d10;                  /*d23;*/
    uint32_t            d9;                   /*d24;*/
    uint32_t            d8;                   /*d25;*/
    uint32_t            d7;                   /*d26;*/
    uint32_t            d6;                   /*d27;*/
    uint32_t            d5;                   /*d28;*/
    uint32_t            d4;                   /*d29;*/
    uint32_t            d3;                   /*d30;*/
    uint32_t            d2;                   /*d31;*/
    uint32_t            MSW;                  /*LSW;*/
  }                       s;
};
typedef union vS1024                    vS1024;
#endif  /*  */

#if defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE2__
/************************************************************************************
*                                                                                   *
*                                Division operations                                *
*                                                                                   *
************************************************************************************/
/*
 *  vU256Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256Divide(
  const vU256       *numerator,
  const vU256       *divisor,
  vU256             *result,
  vU256 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256Divide(
  const vS256       *numerator,
  const vS256       *divisor,
  vS256             *result,
  vS256 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512Divide(
  const vU512       *numerator,
  const vU512       *divisor,
  vU512             *result,
  vU512 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512Divide(
  const vS512       *numerator,
  const vS512       *divisor,
  vS512             *result,
  vS512 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024Divide(
  const vU1024       *numerator,
  const vU1024       *divisor,
  vU1024             *result,
  vU1024 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024Divide(
  const vS1024       *numerator,
  const vS1024       *divisor,
  vS1024             *result,
  vS1024 * __nullable remainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                              Multiply operations                                  *
*                                                                                   *
************************************************************************************/

/*
 *  vU128FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU128FullMultiply(
  const vU128 *  a,
  const vU128 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS128FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS128FullMultiply(
  const vS128 *  a,
  const vS128 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU256FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256FullMultiply(
  const vU256 *  a,
  const vU256 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256FullMultiply(
  const vS256 *  a,
  const vS256 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512FullMultiply(
  const vU512 *  a,
  const vU512 *  b,
  vU1024 *       result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512FullMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512FullMultiply(
  const vS512 *  a,
  const vS512 *  b,
  vS1024 *       result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU256HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256HalfMultiply(
  const vU256 *  a,
  const vU256 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256HalfMultiply(
  const vS256 *  a,
  const vS256 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512HalfMultiply(
  const vU512 *  a,
  const vU512 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512HalfMultiply(
  const vS512 *  a,
  const vS512 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024HalfMultiply(
  const vU1024 *  a,
  const vU1024 *  b,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024HalfMultiply(
  const vS1024 *  a,
  const vS1024 *  b,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                             Subtraction operations                                *
*                                                                                   *
************************************************************************************/

/*
 *  vU256Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256Sub(
  const vU256 *  a,
  const vU256 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256Sub(
  const vS256 *  a,
  const vS256 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU256SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256SubS(
  const vU256 *  a,
  const vU256 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256SubS(
  const vS256 *  a,
  const vS256 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512Sub(
  const vU512 *  a,
  const vU512 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512Sub(
  const vS512 *  a,
  const vS512 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512SubS(
  const vU512 *  a,
  const vU512 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512SubS(
  const vS512 *  a,
  const vS512 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024Sub(
  const vU1024 *  a,
  const vU1024 *  b,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024Sub(
  const vS1024 *  a,
  const vS1024 *  b,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024SubS(
  const vU1024 *  a,
  const vU1024 *  b,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024SubS(
  const vS1024 *  a,
  const vS1024 *  b,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                                Negate operations                                  *
*                                                                                   *
************************************************************************************/

/*
 *  vU256Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256Neg(
  const vU256 *  a,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256Neg(
  const vS256 *  a,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512Neg(
  const vU512 *  a,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512Neg(
  const vS512 *  a,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024Neg(
  const vU1024 *  a,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024Neg()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024Neg(
  const vS1024 *  a,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                                Addition operations                                *
*                                                                                   *
************************************************************************************/

/*
 *  vU256Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256Add(
  const vU256 *  a,
  const vU256 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256Add(
  const vS256 *  a,
  const vS256 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU256AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256AddS(
  const vU256 *  a,
  const vU256 *  b,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256AddS(
  const vS256 *  a,
  const vS256 *  b,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512Add(
  const vU512 *  a,
  const vU512 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512Add(
  const vS512 *  a,
  const vS512 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512AddS(
  const vU512 *  a,
  const vU512 *  b,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512AddS(
  const vS512 *  a,
  const vS512 *  b,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024Add(
  const vU1024 *  a,
  const vU1024 *  b,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024Add(
  const vS1024 *  a,
  const vS1024 *  b,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024AddS(
  const vU1024 *  a,
  const vU1024 *  b,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024AddS(
  const vS1024 *  a,
  const vS1024 *  b,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                                   Mod operations                                  *
*                                                                                   *
************************************************************************************/

/*
 *  vU256Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU256Mod(
  const vU256 *  numerator,
  const vU256 *  divisor,
  vU256 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS256Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS256Mod(
  const vS256 *  numerator,
  const vS256 *  divisor,
  vS256 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU512Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU512Mod(
  const vU512 *  numerator,
  const vU512 *  divisor,
  vU512 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS512Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS512Mod(
  const vS512 *  numerator,
  const vS512 *  divisor,
  vS512 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vU1024Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vU1024Mod(
  const vU1024 *  numerator,
  const vU1024 *  divisor,
  vU1024 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vS1024Mod()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vS1024Mod(
  const vS1024 *  numerator,
  const vS1024 *  divisor,
  vS1024 *        remainder) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                                Shift operations                                   *
*                                                                                   *
************************************************************************************/

/*
 *  vLL256Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLL256Shift(
  const vU256 *  a,
  uint32_t       shiftAmount,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vLL512Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLL512Shift(
  const vU512 *  a,
  uint32_t       shiftAmount,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vLL1024Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLL1024Shift(
  const vU1024 *  a,
  uint32_t        shiftAmount,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vLR256Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLR256Shift(
  const vU256 *  a,
  uint32_t       shiftAmount,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vLR512Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLR512Shift(
  const vU512 *  a,
  uint32_t       shiftAmount,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vLR1024Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vLR1024Shift(
  const vU1024 *  a,
  uint32_t        shiftAmount,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vA256Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vA256Shift(
  const vS256 *  a,
  uint32_t       shiftAmount,
  vS256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vA512Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vA512Shift(
  const vS512 *  a,
  uint32_t       shiftAmount,
  vS512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vA1024Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vA1024Shift(
  const vS1024 *  a,
  uint32_t        shiftAmount,
  vS1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);



/************************************************************************************
*                                                                                   *
*                                  Rotate operations                                *
*                                                                                   *
************************************************************************************/

/*
 *  vL256Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vL256Rotate(
  const vU256 *  a,
  uint32_t       rotateAmount,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vL512Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vL512Rotate(
  const vU512 *  a,
  uint32_t       rotateAmount,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vL1024Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vL1024Rotate(
  const vU1024 *  a,
  uint32_t        rotateAmount,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vR256Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vR256Rotate(
  const vU256 *  a,
  uint32_t       rotateAmount,
  vU256 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vR512Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vR512Rotate(
  const vU512 *  a,
  uint32_t       rotateAmount,
  vU512 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


/*
 *  vR1024Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vR1024Rotate(
  const vU1024 *  a,
  uint32_t        rotateAmount,
  vU1024 *        result) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios);


#endif  // defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE2__

#endif  /* defined(__ppc__) || defined(__ppc64__) || defined(__i386__) || defined(__x86_64__) */


#pragma options align=reset

#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif

#ifdef __cplusplus
}
#endif

#endif /* __VBIGNUM__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/clapack.h
/*
 =================================================================================================
 Definitions and prototypes for LAPACK v3.2.1 as provided Apple Computer.

 Documentation of the LAPACK interfaces, including reference implementations, can be found on
 the web starting from the LAPACK FAQ page at this URL (verified live as of January 2010):
 http://netlib.org/lapack/faq.html

 A hardcopy maanual is:
 LAPACK Users' Guide, Third Edition. 
 @BOOK{laug,
 AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
 Blackford, S. and Demmel, J. and Dongarra, J. and
 Du Croz, J. and Greenbaum, A. and Hammarling, S. and
 McKenney, A. and Sorensen, D.},
 TITLE = {{LAPACK} Users' Guide},
 EDITION = {Third},
 PUBLISHER = {Society for Industrial and Applied Mathematics},
 YEAR = {1999},
 ADDRESS = {Philadelphia, PA},
 ISBN = {0-89871-447-8 (paperback)} }

 =================================================================================================
 */

#ifndef __CLAPACK_H
#define __CLAPACK_H

#ifdef __cplusplus
extern "C" {
#endif

#if defined(__LP64__) /* In LP64 match sizes with the 32 bit ABI */
    typedef int 		__CLPK_integer;
    typedef int 		__CLPK_logical;
    typedef float 		__CLPK_real;
    typedef double 		__CLPK_doublereal;
    
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wstrict-prototypes"
    typedef __CLPK_logical 	(*__CLPK_L_fp)();
#pragma clang diagnostic pop
    
    typedef int 		__CLPK_ftnlen;
#else
    typedef long int 	__CLPK_integer;
    typedef long int 	__CLPK_logical;
    typedef float 		__CLPK_real;
    typedef double 		__CLPK_doublereal;
    
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wstrict-prototypes"
    typedef __CLPK_logical 	(*__CLPK_L_fp)();
#pragma clang diagnostic pop
    
    typedef long int 	__CLPK_ftnlen;
#endif

typedef struct { __CLPK_real r, i; } __CLPK_complex;
typedef struct { __CLPK_doublereal r, i; } __CLPK_doublecomplex;

#include <stdint.h>
#if __has_include(<os/availability.h>)
# include <os/availability.h>
#else // __has_include(<os/availability.h>)
# undef API_AVAILABLE
# define API_AVAILABLE(...) /* Nothing */
#endif // __has_include(<os/availability.h>)

int cbdsqr_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__ncvt,
        __CLPK_integer *__nru, __CLPK_integer *__ncc, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__vt, __CLPK_integer *__ldvt,
        __CLPK_complex *__u, __CLPK_integer *__ldu, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbbrd_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__ncc, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__q, __CLPK_integer *__ldq,
        __CLPK_complex *__pt, __CLPK_integer *__ldpt, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbcon_(char *__norm, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__afb, __CLPK_integer *__ldafb,
        __CLPK_integer *__ipiv, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbsv_(__CLPK_integer *__n, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_integer *__nrhs, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_complex *__afb,
        __CLPK_integer *__ldafb, __CLPK_integer *__ipiv, char *__equed,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbtf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbtrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgbtrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgebak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__scale,
        __CLPK_integer *__m, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgebal_(char *__job, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgebd2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_complex *__tauq, __CLPK_complex *__taup, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgebrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_complex *__tauq, __CLPK_complex *__taup, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgecon_(char *__norm, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgees_(char *__jobvs, char *__sort, __CLPK_L_fp __select,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_complex *__w, __CLPK_complex *__vs,
        __CLPK_integer *__ldvs, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeesx_(char *__jobvs, char *__sort, __CLPK_L_fp __select, char *__sense,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_complex *__w, __CLPK_complex *__vs,
        __CLPK_integer *__ldvs, __CLPK_real *__rconde, __CLPK_real *__rcondv,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__w,
        __CLPK_complex *__vl, __CLPK_integer *__ldvl, __CLPK_complex *__vr,
        __CLPK_integer *__ldvr, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__w, __CLPK_complex *__vl, __CLPK_integer *__ldvl,
        __CLPK_complex *__vr, __CLPK_integer *__ldvr, __CLPK_integer *__ilo,
        __CLPK_integer *__ihi, __CLPK_real *__scale, __CLPK_real *__abnrm,
        __CLPK_real *__rconde, __CLPK_real *__rcondv, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgegs_(char *__jobvsl, char *__jobvsr, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__alpha, __CLPK_complex *__beta,
        __CLPK_complex *__vsl, __CLPK_integer *__ldvsl, __CLPK_complex *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgegv_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__alpha, __CLPK_complex *__beta,
        __CLPK_complex *__vl, __CLPK_integer *__ldvl, __CLPK_complex *__vr,
        __CLPK_integer *__ldvr, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgehd2_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgehrd_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgels_(char *__trans, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelsd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_real *__s, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelss_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_real *__s, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelsx_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgelsy_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeql2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeqlf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeqp3_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeqpf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeqr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgeqrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgerfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgerq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgerqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgesc2_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__rhs, __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_real *__scale)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgesdd_(char *__jobz, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__s,
        __CLPK_complex *__u, __CLPK_integer *__ldu, __CLPK_complex *__vt,
        __CLPK_integer *__ldvt, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgesvd_(char *__jobu, char *__jobvt, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_complex *__u, __CLPK_integer *__ldu,
        __CLPK_complex *__vt, __CLPK_integer *__ldvt, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgesvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        char *__equed, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgetc2_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgetf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgetrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgetri_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgetrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggbak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__lscale,
        __CLPK_real *__rscale, __CLPK_integer *__m, __CLPK_complex *__v,
        __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggbal_(char *__job, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__lscale,
        __CLPK_real *__rscale, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgges_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_integer *__sdim,
        __CLPK_complex *__alpha, __CLPK_complex *__beta, __CLPK_complex *__vsl,
        __CLPK_integer *__ldvsl, __CLPK_complex *__vsr, __CLPK_integer *__ldvsr,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggesx_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        char *__sense, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__sdim, __CLPK_complex *__alpha, __CLPK_complex *__beta,
        __CLPK_complex *__vsl, __CLPK_integer *__ldvsl, __CLPK_complex *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_real *__rconde, __CLPK_real *__rcondv,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__alpha, __CLPK_complex *__beta,
        __CLPK_complex *__vl, __CLPK_integer *__ldvl, __CLPK_complex *__vr,
        __CLPK_integer *__ldvr, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__alpha,
        __CLPK_complex *__beta, __CLPK_complex *__vl, __CLPK_integer *__ldvl,
        __CLPK_complex *__vr, __CLPK_integer *__ldvr, __CLPK_integer *__ilo,
        __CLPK_integer *__ihi, __CLPK_real *__lscale, __CLPK_real *__rscale,
        __CLPK_real *__abnrm, __CLPK_real *__bbnrm, __CLPK_real *__rconde,
        __CLPK_real *__rcondv, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork, __CLPK_integer *__iwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggglm_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__d__, __CLPK_complex *__x,
        __CLPK_complex *__y, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgghrd_(char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_complex *__z__,
        __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgglse_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__p,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__c__, __CLPK_complex *__d__,
        __CLPK_complex *__x, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggqrf_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__taua,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__taub,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggrqf_(__CLPK_integer *__m, __CLPK_integer *__p, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__taua,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__taub,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggsvd_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__p, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__alpha,
        __CLPK_real *__beta, __CLPK_complex *__u, __CLPK_integer *__ldu,
        __CLPK_complex *__v, __CLPK_integer *__ldv, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cggsvp_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_real *__tola, __CLPK_real *__tolb, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_complex *__u, __CLPK_integer *__ldu,
        __CLPK_complex *__v, __CLPK_integer *__ldv, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_integer *__iwork, __CLPK_real *__rwork,
        __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgtcon_(char *__norm, __CLPK_integer *__n, __CLPK_complex *__dl,
        __CLPK_complex *__d__, __CLPK_complex *__du, __CLPK_complex *__du2,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgtrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__dl, __CLPK_complex *__d__, __CLPK_complex *__du,
        __CLPK_complex *__dlf, __CLPK_complex *__df, __CLPK_complex *__duf,
        __CLPK_complex *__du2, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgtsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_complex *__dl,
        __CLPK_complex *__d__, __CLPK_complex *__du, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgtsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__dl, __CLPK_complex *__d__,
        __CLPK_complex *__du, __CLPK_complex *__dlf, __CLPK_complex *__df,
        __CLPK_complex *__duf, __CLPK_complex *__du2, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgttrf_(__CLPK_integer *__n, __CLPK_complex *__dl, __CLPK_complex *__d__,
        __CLPK_complex *__du, __CLPK_complex *__du2, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgttrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__dl, __CLPK_complex *__d__, __CLPK_complex *__du,
        __CLPK_complex *__du2, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cgtts2_(__CLPK_integer *__itrans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__dl, __CLPK_complex *__d__,
        __CLPK_complex *__du, __CLPK_complex *__du2, __CLPK_integer *__ipiv,
        __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_complex *__work,
        __CLPK_real *__rwork, __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbgst_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__bb, __CLPK_integer *__ldbb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbgv_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbgvd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbgvx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__bb, __CLPK_integer *__ldbb,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_complex *__work,
        __CLPK_real *__rwork, __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chbtrd_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int checon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__anorm,
        __CLPK_real *__rcond, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cheequb_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cheev_(char *__jobz, char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__w, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cheevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__w,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cheevr_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cheevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chegs2_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chegst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chegv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__w,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chegvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__w,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chegvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cherfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chesv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chesvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetd2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_complex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetf2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetrd_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_complex *__tau, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chetrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chfrk_(char *__transr, char *__uplo, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__alpha, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__beta,
        __CLPK_complex *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int chgeqz_(char *__job, char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_complex *__h__,
        __CLPK_integer *__ldh, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_complex *__alpha, __CLPK_complex *__beta, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

void chla_transtype__(char *__ret_val, __CLPK_ftnlen __ret_val_len,
        __CLPK_integer *__trans)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpcon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_real *__w, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_real *__w, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork, __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_real *__vl, __CLPK_real *__vu,
        __CLPK_integer *__il, __CLPK_integer *__iu, __CLPK_real *__abstol,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpgst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_complex *__bp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpgv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__ap, __CLPK_complex *__bp,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpgvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__ap, __CLPK_complex *__bp,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpgvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__ap, __CLPK_complex *__bp,
        __CLPK_real *__vl, __CLPK_real *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_real *__abstol, __CLPK_integer *__m,
        __CLPK_real *__w, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_complex *__afp, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chpsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__ap, __CLPK_complex *__afp,
        __CLPK_integer *__ipiv, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chptrd_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_complex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chptri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chsein_(char *__side, char *__eigsrc, char *__initv,
        __CLPK_logical *__select, __CLPK_integer *__n, __CLPK_complex *__h__,
        __CLPK_integer *__ldh, __CLPK_complex *__w, __CLPK_complex *__vl,
        __CLPK_integer *__ldvl, __CLPK_complex *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_complex *__work,
        __CLPK_real *__rwork, __CLPK_integer *__ifaill,
        __CLPK_integer *__ifailr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int chseqr_(char *__job, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_complex *__h__,
        __CLPK_integer *__ldh, __CLPK_complex *__w, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clabrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__tauq, __CLPK_complex *__taup,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_complex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacgv_(__CLPK_integer *__n, __CLPK_complex *__x,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacn2_(__CLPK_integer *__n, __CLPK_complex *__v, __CLPK_complex *__x,
        __CLPK_real *__est, __CLPK_integer *__kase,
        __CLPK_integer *__isave)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacon_(__CLPK_integer *__n, __CLPK_complex *__v, __CLPK_complex *__x,
        __CLPK_real *__est,
        __CLPK_integer *__kase)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacp2_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacpy_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacrm_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__rwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int clacrt_(__CLPK_integer *__n, __CLPK_complex *__cx, __CLPK_integer *__incx,
        __CLPK_complex *__cy, __CLPK_integer *__incy, __CLPK_complex *__c__,
        __CLPK_complex *__s)
API_AVAILABLE(macos(10.2), ios(4.0));

void cladiv_(__CLPK_complex *__ret_val, __CLPK_complex *__x,
        __CLPK_complex *__y)
API_AVAILABLE(macos(10.2), ios(4.0));

int claed0_(__CLPK_integer *__qsiz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__q, __CLPK_integer *__ldq,
        __CLPK_complex *__qstore, __CLPK_integer *__ldqs, __CLPK_real *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claed7_(__CLPK_integer *__n, __CLPK_integer *__cutpnt,
        __CLPK_integer *__qsiz, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm, __CLPK_real *__d__,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_real *__rho,
        __CLPK_integer *__indxq, __CLPK_real *__qstore, __CLPK_integer *__qptr,
        __CLPK_integer *__prmptr, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_real *__givnum, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claed8_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__qsiz,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_real *__d__,
        __CLPK_real *__rho, __CLPK_integer *__cutpnt, __CLPK_real *__z__,
        __CLPK_real *__dlamda, __CLPK_complex *__q2, __CLPK_integer *__ldq2,
        __CLPK_real *__w, __CLPK_integer *__indxp, __CLPK_integer *__indx,
        __CLPK_integer *__indxq, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_real *__givnum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claein_(__CLPK_logical *__rightv, __CLPK_logical *__noinit,
        __CLPK_integer *__n, __CLPK_complex *__h__, __CLPK_integer *__ldh,
        __CLPK_complex *__w, __CLPK_complex *__v, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_real *__rwork, __CLPK_real *__eps3,
        __CLPK_real *__smlnum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claesy_(__CLPK_complex *__a, __CLPK_complex *__b, __CLPK_complex *__c__,
        __CLPK_complex *__rt1, __CLPK_complex *__rt2, __CLPK_complex *__evscal,
        __CLPK_complex *__cs1,
        __CLPK_complex *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int claev2_(__CLPK_complex *__a, __CLPK_complex *__b, __CLPK_complex *__c__,
        __CLPK_real *__rt1, __CLPK_real *__rt2, __CLPK_real *__cs1,
        __CLPK_complex *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int clag2z_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__sa,
        __CLPK_integer *__ldsa, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clags2_(__CLPK_logical *__upper, __CLPK_real *__a1, __CLPK_complex *__a2,
        __CLPK_real *__a3, __CLPK_real *__b1, __CLPK_complex *__b2,
        __CLPK_real *__b3, __CLPK_real *__csu, __CLPK_complex *__snu,
        __CLPK_real *__csv, __CLPK_complex *__snv, __CLPK_real *__csq,
        __CLPK_complex *__snq)
API_AVAILABLE(macos(10.2), ios(4.0));

int clagtm_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__alpha, __CLPK_complex *__dl, __CLPK_complex *__d__,
        __CLPK_complex *__du, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__beta, __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int clahef_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_complex *__w, __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clahqr_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__h__, __CLPK_integer *__ldh, __CLPK_complex *__w,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_complex *__z__,
        __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clahr2_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__t, __CLPK_integer *__ldt, __CLPK_complex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int clahrd_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__t, __CLPK_integer *__ldt, __CLPK_complex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int claic1_(__CLPK_integer *__job, __CLPK_integer *__j, __CLPK_complex *__x,
        __CLPK_real *__sest, __CLPK_complex *__w, __CLPK_complex *__gamma,
        __CLPK_real *__sestpr, __CLPK_complex *__s,
        __CLPK_complex *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int clals0_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__nrhs,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__bx,
        __CLPK_integer *__ldbx, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_real *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_real *__poles, __CLPK_real *__difl,
        __CLPK_real *__difr, __CLPK_real *__z__, __CLPK_integer *__k,
        __CLPK_real *__c__, __CLPK_real *__s, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clalsa_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__bx, __CLPK_integer *__ldbx,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__vt,
        __CLPK_integer *__k, __CLPK_real *__difl, __CLPK_real *__difr,
        __CLPK_real *__z__, __CLPK_real *__poles, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_integer *__perm, __CLPK_real *__givnum, __CLPK_real *__c__,
        __CLPK_real *__s, __CLPK_real *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clalsd_(char *__uplo, __CLPK_integer *__smlsiz, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clangb_(char *__norm, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_complex *__ab,
        __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clange_(char *__norm, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clangt_(char *__norm, __CLPK_integer *__n,
        __CLPK_complex *__dl, __CLPK_complex *__d__,
        __CLPK_complex *__du)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanhb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanhe_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanhf_(char *__norm, char *__transr, char *__uplo,
        __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanhp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanhs_(char *__norm, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clanht_(char *__norm, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_complex *__e)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clansb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clansp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clansy_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clantb_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__ab,
        __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clantp_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal clantr_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clapll_(__CLPK_integer *__n, __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_complex *__y, __CLPK_integer *__incy,
        __CLPK_real *__ssmin)
API_AVAILABLE(macos(10.2), ios(4.0));

int clapmt_(__CLPK_logical *__forwrd, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_integer *__k)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqgb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqge_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqhb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_real *__s,
        __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqhe_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqhp_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqp2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_complex *__tau, __CLPK_real *__vn1, __CLPK_real *__vn2,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqps_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_integer *__nb, __CLPK_integer *__kb, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_complex *__tau,
        __CLPK_real *__vn1, __CLPK_real *__vn2, __CLPK_complex *__auxv,
        __CLPK_complex *__f,
        __CLPK_integer *__ldf)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr0_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__h__, __CLPK_integer *__ldh, __CLPK_complex *__w,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr1_(__CLPK_integer *__n, __CLPK_complex *__h__, __CLPK_integer *__ldh,
        __CLPK_complex *__s1, __CLPK_complex *__s2,
        __CLPK_complex *__v)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr2_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_complex *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__ns, __CLPK_integer *__nd,
        __CLPK_complex *__sh, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__nh, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_integer *__nv, __CLPK_complex *__wv, __CLPK_integer *__ldwv,
        __CLPK_complex *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr3_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_complex *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__ns, __CLPK_integer *__nd,
        __CLPK_complex *__sh, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__nh, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_integer *__nv, __CLPK_complex *__wv, __CLPK_integer *__ldwv,
        __CLPK_complex *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr4_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__h__, __CLPK_integer *__ldh, __CLPK_complex *__w,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqr5_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__kacc22, __CLPK_integer *__n, __CLPK_integer *__ktop,
        __CLPK_integer *__kbot, __CLPK_integer *__nshfts, __CLPK_complex *__s,
        __CLPK_complex *__h__, __CLPK_integer *__ldh, __CLPK_integer *__iloz,
        __CLPK_integer *__ihiz, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__v, __CLPK_integer *__ldv, __CLPK_complex *__u,
        __CLPK_integer *__ldu, __CLPK_integer *__nv, __CLPK_complex *__wv,
        __CLPK_integer *__ldwv, __CLPK_integer *__nh, __CLPK_complex *__wh,
        __CLPK_integer *__ldwh)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqsb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_real *__s,
        __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqsp_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int claqsy_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int clar1v_(__CLPK_integer *__n, __CLPK_integer *__b1, __CLPK_integer *__bn,
        __CLPK_real *__lambda, __CLPK_real *__d__, __CLPK_real *__l,
        __CLPK_real *__ld, __CLPK_real *__lld, __CLPK_real *__pivmin,
        __CLPK_real *__gaptol, __CLPK_complex *__z__, __CLPK_logical *__wantnc,
        __CLPK_integer *__negcnt, __CLPK_real *__ztz, __CLPK_real *__mingma,
        __CLPK_integer *__r__, __CLPK_integer *__isuppz, __CLPK_real *__nrminv,
        __CLPK_real *__resid, __CLPK_real *__rqcorr,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clar2v_(__CLPK_integer *__n, __CLPK_complex *__x, __CLPK_complex *__y,
        __CLPK_complex *__z__, __CLPK_integer *__incx, __CLPK_real *__c__,
        __CLPK_complex *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarcm_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__rwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarf_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__v, __CLPK_integer *__incv, __CLPK_complex *__tau,
        __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarfb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__v, __CLPK_integer *__ldv, __CLPK_complex *__t,
        __CLPK_integer *__ldt, __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_complex *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarfg_(__CLPK_integer *__n, __CLPK_complex *__alpha, __CLPK_complex *__x,
        __CLPK_integer *__incx,
        __CLPK_complex *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarfp_(__CLPK_integer *__n, __CLPK_complex *__alpha, __CLPK_complex *__x,
        __CLPK_integer *__incx,
        __CLPK_complex *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarft_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_complex *__tau, __CLPK_complex *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarfx_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__v, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clargv_(__CLPK_integer *__n, __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_complex *__y, __CLPK_integer *__incy, __CLPK_real *__c__,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarnv_(__CLPK_integer *__idist, __CLPK_integer *__iseed,
        __CLPK_integer *__n,
        __CLPK_complex *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarrv_(__CLPK_integer *__n, __CLPK_real *__vl, __CLPK_real *__vu,
        __CLPK_real *__d__, __CLPK_real *__l, __CLPK_real *__pivmin,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_integer *__dol,
        __CLPK_integer *__dou, __CLPK_real *__minrgp, __CLPK_real *__rtol1,
        __CLPK_real *__rtol2, __CLPK_real *__w, __CLPK_real *__werr,
        __CLPK_real *__wgap, __CLPK_integer *__iblock, __CLPK_integer *__indexw,
        __CLPK_real *__gers, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarscl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_complex *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int clartg_(__CLPK_complex *__f, __CLPK_complex *__g, __CLPK_real *__cs,
        __CLPK_complex *__sn,
        __CLPK_complex *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

int clartv_(__CLPK_integer *__n, __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_complex *__y, __CLPK_integer *__incy, __CLPK_real *__c__,
        __CLPK_complex *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarz_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__l, __CLPK_complex *__v, __CLPK_integer *__incv,
        __CLPK_complex *__tau, __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarzb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_complex *__t, __CLPK_integer *__ldt, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int clarzt_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_complex *__v, __CLPK_integer *__ldv,
        __CLPK_complex *__tau, __CLPK_complex *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int clascl_(char *__type__, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_real *__cfrom, __CLPK_real *__cto, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clascl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_complex *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int claset_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__alpha, __CLPK_complex *__beta, __CLPK_complex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int clasr_(char *__side, char *__pivot, char *__direct, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__c__, __CLPK_real *__s,
        __CLPK_complex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int classq_(__CLPK_integer *__n, __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_real *__scale,
        __CLPK_real *__sumsq)
API_AVAILABLE(macos(10.2), ios(4.0));

int claswp_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__k1, __CLPK_integer *__k2, __CLPK_integer *__ipiv,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int clasyf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_complex *__w, __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatbs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_integer *__kd, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__x, __CLPK_real *__scale,
        __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatdf_(__CLPK_integer *__ijob, __CLPK_integer *__n, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_complex *__rhs, __CLPK_real *__rdsum,
        __CLPK_real *__rdscal, __CLPK_integer *__ipiv,
        __CLPK_integer *__jpiv)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatps_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_complex *__ap, __CLPK_complex *__x,
        __CLPK_real *__scale, __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatrd_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__e,
        __CLPK_complex *__tau, __CLPK_complex *__w,
        __CLPK_integer *__ldw)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatrs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__x, __CLPK_real *__scale, __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatrz_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__l,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clatzm_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__v, __CLPK_integer *__incv, __CLPK_complex *__tau,
        __CLPK_complex *__c1, __CLPK_complex *__c2, __CLPK_integer *__ldc,
        __CLPK_complex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int clauu2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int clauum_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbcon_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_real *__anorm,
        __CLPK_real *__rcond, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbequ_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab, __CLPK_real *__s,
        __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_complex *__afb, __CLPK_integer *__ldafb, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbstf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__afb, __CLPK_integer *__ldafb,
        char *__equed, __CLPK_real *__s, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbtf2_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbtrf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpbtrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpftrf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpftri_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpftrs_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpocon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpoequ_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpoequb_(__CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cporfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__af,
        __CLPK_integer *__ldaf, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cposvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__af, __CLPK_integer *__ldaf, char *__equed,
        __CLPK_real *__s, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpotf2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpotrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpotri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpotrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cppcon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__anorm, __CLPK_real *__rcond, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cppequ_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_complex *__afp, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cppsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cppsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__ap, __CLPK_complex *__afp,
        char *__equed, __CLPK_real *__s, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpptri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpstf2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_real *__tol, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpstrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_real *__tol, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cptcon_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_complex *__e,
        __CLPK_real *__anorm, __CLPK_real *__rcond, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpteqr_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cptrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__d__, __CLPK_complex *__e, __CLPK_real *__df,
        __CLPK_complex *__ef, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cptsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__d__,
        __CLPK_complex *__e, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cptsvx_(char *__fact, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__d__, __CLPK_complex *__e, __CLPK_real *__df,
        __CLPK_complex *__ef, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpttrf_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_complex *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cpttrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__d__, __CLPK_complex *__e, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cptts2_(__CLPK_integer *__iuplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__d__, __CLPK_complex *__e,
        __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int crot_(__CLPK_integer *__n, __CLPK_complex *__cx, __CLPK_integer *__incx,
        __CLPK_complex *__cy, __CLPK_integer *__incy, __CLPK_real *__c__,
        __CLPK_complex *__s)
API_AVAILABLE(macos(10.2), ios(4.0));

int cspcon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cspmv_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__alpha,
        __CLPK_complex *__ap, __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_complex *__beta, __CLPK_complex *__y,
        __CLPK_integer *__incy)
API_AVAILABLE(macos(10.2), ios(4.0));

int cspr_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__alpha,
        __CLPK_complex *__x, __CLPK_integer *__incx,
        __CLPK_complex *__ap)
API_AVAILABLE(macos(10.2), ios(4.0));

int csprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_complex *__afp, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cspsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cspsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__ap, __CLPK_complex *__afp,
        __CLPK_integer *__ipiv, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csptri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__ap, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csrscl_(__CLPK_integer *__n, __CLPK_real *__sa, __CLPK_complex *__sx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int cstedc_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_complex *__work, __CLPK_integer *__lwork, __CLPK_real *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cstegr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_complex *__z__, __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cstein_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_integer *__iblock,
        __CLPK_integer *__isplit, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cstemr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__nzc, __CLPK_integer *__isuppz,
        __CLPK_logical *__tryrac, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csteqr_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csycon_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__anorm,
        __CLPK_real *__rcond, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csyequb_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csymv_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__alpha,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__x,
        __CLPK_integer *__incx, __CLPK_complex *__beta, __CLPK_complex *__y,
        __CLPK_integer *__incy)
API_AVAILABLE(macos(10.2), ios(4.0));

int csyr_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__alpha,
        __CLPK_complex *__x, __CLPK_integer *__incx, __CLPK_complex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int csyrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csysv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csysvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csytf2_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csytrf_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csytri_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int csytrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctbcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_complex *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__rcond, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctbrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctbtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_complex *__ab,
        __CLPK_integer *__ldab, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctfsm_(char *__transr, char *__side, char *__uplo, char *__trans,
        char *__diag, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__alpha, __CLPK_complex *__a, __CLPK_complex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctftri_(char *__transr, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctfttp_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__arf, __CLPK_complex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctfttr_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__arf, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_complex *__s, __CLPK_integer *__lds,
        __CLPK_complex *__p, __CLPK_integer *__ldp, __CLPK_complex *__vl,
        __CLPK_integer *__ldvl, __CLPK_complex *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgex2_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__j1,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgexc_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__q,
        __CLPK_integer *__ldq, __CLPK_complex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgsen_(__CLPK_integer *__ijob, __CLPK_logical *__wantq,
        __CLPK_logical *__wantz, __CLPK_logical *__select, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__alpha, __CLPK_complex *__beta,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_complex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__m, __CLPK_real *__pl,
        __CLPK_real *__pr, __CLPK_real *__dif, __CLPK_complex *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgsja_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_real *__tola,
        __CLPK_real *__tolb, __CLPK_real *__alpha, __CLPK_real *__beta,
        __CLPK_complex *__u, __CLPK_integer *__ldu, __CLPK_complex *__v,
        __CLPK_integer *__ldv, __CLPK_complex *__q, __CLPK_integer *__ldq,
        __CLPK_complex *__work, __CLPK_integer *__ncycle,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__vl,
        __CLPK_integer *__ldvl, __CLPK_complex *__vr, __CLPK_integer *__ldvr,
        __CLPK_real *__s, __CLPK_real *__dif, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgsy2_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__d__, __CLPK_integer *__ldd,
        __CLPK_complex *__e, __CLPK_integer *__lde, __CLPK_complex *__f,
        __CLPK_integer *__ldf, __CLPK_real *__scale, __CLPK_real *__rdsum,
        __CLPK_real *__rdscal,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctgsyl_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__d__, __CLPK_integer *__ldd,
        __CLPK_complex *__e, __CLPK_integer *__lde, __CLPK_complex *__f,
        __CLPK_integer *__ldf, __CLPK_real *__scale, __CLPK_real *__dif,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctpcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_real *__rcond, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctprfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__ap, __CLPK_complex *__b,
        __CLPK_integer *__ldb, __CLPK_complex *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_complex *__work,
        __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctptri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctptrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__ap, __CLPK_complex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctpttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__ap, __CLPK_complex *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctpttr_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_real *__rcond,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_complex *__vl, __CLPK_integer *__ldvl, __CLPK_complex *__vr,
        __CLPK_integer *__ldvr, __CLPK_integer *__mm, __CLPK_integer *__m,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrexc_(char *__compq, __CLPK_integer *__n, __CLPK_complex *__t,
        __CLPK_integer *__ldt, __CLPK_complex *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb, __CLPK_complex *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_complex *__work, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrsen_(char *__job, char *__compq, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_complex *__q, __CLPK_integer *__ldq, __CLPK_complex *__w,
        __CLPK_integer *__m, __CLPK_real *__s, __CLPK_real *__sep,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_complex *__t, __CLPK_integer *__ldt,
        __CLPK_complex *__vl, __CLPK_integer *__ldvl, __CLPK_complex *__vr,
        __CLPK_integer *__ldvr, __CLPK_real *__s, __CLPK_real *__sep,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_complex *__work,
        __CLPK_integer *__ldwork, __CLPK_real *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrsyl_(char *__trana, char *__tranb, __CLPK_integer *__isgn,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_complex *__c__, __CLPK_integer *__ldc, __CLPK_real *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrti2_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrtri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctrttp_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctzrqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ctzrzf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cung2l_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cung2r_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungbr_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__tau, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunghr_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunglq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungql_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungqr_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungrq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cungtr_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunm2l_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunm2r_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmbr_(char *__vect, char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmhr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__c__, __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunml2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmlq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmql_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmqr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmr2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmr3_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__c__, __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmrq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_complex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__tau, __CLPK_complex *__c__,
        __CLPK_integer *__ldc, __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmrz_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_complex *__a, __CLPK_integer *__lda, __CLPK_complex *__tau,
        __CLPK_complex *__c__, __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cunmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__a, __CLPK_integer *__lda,
        __CLPK_complex *__tau, __CLPK_complex *__c__, __CLPK_integer *__ldc,
        __CLPK_complex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cupgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_complex *__ap,
        __CLPK_complex *__tau, __CLPK_complex *__q, __CLPK_integer *__ldq,
        __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int cupmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_complex *__ap, __CLPK_complex *__tau,
        __CLPK_complex *__c__, __CLPK_integer *__ldc, __CLPK_complex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dbdsdc_(char *__uplo, char *__compq, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublereal *__q, __CLPK_integer *__iq,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dbdsqr_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__ncvt,
        __CLPK_integer *__nru, __CLPK_integer *__ncc, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__vt, __CLPK_integer *__ldvt,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ddisna_(char *__job, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__sep,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbbrd_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__ncc, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__pt,
        __CLPK_integer *__ldpt, __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbcon_(char *__norm, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__rowcnd, __CLPK_doublereal *__colcnd,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__rowcnd, __CLPK_doublereal *__colcnd,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__afb,
        __CLPK_integer *__ldafb, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbsv_(__CLPK_integer *__n, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__afb, __CLPK_integer *__ldafb,
        __CLPK_integer *__ipiv, char *__equed, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbtf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbtrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgbtrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgebak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__scale, __CLPK_integer *__m, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgebal_(char *__job, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgebd2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tauq, __CLPK_doublereal *__taup,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgebrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tauq, __CLPK_doublereal *__taup,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgecon_(char *__norm, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgees_(char *__jobvs, char *__sort, __CLPK_L_fp __select,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_doublereal *__wr,
        __CLPK_doublereal *__wi, __CLPK_doublereal *__vs,
        __CLPK_integer *__ldvs, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeesx_(char *__jobvs, char *__sort, __CLPK_L_fp __select, char *__sense,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_doublereal *__wr,
        __CLPK_doublereal *__wi, __CLPK_doublereal *__vs,
        __CLPK_integer *__ldvs, __CLPK_doublereal *__rconde,
        __CLPK_doublereal *__rcondv, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__wr,
        __CLPK_doublereal *__wi, __CLPK_doublereal *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr, __CLPK_integer *__ilo,
        __CLPK_integer *__ihi, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__abnrm, __CLPK_doublereal *__rconde,
        __CLPK_doublereal *__rcondv, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgegs_(char *__jobvsl, char *__jobvsr, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__alphar,
        __CLPK_doublereal *__alphai, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__vsl, __CLPK_integer *__ldvsl,
        __CLPK_doublereal *__vsr, __CLPK_integer *__ldvsr,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgegv_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__alphar,
        __CLPK_doublereal *__alphai, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgehd2_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgehrd_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgejsv_(char *__joba, char *__jobu, char *__jobv, char *__jobr,
        char *__jobt, char *__jobp, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__sva,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgels_(char *__trans, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelsd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__s,
        __CLPK_doublereal *__rcond, __CLPK_integer *__rank,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelss_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__s,
        __CLPK_doublereal *__rcond, __CLPK_integer *__rank,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelsx_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt,
        __CLPK_doublereal *__rcond, __CLPK_integer *__rank,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgelsy_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt,
        __CLPK_doublereal *__rcond, __CLPK_integer *__rank,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeql2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeqlf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeqp3_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeqpf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeqr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgeqrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgerfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgerq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgerqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesc2_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__rhs, __CLPK_integer *__ipiv,
        __CLPK_integer *__jpiv,
        __CLPK_doublereal *__scale)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesdd_(char *__jobz, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesvd_(char *__jobu, char *__jobvt, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__s, __CLPK_doublereal *__u, __CLPK_integer *__ldu,
        __CLPK_doublereal *__vt, __CLPK_integer *__ldvt,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesvj_(char *__joba, char *__jobu, char *__jobv, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__sva, __CLPK_integer *__mv, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgesvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        char *__equed, __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgetc2_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgetf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgetrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgetri_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgetrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggbak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_integer *__m, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggbal_(char *__job, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgges_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_integer *__sdim,
        __CLPK_doublereal *__alphar, __CLPK_doublereal *__alphai,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__vsl,
        __CLPK_integer *__ldvsl, __CLPK_doublereal *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggesx_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        char *__sense, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__sdim, __CLPK_doublereal *__alphar,
        __CLPK_doublereal *__alphai, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__vsl, __CLPK_integer *__ldvsl,
        __CLPK_doublereal *__vsr, __CLPK_integer *__ldvsr,
        __CLPK_doublereal *__rconde, __CLPK_doublereal *__rcondv,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__alphar,
        __CLPK_doublereal *__alphai, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__alphar, __CLPK_doublereal *__alphai,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_doublereal *__abnrm, __CLPK_doublereal *__bbnrm,
        __CLPK_doublereal *__rconde, __CLPK_doublereal *__rcondv,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggglm_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__d__, __CLPK_doublereal *__x,
        __CLPK_doublereal *__y, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgghrd_(char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__z__,
        __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgglse_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__p,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__x,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggqrf_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__taua, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__taub,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggrqf_(__CLPK_integer *__m, __CLPK_integer *__p, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__taua, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__taub,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggsvd_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__p, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dggsvp_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__tola, __CLPK_doublereal *__tolb,
        __CLPK_integer *__k, __CLPK_integer *__l, __CLPK_doublereal *__u,
        __CLPK_integer *__ldu, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_integer *__iwork,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgsvj0_(char *__jobv, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__sva, __CLPK_integer *__mv, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv, __CLPK_doublereal *__eps,
        __CLPK_doublereal *__sfmin, __CLPK_doublereal *__tol,
        __CLPK_integer *__nsweep, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgsvj1_(char *__jobv, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__n1, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__sva,
        __CLPK_integer *__mv, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_doublereal *__eps, __CLPK_doublereal *__sfmin,
        __CLPK_doublereal *__tol, __CLPK_integer *__nsweep,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgtcon_(char *__norm, __CLPK_integer *__n, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgtrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__dl, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__du, __CLPK_doublereal *__dlf,
        __CLPK_doublereal *__df, __CLPK_doublereal *__duf,
        __CLPK_doublereal *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgtsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgtsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__dlf, __CLPK_doublereal *__df,
        __CLPK_doublereal *__duf, __CLPK_doublereal *__du2,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgttrf_(__CLPK_integer *__n, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__du2, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgttrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__dl, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__du, __CLPK_doublereal *__du2,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dgtts2_(__CLPK_integer *__itrans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int dhgeqz_(char *__job, char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_doublereal *__h__,
        __CLPK_integer *__ldh, __CLPK_doublereal *__t, __CLPK_integer *__ldt,
        __CLPK_doublereal *__alphar, __CLPK_doublereal *__alphai,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dhsein_(char *__side, char *__eigsrc, char *__initv,
        __CLPK_logical *__select, __CLPK_integer *__n, __CLPK_doublereal *__h__,
        __CLPK_integer *__ldh, __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_doublereal *__work,
        __CLPK_integer *__ifaill, __CLPK_integer *__ifailr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dhseqr_(char *__job, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_doublereal *__h__,
        __CLPK_integer *__ldh, __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));


        __CLPK_logical disnan_(__CLPK_doublereal *__din)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlabad_(__CLPK_doublereal *__small,
        __CLPK_doublereal *__large)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlabrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__tauq,
        __CLPK_doublereal *__taup, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlacn2_(__CLPK_integer *__n, __CLPK_doublereal *__v, __CLPK_doublereal *__x,
        __CLPK_integer *__isgn, __CLPK_doublereal *__est,
        __CLPK_integer *__kase,
        __CLPK_integer *__isave)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlacon_(__CLPK_integer *__n, __CLPK_doublereal *__v, __CLPK_doublereal *__x,
        __CLPK_integer *__isgn, __CLPK_doublereal *__est,
        __CLPK_integer *__kase)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlacpy_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int dladiv_(__CLPK_doublereal *__a, __CLPK_doublereal *__b,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__p,
        __CLPK_doublereal *__q)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlae2_(__CLPK_doublereal *__a, __CLPK_doublereal *__b,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rt1,
        __CLPK_doublereal *__rt2)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaebz_(__CLPK_integer *__ijob, __CLPK_integer *__nitmax,
        __CLPK_integer *__n, __CLPK_integer *__mmax, __CLPK_integer *__minp,
        __CLPK_integer *__nbmin, __CLPK_doublereal *__abstol,
        __CLPK_doublereal *__reltol, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__e2, __CLPK_integer *__nval,
        __CLPK_doublereal *__ab, __CLPK_doublereal *__c__,
        __CLPK_integer *__mout, __CLPK_integer *__nab,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed0_(__CLPK_integer *__icompq, __CLPK_integer *__qsiz,
        __CLPK_integer *__n, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__qstore, __CLPK_integer *__ldqs,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed1_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_integer *__indxq,
        __CLPK_doublereal *__rho, __CLPK_integer *__cutpnt,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed2_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__n1,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__indxq, __CLPK_doublereal *__rho,
        __CLPK_doublereal *__z__, __CLPK_doublereal *__dlamda,
        __CLPK_doublereal *__w, __CLPK_doublereal *__q2, __CLPK_integer *__indx,
        __CLPK_integer *__indxc, __CLPK_integer *__indxp,
        __CLPK_integer *__coltyp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed3_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__n1,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__rho, __CLPK_doublereal *__dlamda,
        __CLPK_doublereal *__q2, __CLPK_integer *__indx, __CLPK_integer *__ctot,
        __CLPK_doublereal *__w, __CLPK_doublereal *__s,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed4_(__CLPK_integer *__n, __CLPK_integer *__i__,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__delta, __CLPK_doublereal *__rho,
        __CLPK_doublereal *__dlam,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed5_(__CLPK_integer *__i__, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__z__, __CLPK_doublereal *__delta,
        __CLPK_doublereal *__rho,
        __CLPK_doublereal *__dlam)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed6_(__CLPK_integer *__kniter, __CLPK_logical *__orgati,
        __CLPK_doublereal *__rho, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__z__, __CLPK_doublereal *__finit,
        __CLPK_doublereal *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed7_(__CLPK_integer *__icompq, __CLPK_integer *__n,
        __CLPK_integer *__qsiz, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__indxq, __CLPK_doublereal *__rho,
        __CLPK_integer *__cutpnt, __CLPK_doublereal *__qstore,
        __CLPK_integer *__qptr, __CLPK_integer *__prmptr,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_doublereal *__givnum,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed8_(__CLPK_integer *__icompq, __CLPK_integer *__k, __CLPK_integer *__n,
        __CLPK_integer *__qsiz, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_integer *__indxq,
        __CLPK_doublereal *__rho, __CLPK_integer *__cutpnt,
        __CLPK_doublereal *__z__, __CLPK_doublereal *__dlamda,
        __CLPK_doublereal *__q2, __CLPK_integer *__ldq2, __CLPK_doublereal *__w,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_doublereal *__givnum,
        __CLPK_integer *__indxp, __CLPK_integer *__indx,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaed9_(__CLPK_integer *__k, __CLPK_integer *__kstart,
        __CLPK_integer *__kstop, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__rho,
        __CLPK_doublereal *__dlamda, __CLPK_doublereal *__w,
        __CLPK_doublereal *__s, __CLPK_integer *__lds,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaeda_(__CLPK_integer *__n, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm,
        __CLPK_integer *__prmptr, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_doublereal *__givnum, __CLPK_doublereal *__q,
        __CLPK_integer *__qptr, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__ztemp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaein_(__CLPK_logical *__rightv, __CLPK_logical *__noinit,
        __CLPK_integer *__n, __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_doublereal *__vr, __CLPK_doublereal *__vi,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__work, __CLPK_doublereal *__eps3,
        __CLPK_doublereal *__smlnum, __CLPK_doublereal *__bignum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaev2_(__CLPK_doublereal *__a, __CLPK_doublereal *__b,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rt1,
        __CLPK_doublereal *__rt2, __CLPK_doublereal *__cs1,
        __CLPK_doublereal *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaexc_(__CLPK_logical *__wantq, __CLPK_integer *__n,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_doublereal *__q,
        __CLPK_integer *__ldq, __CLPK_integer *__j1, __CLPK_integer *__n1,
        __CLPK_integer *__n2, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlag2_(__CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__safmin, __CLPK_doublereal *__scale1,
        __CLPK_doublereal *__scale2, __CLPK_doublereal *__wr1,
        __CLPK_doublereal *__wr2,
        __CLPK_doublereal *__wi)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlag2s_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_real *__sa, __CLPK_integer *__ldsa,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlags2_(__CLPK_logical *__upper, __CLPK_doublereal *__a1,
        __CLPK_doublereal *__a2, __CLPK_doublereal *__a3,
        __CLPK_doublereal *__b1, __CLPK_doublereal *__b2,
        __CLPK_doublereal *__b3, __CLPK_doublereal *__csu,
        __CLPK_doublereal *__snu, __CLPK_doublereal *__csv,
        __CLPK_doublereal *__snv, __CLPK_doublereal *__csq,
        __CLPK_doublereal *__snq)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlagtf_(__CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_doublereal *__lambda, __CLPK_doublereal *__b,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__tol,
        __CLPK_doublereal *__d__, __CLPK_integer *__in,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlagtm_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__dl,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__du,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlagts_(__CLPK_integer *__job, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_doublereal *__b, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__d__, __CLPK_integer *__in, __CLPK_doublereal *__y,
        __CLPK_doublereal *__tol,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlagv2_(__CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__alphar, __CLPK_doublereal *__alphai,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__csl,
        __CLPK_doublereal *__snl, __CLPK_doublereal *__csr,
        __CLPK_doublereal *__snr)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlahqr_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlahr2_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_doublereal *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlahrd_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_doublereal *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaic1_(__CLPK_integer *__job, __CLPK_integer *__j, __CLPK_doublereal *__x,
        __CLPK_doublereal *__sest, __CLPK_doublereal *__w,
        __CLPK_doublereal *__gamma, __CLPK_doublereal *__sestpr,
        __CLPK_doublereal *__s,
        __CLPK_doublereal *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_logical dlaisnan_(__CLPK_doublereal *__din1,
        __CLPK_doublereal *__din2)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaln2_(__CLPK_logical *__ltrans, __CLPK_integer *__na,
        __CLPK_integer *__nw, __CLPK_doublereal *__smin,
        __CLPK_doublereal *__ca, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__d1, __CLPK_doublereal *__d2,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__wr,
        __CLPK_doublereal *__wi, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__xnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlals0_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__bx,
        __CLPK_integer *__ldbx, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_doublereal *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_doublereal *__poles,
        __CLPK_doublereal *__difl, __CLPK_doublereal *__difr,
        __CLPK_doublereal *__z__, __CLPK_integer *__k, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__s, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlalsa_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__bx, __CLPK_integer *__ldbx,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__vt,
        __CLPK_integer *__k, __CLPK_doublereal *__difl,
        __CLPK_doublereal *__difr, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__poles, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_integer *__perm, __CLPK_doublereal *__givnum,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlalsd_(char *__uplo, __CLPK_integer *__smlsiz, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__rcond, __CLPK_integer *__rank,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamrg_(__CLPK_integer *__n1, __CLPK_integer *__n2, __CLPK_doublereal *__a,
        __CLPK_integer *__dtrd1, __CLPK_integer *__dtrd2,
        __CLPK_integer *__index)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer dlaneg_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__lld, __CLPK_doublereal *__sigma,
        __CLPK_doublereal *__pivmin,
        __CLPK_integer *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlangb_(char *__norm, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlange_(char *__norm, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlangt_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublereal *__dl, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__du)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlanhs_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlansb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlansf_(char *__norm, char *__transr, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlansp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlanst_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlansy_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlantb_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlantp_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlantr_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlanv2_(__CLPK_doublereal *__a, __CLPK_doublereal *__b,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__rt1r, __CLPK_doublereal *__rt1i,
        __CLPK_doublereal *__rt2r, __CLPK_doublereal *__rt2i,
        __CLPK_doublereal *__cs,
        __CLPK_doublereal *__sn)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlapll_(__CLPK_integer *__n, __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__y, __CLPK_integer *__incy,
        __CLPK_doublereal *__ssmin)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlapmt_(__CLPK_logical *__forwrd, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_integer *__k)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlapy2_(__CLPK_doublereal *__x,
        __CLPK_doublereal *__y)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlapy3_(__CLPK_doublereal *__x, __CLPK_doublereal *__y,
        __CLPK_doublereal *__z__)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqgb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__rowcnd, __CLPK_doublereal *__colcnd,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqge_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqp2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__vn1,
        __CLPK_doublereal *__vn2,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqps_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_integer *__nb, __CLPK_integer *__kb, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__vn1, __CLPK_doublereal *__vn2,
        __CLPK_doublereal *__auxv, __CLPK_doublereal *__f,
        __CLPK_integer *__ldf)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr0_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr1_(__CLPK_integer *__n, __CLPK_doublereal *__h__,
        __CLPK_integer *__ldh, __CLPK_doublereal *__sr1,
        __CLPK_doublereal *__si1, __CLPK_doublereal *__sr2,
        __CLPK_doublereal *__si2,
        __CLPK_doublereal *__v)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr2_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz, __CLPK_integer *__ns,
        __CLPK_integer *__nd, __CLPK_doublereal *__sr, __CLPK_doublereal *__si,
        __CLPK_doublereal *__v, __CLPK_integer *__ldv, __CLPK_integer *__nh,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_integer *__nv,
        __CLPK_doublereal *__wv, __CLPK_integer *__ldwv,
        __CLPK_doublereal *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr3_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz, __CLPK_integer *__ns,
        __CLPK_integer *__nd, __CLPK_doublereal *__sr, __CLPK_doublereal *__si,
        __CLPK_doublereal *__v, __CLPK_integer *__ldv, __CLPK_integer *__nh,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_integer *__nv,
        __CLPK_doublereal *__wv, __CLPK_integer *__ldwv,
        __CLPK_doublereal *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr4_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__h__, __CLPK_integer *__ldh,
        __CLPK_doublereal *__wr, __CLPK_doublereal *__wi,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqr5_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__kacc22, __CLPK_integer *__n, __CLPK_integer *__ktop,
        __CLPK_integer *__kbot, __CLPK_integer *__nshfts,
        __CLPK_doublereal *__sr, __CLPK_doublereal *__si,
        __CLPK_doublereal *__h__, __CLPK_integer *__ldh, __CLPK_integer *__iloz,
        __CLPK_integer *__ihiz, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__v, __CLPK_integer *__ldv, __CLPK_doublereal *__u,
        __CLPK_integer *__ldu, __CLPK_integer *__nv, __CLPK_doublereal *__wv,
        __CLPK_integer *__ldwv, __CLPK_integer *__nh, __CLPK_doublereal *__wh,
        __CLPK_integer *__ldwh)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqsb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqsp_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqsy_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaqtr_(__CLPK_logical *__ltran, __CLPK_logical *__l__CLPK_real,
        __CLPK_integer *__n, __CLPK_doublereal *__t, __CLPK_integer *__ldt,
        __CLPK_doublereal *__b, __CLPK_doublereal *__w,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__x,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlar1v_(__CLPK_integer *__n, __CLPK_integer *__b1, __CLPK_integer *__bn,
        __CLPK_doublereal *__lambda, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__l, __CLPK_doublereal *__ld,
        __CLPK_doublereal *__lld, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__gaptol, __CLPK_doublereal *__z__,
        __CLPK_logical *__wantnc, __CLPK_integer *__negcnt,
        __CLPK_doublereal *__ztz, __CLPK_doublereal *__mingma,
        __CLPK_integer *__r__, __CLPK_integer *__isuppz,
        __CLPK_doublereal *__nrminv, __CLPK_doublereal *__resid,
        __CLPK_doublereal *__rqcorr,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlar2v_(__CLPK_integer *__n, __CLPK_doublereal *__x, __CLPK_doublereal *__y,
        __CLPK_doublereal *__z__, __CLPK_integer *__incx,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarf_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__v, __CLPK_integer *__incv,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarfb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__v, __CLPK_integer *__ldv, __CLPK_doublereal *__t,
        __CLPK_integer *__ldt, __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarfg_(__CLPK_integer *__n, __CLPK_doublereal *__alpha,
        __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarfp_(__CLPK_integer *__n, __CLPK_doublereal *__alpha,
        __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarft_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarfx_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__v, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlargv_(__CLPK_integer *__n, __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__y, __CLPK_integer *__incy,
        __CLPK_doublereal *__c__,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarnv_(__CLPK_integer *__idist, __CLPK_integer *__iseed,
        __CLPK_integer *__n,
        __CLPK_doublereal *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarra_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__e2,
        __CLPK_doublereal *__spltol, __CLPK_doublereal *__tnrm,
        __CLPK_integer *__nsplit, __CLPK_integer *__isplit,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrb_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__lld, __CLPK_integer *__ifirst,
        __CLPK_integer *__ilast, __CLPK_doublereal *__rtol1,
        __CLPK_doublereal *__rtol2, __CLPK_integer *__offset,
        __CLPK_doublereal *__w, __CLPK_doublereal *__wgap,
        __CLPK_doublereal *__werr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__spdiam, __CLPK_integer *__twist,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrc_(char *__jobt, __CLPK_integer *__n, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__pivmin,
        __CLPK_integer *__eigcnt, __CLPK_integer *__lcnt,
        __CLPK_integer *__rcnt,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrd_(char *__range, char *__order, __CLPK_integer *__n,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__gers,
        __CLPK_doublereal *__reltol, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__e2,
        __CLPK_doublereal *__pivmin, __CLPK_integer *__nsplit,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_doublereal *__werr, __CLPK_doublereal *__wl,
        __CLPK_doublereal *__wu, __CLPK_integer *__iblock,
        __CLPK_integer *__indexw, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarre_(char *__range, __CLPK_integer *__n, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__e2, __CLPK_doublereal *__rtol1,
        __CLPK_doublereal *__rtol2, __CLPK_doublereal *__spltol,
        __CLPK_integer *__nsplit, __CLPK_integer *__isplit, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__werr,
        __CLPK_doublereal *__wgap, __CLPK_integer *__iblock,
        __CLPK_integer *__indexw, __CLPK_doublereal *__gers,
        __CLPK_doublereal *__pivmin, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrf_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__l, __CLPK_doublereal *__ld,
        __CLPK_integer *__clstrt, __CLPK_integer *__clend,
        __CLPK_doublereal *__w, __CLPK_doublereal *__wgap,
        __CLPK_doublereal *__werr, __CLPK_doublereal *__spdiam,
        __CLPK_doublereal *__clgapl, __CLPK_doublereal *__clgapr,
        __CLPK_doublereal *__pivmin, __CLPK_doublereal *__sigma,
        __CLPK_doublereal *__dplus, __CLPK_doublereal *__lplus,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrj_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e2, __CLPK_integer *__ifirst,
        __CLPK_integer *__ilast, __CLPK_doublereal *__rtol,
        __CLPK_integer *__offset, __CLPK_doublereal *__w,
        __CLPK_doublereal *__werr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__spdiam,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrk_(__CLPK_integer *__n, __CLPK_integer *__iw, __CLPK_doublereal *__gl,
        __CLPK_doublereal *__gu, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e2, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__reltol, __CLPK_doublereal *__w,
        __CLPK_doublereal *__werr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrr_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarrv_(__CLPK_integer *__n, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__l, __CLPK_doublereal *__pivmin,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_integer *__dol,
        __CLPK_integer *__dou, __CLPK_doublereal *__minrgp,
        __CLPK_doublereal *__rtol1, __CLPK_doublereal *__rtol2,
        __CLPK_doublereal *__w, __CLPK_doublereal *__werr,
        __CLPK_doublereal *__wgap, __CLPK_integer *__iblock,
        __CLPK_integer *__indexw, __CLPK_doublereal *__gers,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarscl2_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlartg_(__CLPK_doublereal *__f, __CLPK_doublereal *__g,
        __CLPK_doublereal *__cs, __CLPK_doublereal *__sn,
        __CLPK_doublereal *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlartv_(__CLPK_integer *__n, __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__y, __CLPK_integer *__incy,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaruv_(__CLPK_integer *__iseed, __CLPK_integer *__n,
        __CLPK_doublereal *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarz_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__l, __CLPK_doublereal *__v, __CLPK_integer *__incv,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarzb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_doublereal *__t, __CLPK_integer *__ldt, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlarzt_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__v, __CLPK_integer *__ldv,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlas2_(__CLPK_doublereal *__f, __CLPK_doublereal *__g,
        __CLPK_doublereal *__h__, __CLPK_doublereal *__ssmin,
        __CLPK_doublereal *__ssmax)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlascl_(char *__type__, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_doublereal *__cfrom, __CLPK_doublereal *__cto,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlascl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd0_(__CLPK_integer *__n, __CLPK_integer *__sqre,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__vt,
        __CLPK_integer *__ldvt, __CLPK_integer *__smlsiz,
        __CLPK_integer *__iwork, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd1_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__alpha,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__u,
        __CLPK_integer *__ldu, __CLPK_doublereal *__vt, __CLPK_integer *__ldvt,
        __CLPK_integer *__idxq, __CLPK_integer *__iwork,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd2_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_integer *__k, __CLPK_doublereal *__d__, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublereal *__dsigma,
        __CLPK_doublereal *__u2, __CLPK_integer *__ldu2,
        __CLPK_doublereal *__vt2, __CLPK_integer *__ldvt2,
        __CLPK_integer *__idxp, __CLPK_integer *__idx, __CLPK_integer *__idxc,
        __CLPK_integer *__idxq, __CLPK_integer *__coltyp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd3_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_integer *__k, __CLPK_doublereal *__d__, __CLPK_doublereal *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__dsigma,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__u2,
        __CLPK_integer *__ldu2, __CLPK_doublereal *__vt, __CLPK_integer *__ldvt,
        __CLPK_doublereal *__vt2, __CLPK_integer *__ldvt2,
        __CLPK_integer *__idxc, __CLPK_integer *__ctot,
        __CLPK_doublereal *__z__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd4_(__CLPK_integer *__n, __CLPK_integer *__i__,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__delta, __CLPK_doublereal *__rho,
        __CLPK_doublereal *__sigma, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd5_(__CLPK_integer *__i__, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__z__, __CLPK_doublereal *__delta,
        __CLPK_doublereal *__rho, __CLPK_doublereal *__dsigma,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd6_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__vf, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_integer *__idxq, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_doublereal *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_doublereal *__poles,
        __CLPK_doublereal *__difl, __CLPK_doublereal *__difr,
        __CLPK_doublereal *__z__, __CLPK_integer *__k, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__s, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd7_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__k,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__zw, __CLPK_doublereal *__vf,
        __CLPK_doublereal *__vfw, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vlw, __CLPK_doublereal *__alpha,
        __CLPK_doublereal *__beta, __CLPK_doublereal *__dsigma,
        __CLPK_integer *__idx, __CLPK_integer *__idxp, __CLPK_integer *__idxq,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_doublereal *__givnum, __CLPK_integer *__ldgnum,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasd8_(__CLPK_integer *__icompq, __CLPK_integer *__k,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__vf, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__difl, __CLPK_doublereal *__difr,
        __CLPK_integer *__lddifr, __CLPK_doublereal *__dsigma,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasda_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__sqre, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__u, __CLPK_integer *__ldu,
        __CLPK_doublereal *__vt, __CLPK_integer *__k, __CLPK_doublereal *__difl,
        __CLPK_doublereal *__difr, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__poles, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_integer *__perm, __CLPK_doublereal *__givnum,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasdq_(char *__uplo, __CLPK_integer *__sqre, __CLPK_integer *__n,
        __CLPK_integer *__ncvt, __CLPK_integer *__nru, __CLPK_integer *__ncc,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vt, __CLPK_integer *__ldvt, __CLPK_doublereal *__u,
        __CLPK_integer *__ldu, __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasdt_(__CLPK_integer *__n, __CLPK_integer *__lvl, __CLPK_integer *__nd,
        __CLPK_integer *__inode, __CLPK_integer *__ndiml,
        __CLPK_integer *__ndimr,
        __CLPK_integer *__msub)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaset_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq1_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq2_(__CLPK_integer *__n, __CLPK_doublereal *__z__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq3_(__CLPK_integer *__i0, __CLPK_integer *__n0,
        __CLPK_doublereal *__z__, __CLPK_integer *__pp,
        __CLPK_doublereal *__dmin__, __CLPK_doublereal *__sigma,
        __CLPK_doublereal *__desig, __CLPK_doublereal *__qmax,
        __CLPK_integer *__nfail, __CLPK_integer *__iter, __CLPK_integer *__ndiv,
        __CLPK_logical *__ieee, __CLPK_integer *__ttype,
        __CLPK_doublereal *__dmin1, __CLPK_doublereal *__dmin2,
        __CLPK_doublereal *__dn, __CLPK_doublereal *__dn1,
        __CLPK_doublereal *__dn2, __CLPK_doublereal *__g,
        __CLPK_doublereal *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq4_(__CLPK_integer *__i0, __CLPK_integer *__n0,
        __CLPK_doublereal *__z__, __CLPK_integer *__pp, __CLPK_integer *__n0in,
        __CLPK_doublereal *__dmin__, __CLPK_doublereal *__dmin1,
        __CLPK_doublereal *__dmin2, __CLPK_doublereal *__dn,
        __CLPK_doublereal *__dn1, __CLPK_doublereal *__dn2,
        __CLPK_doublereal *__tau, __CLPK_integer *__ttype,
        __CLPK_doublereal *__g)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq5_(__CLPK_integer *__i0, __CLPK_integer *__n0,
        __CLPK_doublereal *__z__, __CLPK_integer *__pp,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__dmin__,
        __CLPK_doublereal *__dmin1, __CLPK_doublereal *__dmin2,
        __CLPK_doublereal *__dn, __CLPK_doublereal *__dnm1,
        __CLPK_doublereal *__dnm2,
        __CLPK_logical *__ieee)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasq6_(__CLPK_integer *__i0, __CLPK_integer *__n0,
        __CLPK_doublereal *__z__, __CLPK_integer *__pp,
        __CLPK_doublereal *__dmin__, __CLPK_doublereal *__dmin1,
        __CLPK_doublereal *__dmin2, __CLPK_doublereal *__dn,
        __CLPK_doublereal *__dnm1,
        __CLPK_doublereal *__dnm2)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasr_(char *__side, char *__pivot, char *__direct, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublereal *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasrt_(char *__id, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlassq_(__CLPK_integer *__n, __CLPK_doublereal *__x, __CLPK_integer *__incx,
        __CLPK_doublereal *__scale,
        __CLPK_doublereal *__sumsq)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasv2_(__CLPK_doublereal *__f, __CLPK_doublereal *__g,
        __CLPK_doublereal *__h__, __CLPK_doublereal *__ssmin,
        __CLPK_doublereal *__ssmax, __CLPK_doublereal *__snr,
        __CLPK_doublereal *__csr, __CLPK_doublereal *__snl,
        __CLPK_doublereal *__csl)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlaswp_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__k1, __CLPK_integer *__k2, __CLPK_integer *__ipiv,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasy2_(__CLPK_logical *__ltranl, __CLPK_logical *__ltranr,
        __CLPK_integer *__isgn, __CLPK_integer *__n1, __CLPK_integer *__n2,
        __CLPK_doublereal *__tl, __CLPK_integer *__ldtl,
        __CLPK_doublereal *__tr, __CLPK_integer *__ldtr, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__xnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlasyf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__w, __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlat2s_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_real *__sa, __CLPK_integer *__ldsa,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatbs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_integer *__kd, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__x,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatdf_(__CLPK_integer *__ijob, __CLPK_integer *__n,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__rhs, __CLPK_doublereal *__rdsum,
        __CLPK_doublereal *__rdscal, __CLPK_integer *__ipiv,
        __CLPK_integer *__jpiv)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatps_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_doublereal *__ap, __CLPK_doublereal *__x,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatrd_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__w,
        __CLPK_integer *__ldw)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatrs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__x, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatrz_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__l,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlatzm_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__v, __CLPK_integer *__incv,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__c1,
        __CLPK_doublereal *__c2, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlauu2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlauum_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dopgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dopmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__ap, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorg2l_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorg2r_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgbr_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorghr_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorglq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgql_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgqr_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgrq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorm2l_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorm2r_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormbr_(char *__vect, char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormhr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dorml2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormlq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormql_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormqr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormr2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormr3_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormrq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormrz_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dormtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbcon_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbequ_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__afb, __CLPK_integer *__ldafb,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbstf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__afb,
        __CLPK_integer *__ldafb, char *__equed, __CLPK_doublereal *__s,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbtf2_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbtrf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpbtrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpftrf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpftri_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpftrs_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpocon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpoequ_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpoequb_(__CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dporfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__af,
        __CLPK_integer *__ldaf, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dposvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__af, __CLPK_integer *__ldaf, char *__equed,
        __CLPK_doublereal *__s, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpotf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpotrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpotri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpotrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dppcon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dppequ_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__afp,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dppsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dppsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__afp, char *__equed, __CLPK_doublereal *__s,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpptri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpstf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_doublereal *__tol, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpstrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_doublereal *__tol, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dptcon_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpteqr_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dptrfs_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__df, __CLPK_doublereal *__ef,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dptsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dptsvx_(char *__fact, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__df, __CLPK_doublereal *__ef,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpttrf_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dpttrs_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dptts2_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int drscl_(__CLPK_integer *__n, __CLPK_doublereal *__sa,
        __CLPK_doublereal *__sx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbgst_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__bb, __CLPK_integer *__ldbb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbgv_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__bb, __CLPK_integer *__ldbb,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbgvd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__bb, __CLPK_integer *__ldbb,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbgvx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__bb, __CLPK_integer *__ldbb,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsbtrd_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsfrk_(char *__transr, char *__uplo, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__alpha, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__work, __CLPK_real *__swork, __CLPK_integer *__iter,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspcon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__w,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__w,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspgst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__bp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspgv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__ap, __CLPK_doublereal *__bp,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspgvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__ap, __CLPK_doublereal *__bp,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspgvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__ap, __CLPK_doublereal *__bp,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__work, __CLPK_real *__swork, __CLPK_integer *__iter,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__afp,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dspsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__afp, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsptrd_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsptri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__ap, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstebz_(char *__range, char *__order, __CLPK_integer *__n,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e, __CLPK_integer *__m,
        __CLPK_integer *__nsplit, __CLPK_doublereal *__w,
        __CLPK_integer *__iblock, __CLPK_integer *__isplit,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstedc_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstegr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstein_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_integer *__iblock, __CLPK_integer *__isplit,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstemr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_doublereal *__z__, __CLPK_integer *__ldz, __CLPK_integer *__nzc,
        __CLPK_integer *__isuppz, __CLPK_logical *__tryrac,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsteqr_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsterf_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstev_(char *__jobz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstevd_(char *__jobz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstevr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dstevx_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsycon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyequb_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__w,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__w,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyevr_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsygs2_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsygst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsygv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__w,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsygvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__w,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsygvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsyrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsysv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsysvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytd2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytrd_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__tau, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dsytrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtbcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublereal *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtbrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtbtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_doublereal *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtfsm_(char *__transr, char *__side, char *__uplo, char *__trans,
        char *__diag, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__a,
        __CLPK_doublereal *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtftri_(char *__transr, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtfttp_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__arf, __CLPK_doublereal *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtfttr_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__arf, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublereal *__s, __CLPK_integer *__lds,
        __CLPK_doublereal *__p, __CLPK_integer *__ldp, __CLPK_doublereal *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgex2_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__j1, __CLPK_integer *__n1, __CLPK_integer *__n2,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgexc_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgsen_(__CLPK_integer *__ijob, __CLPK_logical *__wantq,
        __CLPK_logical *__wantz, __CLPK_logical *__select, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__alphar,
        __CLPK_doublereal *__alphai, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__m, __CLPK_doublereal *__pl,
        __CLPK_doublereal *__pr, __CLPK_doublereal *__dif,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgsja_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__tola, __CLPK_doublereal *__tolb,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublereal *__u, __CLPK_integer *__ldu, __CLPK_doublereal *__v,
        __CLPK_integer *__ldv, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__work, __CLPK_integer *__ncycle,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublereal *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__s, __CLPK_doublereal *__dif, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgsy2_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__d__, __CLPK_integer *__ldd,
        __CLPK_doublereal *__e, __CLPK_integer *__lde, __CLPK_doublereal *__f,
        __CLPK_integer *__ldf, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__rdsum, __CLPK_doublereal *__rdscal,
        __CLPK_integer *__iwork, __CLPK_integer *__pq,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtgsyl_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__d__, __CLPK_integer *__ldd,
        __CLPK_doublereal *__e, __CLPK_integer *__lde, __CLPK_doublereal *__f,
        __CLPK_integer *__ldf, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__dif, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtpcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__rcond,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtprfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ap, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtptri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtptrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__ap, __CLPK_doublereal *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtpttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__ap, __CLPK_doublereal *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtpttr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__ap,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublereal *__t, __CLPK_integer *__ldt,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrexc_(char *__compq, __CLPK_integer *__n, __CLPK_doublereal *__t,
        __CLPK_integer *__ldt, __CLPK_doublereal *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb, __CLPK_doublereal *__x,
        __CLPK_integer *__ldx, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrsen_(char *__job, char *__compq, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublereal *__t, __CLPK_integer *__ldt,
        __CLPK_doublereal *__q, __CLPK_integer *__ldq, __CLPK_doublereal *__wr,
        __CLPK_doublereal *__wi, __CLPK_integer *__m, __CLPK_doublereal *__s,
        __CLPK_doublereal *__sep, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublereal *__t, __CLPK_integer *__ldt,
        __CLPK_doublereal *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublereal *__vr, __CLPK_integer *__ldvr, __CLPK_doublereal *__s,
        __CLPK_doublereal *__sep, __CLPK_integer *__mm, __CLPK_integer *__m,
        __CLPK_doublereal *__work, __CLPK_integer *__ldwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrsyl_(char *__trana, char *__tranb, __CLPK_integer *__isgn,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrti2_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrtri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda, __CLPK_doublereal *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtrttp_(char *__uplo, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtzrqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dtzrzf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__tau,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dzsum1_(__CLPK_integer *__n, __CLPK_doublecomplex *__cx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer icmax1_(__CLPK_integer *__n, __CLPK_complex *__cx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ieeeck_(__CLPK_integer *__ispec, __CLPK_real *__zero,
        __CLPK_real *__one)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaclc_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaclr_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_complex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer iladiag_(char *__diag)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer iladlc_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer iladlr_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaenv_(__CLPK_integer *__ispec, char *__name__, char *__opts,
        __CLPK_integer *__n1, __CLPK_integer *__n2, __CLPK_integer *__n3,
        __CLPK_integer *__n4)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaprec_(char *__prec)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaslc_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaslr_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilatrans_(char *__trans)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilauplo_(char *__uplo)
API_AVAILABLE(macos(10.2), ios(4.0));

int ilaver_(__CLPK_integer *__vers_major__, __CLPK_integer *__vers_minor__,
        __CLPK_integer *__vers_patch__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilazlc_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilazlr_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer iparmq_(__CLPK_integer *__ispec, char *__name__, char *__opts,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer izmax1_(__CLPK_integer *__n, __CLPK_doublecomplex *__cx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_logical lsamen_(__CLPK_integer *__n, char *__ca,
        char *__cb)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer smaxloc_(__CLPK_real *__a,
        __CLPK_integer *__dimm)
API_AVAILABLE(macos(10.2), ios(4.0));

int sbdsdc_(char *__uplo, char *__compq, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__u,
        __CLPK_integer *__ldu, __CLPK_real *__vt, __CLPK_integer *__ldvt,
        __CLPK_real *__q, __CLPK_integer *__iq, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sbdsqr_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__ncvt,
        __CLPK_integer *__nru, __CLPK_integer *__ncc, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__vt, __CLPK_integer *__ldvt,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal scsum1_(__CLPK_integer *__n, __CLPK_complex *__cx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int sdisna_(char *__job, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__sep,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbbrd_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__ncc, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_real *__ab, __CLPK_integer *__ldab, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_real *__pt, __CLPK_integer *__ldpt, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbcon_(char *__norm, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__afb, __CLPK_integer *__ldafb,
        __CLPK_integer *__ipiv, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbsv_(__CLPK_integer *__n, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_integer *__nrhs, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_real *__ab, __CLPK_integer *__ldab, __CLPK_real *__afb,
        __CLPK_integer *__ldafb, __CLPK_integer *__ipiv, char *__equed,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbtf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbtrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgbtrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgebak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__scale,
        __CLPK_integer *__m, __CLPK_real *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgebal_(char *__job, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgebd2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__tauq, __CLPK_real *__taup, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgebrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__tauq, __CLPK_real *__taup, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgecon_(char *__norm, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgees_(char *__jobvs, char *__sort, __CLPK_L_fp __select,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_real *__wr, __CLPK_real *__wi,
        __CLPK_real *__vs, __CLPK_integer *__ldvs, __CLPK_real *__work,
        __CLPK_integer *__lwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeesx_(char *__jobvs, char *__sort, __CLPK_L_fp __select, char *__sense,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_real *__wr, __CLPK_real *__wi,
        __CLPK_real *__vs, __CLPK_integer *__ldvs, __CLPK_real *__rconde,
        __CLPK_real *__rcondv, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__wr, __CLPK_real *__wi,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__wr, __CLPK_real *__wi, __CLPK_real *__vl,
        __CLPK_integer *__ldvl, __CLPK_real *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__scale,
        __CLPK_real *__abnrm, __CLPK_real *__rconde, __CLPK_real *__rcondv,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgegs_(char *__jobvsl, char *__jobvsr, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__alphar, __CLPK_real *__alphai,
        __CLPK_real *__beta, __CLPK_real *__vsl, __CLPK_integer *__ldvsl,
        __CLPK_real *__vsr, __CLPK_integer *__ldvsr, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgegv_(char *__jobvl, char *__jobvr, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__alphar, __CLPK_real *__alphai, __CLPK_real *__beta,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgehd2_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgehrd_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgejsv_(char *__joba, char *__jobu, char *__jobv, char *__jobr,
        char *__jobt, char *__jobp, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__sva,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgels_(char *__trans, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelsd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__s, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelss_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__s, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelsx_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgelsy_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_integer *__jpvt, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeql2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeqlf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeqp3_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeqpf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeqr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgeqrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgerfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgerq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgerqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesc2_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__rhs, __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_real *__scale)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesdd_(char *__jobz, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__s,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__vt,
        __CLPK_integer *__ldvt, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesvd_(char *__jobu, char *__jobvt, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__vt, __CLPK_integer *__ldvt, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesvj_(char *__joba, char *__jobu, char *__jobv, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__sva, __CLPK_integer *__mv, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgesvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        char *__equed, __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgetc2_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgetf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgetrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgetri_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgetrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggbak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__lscale,
        __CLPK_real *__rscale, __CLPK_integer *__m, __CLPK_real *__v,
        __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggbal_(char *__job, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__lscale,
        __CLPK_real *__rscale, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgges_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_integer *__sdim,
        __CLPK_real *__alphar, __CLPK_real *__alphai, __CLPK_real *__beta,
        __CLPK_real *__vsl, __CLPK_integer *__ldvsl, __CLPK_real *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggesx_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        char *__sense, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__sdim, __CLPK_real *__alphar, __CLPK_real *__alphai,
        __CLPK_real *__beta, __CLPK_real *__vsl, __CLPK_integer *__ldvsl,
        __CLPK_real *__vsr, __CLPK_integer *__ldvsr, __CLPK_real *__rconde,
        __CLPK_real *__rcondv, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__alphar, __CLPK_real *__alphai, __CLPK_real *__beta,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__alphar,
        __CLPK_real *__alphai, __CLPK_real *__beta, __CLPK_real *__vl,
        __CLPK_integer *__ldvl, __CLPK_real *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__lscale,
        __CLPK_real *__rscale, __CLPK_real *__abnrm, __CLPK_real *__bbnrm,
        __CLPK_real *__rconde, __CLPK_real *__rcondv, __CLPK_real *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggglm_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__d__, __CLPK_real *__x,
        __CLPK_real *__y, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgghrd_(char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__z__,
        __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgglse_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__p,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__c__, __CLPK_real *__d__,
        __CLPK_real *__x, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggqrf_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__taua,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__taub,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggrqf_(__CLPK_integer *__m, __CLPK_integer *__p, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__taua,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__taub,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggsvd_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__p, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__alpha,
        __CLPK_real *__beta, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__v, __CLPK_integer *__ldv, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sggsvp_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__tola, __CLPK_real *__tolb, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__v, __CLPK_integer *__ldv, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_integer *__iwork, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgsvj0_(char *__jobv, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__d__,
        __CLPK_real *__sva, __CLPK_integer *__mv, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_real *__eps, __CLPK_real *__sfmin,
        __CLPK_real *__tol, __CLPK_integer *__nsweep, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgsvj1_(char *__jobv, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__n1, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__d__, __CLPK_real *__sva, __CLPK_integer *__mv,
        __CLPK_real *__v, __CLPK_integer *__ldv, __CLPK_real *__eps,
        __CLPK_real *__sfmin, __CLPK_real *__tol, __CLPK_integer *__nsweep,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgtcon_(char *__norm, __CLPK_integer *__n, __CLPK_real *__dl,
        __CLPK_real *__d__, __CLPK_real *__du, __CLPK_real *__du2,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgtrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__dl, __CLPK_real *__d__, __CLPK_real *__du,
        __CLPK_real *__dlf, __CLPK_real *__df, __CLPK_real *__duf,
        __CLPK_real *__du2, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgtsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__dl,
        __CLPK_real *__d__, __CLPK_real *__du, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgtsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__dl, __CLPK_real *__d__,
        __CLPK_real *__du, __CLPK_real *__dlf, __CLPK_real *__df,
        __CLPK_real *__duf, __CLPK_real *__du2, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgttrf_(__CLPK_integer *__n, __CLPK_real *__dl, __CLPK_real *__d__,
        __CLPK_real *__du, __CLPK_real *__du2, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgttrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__dl, __CLPK_real *__d__, __CLPK_real *__du,
        __CLPK_real *__du2, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sgtts2_(__CLPK_integer *__itrans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__dl, __CLPK_real *__d__,
        __CLPK_real *__du, __CLPK_real *__du2, __CLPK_integer *__ipiv,
        __CLPK_real *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int shgeqz_(char *__job, char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__h__,
        __CLPK_integer *__ldh, __CLPK_real *__t, __CLPK_integer *__ldt,
        __CLPK_real *__alphar, __CLPK_real *__alphai, __CLPK_real *__beta,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int shsein_(char *__side, char *__eigsrc, char *__initv,
        __CLPK_logical *__select, __CLPK_integer *__n, __CLPK_real *__h__,
        __CLPK_integer *__ldh, __CLPK_real *__wr, __CLPK_real *__wi,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_integer *__mm, __CLPK_integer *__m,
        __CLPK_real *__work, __CLPK_integer *__ifaill, __CLPK_integer *__ifailr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int shseqr_(char *__job, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_real *__h__,
        __CLPK_integer *__ldh, __CLPK_real *__wr, __CLPK_real *__wi,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));


        __CLPK_logical sisnan_(__CLPK_real *__sin__)
API_AVAILABLE(macos(10.2), ios(4.0));

int slabad_(__CLPK_real *__small,
        __CLPK_real *__large)
API_AVAILABLE(macos(10.2), ios(4.0));

int slabrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__tauq, __CLPK_real *__taup,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int slacn2_(__CLPK_integer *__n, __CLPK_real *__v, __CLPK_real *__x,
        __CLPK_integer *__isgn, __CLPK_real *__est, __CLPK_integer *__kase,
        __CLPK_integer *__isave)
API_AVAILABLE(macos(10.2), ios(4.0));

int slacon_(__CLPK_integer *__n, __CLPK_real *__v, __CLPK_real *__x,
        __CLPK_integer *__isgn, __CLPK_real *__est,
        __CLPK_integer *__kase)
API_AVAILABLE(macos(10.2), ios(4.0));

int slacpy_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int sladiv_(__CLPK_real *__a, __CLPK_real *__b, __CLPK_real *__c__,
        __CLPK_real *__d__, __CLPK_real *__p,
        __CLPK_real *__q)
API_AVAILABLE(macos(10.2), ios(4.0));

int slae2_(__CLPK_real *__a, __CLPK_real *__b, __CLPK_real *__c__,
        __CLPK_real *__rt1,
        __CLPK_real *__rt2)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaebz_(__CLPK_integer *__ijob, __CLPK_integer *__nitmax,
        __CLPK_integer *__n, __CLPK_integer *__mmax, __CLPK_integer *__minp,
        __CLPK_integer *__nbmin, __CLPK_real *__abstol, __CLPK_real *__reltol,
        __CLPK_real *__pivmin, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__e2, __CLPK_integer *__nval, __CLPK_real *__ab,
        __CLPK_real *__c__, __CLPK_integer *__mout, __CLPK_integer *__nab,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed0_(__CLPK_integer *__icompq, __CLPK_integer *__qsiz,
        __CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__qstore,
        __CLPK_integer *__ldqs, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed1_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_integer *__indxq, __CLPK_real *__rho,
        __CLPK_integer *__cutpnt, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed2_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__n1,
        __CLPK_real *__d__, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__indxq, __CLPK_real *__rho, __CLPK_real *__z__,
        __CLPK_real *__dlamda, __CLPK_real *__w, __CLPK_real *__q2,
        __CLPK_integer *__indx, __CLPK_integer *__indxc,
        __CLPK_integer *__indxp, __CLPK_integer *__coltyp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed3_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__n1,
        __CLPK_real *__d__, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_real *__rho, __CLPK_real *__dlamda, __CLPK_real *__q2,
        __CLPK_integer *__indx, __CLPK_integer *__ctot, __CLPK_real *__w,
        __CLPK_real *__s,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed4_(__CLPK_integer *__n, __CLPK_integer *__i__, __CLPK_real *__d__,
        __CLPK_real *__z__, __CLPK_real *__delta, __CLPK_real *__rho,
        __CLPK_real *__dlam,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed5_(__CLPK_integer *__i__, __CLPK_real *__d__, __CLPK_real *__z__,
        __CLPK_real *__delta, __CLPK_real *__rho,
        __CLPK_real *__dlam)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed6_(__CLPK_integer *__kniter, __CLPK_logical *__orgati,
        __CLPK_real *__rho, __CLPK_real *__d__, __CLPK_real *__z__,
        __CLPK_real *__finit, __CLPK_real *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed7_(__CLPK_integer *__icompq, __CLPK_integer *__n,
        __CLPK_integer *__qsiz, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm, __CLPK_real *__d__,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_integer *__indxq,
        __CLPK_real *__rho, __CLPK_integer *__cutpnt, __CLPK_real *__qstore,
        __CLPK_integer *__qptr, __CLPK_integer *__prmptr,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_real *__givnum, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed8_(__CLPK_integer *__icompq, __CLPK_integer *__k, __CLPK_integer *__n,
        __CLPK_integer *__qsiz, __CLPK_real *__d__, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_integer *__indxq, __CLPK_real *__rho,
        __CLPK_integer *__cutpnt, __CLPK_real *__z__, __CLPK_real *__dlamda,
        __CLPK_real *__q2, __CLPK_integer *__ldq2, __CLPK_real *__w,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_real *__givnum,
        __CLPK_integer *__indxp, __CLPK_integer *__indx,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaed9_(__CLPK_integer *__k, __CLPK_integer *__kstart,
        __CLPK_integer *__kstop, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__rho,
        __CLPK_real *__dlamda, __CLPK_real *__w, __CLPK_real *__s,
        __CLPK_integer *__lds,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaeda_(__CLPK_integer *__n, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm,
        __CLPK_integer *__prmptr, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_real *__givnum, __CLPK_real *__q, __CLPK_integer *__qptr,
        __CLPK_real *__z__, __CLPK_real *__ztemp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaein_(__CLPK_logical *__rightv, __CLPK_logical *__noinit,
        __CLPK_integer *__n, __CLPK_real *__h__, __CLPK_integer *__ldh,
        __CLPK_real *__wr, __CLPK_real *__wi, __CLPK_real *__vr,
        __CLPK_real *__vi, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__work, __CLPK_real *__eps3, __CLPK_real *__smlnum,
        __CLPK_real *__bignum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaev2_(__CLPK_real *__a, __CLPK_real *__b, __CLPK_real *__c__,
        __CLPK_real *__rt1, __CLPK_real *__rt2, __CLPK_real *__cs1,
        __CLPK_real *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaexc_(__CLPK_logical *__wantq, __CLPK_integer *__n, __CLPK_real *__t,
        __CLPK_integer *__ldt, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__j1, __CLPK_integer *__n1, __CLPK_integer *__n2,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slag2_(__CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__safmin, __CLPK_real *__scale1,
        __CLPK_real *__scale2, __CLPK_real *__wr1, __CLPK_real *__wr2,
        __CLPK_real *__wi)
API_AVAILABLE(macos(10.2), ios(4.0));

int slag2d_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__sa,
        __CLPK_integer *__ldsa, __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slags2_(__CLPK_logical *__upper, __CLPK_real *__a1, __CLPK_real *__a2,
        __CLPK_real *__a3, __CLPK_real *__b1, __CLPK_real *__b2,
        __CLPK_real *__b3, __CLPK_real *__csu, __CLPK_real *__snu,
        __CLPK_real *__csv, __CLPK_real *__snv, __CLPK_real *__csq,
        __CLPK_real *__snq)
API_AVAILABLE(macos(10.2), ios(4.0));

int slagtf_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_real *__lambda,
        __CLPK_real *__b, __CLPK_real *__c__, __CLPK_real *__tol,
        __CLPK_real *__d__, __CLPK_integer *__in,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slagtm_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__alpha, __CLPK_real *__dl, __CLPK_real *__d__,
        __CLPK_real *__du, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__beta, __CLPK_real *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int slagts_(__CLPK_integer *__job, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_real *__b, __CLPK_real *__c__, __CLPK_real *__d__,
        __CLPK_integer *__in, __CLPK_real *__y, __CLPK_real *__tol,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slagv2_(__CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__alphar, __CLPK_real *__alphai,
        __CLPK_real *__beta, __CLPK_real *__csl, __CLPK_real *__snl,
        __CLPK_real *__csr,
        __CLPK_real *__snr)
API_AVAILABLE(macos(10.2), ios(4.0));

int slahqr_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__h__, __CLPK_integer *__ldh, __CLPK_real *__wr,
        __CLPK_real *__wi, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slahr2_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__t, __CLPK_integer *__ldt, __CLPK_real *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int slahrd_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__t, __CLPK_integer *__ldt, __CLPK_real *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaic1_(__CLPK_integer *__job, __CLPK_integer *__j, __CLPK_real *__x,
        __CLPK_real *__sest, __CLPK_real *__w, __CLPK_real *__gamma,
        __CLPK_real *__sestpr, __CLPK_real *__s,
        __CLPK_real *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_logical slaisnan_(__CLPK_real *__sin1,
        __CLPK_real *__sin2)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaln2_(__CLPK_logical *__ltrans, __CLPK_integer *__na,
        __CLPK_integer *__nw, __CLPK_real *__smin, __CLPK_real *__ca,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__d1,
        __CLPK_real *__d2, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__wr, __CLPK_real *__wi, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__scale, __CLPK_real *__xnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slals0_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__nrhs,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__bx,
        __CLPK_integer *__ldbx, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_real *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_real *__poles, __CLPK_real *__difl,
        __CLPK_real *__difr, __CLPK_real *__z__, __CLPK_integer *__k,
        __CLPK_real *__c__, __CLPK_real *__s, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slalsa_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__bx, __CLPK_integer *__ldbx,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__vt,
        __CLPK_integer *__k, __CLPK_real *__difl, __CLPK_real *__difr,
        __CLPK_real *__z__, __CLPK_real *__poles, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_integer *__perm, __CLPK_real *__givnum, __CLPK_real *__c__,
        __CLPK_real *__s, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slalsd_(char *__uplo, __CLPK_integer *__smlsiz, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__rcond,
        __CLPK_integer *__rank, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slamrg_(__CLPK_integer *__n1, __CLPK_integer *__n2, __CLPK_real *__a,
        __CLPK_integer *__strd1, __CLPK_integer *__strd2,
        __CLPK_integer *__index)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer slaneg_(__CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__lld, __CLPK_real *__sigma, __CLPK_real *__pivmin,
        __CLPK_integer *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slangb_(char *__norm, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_real *__ab,
        __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slange_(char *__norm, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slangt_(char *__norm, __CLPK_integer *__n, __CLPK_real *__dl,
        __CLPK_real *__d__,
        __CLPK_real *__du)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slanhs_(char *__norm, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slansb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slansf_(char *__norm, char *__transr, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slansp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__ap,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slanst_(char *__norm, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slansy_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slantb_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__ab,
        __CLPK_integer *__ldab,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slantp_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slantr_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slanv2_(__CLPK_real *__a, __CLPK_real *__b, __CLPK_real *__c__,
        __CLPK_real *__d__, __CLPK_real *__rt1r, __CLPK_real *__rt1i,
        __CLPK_real *__rt2r, __CLPK_real *__rt2i, __CLPK_real *__cs,
        __CLPK_real *__sn)
API_AVAILABLE(macos(10.2), ios(4.0));

int slapll_(__CLPK_integer *__n, __CLPK_real *__x, __CLPK_integer *__incx,
        __CLPK_real *__y, __CLPK_integer *__incy,
        __CLPK_real *__ssmin)
API_AVAILABLE(macos(10.2), ios(4.0));

int slapmt_(__CLPK_logical *__forwrd, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_integer *__k)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slapy2_(__CLPK_real *__x,
        __CLPK_real *__y)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slapy3_(__CLPK_real *__x, __CLPK_real *__y,
        __CLPK_real *__z__)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqgb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__r__, __CLPK_real *__c__, __CLPK_real *__rowcnd,
        __CLPK_real *__colcnd, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqge_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__r__, __CLPK_real *__c__,
        __CLPK_real *__rowcnd, __CLPK_real *__colcnd, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqp2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_real *__tau, __CLPK_real *__vn1, __CLPK_real *__vn2,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqps_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_integer *__nb, __CLPK_integer *__kb, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt, __CLPK_real *__tau,
        __CLPK_real *__vn1, __CLPK_real *__vn2, __CLPK_real *__auxv,
        __CLPK_real *__f,
        __CLPK_integer *__ldf)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr0_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__h__, __CLPK_integer *__ldh, __CLPK_real *__wr,
        __CLPK_real *__wi, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr1_(__CLPK_integer *__n, __CLPK_real *__h__, __CLPK_integer *__ldh,
        __CLPK_real *__sr1, __CLPK_real *__si1, __CLPK_real *__sr2,
        __CLPK_real *__si2,
        __CLPK_real *__v)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr2_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_real *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__ns, __CLPK_integer *__nd,
        __CLPK_real *__sr, __CLPK_real *__si, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_integer *__nh, __CLPK_real *__t,
        __CLPK_integer *__ldt, __CLPK_integer *__nv, __CLPK_real *__wv,
        __CLPK_integer *__ldwv, __CLPK_real *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr3_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_real *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__ns, __CLPK_integer *__nd,
        __CLPK_real *__sr, __CLPK_real *__si, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_integer *__nh, __CLPK_real *__t,
        __CLPK_integer *__ldt, __CLPK_integer *__nv, __CLPK_real *__wv,
        __CLPK_integer *__ldwv, __CLPK_real *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr4_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__h__, __CLPK_integer *__ldh, __CLPK_real *__wr,
        __CLPK_real *__wi, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqr5_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__kacc22, __CLPK_integer *__n, __CLPK_integer *__ktop,
        __CLPK_integer *__kbot, __CLPK_integer *__nshfts, __CLPK_real *__sr,
        __CLPK_real *__si, __CLPK_real *__h__, __CLPK_integer *__ldh,
        __CLPK_integer *__iloz, __CLPK_integer *__ihiz, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_real *__v, __CLPK_integer *__ldv,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_integer *__nv,
        __CLPK_real *__wv, __CLPK_integer *__ldwv, __CLPK_integer *__nh,
        __CLPK_real *__wh,
        __CLPK_integer *__ldwh)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqsb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab, __CLPK_real *__s,
        __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqsp_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqsy_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaqtr_(__CLPK_logical *__ltran, __CLPK_logical *__l__CLPK_real,
        __CLPK_integer *__n, __CLPK_real *__t, __CLPK_integer *__ldt,
        __CLPK_real *__b, __CLPK_real *__w, __CLPK_real *__scale,
        __CLPK_real *__x, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slar1v_(__CLPK_integer *__n, __CLPK_integer *__b1, __CLPK_integer *__bn,
        __CLPK_real *__lambda, __CLPK_real *__d__, __CLPK_real *__l,
        __CLPK_real *__ld, __CLPK_real *__lld, __CLPK_real *__pivmin,
        __CLPK_real *__gaptol, __CLPK_real *__z__, __CLPK_logical *__wantnc,
        __CLPK_integer *__negcnt, __CLPK_real *__ztz, __CLPK_real *__mingma,
        __CLPK_integer *__r__, __CLPK_integer *__isuppz, __CLPK_real *__nrminv,
        __CLPK_real *__resid, __CLPK_real *__rqcorr,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slar2v_(__CLPK_integer *__n, __CLPK_real *__x, __CLPK_real *__y,
        __CLPK_real *__z__, __CLPK_integer *__incx, __CLPK_real *__c__,
        __CLPK_real *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarf_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__v, __CLPK_integer *__incv, __CLPK_real *__tau,
        __CLPK_real *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarfb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__v, __CLPK_integer *__ldv, __CLPK_real *__t,
        __CLPK_integer *__ldt, __CLPK_real *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarfg_(__CLPK_integer *__n, __CLPK_real *__alpha, __CLPK_real *__x,
        __CLPK_integer *__incx,
        __CLPK_real *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarfp_(__CLPK_integer *__n, __CLPK_real *__alpha, __CLPK_real *__x,
        __CLPK_integer *__incx,
        __CLPK_real *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarft_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__v, __CLPK_integer *__ldv,
        __CLPK_real *__tau, __CLPK_real *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarfx_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__v, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slargv_(__CLPK_integer *__n, __CLPK_real *__x, __CLPK_integer *__incx,
        __CLPK_real *__y, __CLPK_integer *__incy, __CLPK_real *__c__,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarnv_(__CLPK_integer *__idist, __CLPK_integer *__iseed,
        __CLPK_integer *__n,
        __CLPK_real *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarra_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__e2, __CLPK_real *__spltol, __CLPK_real *__tnrm,
        __CLPK_integer *__nsplit, __CLPK_integer *__isplit,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrb_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__lld,
        __CLPK_integer *__ifirst, __CLPK_integer *__ilast, __CLPK_real *__rtol1,
        __CLPK_real *__rtol2, __CLPK_integer *__offset, __CLPK_real *__w,
        __CLPK_real *__wgap, __CLPK_real *__werr, __CLPK_real *__work,
        __CLPK_integer *__iwork, __CLPK_real *__pivmin, __CLPK_real *__spdiam,
        __CLPK_integer *__twist,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrc_(char *__jobt, __CLPK_integer *__n, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__pivmin, __CLPK_integer *__eigcnt, __CLPK_integer *__lcnt,
        __CLPK_integer *__rcnt,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrd_(char *__range, char *__order, __CLPK_integer *__n,
        __CLPK_real *__vl, __CLPK_real *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_real *__gers, __CLPK_real *__reltol,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__e2,
        __CLPK_real *__pivmin, __CLPK_integer *__nsplit,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__werr, __CLPK_real *__wl, __CLPK_real *__wu,
        __CLPK_integer *__iblock, __CLPK_integer *__indexw, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarre_(char *__range, __CLPK_integer *__n, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__e2,
        __CLPK_real *__rtol1, __CLPK_real *__rtol2, __CLPK_real *__spltol,
        __CLPK_integer *__nsplit, __CLPK_integer *__isplit, __CLPK_integer *__m,
        __CLPK_real *__w, __CLPK_real *__werr, __CLPK_real *__wgap,
        __CLPK_integer *__iblock, __CLPK_integer *__indexw, __CLPK_real *__gers,
        __CLPK_real *__pivmin, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrf_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__l,
        __CLPK_real *__ld, __CLPK_integer *__clstrt, __CLPK_integer *__clend,
        __CLPK_real *__w, __CLPK_real *__wgap, __CLPK_real *__werr,
        __CLPK_real *__spdiam, __CLPK_real *__clgapl, __CLPK_real *__clgapr,
        __CLPK_real *__pivmin, __CLPK_real *__sigma, __CLPK_real *__dplus,
        __CLPK_real *__lplus, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrj_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e2,
        __CLPK_integer *__ifirst, __CLPK_integer *__ilast, __CLPK_real *__rtol,
        __CLPK_integer *__offset, __CLPK_real *__w, __CLPK_real *__werr,
        __CLPK_real *__work, __CLPK_integer *__iwork, __CLPK_real *__pivmin,
        __CLPK_real *__spdiam,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrk_(__CLPK_integer *__n, __CLPK_integer *__iw, __CLPK_real *__gl,
        __CLPK_real *__gu, __CLPK_real *__d__, __CLPK_real *__e2,
        __CLPK_real *__pivmin, __CLPK_real *__reltol, __CLPK_real *__w,
        __CLPK_real *__werr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrr_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarrv_(__CLPK_integer *__n, __CLPK_real *__vl, __CLPK_real *__vu,
        __CLPK_real *__d__, __CLPK_real *__l, __CLPK_real *__pivmin,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_integer *__dol,
        __CLPK_integer *__dou, __CLPK_real *__minrgp, __CLPK_real *__rtol1,
        __CLPK_real *__rtol2, __CLPK_real *__w, __CLPK_real *__werr,
        __CLPK_real *__wgap, __CLPK_integer *__iblock, __CLPK_integer *__indexw,
        __CLPK_real *__gers, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarscl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int slartg_(__CLPK_real *__f, __CLPK_real *__g, __CLPK_real *__cs,
        __CLPK_real *__sn,
        __CLPK_real *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

int slartv_(__CLPK_integer *__n, __CLPK_real *__x, __CLPK_integer *__incx,
        __CLPK_real *__y, __CLPK_integer *__incy, __CLPK_real *__c__,
        __CLPK_real *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaruv_(__CLPK_integer *__iseed, __CLPK_integer *__n,
        __CLPK_real *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarz_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__l, __CLPK_real *__v, __CLPK_integer *__incv,
        __CLPK_real *__tau, __CLPK_real *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarzb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_real *__v, __CLPK_integer *__ldv,
        __CLPK_real *__t, __CLPK_integer *__ldt, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int slarzt_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__v, __CLPK_integer *__ldv,
        __CLPK_real *__tau, __CLPK_real *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int slas2_(__CLPK_real *__f, __CLPK_real *__g, __CLPK_real *__h__,
        __CLPK_real *__ssmin,
        __CLPK_real *__ssmax)
API_AVAILABLE(macos(10.2), ios(4.0));

int slascl_(char *__type__, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_real *__cfrom, __CLPK_real *__cto, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slascl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd0_(__CLPK_integer *__n, __CLPK_integer *__sqre, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__vt, __CLPK_integer *__ldvt, __CLPK_integer *__smlsiz,
        __CLPK_integer *__iwork, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd1_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_real *__d__, __CLPK_real *__alpha, __CLPK_real *__beta,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__vt,
        __CLPK_integer *__ldvt, __CLPK_integer *__idxq, __CLPK_integer *__iwork,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd2_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_integer *__k, __CLPK_real *__d__, __CLPK_real *__z__,
        __CLPK_real *__alpha, __CLPK_real *__beta, __CLPK_real *__u,
        __CLPK_integer *__ldu, __CLPK_real *__vt, __CLPK_integer *__ldvt,
        __CLPK_real *__dsigma, __CLPK_real *__u2, __CLPK_integer *__ldu2,
        __CLPK_real *__vt2, __CLPK_integer *__ldvt2, __CLPK_integer *__idxp,
        __CLPK_integer *__idx, __CLPK_integer *__idxc, __CLPK_integer *__idxq,
        __CLPK_integer *__coltyp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd3_(__CLPK_integer *__nl, __CLPK_integer *__nr, __CLPK_integer *__sqre,
        __CLPK_integer *__k, __CLPK_real *__d__, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_real *__dsigma, __CLPK_real *__u,
        __CLPK_integer *__ldu, __CLPK_real *__u2, __CLPK_integer *__ldu2,
        __CLPK_real *__vt, __CLPK_integer *__ldvt, __CLPK_real *__vt2,
        __CLPK_integer *__ldvt2, __CLPK_integer *__idxc, __CLPK_integer *__ctot,
        __CLPK_real *__z__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd4_(__CLPK_integer *__n, __CLPK_integer *__i__, __CLPK_real *__d__,
        __CLPK_real *__z__, __CLPK_real *__delta, __CLPK_real *__rho,
        __CLPK_real *__sigma, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd5_(__CLPK_integer *__i__, __CLPK_real *__d__, __CLPK_real *__z__,
        __CLPK_real *__delta, __CLPK_real *__rho, __CLPK_real *__dsigma,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd6_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_real *__d__,
        __CLPK_real *__vf, __CLPK_real *__vl, __CLPK_real *__alpha,
        __CLPK_real *__beta, __CLPK_integer *__idxq, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_real *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_real *__poles, __CLPK_real *__difl,
        __CLPK_real *__difr, __CLPK_real *__z__, __CLPK_integer *__k,
        __CLPK_real *__c__, __CLPK_real *__s, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd7_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__k,
        __CLPK_real *__d__, __CLPK_real *__z__, __CLPK_real *__zw,
        __CLPK_real *__vf, __CLPK_real *__vfw, __CLPK_real *__vl,
        __CLPK_real *__vlw, __CLPK_real *__alpha, __CLPK_real *__beta,
        __CLPK_real *__dsigma, __CLPK_integer *__idx, __CLPK_integer *__idxp,
        __CLPK_integer *__idxq, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_real *__givnum,
        __CLPK_integer *__ldgnum, __CLPK_real *__c__, __CLPK_real *__s,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasd8_(__CLPK_integer *__icompq, __CLPK_integer *__k, __CLPK_real *__d__,
        __CLPK_real *__z__, __CLPK_real *__vf, __CLPK_real *__vl,
        __CLPK_real *__difl, __CLPK_real *__difr, __CLPK_integer *__lddifr,
        __CLPK_real *__dsigma, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasda_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__sqre, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__vt, __CLPK_integer *__k, __CLPK_real *__difl,
        __CLPK_real *__difr, __CLPK_real *__z__, __CLPK_real *__poles,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_integer *__ldgcol, __CLPK_integer *__perm, __CLPK_real *__givnum,
        __CLPK_real *__c__, __CLPK_real *__s, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasdq_(char *__uplo, __CLPK_integer *__sqre, __CLPK_integer *__n,
        __CLPK_integer *__ncvt, __CLPK_integer *__nru, __CLPK_integer *__ncc,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vt,
        __CLPK_integer *__ldvt, __CLPK_real *__u, __CLPK_integer *__ldu,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasdt_(__CLPK_integer *__n, __CLPK_integer *__lvl, __CLPK_integer *__nd,
        __CLPK_integer *__inode, __CLPK_integer *__ndiml,
        __CLPK_integer *__ndimr,
        __CLPK_integer *__msub)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaset_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__alpha, __CLPK_real *__beta, __CLPK_real *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq1_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq2_(__CLPK_integer *__n, __CLPK_real *__z__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq3_(__CLPK_integer *__i0, __CLPK_integer *__n0, __CLPK_real *__z__,
        __CLPK_integer *__pp, __CLPK_real *__dmin__, __CLPK_real *__sigma,
        __CLPK_real *__desig, __CLPK_real *__qmax, __CLPK_integer *__nfail,
        __CLPK_integer *__iter, __CLPK_integer *__ndiv, __CLPK_logical *__ieee,
        __CLPK_integer *__ttype, __CLPK_real *__dmin1, __CLPK_real *__dmin2,
        __CLPK_real *__dn, __CLPK_real *__dn1, __CLPK_real *__dn2,
        __CLPK_real *__g,
        __CLPK_real *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq4_(__CLPK_integer *__i0, __CLPK_integer *__n0, __CLPK_real *__z__,
        __CLPK_integer *__pp, __CLPK_integer *__n0in, __CLPK_real *__dmin__,
        __CLPK_real *__dmin1, __CLPK_real *__dmin2, __CLPK_real *__dn,
        __CLPK_real *__dn1, __CLPK_real *__dn2, __CLPK_real *__tau,
        __CLPK_integer *__ttype,
        __CLPK_real *__g)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq5_(__CLPK_integer *__i0, __CLPK_integer *__n0, __CLPK_real *__z__,
        __CLPK_integer *__pp, __CLPK_real *__tau, __CLPK_real *__dmin__,
        __CLPK_real *__dmin1, __CLPK_real *__dmin2, __CLPK_real *__dn,
        __CLPK_real *__dnm1, __CLPK_real *__dnm2,
        __CLPK_logical *__ieee)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasq6_(__CLPK_integer *__i0, __CLPK_integer *__n0, __CLPK_real *__z__,
        __CLPK_integer *__pp, __CLPK_real *__dmin__, __CLPK_real *__dmin1,
        __CLPK_real *__dmin2, __CLPK_real *__dn, __CLPK_real *__dnm1,
        __CLPK_real *__dnm2)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasr_(char *__side, char *__pivot, char *__direct, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__c__, __CLPK_real *__s,
        __CLPK_real *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasrt_(char *__id, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slassq_(__CLPK_integer *__n, __CLPK_real *__x, __CLPK_integer *__incx,
        __CLPK_real *__scale,
        __CLPK_real *__sumsq)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasv2_(__CLPK_real *__f, __CLPK_real *__g, __CLPK_real *__h__,
        __CLPK_real *__ssmin, __CLPK_real *__ssmax, __CLPK_real *__snr,
        __CLPK_real *__csr, __CLPK_real *__snl,
        __CLPK_real *__csl)
API_AVAILABLE(macos(10.2), ios(4.0));

int slaswp_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__k1, __CLPK_integer *__k2, __CLPK_integer *__ipiv,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasy2_(__CLPK_logical *__ltranl, __CLPK_logical *__ltranr,
        __CLPK_integer *__isgn, __CLPK_integer *__n1, __CLPK_integer *__n2,
        __CLPK_real *__tl, __CLPK_integer *__ldtl, __CLPK_real *__tr,
        __CLPK_integer *__ldtr, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__scale, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__xnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slasyf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_real *__w, __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatbs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_integer *__kd, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__x, __CLPK_real *__scale,
        __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatdf_(__CLPK_integer *__ijob, __CLPK_integer *__n, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_real *__rhs, __CLPK_real *__rdsum,
        __CLPK_real *__rdscal, __CLPK_integer *__ipiv,
        __CLPK_integer *__jpiv)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatps_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_real *__ap, __CLPK_real *__x,
        __CLPK_real *__scale, __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatrd_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__e,
        __CLPK_real *__tau, __CLPK_real *__w,
        __CLPK_integer *__ldw)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatrs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__x, __CLPK_real *__scale, __CLPK_real *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatrz_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__l,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slatzm_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__v, __CLPK_integer *__incv, __CLPK_real *__tau,
        __CLPK_real *__c1, __CLPK_real *__c2, __CLPK_integer *__ldc,
        __CLPK_real *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int slauu2_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int slauum_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sopgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__tau, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sopmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__ap, __CLPK_real *__tau,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorg2l_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorg2r_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgbr_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__tau, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorghr_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorglq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgql_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgqr_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgrq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorm2l_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorm2r_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormbr_(char *__vect, char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormhr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sorml2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormlq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormql_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormqr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormr2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormr3_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormrq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormrz_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sormtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__tau, __CLPK_real *__c__, __CLPK_integer *__ldc,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbcon_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab, __CLPK_real *__anorm,
        __CLPK_real *__rcond, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbequ_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab, __CLPK_real *__s,
        __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__afb, __CLPK_integer *__ldafb, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbstf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__afb, __CLPK_integer *__ldafb,
        char *__equed, __CLPK_real *__s, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbtf2_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbtrf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spbtrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spftrf_(char *__transr, char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spftri_(char *__transr, char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spftrs_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spocon_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spoequ_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spoequb_(__CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sporfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__af,
        __CLPK_integer *__ldaf, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sposvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__af, __CLPK_integer *__ldaf, char *__equed,
        __CLPK_real *__s, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spotf2_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spotrf_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spotri_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spotrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sppcon_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__anorm, __CLPK_real *__rcond, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sppequ_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__s, __CLPK_real *__scond, __CLPK_real *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_real *__afp, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sppsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sppsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__ap, __CLPK_real *__afp,
        char *__equed, __CLPK_real *__s, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__rcond, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spptri_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spstf2_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_real *__tol, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spstrf_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_real *__tol, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sptcon_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__anorm, __CLPK_real *__rcond, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spteqr_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sptrfs_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__df, __CLPK_real *__ef,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sptsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sptsvx_(char *__fact, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__df,
        __CLPK_real *__ef, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spttrf_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int spttrs_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sptts2_(__CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int srscl_(__CLPK_integer *__n, __CLPK_real *__sa, __CLPK_real *__sx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbgst_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbgv_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbgvd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbgvx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__bb, __CLPK_integer *__ldbb,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssbtrd_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssfrk_(char *__transr, char *__uplo, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_real *__alpha, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__beta,
        __CLPK_real *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspcon_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__ipiv, __CLPK_real *__anorm, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspev_(char *__jobz, char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspevd_(char *__jobz, char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__ap, __CLPK_real *__vl, __CLPK_real *__vu,
        __CLPK_integer *__il, __CLPK_integer *__iu, __CLPK_real *__abstol,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspgst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__ap, __CLPK_real *__bp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspgv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__ap, __CLPK_real *__bp,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspgvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__ap, __CLPK_real *__bp,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspgvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__ap, __CLPK_real *__bp,
        __CLPK_real *__vl, __CLPK_real *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_real *__abstol, __CLPK_integer *__m,
        __CLPK_real *__w, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_real *__afp, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sspsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__ap, __CLPK_real *__afp,
        __CLPK_integer *__ipiv, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__rcond,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssptrd_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssptri_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__ipiv, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__ap, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstebz_(char *__range, char *__order, __CLPK_integer *__n,
        __CLPK_real *__vl, __CLPK_real *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_real *__abstol, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_integer *__m, __CLPK_integer *__nsplit,
        __CLPK_real *__w, __CLPK_integer *__iblock, __CLPK_integer *__isplit,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstedc_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstegr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstein_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_integer *__iblock,
        __CLPK_integer *__isplit, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstemr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_integer *__m, __CLPK_real *__w, __CLPK_real *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__nzc, __CLPK_integer *__isuppz,
        __CLPK_logical *__tryrac, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssteqr_(char *__compz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssterf_(__CLPK_integer *__n, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstev_(char *__jobz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstevd_(char *__jobz, __CLPK_integer *__n, __CLPK_real *__d__,
        __CLPK_real *__e, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstevr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int sstevx_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_real *__d__, __CLPK_real *__e, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssycon_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__anorm,
        __CLPK_real *__rcond, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyequb_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__s, __CLPK_real *__scond,
        __CLPK_real *__amax, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyev_(char *__jobz, char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__w, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyevd_(char *__jobz, char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__w, __CLPK_real *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyevr_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssygs2_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssygst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssygv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__w,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssygvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__w,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssygvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__vl,
        __CLPK_real *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_real *__abstol, __CLPK_integer *__m, __CLPK_real *__w,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_real *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssyrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssysv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssysvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__af, __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__rcond, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytd2_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytf2_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytrd_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__d__, __CLPK_real *__e,
        __CLPK_real *__tau, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytrf_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytri_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ssytrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stbcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_real *__ab, __CLPK_integer *__ldab,
        __CLPK_real *__rcond, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stbrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__x, __CLPK_integer *__ldx, __CLPK_real *__ferr,
        __CLPK_real *__berr, __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stbtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs, __CLPK_real *__ab,
        __CLPK_integer *__ldab, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stfsm_(char *__transr, char *__side, char *__uplo, char *__trans,
        char *__diag, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_real *__alpha, __CLPK_real *__a, __CLPK_real *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int stftri_(char *__transr, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_real *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stfttp_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__arf, __CLPK_real *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stfttr_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__arf, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_real *__s, __CLPK_integer *__lds,
        __CLPK_real *__p, __CLPK_integer *__ldp, __CLPK_real *__vl,
        __CLPK_integer *__ldvl, __CLPK_real *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgex2_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__j1, __CLPK_integer *__n1, __CLPK_integer *__n2,
        __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgexc_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__q,
        __CLPK_integer *__ldq, __CLPK_real *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgsen_(__CLPK_integer *__ijob, __CLPK_logical *__wantq,
        __CLPK_logical *__wantz, __CLPK_logical *__select, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__alphar, __CLPK_real *__alphai,
        __CLPK_real *__beta, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_real *__z__, __CLPK_integer *__ldz, __CLPK_integer *__m,
        __CLPK_real *__pl, __CLPK_real *__pr, __CLPK_real *__dif,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgsja_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__tola,
        __CLPK_real *__tolb, __CLPK_real *__alpha, __CLPK_real *__beta,
        __CLPK_real *__u, __CLPK_integer *__ldu, __CLPK_real *__v,
        __CLPK_integer *__ldv, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_real *__work, __CLPK_integer *__ncycle,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__vl,
        __CLPK_integer *__ldvl, __CLPK_real *__vr, __CLPK_integer *__ldvr,
        __CLPK_real *__s, __CLPK_real *__dif, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgsy2_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__d__, __CLPK_integer *__ldd,
        __CLPK_real *__e, __CLPK_integer *__lde, __CLPK_real *__f,
        __CLPK_integer *__ldf, __CLPK_real *__scale, __CLPK_real *__rdsum,
        __CLPK_real *__rdscal, __CLPK_integer *__iwork, __CLPK_integer *__pq,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stgsyl_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__c__,
        __CLPK_integer *__ldc, __CLPK_real *__d__, __CLPK_integer *__ldd,
        __CLPK_real *__e, __CLPK_integer *__lde, __CLPK_real *__f,
        __CLPK_integer *__ldf, __CLPK_real *__scale, __CLPK_real *__dif,
        __CLPK_real *__work, __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stpcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_real *__ap, __CLPK_real *__rcond, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stprfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__ap, __CLPK_real *__b,
        __CLPK_integer *__ldb, __CLPK_real *__x, __CLPK_integer *__ldx,
        __CLPK_real *__ferr, __CLPK_real *__berr, __CLPK_real *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stptri_(char *__uplo, char *__diag, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stptrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__ap, __CLPK_real *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stpttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_real *__ap, __CLPK_real *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stpttr_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__ap,
        __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_real *__a, __CLPK_integer *__lda, __CLPK_real *__rcond,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_real *__t, __CLPK_integer *__ldt,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_integer *__mm, __CLPK_integer *__m,
        __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strexc_(char *__compq, __CLPK_integer *__n, __CLPK_real *__t,
        __CLPK_integer *__ldt, __CLPK_real *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst, __CLPK_real *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb, __CLPK_real *__x,
        __CLPK_integer *__ldx, __CLPK_real *__ferr, __CLPK_real *__berr,
        __CLPK_real *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strsen_(char *__job, char *__compq, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_real *__t, __CLPK_integer *__ldt,
        __CLPK_real *__q, __CLPK_integer *__ldq, __CLPK_real *__wr,
        __CLPK_real *__wi, __CLPK_integer *__m, __CLPK_real *__s,
        __CLPK_real *__sep, __CLPK_real *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_real *__t, __CLPK_integer *__ldt,
        __CLPK_real *__vl, __CLPK_integer *__ldvl, __CLPK_real *__vr,
        __CLPK_integer *__ldvr, __CLPK_real *__s, __CLPK_real *__sep,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_real *__work,
        __CLPK_integer *__ldwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strsyl_(char *__trana, char *__tranb, __CLPK_integer *__isgn,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_real *__c__, __CLPK_integer *__ldc, __CLPK_real *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strti2_(char *__uplo, char *__diag, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strtri_(char *__uplo, char *__diag, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_real *__a, __CLPK_integer *__lda,
        __CLPK_real *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strttf_(char *__transr, char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int strttp_(char *__uplo, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stzrqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int stzrzf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_real *__a,
        __CLPK_integer *__lda, __CLPK_real *__tau, __CLPK_real *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zbdsqr_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__ncvt,
        __CLPK_integer *__nru, __CLPK_integer *__ncc, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublecomplex *__u,
        __CLPK_integer *__ldu, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zcgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublecomplex *__work, __CLPK_complex *__swork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iter,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zcposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublecomplex *__work, __CLPK_complex *__swork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iter,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zdrscl_(__CLPK_integer *__n, __CLPK_doublereal *__sa,
        __CLPK_doublecomplex *__sx,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbbrd_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__ncc, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__pt, __CLPK_integer *__ldpt,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbcon_(char *__norm, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbequb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublecomplex *__afb, __CLPK_integer *__ldafb,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbsv_(__CLPK_integer *__n, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublecomplex *__afb, __CLPK_integer *__ldafb,
        __CLPK_integer *__ipiv, char *__equed, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbtf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbtrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgbtrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgebak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__scale, __CLPK_integer *__m,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgebal_(char *__job, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgebd2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tauq, __CLPK_doublecomplex *__taup,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgebrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tauq, __CLPK_doublecomplex *__taup,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgecon_(char *__norm, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeequ_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeequb_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublereal *__rowcnd, __CLPK_doublereal *__colcnd,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgees_(char *__jobvs, char *__sort, __CLPK_L_fp __select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_doublecomplex *__w,
        __CLPK_doublecomplex *__vs, __CLPK_integer *__ldvs,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeesx_(char *__jobvs, char *__sort, __CLPK_L_fp __select, char *__sense,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__sdim, __CLPK_doublecomplex *__w,
        __CLPK_doublecomplex *__vs, __CLPK_integer *__ldvs,
        __CLPK_doublereal *__rconde, __CLPK_doublereal *__rcondv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublecomplex *__vr,
        __CLPK_integer *__ldvr, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublecomplex *__vr,
        __CLPK_integer *__ldvr, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__abnrm,
        __CLPK_doublereal *__rconde, __CLPK_doublereal *__rcondv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgegs_(char *__jobvsl, char *__jobvsr, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__vsl, __CLPK_integer *__ldvsl,
        __CLPK_doublecomplex *__vsr, __CLPK_integer *__ldvsr,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgegv_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgehd2_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgehrd_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgels_(char *__trans, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelsd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__s, __CLPK_doublereal *__rcond,
        __CLPK_integer *__rank, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelss_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__s, __CLPK_doublereal *__rcond,
        __CLPK_integer *__rank, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelsx_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__jpvt, __CLPK_doublereal *__rcond,
        __CLPK_integer *__rank, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgelsy_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__jpvt, __CLPK_doublereal *__rcond,
        __CLPK_integer *__rank, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeql2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeqlf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeqp3_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeqpf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeqr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgeqrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgerfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__af, __CLPK_integer *__ldaf,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgerq2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgerqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgesc2_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__rhs,
        __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_doublereal *__scale)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgesdd_(char *__jobz, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__s, __CLPK_doublecomplex *__u,
        __CLPK_integer *__ldu, __CLPK_doublecomplex *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgesv_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgesvd_(char *__jobu, char *__jobvt, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__s, __CLPK_doublecomplex *__u,
        __CLPK_integer *__ldu, __CLPK_doublecomplex *__vt,
        __CLPK_integer *__ldvt, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgesvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv, char *__equed,
        __CLPK_doublereal *__r__, __CLPK_doublereal *__c__,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgetc2_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv, __CLPK_integer *__jpiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgetf2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgetrf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgetri_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgetrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggbak_(char *__job, char *__side, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_integer *__m, __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggbal_(char *__job, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgges_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__sdim, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__beta, __CLPK_doublecomplex *__vsl,
        __CLPK_integer *__ldvsl, __CLPK_doublecomplex *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggesx_(char *__jobvsl, char *__jobvsr, char *__sort, __CLPK_L_fp __selctg,
        char *__sense, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__sdim, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__beta, __CLPK_doublecomplex *__vsl,
        __CLPK_integer *__ldvsl, __CLPK_doublecomplex *__vsr,
        __CLPK_integer *__ldvsr, __CLPK_doublereal *__rconde,
        __CLPK_doublereal *__rcondv, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggev_(char *__jobvl, char *__jobvr, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggevx_(char *__balanc, char *__jobvl, char *__jobvr, char *__sense,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublereal *__lscale, __CLPK_doublereal *__rscale,
        __CLPK_doublereal *__abnrm, __CLPK_doublereal *__bbnrm,
        __CLPK_doublereal *__rconde, __CLPK_doublereal *__rcondv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_logical *__bwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggglm_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__x,
        __CLPK_doublecomplex *__y, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgghrd_(char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgglse_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__p,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_doublecomplex *__d__,
        __CLPK_doublecomplex *__x, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggqrf_(__CLPK_integer *__n, __CLPK_integer *__m, __CLPK_integer *__p,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__taua, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__taub,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggrqf_(__CLPK_integer *__m, __CLPK_integer *__p, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__taua, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__taub,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggsvd_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__p, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublecomplex *__u, __CLPK_integer *__ldu,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zggsvp_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__tola, __CLPK_doublereal *__tolb,
        __CLPK_integer *__k, __CLPK_integer *__l, __CLPK_doublecomplex *__u,
        __CLPK_integer *__ldu, __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__iwork, __CLPK_doublereal *__rwork,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgtcon_(char *__norm, __CLPK_integer *__n, __CLPK_doublecomplex *__dl,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__du,
        __CLPK_doublecomplex *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgtrfs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__dl, __CLPK_doublecomplex *__d__,
        __CLPK_doublecomplex *__du, __CLPK_doublecomplex *__dlf,
        __CLPK_doublecomplex *__df, __CLPK_doublecomplex *__duf,
        __CLPK_doublecomplex *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgtsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__dl, __CLPK_doublecomplex *__d__,
        __CLPK_doublecomplex *__du, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgtsvx_(char *__fact, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__dl,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__du,
        __CLPK_doublecomplex *__dlf, __CLPK_doublecomplex *__df,
        __CLPK_doublecomplex *__duf, __CLPK_doublecomplex *__du2,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgttrf_(__CLPK_integer *__n, __CLPK_doublecomplex *__dl,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__du,
        __CLPK_doublecomplex *__du2, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgttrs_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__dl, __CLPK_doublecomplex *__d__,
        __CLPK_doublecomplex *__du, __CLPK_doublecomplex *__du2,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zgtts2_(__CLPK_integer *__itrans, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__dl,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__du,
        __CLPK_doublecomplex *__du2, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__lrwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__vl, __CLPK_doublereal *__vu,
        __CLPK_integer *__il, __CLPK_integer *__iu, __CLPK_doublereal *__abstol,
        __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbgst_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__bb,
        __CLPK_integer *__ldbb, __CLPK_doublecomplex *__x,
        __CLPK_integer *__ldx, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbgv_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__bb,
        __CLPK_integer *__ldbb, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbgvd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__bb,
        __CLPK_integer *__ldbb, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__lrwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbgvx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__ka, __CLPK_integer *__kb, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__bb,
        __CLPK_integer *__ldbb, __CLPK_doublecomplex *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__vl, __CLPK_doublereal *__vu,
        __CLPK_integer *__il, __CLPK_integer *__iu, __CLPK_doublereal *__abstol,
        __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhbtrd_(char *__vect, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__q,
        __CLPK_integer *__ldq, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhecon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zheequb_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zheev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zheevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zheevr_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__lrwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zheevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhegs2_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhegst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhegv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhegvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhegvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork, __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zherfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__af, __CLPK_integer *__ldaf,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhesv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhesvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetd2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetrd_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhetrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhfrk_(char *__transr, char *__uplo, char *__trans, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublereal *__alpha,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__beta,
        __CLPK_doublecomplex *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhgeqz_(char *__job, char *__compq, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpcon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpev_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpevd_(char *__jobz, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__lrwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpevx_(char *__jobz, char *__range, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpgst_(__CLPK_integer *__itype, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__bp,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpgv_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__bp, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpgvd_(__CLPK_integer *__itype, char *__jobz, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__bp, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_doublereal *__rwork, __CLPK_integer *__lrwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpgvx_(__CLPK_integer *__itype, char *__jobz, char *__range, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__bp, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_integer *__il, __CLPK_integer *__iu,
        __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__afp,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhpsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__afp, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhptrd_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhptri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhsein_(char *__side, char *__eigsrc, char *__initv,
        __CLPK_logical *__select, __CLPK_integer *__n,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__vl,
        __CLPK_integer *__ldvl, __CLPK_doublecomplex *__vr,
        __CLPK_integer *__ldvr, __CLPK_integer *__mm, __CLPK_integer *__m,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__ifaill, __CLPK_integer *__ifailr,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zhseqr_(char *__job, char *__compz, __CLPK_integer *__n,
        __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlabrd_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__tauq, __CLPK_doublecomplex *__taup,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublecomplex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacgv_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacn2_(__CLPK_integer *__n, __CLPK_doublecomplex *__v,
        __CLPK_doublecomplex *__x, __CLPK_doublereal *__est,
        __CLPK_integer *__kase,
        __CLPK_integer *__isave)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacon_(__CLPK_integer *__n, __CLPK_doublecomplex *__v,
        __CLPK_doublecomplex *__x, __CLPK_doublereal *__est,
        __CLPK_integer *__kase)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacp2_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacpy_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacrm_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__rwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlacrt_(__CLPK_integer *__n, __CLPK_doublecomplex *__cx,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__cy,
        __CLPK_integer *__incy, __CLPK_doublecomplex *__c__,
        __CLPK_doublecomplex *__s)
API_AVAILABLE(macos(10.2), ios(4.0));

void zladiv_(__CLPK_doublecomplex *__ret_val, __CLPK_doublecomplex *__x,
        __CLPK_doublecomplex *__y)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaed0_(__CLPK_integer *__qsiz, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__qstore, __CLPK_integer *__ldqs,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaed7_(__CLPK_integer *__n, __CLPK_integer *__cutpnt,
        __CLPK_integer *__qsiz, __CLPK_integer *__tlvls,
        __CLPK_integer *__curlvl, __CLPK_integer *__curpbm,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__q,
        __CLPK_integer *__ldq, __CLPK_doublereal *__rho,
        __CLPK_integer *__indxq, __CLPK_doublereal *__qstore,
        __CLPK_integer *__qptr, __CLPK_integer *__prmptr,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_doublereal *__givnum,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaed8_(__CLPK_integer *__k, __CLPK_integer *__n, __CLPK_integer *__qsiz,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__rho,
        __CLPK_integer *__cutpnt, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__dlamda, __CLPK_doublecomplex *__q2,
        __CLPK_integer *__ldq2, __CLPK_doublereal *__w, __CLPK_integer *__indxp,
        __CLPK_integer *__indx, __CLPK_integer *__indxq, __CLPK_integer *__perm,
        __CLPK_integer *__givptr, __CLPK_integer *__givcol,
        __CLPK_doublereal *__givnum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaein_(__CLPK_logical *__rightv, __CLPK_logical *__noinit,
        __CLPK_integer *__n, __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__v,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__rwork, __CLPK_doublereal *__eps3,
        __CLPK_doublereal *__smlnum,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaesy_(__CLPK_doublecomplex *__a, __CLPK_doublecomplex *__b,
        __CLPK_doublecomplex *__c__, __CLPK_doublecomplex *__rt1,
        __CLPK_doublecomplex *__rt2, __CLPK_doublecomplex *__evscal,
        __CLPK_doublecomplex *__cs1,
        __CLPK_doublecomplex *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaev2_(__CLPK_doublecomplex *__a, __CLPK_doublecomplex *__b,
        __CLPK_doublecomplex *__c__, __CLPK_doublereal *__rt1,
        __CLPK_doublereal *__rt2, __CLPK_doublereal *__cs1,
        __CLPK_doublecomplex *__sn1)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlag2c_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__sa, __CLPK_integer *__ldsa,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlags2_(__CLPK_logical *__upper, __CLPK_doublereal *__a1,
        __CLPK_doublecomplex *__a2, __CLPK_doublereal *__a3,
        __CLPK_doublereal *__b1, __CLPK_doublecomplex *__b2,
        __CLPK_doublereal *__b3, __CLPK_doublereal *__csu,
        __CLPK_doublecomplex *__snu, __CLPK_doublereal *__csv,
        __CLPK_doublecomplex *__snv, __CLPK_doublereal *__csq,
        __CLPK_doublecomplex *__snq)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlagtm_(char *__trans, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__alpha, __CLPK_doublecomplex *__dl,
        __CLPK_doublecomplex *__d__, __CLPK_doublecomplex *__du,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__beta, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlahef_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__w,
        __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlahqr_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_integer *__iloz,
        __CLPK_integer *__ihiz, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlahr2_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__t,
        __CLPK_integer *__ldt, __CLPK_doublecomplex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlahrd_(__CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__nb,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__t,
        __CLPK_integer *__ldt, __CLPK_doublecomplex *__y,
        __CLPK_integer *__ldy)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaic1_(__CLPK_integer *__job, __CLPK_integer *__j,
        __CLPK_doublecomplex *__x, __CLPK_doublereal *__sest,
        __CLPK_doublecomplex *__w, __CLPK_doublecomplex *__gamma,
        __CLPK_doublereal *__sestpr, __CLPK_doublecomplex *__s,
        __CLPK_doublecomplex *__c__)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlals0_(__CLPK_integer *__icompq, __CLPK_integer *__nl,
        __CLPK_integer *__nr, __CLPK_integer *__sqre, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__bx, __CLPK_integer *__ldbx,
        __CLPK_integer *__perm, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_doublereal *__givnum, __CLPK_integer *__ldgnum,
        __CLPK_doublereal *__poles, __CLPK_doublereal *__difl,
        __CLPK_doublereal *__difr, __CLPK_doublereal *__z__,
        __CLPK_integer *__k, __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlalsa_(__CLPK_integer *__icompq, __CLPK_integer *__smlsiz,
        __CLPK_integer *__n, __CLPK_integer *__nrhs, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__bx,
        __CLPK_integer *__ldbx, __CLPK_doublereal *__u, __CLPK_integer *__ldu,
        __CLPK_doublereal *__vt, __CLPK_integer *__k, __CLPK_doublereal *__difl,
        __CLPK_doublereal *__difr, __CLPK_doublereal *__z__,
        __CLPK_doublereal *__poles, __CLPK_integer *__givptr,
        __CLPK_integer *__givcol, __CLPK_integer *__ldgcol,
        __CLPK_integer *__perm, __CLPK_doublereal *__givnum,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlalsd_(char *__uplo, __CLPK_integer *__smlsiz, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublereal *__rcond,
        __CLPK_integer *__rank, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlangb_(char *__norm, __CLPK_integer *__n,
        __CLPK_integer *__kl, __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlange_(char *__norm, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlangt_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublecomplex *__dl, __CLPK_doublecomplex *__d__,
        __CLPK_doublecomplex *__du)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanhb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanhe_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanhf_(char *__norm, char *__transr, char *__uplo,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanhp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanhs_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlanht_(char *__norm, __CLPK_integer *__n,
        __CLPK_doublereal *__d__,
        __CLPK_doublecomplex *__e)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlansb_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlansp_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlansy_(char *__norm, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlantb_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlantp_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal zlantr_(char *__norm, char *__uplo, char *__diag,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlapll_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__y,
        __CLPK_integer *__incy,
        __CLPK_doublereal *__ssmin)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlapmt_(__CLPK_logical *__forwrd, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_integer *__k)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqgb_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__kl,
        __CLPK_integer *__ku, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqge_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__r__,
        __CLPK_doublereal *__c__, __CLPK_doublereal *__rowcnd,
        __CLPK_doublereal *__colcnd, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqhb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqhe_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqhp_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqp2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__jpvt, __CLPK_doublecomplex *__tau,
        __CLPK_doublereal *__vn1, __CLPK_doublereal *__vn2,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqps_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__offset,
        __CLPK_integer *__nb, __CLPK_integer *__kb, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__jpvt,
        __CLPK_doublecomplex *__tau, __CLPK_doublereal *__vn1,
        __CLPK_doublereal *__vn2, __CLPK_doublecomplex *__auxv,
        __CLPK_doublecomplex *__f,
        __CLPK_integer *__ldf)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr0_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_integer *__iloz,
        __CLPK_integer *__ihiz, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr1_(__CLPK_integer *__n, __CLPK_doublecomplex *__h__,
        __CLPK_integer *__ldh, __CLPK_doublecomplex *__s1,
        __CLPK_doublecomplex *__s2,
        __CLPK_doublecomplex *__v)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr2_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_doublecomplex *__h__,
        __CLPK_integer *__ldh, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ns, __CLPK_integer *__nd, __CLPK_doublecomplex *__sh,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv, __CLPK_integer *__nh,
        __CLPK_doublecomplex *__t, __CLPK_integer *__ldt, __CLPK_integer *__nv,
        __CLPK_doublecomplex *__wv, __CLPK_integer *__ldwv,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr3_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ktop, __CLPK_integer *__kbot,
        __CLPK_integer *__nw, __CLPK_doublecomplex *__h__,
        __CLPK_integer *__ldh, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ns, __CLPK_integer *__nd, __CLPK_doublecomplex *__sh,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv, __CLPK_integer *__nh,
        __CLPK_doublecomplex *__t, __CLPK_integer *__ldt, __CLPK_integer *__nv,
        __CLPK_doublecomplex *__wv, __CLPK_integer *__ldwv,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr4_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__h__, __CLPK_integer *__ldh,
        __CLPK_doublecomplex *__w, __CLPK_integer *__iloz,
        __CLPK_integer *__ihiz, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqr5_(__CLPK_logical *__wantt, __CLPK_logical *__wantz,
        __CLPK_integer *__kacc22, __CLPK_integer *__n, __CLPK_integer *__ktop,
        __CLPK_integer *__kbot, __CLPK_integer *__nshfts,
        __CLPK_doublecomplex *__s, __CLPK_doublecomplex *__h__,
        __CLPK_integer *__ldh, __CLPK_integer *__iloz, __CLPK_integer *__ihiz,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__u, __CLPK_integer *__ldu, __CLPK_integer *__nv,
        __CLPK_doublecomplex *__wv, __CLPK_integer *__ldwv,
        __CLPK_integer *__nh, __CLPK_doublecomplex *__wh,
        __CLPK_integer *__ldwh)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqsb_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqsp_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaqsy_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        char *__equed)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlar1v_(__CLPK_integer *__n, __CLPK_integer *__b1, __CLPK_integer *__bn,
        __CLPK_doublereal *__lambda, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__l, __CLPK_doublereal *__ld,
        __CLPK_doublereal *__lld, __CLPK_doublereal *__pivmin,
        __CLPK_doublereal *__gaptol, __CLPK_doublecomplex *__z__,
        __CLPK_logical *__wantnc, __CLPK_integer *__negcnt,
        __CLPK_doublereal *__ztz, __CLPK_doublereal *__mingma,
        __CLPK_integer *__r__, __CLPK_integer *__isuppz,
        __CLPK_doublereal *__nrminv, __CLPK_doublereal *__resid,
        __CLPK_doublereal *__rqcorr,
        __CLPK_doublereal *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlar2v_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_doublecomplex *__y, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__incx, __CLPK_doublereal *__c__,
        __CLPK_doublecomplex *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarcm_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__rwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarf_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__v, __CLPK_integer *__incv,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarfb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarfg_(__CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__x, __CLPK_integer *__incx,
        __CLPK_doublecomplex *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarfp_(__CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__x, __CLPK_integer *__incx,
        __CLPK_doublecomplex *__tau)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarft_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarfx_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__v, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlargv_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__y,
        __CLPK_integer *__incy, __CLPK_doublereal *__c__,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarnv_(__CLPK_integer *__idist, __CLPK_integer *__iseed,
        __CLPK_integer *__n,
        __CLPK_doublecomplex *__x)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarrv_(__CLPK_integer *__n, __CLPK_doublereal *__vl,
        __CLPK_doublereal *__vu, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__l, __CLPK_doublereal *__pivmin,
        __CLPK_integer *__isplit, __CLPK_integer *__m, __CLPK_integer *__dol,
        __CLPK_integer *__dou, __CLPK_doublereal *__minrgp,
        __CLPK_doublereal *__rtol1, __CLPK_doublereal *__rtol2,
        __CLPK_doublereal *__w, __CLPK_doublereal *__werr,
        __CLPK_doublereal *__wgap, __CLPK_integer *__iblock,
        __CLPK_integer *__indexw, __CLPK_doublereal *__gers,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__isuppz, __CLPK_doublereal *__work,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarscl2_(__CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlartg_(__CLPK_doublecomplex *__f, __CLPK_doublecomplex *__g,
        __CLPK_doublereal *__cs, __CLPK_doublecomplex *__sn,
        __CLPK_doublecomplex *__r__)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlartv_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__y,
        __CLPK_integer *__incy, __CLPK_doublereal *__c__,
        __CLPK_doublecomplex *__s,
        __CLPK_integer *__incc)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarz_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__l, __CLPK_doublecomplex *__v, __CLPK_integer *__incv,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarzb_(char *__side, char *__trans, char *__direct, char *__storev,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__ldwork)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlarzt_(char *__direct, char *__storev, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__t,
        __CLPK_integer *__ldt)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlascl_(char *__type__, __CLPK_integer *__kl, __CLPK_integer *__ku,
        __CLPK_doublereal *__cfrom, __CLPK_doublereal *__cto,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlascl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublecomplex *__x,
        __CLPK_integer *__ldx)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaset_(char *__uplo, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlasr_(char *__side, char *__pivot, char *__direct, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublereal *__c__, __CLPK_doublereal *__s,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlassq_(__CLPK_integer *__n, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__sumsq)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlaswp_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__k1, __CLPK_integer *__k2,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__incx)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlasyf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_integer *__kb, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__w,
        __CLPK_integer *__ldw,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlat2c_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_complex *__sa, __CLPK_integer *__ldsa,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatbs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__x,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatdf_(__CLPK_integer *__ijob, __CLPK_integer *__n,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublecomplex *__rhs, __CLPK_doublereal *__rdsum,
        __CLPK_doublereal *__rdscal, __CLPK_integer *__ipiv,
        __CLPK_integer *__jpiv)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatps_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__x, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatrd_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nb,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__w,
        __CLPK_integer *__ldw)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatrs_(char *__uplo, char *__trans, char *__diag, char *__normin,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__x, __CLPK_doublereal *__scale,
        __CLPK_doublereal *__cnorm,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatrz_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__l,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlatzm_(char *__side, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__v, __CLPK_integer *__incv,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c1,
        __CLPK_doublecomplex *__c2, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlauu2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zlauum_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbcon_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbequ_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__afb,
        __CLPK_integer *__ldafb, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbstf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublecomplex *__afb, __CLPK_integer *__ldafb, char *__equed,
        __CLPK_doublereal *__s, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbtf2_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbtrf_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpbtrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__kd,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpftrf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpftri_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpftrs_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpocon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpoequ_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpoequb_(__CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zporfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__af, __CLPK_integer *__ldaf,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zposv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zposvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__af,
        __CLPK_integer *__ldaf, char *__equed, __CLPK_doublereal *__s,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpotf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpotrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpotri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpotrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zppcon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zppequ_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublereal *__s, __CLPK_doublereal *__scond,
        __CLPK_doublereal *__amax,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__afp,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zppsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zppsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__afp, char *__equed, __CLPK_doublereal *__s,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpptri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpstf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_doublereal *__tol, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpstrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__piv, __CLPK_integer *__rank,
        __CLPK_doublereal *__tol, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zptcon_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublecomplex *__e, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpteqr_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zptrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__e,
        __CLPK_doublereal *__df, __CLPK_doublecomplex *__ef,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zptsv_(__CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__e,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zptsvx_(char *__fact, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__e,
        __CLPK_doublereal *__df, __CLPK_doublecomplex *__ef,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpttrf_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublecomplex *__e,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zpttrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublereal *__d__, __CLPK_doublecomplex *__e,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zptts2_(__CLPK_integer *__iuplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublereal *__d__,
        __CLPK_doublecomplex *__e, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int zrot_(__CLPK_integer *__n, __CLPK_doublecomplex *__cx,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__cy,
        __CLPK_integer *__incy, __CLPK_doublereal *__c__,
        __CLPK_doublecomplex *__s)
API_AVAILABLE(macos(10.2), ios(4.0));

int zspcon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublereal *__anorm,
        __CLPK_doublereal *__rcond, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zspmv_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__x,
        __CLPK_integer *__incx, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__y,
        __CLPK_integer *__incy)
API_AVAILABLE(macos(10.2), ios(4.0));

int zspr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__x, __CLPK_integer *__incx,
        __CLPK_doublecomplex *__ap)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsprfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__afp,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zspsv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zspsvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__afp, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsptrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsptri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsptrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ap, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zstedc_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__lrwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zstegr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_doublereal *__abstol, __CLPK_integer *__m,
        __CLPK_doublereal *__w, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_integer *__isuppz,
        __CLPK_doublereal *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork, __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zstein_(__CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_integer *__iblock, __CLPK_integer *__isplit,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_doublereal *__work, __CLPK_integer *__iwork,
        __CLPK_integer *__ifail,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zstemr_(char *__jobz, char *__range, __CLPK_integer *__n,
        __CLPK_doublereal *__d__, __CLPK_doublereal *__e,
        __CLPK_doublereal *__vl, __CLPK_doublereal *__vu, __CLPK_integer *__il,
        __CLPK_integer *__iu, __CLPK_integer *__m, __CLPK_doublereal *__w,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__nzc, __CLPK_integer *__isuppz,
        __CLPK_logical *__tryrac, __CLPK_doublereal *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsteqr_(char *__compz, __CLPK_integer *__n, __CLPK_doublereal *__d__,
        __CLPK_doublereal *__e, __CLPK_doublecomplex *__z__,
        __CLPK_integer *__ldz, __CLPK_doublereal *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsycon_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublereal *__anorm, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsyequb_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublereal *__s,
        __CLPK_doublereal *__scond, __CLPK_doublereal *__amax,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsymv_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__x, __CLPK_integer *__incx,
        __CLPK_doublecomplex *__beta, __CLPK_doublecomplex *__y,
        __CLPK_integer *__incy)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsyr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__alpha,
        __CLPK_doublecomplex *__x, __CLPK_integer *__incx,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsyrfs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__af, __CLPK_integer *__ldaf,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsysv_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsysvx_(char *__fact, char *__uplo, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__af,
        __CLPK_integer *__ldaf, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__rcond, __CLPK_doublereal *__ferr,
        __CLPK_doublereal *__berr, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsytf2_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsytrf_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsytri_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_integer *__ipiv,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zsytrs_(char *__uplo, __CLPK_integer *__n, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__ipiv, __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztbcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_doublecomplex *__ab,
        __CLPK_integer *__ldab, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztbrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztbtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__kd, __CLPK_integer *__nrhs,
        __CLPK_doublecomplex *__ab, __CLPK_integer *__ldab,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztfsm_(char *__transr, char *__side, char *__uplo, char *__trans,
        char *__diag, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__a,
        __CLPK_doublecomplex *__b,
        __CLPK_integer *__ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztftri_(char *__transr, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztfttp_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__arf, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztfttr_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__arf, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__s, __CLPK_integer *__lds,
        __CLPK_doublecomplex *__p, __CLPK_integer *__ldp,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgex2_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__j1,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgexc_(__CLPK_logical *__wantq, __CLPK_logical *__wantz,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgsen_(__CLPK_integer *__ijob, __CLPK_logical *__wantq,
        __CLPK_logical *__wantz, __CLPK_logical *__select, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__alpha, __CLPK_doublecomplex *__beta,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__z__, __CLPK_integer *__ldz, __CLPK_integer *__m,
        __CLPK_doublereal *__pl, __CLPK_doublereal *__pr,
        __CLPK_doublereal *__dif, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__liwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgsja_(char *__jobu, char *__jobv, char *__jobq, __CLPK_integer *__m,
        __CLPK_integer *__p, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_integer *__l, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublereal *__tola, __CLPK_doublereal *__tolb,
        __CLPK_doublereal *__alpha, __CLPK_doublereal *__beta,
        __CLPK_doublecomplex *__u, __CLPK_integer *__ldu,
        __CLPK_doublecomplex *__v, __CLPK_integer *__ldv,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__work, __CLPK_integer *__ncycle,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__s, __CLPK_doublereal *__dif, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork, __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgsy2_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__d__, __CLPK_integer *__ldd,
        __CLPK_doublecomplex *__e, __CLPK_integer *__lde,
        __CLPK_doublecomplex *__f, __CLPK_integer *__ldf,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__rdsum,
        __CLPK_doublereal *__rdscal,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztgsyl_(char *__trans, __CLPK_integer *__ijob, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__d__, __CLPK_integer *__ldd,
        __CLPK_doublecomplex *__e, __CLPK_integer *__lde,
        __CLPK_doublecomplex *__f, __CLPK_integer *__ldf,
        __CLPK_doublereal *__scale, __CLPK_doublereal *__dif,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__iwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztpcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublereal *__rcond,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztprfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztptri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztptrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztpttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__ap, __CLPK_doublecomplex *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztpttr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrcon_(char *__norm, char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublereal *__rcond, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrevc_(char *__side, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_integer *__mm, __CLPK_integer *__m, __CLPK_doublecomplex *__work,
        __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrexc_(char *__compq, __CLPK_integer *__n, __CLPK_doublecomplex *__t,
        __CLPK_integer *__ldt, __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_integer *__ifst, __CLPK_integer *__ilst,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrrfs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__x, __CLPK_integer *__ldx,
        __CLPK_doublereal *__ferr, __CLPK_doublereal *__berr,
        __CLPK_doublecomplex *__work, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrsen_(char *__job, char *__compq, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__q, __CLPK_integer *__ldq,
        __CLPK_doublecomplex *__w, __CLPK_integer *__m, __CLPK_doublereal *__s,
        __CLPK_doublereal *__sep, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrsna_(char *__job, char *__howmny, __CLPK_logical *__select,
        __CLPK_integer *__n, __CLPK_doublecomplex *__t, __CLPK_integer *__ldt,
        __CLPK_doublecomplex *__vl, __CLPK_integer *__ldvl,
        __CLPK_doublecomplex *__vr, __CLPK_integer *__ldvr,
        __CLPK_doublereal *__s, __CLPK_doublereal *__sep, __CLPK_integer *__mm,
        __CLPK_integer *__m, __CLPK_doublecomplex *__work,
        __CLPK_integer *__ldwork, __CLPK_doublereal *__rwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrsyl_(char *__trana, char *__tranb, __CLPK_integer *__isgn,
        __CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublereal *__scale,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrti2_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrtri_(char *__uplo, char *__diag, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrtrs_(char *__uplo, char *__trans, char *__diag, __CLPK_integer *__n,
        __CLPK_integer *__nrhs, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__b, __CLPK_integer *__ldb,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrttf_(char *__transr, char *__uplo, __CLPK_integer *__n,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__arf,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztrttp_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__ap,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztzrqf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int ztzrzf_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zung2l_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zung2r_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungbr_(char *__vect, __CLPK_integer *__m, __CLPK_integer *__n,
        __CLPK_integer *__k, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunghr_(__CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungl2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunglq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungql_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungqr_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungr2_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungrq_(__CLPK_integer *__m, __CLPK_integer *__n, __CLPK_integer *__k,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zungtr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunm2l_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunm2r_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmbr_(char *__vect, char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmhr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__ilo, __CLPK_integer *__ihi,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunml2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmlq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmql_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmqr_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmr2_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmr3_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmrq_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_doublecomplex *__a,
        __CLPK_integer *__lda, __CLPK_doublecomplex *__tau,
        __CLPK_doublecomplex *__c__, __CLPK_integer *__ldc,
        __CLPK_doublecomplex *__work, __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmrz_(char *__side, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_integer *__k, __CLPK_integer *__l,
        __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zunmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__a, __CLPK_integer *__lda,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublecomplex *__work,
        __CLPK_integer *__lwork,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zupgtr_(char *__uplo, __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__q,
        __CLPK_integer *__ldq, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int zupmtr_(char *__side, char *__uplo, char *__trans, __CLPK_integer *__m,
        __CLPK_integer *__n, __CLPK_doublecomplex *__ap,
        __CLPK_doublecomplex *__tau, __CLPK_doublecomplex *__c__,
        __CLPK_integer *__ldc, __CLPK_doublecomplex *__work,
        __CLPK_integer *__info)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamc1_(__CLPK_integer *__beta, __CLPK_integer *__t, __CLPK_logical *__rnd,
        __CLPK_logical *__ieee1)
API_AVAILABLE(macos(10.2), ios(4.0));

int ilaver_(__CLPK_integer *__vers_major__, __CLPK_integer *__vers_minor__,
        __CLPK_integer *__vers_patch__)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slamch_(char *__cmach)
API_AVAILABLE(macos(10.2), ios(4.0));

int slamc1_(__CLPK_integer *__beta, __CLPK_integer *__t, __CLPK_logical *__rnd,
        __CLPK_logical *__ieee1)
API_AVAILABLE(macos(10.2), ios(4.0));

int slamc2_(__CLPK_integer *__beta, __CLPK_integer *__t, __CLPK_logical *__rnd,
        __CLPK_real *__eps, __CLPK_integer *__emin, __CLPK_real *__rmin,
        __CLPK_integer *__emax,
        __CLPK_real *__rmax)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal slamc3_(__CLPK_real *__a,
        __CLPK_real *__b)
API_AVAILABLE(macos(10.2), ios(4.0));

int slamc4_(__CLPK_integer *__emin, __CLPK_real *__start,
        __CLPK_integer *__base)
API_AVAILABLE(macos(10.2), ios(4.0));

int slamc5_(__CLPK_integer *__beta, __CLPK_integer *__p, __CLPK_integer *__emin,
        __CLPK_logical *__ieee, __CLPK_integer *__emax,
        __CLPK_real *__rmax)
API_AVAILABLE(macos(10.2), ios(4.0));


__CLPK_doublereal dlamch_(char *__cmach)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamc1_(__CLPK_integer *__beta, __CLPK_integer *__t, __CLPK_logical *__rnd,
        __CLPK_logical *__ieee1)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamc2_(__CLPK_integer *__beta, __CLPK_integer *__t, __CLPK_logical *__rnd,
        __CLPK_doublereal *__eps, __CLPK_integer *__emin,
        __CLPK_doublereal *__rmin, __CLPK_integer *__emax,
        __CLPK_doublereal *__rmax)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_doublereal dlamc3_(__CLPK_doublereal *__a,
        __CLPK_doublereal *__b)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamc4_(__CLPK_integer *__emin, __CLPK_doublereal *__start,
        __CLPK_integer *__base)
API_AVAILABLE(macos(10.2), ios(4.0));

int dlamc5_(__CLPK_integer *__beta, __CLPK_integer *__p, __CLPK_integer *__emin,
        __CLPK_logical *__ieee, __CLPK_integer *__emax,
        __CLPK_doublereal *__rmax)
API_AVAILABLE(macos(10.2), ios(4.0));

__CLPK_integer ilaenv_(__CLPK_integer *__ispec, char *__name__, char *__opts,
        __CLPK_integer *__n1, __CLPK_integer *__n2, __CLPK_integer *__n3,
        __CLPK_integer *__n4)
API_AVAILABLE(macos(10.2), ios(4.0));


#ifdef __cplusplus
}
#endif
#endif /* __CLAPACK_H */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Quadrature/Quadrature.h
// Numerical Analysis Library

#ifndef __QUADRATURE_PUBLIC_HEADER__
#define __QUADRATURE_PUBLIC_HEADER__

#ifdef __cplusplus
extern "C" {
#endif

/*! @enum quadrature_status

@abstract Quadrature return status

@discussion Success is 0, and errors have a negative value.

@constant QUADRATURE_SUCCESS Success.
@constant QUADRATURE_ERROR Generic error.
@constant QUADRATURE_INVALID_ARG_ERROR Invalid argument.
@constant QUADRATURE_ALLOC_ERROR Memory allocation failed.
@constant QUADRATURE_INTERNAL_ERROR This is a bug in the Quadrature code, please file a bug report.
@constant QUADRATURE_INTEGRATE_MAX_EVAL_ERROR The requested accuracy limit could not be reached with the allowed number of evals/subdivisions.
@constant QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR Extremely bad integrand behaviour, or excessive roundoff error occurs at some points of the integration interval.
*/
typedef enum {

  QUADRATURE_SUCCESS                          =    0,
  
  QUADRATURE_ERROR                            =   -1,
  QUADRATURE_INVALID_ARG_ERROR                =   -2,
  QUADRATURE_ALLOC_ERROR                      =   -3,
  QUADRATURE_INTERNAL_ERROR                   =  -99,

  QUADRATURE_INTEGRATE_MAX_EVAL_ERROR         = -101,
  QUADRATURE_INTEGRATE_BAD_BEHAVIOUR_ERROR    = -102

} quadrature_status;

#include "Integration.h"

#ifdef __cplusplus
}
#endif

#endif // __QUADRATURE_PUBLIC_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Quadrature/Integration.h
// Numerical integration

#ifndef QUADRATURE_INTEGRATION_H
#define QUADRATURE_INTEGRATION_H

#include <stddef.h>

// Availability
#if __has_include( <Availability.h> )
#include <Availability.h>
#else
#define __API_AVAILABLE(...)
#endif

// Nullability
#if __has_feature(assume_nonnull)
  _Pragma("clang assume_nonnull begin")
#else
#define _Null_unspecified
#define _Nullable
#endif

/*!
@abstract Y=F(X) one-dimensional real function, array form, double precision

@discussion Should set values <tt>y[i] = F(x[i])</tt> for i=0..n-1.

@param arg User argument passed back to the function when evaluated
@param n Dimension of arrays X and Y
@param x Array of points to evaluate the function
@param y Array receiving the values
*/
typedef void (*quadrature_function_array)(void * _Null_unspecified __arg, size_t __n, const double *__x, double *__y);

/*! @enum quadrature_integrator

@abstract Integrators

@constant QUADRATURE_INTEGRATE_QNG
Simple non-adaptive automatic integrator using Gauss-Kronrod-Patterson quadrature coefficients.
Evaluates 21, or 43, or 87 points in the interval until the requested accuracy is reached.
No workspace is necessary for this integrator.

@constant QUADRATURE_INTEGRATE_QAG
Simple globally adaptive integrator.
Allows selection of the number of Gauss-Kronrod points used in each subinterval, and the max number of subintervals.

@constant QUADRATURE_INTEGRATE_QAGS
Global adaptive quadrature based on 21-point or 15-point (if at least one bound is infinite) GaussKronrod quadrature within each subinterval, with acceleration by Peter Wynn's epsilon algorithm.
If at least one of the interval bounds is infinite, this is equivalent to the QUADPACK QAGI routine. Otherwise, this is equivalent to the QUADPACK QAGS routine.

*/
typedef enum {

  QUADRATURE_INTEGRATE_QNG,
  QUADRATURE_INTEGRATE_QAG,
  QUADRATURE_INTEGRATE_QAGS

} quadrature_integrator;

/*!
@abstract Bytes per interval to allocate in workspace for the QAG integrator. */
#define QUADRATURE_INTEGRATE_QAG_WORKSPACE_PER_INTERVAL 32

/*!
@abstract Bytes per interval to allocate in workspace for the QAGS integrator. */
#define QUADRATURE_INTEGRATE_QAGS_WORKSPACE_PER_INTERVAL 152

/*!
@abstract Function to integrate

@discussion Describes a real function Y=F(X). Since most of the integration time is spent
evaluating F, we allow the caller to provide a array callback, computing several values of F
in a single call.

@field fun Y=F(X) callback.
@field fun_arg User pointer passed as first argument to all invocations of <tt>fun</tt>.
*/
typedef struct {

  quadrature_function_array fun;
  void * _Null_unspecified fun_arg;

} quadrature_integrate_function;

/*!
@abstract Integration options

@field integrator Integration algorithm to apply.
@field abs_tolerance Requested absolute tolerance on the result.
@field rel_tolerance Requested relative tolerance on the result.

@field qag_points_per_interval Number of Gauss-Kronrod points per interval used by the QAG integrator.
Can be 0, 15, 21, 31, 41, 51, 61. 0 maps to the default 21.
Used by the QAG integrator only. Other integrators ignore this value.

@field max_intervals Limit the number of intervals in the subdivision used by adaptive integrators.
If a workspace is provided, this value is ignored, and the number of intervals is limited by <tt>workspace_size</tt>.
<br>The QNG integrator doesn't require a workspace.
<br>The QAG integrator requires at least <tt>max_intervals * QUADRATURE_INTEGRATE_QAG_WORKSPACE_PER_INTERVAL</tt> bytes in <tt>workspace</tt>.
<br>The QAGS integrator requires at least <tt>max_intervals * QUADRATURE_INTEGRATE_QAGS_WORKSPACE_PER_INTERVAL</tt> bytes in <tt>workspace</tt>.

*/
typedef struct {

  quadrature_integrator integrator;
  double abs_tolerance;
  double rel_tolerance;
  size_t qag_points_per_interval;
  size_t max_intervals;
  
} quadrature_integrate_options;

/*!
@abstract Integrate a function F over ]A,B[.

@discussion
This function provides a set of algorithms (integrators) to compute an approximation S' of the integral S = &int; F(x) dx over the interval ]A,B[.

The QNG (simple non-adaptive Gauss-Kronrod integration) and QAG (simple adaptive Gauss-Kronrod integration)
integrators are C ports of the QUADPACK library corresponding routines.
The QAGS integrator provides the functionality offered by the QAGS and QAGI QUADPACK routines.

On success, S' is assumed to verify abs(S-S') &le; max(abs_tolerance, rel_tolerance * abs(S)).
The integration algorithms will identify <b>most</b> cases of divergence, slow convergence, and bad behaviour, and report an error.
The bounds a,b do not need to verify a &le; b. If the integrator is QAGS, one or both of the interval bounds can be infinite (-INFINITY or +INFINITY).

Unless F is known to be smooth enough to guarantee success, it is strongly advised to always check the returned status and absolute error.

The QUADPACK library documentation provides a decision tree to select the integrator.
Adapted to match the contents of this library, this decision tree becomes:

<b>Integration over a finite region</b>

<ul>
<li>If performance is not a concern and you dont know much about the specifics of the problem, use QAGS.
<li>Otherwise, if the integrand is smooth, use QNG or QAG if the requested tolerance couldn't be reached with QNG.
<li>Otherwise, if there are discontinuities or singularities of the integrand or of its derivative, and you know where they are,
split the integration range at these points and analyze each subinterval.
<li>Otherwise, if the integrand has end point singularities, use QAGS.
<li>Otherwise, if the integrand has an oscillatory behavior of nonspecific type, and no singularities, use QAG with 61 points per interval.
<li>Otherwise, use QAGS.
</ul>

<b>Integration over an infinite region</b>

<ul>
<li>If the integrand decays rapidly to zero, truncate the interval and use the finite interval decision tree.
<li>Otherwise, if you are not constrained by computer time, and do not wish to analyze the problem further, use QAGS.
<li>Otherwise, if the integrand has a non-smooth behavior in the range, and you know where it occurs, split off these regions and use
the appropriate finite range routines to integrate over them. Then begin this tree again to handle the remainder of the region.
<li>Otherwise, truncation of the interval, or application of a suitable transformation for reducing the problem to a finite range may be possible.
</ul>

QUADPACK Reference:
Robert Piessens, Elise deDoncker-Kapenga, Christian Ueberhuber, David Kahaner,
QUADPACK: A Subroutine Package for Automatic Integration,
Springer, 1983, ISBN: 3540125531, LC: QA299.3.Q36.

@param f Function to integrate. Can't be NULL.
@param a First bound of the integration interval. May be +/- INFINITY for QAGS.
@param b Second bound of the integration interval. May be +/- INFINITY for QAGS.
@param options Integration options. Can't be NULL.
@param status If not NULL, <tt>*status</tt> receives QUADRATURE_SUCCESS if the integration was successful, and a negative QUADRATURE_..._ERROR code on failure.
@param abs_error If not NULL, <tt>*abs_error</tt> receives an estimate of the absolute error on the result.
@param workspace_size Number of bytes allocated in <tt>workspace</tt>, or 0.
@param workspace If not NULL, points to a work buffer of <tt>workspace_size</tt> bytes. In that case, no additional memory will be allocated.
If NULL, the function will internally allocate a workspace buffer if one is needed.

@return Returns an approximation to the integral.
*/
extern double quadrature_integrate(const quadrature_integrate_function * __f,
                                   double __a,
                                   double __b,
                                   const quadrature_integrate_options * options,
                                   quadrature_status * _Nullable status,
                                   double * _Nullable abs_error,
                                   size_t workspace_size,
                                   void * __restrict _Nullable workspace)
__API_AVAILABLE(macos(10.12), ios(10.0), tvos(10.0), watchos(3.0));

// Availability
#if !__has_include(<Availability.h>)
#undef __API_AVAILABLE
#endif

// Nullability
#if __has_feature(assume_nonnull)
  _Pragma("clang assume_nonnull end")
#else
#undef _Nullable
#undef _Null_unspecified
#endif

#endif // QUADRATURE_INTEGRATION_H
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vDSP.h
/*
    File:       vecLib/vDSP.h

    Contains:   AltiVec DSP Interfaces

    Version:    vecLib-728.0

    Copyright:  Copyright (c) 2000-2019 by Apple Inc. All rights reserved.

    For vDSP documentation, search for "vDSP" at <http://developer.apple.com>
    or search for one of the routine names below.

    Some documentation for vDSP routines is provided below.

    To report bugs, please use <http://developer.apple.com/bugreporter>.
*/
#ifndef __VDSP__
#define __VDSP__


// Tell compiler this file is idempotent (no need to process it more than once).
#if PRAGMA_ONCE
    #pragma once
#endif


/*  Documentation conventions:

        Many of the routines below are documented with C-like pseudocode that
        describes what they do.  For example, vDSP_vadd is declared with:

            extern void vDSP_vadd(
                const float *__A,
                vDSP_Stride  __IA,
                const float *__B,
                vDSP_Stride  __IB,
                float       *__C,
                vDSP_Stride  __IC,
                vDSP_Length  __N)
 					API_AVAILABLE(macos(10.0), ios(4.0));

        and is described with:

            for (n = 0; n < N; ++n)
                C[n] = A[n] + B[n];

        The pseudocode uses two important simplifications:

            Names are shortened.

                The prefix "__" is removed.  This prefix is used in this
                header file so that Apple parameter names do not conflict with
                other developer macro names that might be used in source files
                that include this header, as when a program might use "#define
                N 1024" to set a preprocessor macro "N" to expand to "1024".

            Vectors are simplified by omitting strides.

                The parameters A and IA (with the prefix omitted) represent a
                vector with its elements at memory locations A[i*IA], for
                appropriate values of i.  In the pseudocode, the stride IA
                is omitted; the vector is treated as a simple mathematical
                vector with elements A[i].

                This default map is assumed for all vDSP routines unless stated
                otherwise.  An array without a stride parameter has unit
                stride.  Some routines have more complicated maps.  These are
                documented with each routine.

    Default maps:

        These default maps are used unless documented otherwise for a routine.
        For real vectors:

            Pseudocode:     Memory:
            C[n]            C[n*IC]

        For complex vectors:

            Pseudocode:     Memory:
            C[n]            C->realp[n*IC] + i * C->imagp[n*IC]

        Observe that C[n] in the pseudocode is a complex number, with a real
        component and an imaginary component.


    Pseudocode:

        The pseudo-code used to describe routines is largely C with some
        additions:

            e, pi, and i are the usual mathematical constants, approximately
            2.71828182845, 3.1415926535, and sqrt(-1).

            "**" is exponentiation.  3**4 is 81.

            Re and Im are the real and imaginary parts of a complex number.
            Re(3+4*i) is 3, and Im(3+4*i) is 4.

            sum(f(j), 0 <= j < N) is the sum of f(j) evaluated for each integer
            j from 0 (inclusive) to N (exclusive).  sum(j**2, 0 <= j < 4) is
            0 + 1 + 4 + 9 = 14.  Multiple dimensions may be used.  Thus,
            sum(f(j, k), 0 <= j < M, 0 <= k < N) is the sum of f(j, k)
            evaluated for each pair of integers (j, k) satisfying the
            constraints.

            conj(z) is the complex conjugate of z (the imaginary part is
            negated).

            |x| is the absolute value of x.

   Exactness, IEEE 754 conformance:

        vDSP routines are not expected to produce results identical to the
        pseudo-code in the descriptions, because vDSP routines are free to
        rearrange calculations for better performance.  These rearrangements
        are mathematical identities, so they would produce identical results
        if exact arithmetic were used.  However, floating-point arithmetic
        is approximate, and the rounding errors will often be different when
        operations are rearranged.

        Generally, vDSP routines are not expected to conform to IEEE 754.
        Notably, results may be not correctly rounded to the last bit even for
        elementary operations, and operations involving infinities and NaNs may
        be handled differently than IEEE 754 specifies.

    Const:

        vDSP does not modify the contents of input arrays (including input
        scalars passed by address).  If the specification of a routine does not
        state that it alters the memory that a parameter points to, then the
        routine does not alter that memory through that parameter.  (It may of
        course alter the same memory if it is also pointed to by an output
        parameter.  Such in-place operation is permitted for some vDSP routines
        and not for others.)

        Unfortunately, C semantics make it impractical to add "const" to
        pointers inside structs, because such structs are type-incompatible
        with structs containing pointers that are not const.  Thus, vDSP
        routines with complex parameters accept those parameters via
        DSPSplitComplex and DSPDoubleSplitComplex structs (among other types)
        and not via const versions of those structures.

    Strides:

        (Note:  This section introduces strides.  For an issue using strides
        with complex data, see "Complex strides" below.)

        Many vDSP routines use strides, which specify that the vector operated
        on is embedded in a larger array in memory.  Consider an array A of
        1024 elements.  Then:

            Passing a vDSP routine:     Says to operate on:

            Address A and stride 1      Each element A[j]

            Address A and stride 2      Every other element, A[j*2]

            Address A+1 and stride 2    Every other element, starting
                                        with A[1], so A[j*2+1]

        Strides may be used to operate on columns of multi-dimensional arrays.
        For example, consider a 32*64 element array, A[32][64].  Then passing
        address A+13 and stride 64 instructs vDSP to operate on the elements of
        column 13.

        When strides are used, generally there is some accompanying parameter
        that specifies the length of the operation.  This length is typically
        the number of elements to be processed, not the number in the larger
        array.  (Some vDSP routines have interactions between parameters so
        that the length may specify some number of output elements but requires
        a different numbe of input elements.  This is documented with each
        routine.)

    Complex strides:

        Strides with complex data (interleaved complex data, not split
        complex data) are complicated by a legacy issue.  Originally, complex
        data was regarded as an array of individual elements, so that memory
        containing values to represent complex numbers 2 + 3i, 4 + 5i, 6 + 7i,
        and so on, contained individual floating-point elements:

            A[0] A[1] A[2] A[3] A[4] A[5]
             2    3    4    5    6    7  

        This arrangement was said to have a stride of two, indicating that a
        new complex number starts every two elements.  In the modern view, we
        would regard this as an array of struct with a stride of one struct.
        Unfortunately, the vDSP interface is bound by requirements of backward
        compatibility and must retain the original use.

        Adding to this issue, a parameter is declared as a pointer to DSPComplex
        or DSPDoubleComplex (both structures of two floating-point elements),
        but its stride is still passed as a stride of floating-point elements.
        This means that, in C, to refer to complex element i of a vector C with
        stride IC, you must divide the stride by 2, using C[i*IC/2].
        Essentially, the floating-point element stride passed to the vDSP
        routine, IA, should be twice the complex-number struct stride.
*/


// For i386, translate new names to legacy names.
#if defined __i386__ && !defined __vDSP_TRANSLATE__
#include <vecLib/vDSP_translate.h>
#endif


#include <os/availability.h>
#include <stdint.h>
#include <stdbool.h>


#ifdef __cplusplus
extern "C" {
#endif


#include <CoreFoundation/CFAvailability.h>
#define vDSP_ENUM   CF_ENUM


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#else
    #define __nullable
    #define __nonnull
#endif


#pragma options align=power


/*  These symbols describe the vecLib version associated with this header.

    vDSP_Version0 is a major version number.
    vDSP_Version1 is a minor version number.
*/
#define vDSP_Version0   728
#define vDSP_Version1   0


/*  Define types:

        vDSP_Length for numbers of elements in arrays and for indices of
        elements in arrays.  (It is also used for the base-two logarithm of
        numbers of elements, although a much smaller type is suitable for
        that.)

        vDSP_Stride for differences of indices of elements (which of course
        includes strides).
*/
typedef unsigned long vDSP_Length;
#if defined __arm64__ && !defined __LP64__
typedef long long     vDSP_Stride;
#else
typedef long          vDSP_Stride;
#endif

/*  A DSPComplex or DSPDoubleComplex is a pair of float or double values that
    together represent a complex value.
*/
typedef struct DSPComplex {
    float  real;
    float  imag;
} DSPComplex;
typedef struct DSPDoubleComplex {
    double real;
    double imag;
} DSPDoubleComplex;


/*  A DSPSplitComplex or DSPDoubleSplitComplex is a structure containing
    two pointers, each to an array of float or double.  These represent arrays
    of complex values, with the real components of the values stored in one
    array and the imaginary components of the values stored in a separate
    array.
*/
typedef struct DSPSplitComplex {
    float  * __nonnull realp;
    float  * __nonnull imagp;
} DSPSplitComplex;
typedef struct DSPDoubleSplitComplex {
    double * __nonnull realp;
    double * __nonnull imagp;
} DSPDoubleSplitComplex;


/*  The following statements declare a few simple types and constants used by
    various vDSP routines.
*/
typedef int FFTDirection;
typedef int FFTRadix;
enum {
    kFFTDirection_Forward         = +1,
    kFFTDirection_Inverse         = -1
};
enum {
    kFFTRadix2                    = 0,
    kFFTRadix3                    = 1,
    kFFTRadix5                    = 2
};
enum {
    vDSP_HALF_WINDOW              = 1,
    vDSP_HANN_DENORM              = 0,
    vDSP_HANN_NORM                = 2
};
    

/*  The following types define 24-bit data.
*/
typedef struct { uint8_t bytes[3]; } vDSP_uint24; // Unsigned 24-bit integer.
typedef struct { uint8_t bytes[3]; } vDSP_int24;  // Signed 24-bit integer.


/*  The following types are pointers to structures that contain data used
    inside vDSP routines to assist FFT and biquad filter operations.  The
    contents of these structures may change from release to release, so
    applications should manipulate the values only via the corresponding vDSP
    setup and destroy routines.
*/
typedef struct OpaqueFFTSetup           *FFTSetup;
typedef struct OpaqueFFTSetupD          *FFTSetupD;
typedef struct vDSP_biquad_SetupStruct  *vDSP_biquad_Setup;
typedef struct vDSP_biquad_SetupStructD *vDSP_biquad_SetupD;

    
/*  vDSP_biquadm_Setup or vDSP_biquadm_SetupD is a pointer to a filter object
    to be used with a multi-channel cascaded biquad IIR.  This object carries
    internal state which may be modified by any routine which uses it.  Upon
    creation, the state is initialized such that all delay elements are zero.
 
    Each filter object should only be used in a single thread at a time.
*/
typedef struct vDSP_biquadm_SetupStruct  *vDSP_biquadm_Setup;
typedef struct vDSP_biquadm_SetupStructD *vDSP_biquadm_SetupD;


/*  vDSP_create_fftsetup and vDSP_create_ffsetupD allocate memory and prepare
    constants used by single- and double-precision FFT routines, respectively.

    vDSP_destroy_fftsetup and vDSP_destroy_fftsetupD free the memory.  They
    may be passed a null pointer, in which case they have no effect.
*/
extern __nullable FFTSetup vDSP_create_fftsetup(
    vDSP_Length __Log2n,
    FFTRadix    __Radix)
		API_AVAILABLE(macos(10.0), ios(4.0));

extern void vDSP_destroy_fftsetup(__nullable FFTSetup __setup)
        API_AVAILABLE(macos(10.0), ios(4.0));

extern __nullable FFTSetupD vDSP_create_fftsetupD(
    vDSP_Length __Log2n,
    FFTRadix    __Radix)
        API_AVAILABLE(macos(10.2), ios(4.0));

extern void vDSP_destroy_fftsetupD(__nullable FFTSetupD __setup)
        API_AVAILABLE(macos(10.2), ios(4.0));


/*  vDSP_biquad_CreateSetup allocates memory and prepares the coefficients for
    processing a cascaded biquad IIR filter.

    vDSP_biquad_DestroySetup frees the memory allocated by
    vDSP_biquad_CreateSetup.
*/
extern __nullable vDSP_biquad_Setup vDSP_biquad_CreateSetup(
    const double *__Coefficients,
    vDSP_Length   __M)
        API_AVAILABLE(macos(10.9), ios(6.0));
extern __nullable vDSP_biquad_SetupD vDSP_biquad_CreateSetupD(
    const double *__Coefficients,
    vDSP_Length   __M)
        API_AVAILABLE(macos(10.9), ios(6.0));

extern void vDSP_biquad_DestroySetup (
    __nullable vDSP_biquad_Setup __setup)
        API_AVAILABLE(macos(10.9), ios(6.0));
extern void vDSP_biquad_DestroySetupD(
    __nullable vDSP_biquad_SetupD __setup)
        API_AVAILABLE(macos(10.9), ios(6.0));


/*  vDSP_biquadm_CreateSetup (for float) or vDSP_biquadm_CreateSetupD (for
    double) allocates memory and prepares the coefficients for processing a
    multi-channel cascaded biquad IIR filter.  Delay values are set to zero.

    Unlike some other setup objects in vDSP, a vDSP_biquadm_Setup or
    vDSP_biquadm_SetupD contains data that is modified during a vDSP_biquadm or
    vDSP_biquadmD call, and it therefore may not be used more than once
    simultaneously, as in multiple threads.
 
    vDSP_biquadm_DestroySetup (for single) or vDSP_biquadm_DestroySetupD (for
    double) frees the memory allocated by the corresponding create-setup
    routine.
*/
extern __nullable vDSP_biquadm_Setup vDSP_biquadm_CreateSetup(
    const double *__coeffs,
    vDSP_Length   __M,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern __nullable vDSP_biquadm_SetupD vDSP_biquadm_CreateSetupD(
    const double *__coeffs,
    vDSP_Length   __M,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
extern void vDSP_biquadm_DestroySetup(
    vDSP_biquadm_Setup __setup)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_biquadm_DestroySetupD(
    vDSP_biquadm_SetupD __setup)
        API_AVAILABLE(macos(10.10), ios(8.0));

/*  vDSP_biquadm_CopyState (for float) or vDSP_biquadm_CopyStateD (for double)
    copies the current state between two biquadm setup objects.  The two
    objects must have been created with the same number of channels and
    sections.
 
    vDSP_biquadm_ResetState (for float) or vDSP_biquadm_ResetStateD (for
    double) sets the delay values of a biquadm setup object to zero.
*/
extern void vDSP_biquadm_CopyState(
    vDSP_biquadm_Setup                     __dest,
    const struct vDSP_biquadm_SetupStruct *__src)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_biquadm_CopyStateD(
    vDSP_biquadm_SetupD                     __dest,
    const struct vDSP_biquadm_SetupStructD *__src)
        API_AVAILABLE(macos(10.10), ios(8.0));
extern void vDSP_biquadm_ResetState(vDSP_biquadm_Setup __setup)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_biquadm_ResetStateD(vDSP_biquadm_SetupD __setup)
        API_AVAILABLE(macos(10.10), ios(8.0));

/*
    vDSP_biquadm_SetCoefficientsDouble will
    update the filter coefficients within a valid vDSP_biquadm_Setup object.
 */
    
extern void vDSP_biquadm_SetCoefficientsDouble(
    vDSP_biquadm_Setup                  __setup,
    const double                       *__coeffs,
    vDSP_Length                         __start_sec,
    vDSP_Length                         __start_chn,
    vDSP_Length                         __nsec,
    vDSP_Length                         __nchn)
        API_AVAILABLE(macos(10.11), ios(9.0));
    
/*
    vDSP_biquadm_SetTargetsDouble will
    set the target coefficients within a valid vDSP_biquadm_Setup object.
 */
    
extern void vDSP_biquadm_SetTargetsDouble(
    vDSP_biquadm_Setup                  __setup,
    const double                       *__targets,
    float                               __interp_rate,
    float                               __interp_threshold,
    vDSP_Length                         __start_sec,
    vDSP_Length                         __start_chn,
    vDSP_Length                         __nsec,
    vDSP_Length                         __nchn)
        API_AVAILABLE(macos(10.11), ios(9.0));
    
/*
    vDSP_biquadm_SetCoefficientsSingle will
    update the filter coefficients within a valid vDSP_biquadm_Setup object.
 
    Coefficients are specified in single precision.
 */
    
extern void vDSP_biquadm_SetCoefficientsSingle(
    vDSP_biquadm_Setup                  __setup,
    const float                         *__coeffs,
    vDSP_Length                         __start_sec,
    vDSP_Length                         __start_chn,
    vDSP_Length                         __nsec,
    vDSP_Length                         __nchn)
        API_AVAILABLE(macos(10.11), ios(9.0));
    
/*
    vDSP_biquadm_SetTargetsSingle will
    set the target coefficients within a valid vDSP_biquadm_Setup object.
    The target values are specified in single precision.
 */
    
extern void vDSP_biquadm_SetTargetsSingle(
    vDSP_biquadm_Setup                  __setup,
    const float                        *__targets,
    float                               __interp_rate,
    float                               __interp_threshold,
    vDSP_Length                         __start_sec,
    vDSP_Length                         __start_chn,
    vDSP_Length                         __nsec,
    vDSP_Length                         __nchn)
        API_AVAILABLE(macos(10.11), ios(9.0));
/*
    vDSP_biquadm_SetActiveFilters will set the overall active/inactive filter
    state of a valid vDSP_biquadm_Setup object.
 */
extern void vDSP_biquadm_SetActiveFilters(
    vDSP_biquadm_Setup                  __setup,
    const bool                         *__filter_states)
        API_AVAILABLE(macos(10.11), ios(9.0));

// Convert a complex array to a complex-split array.
extern void vDSP_ctoz(
    const DSPComplex      *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Z,
    vDSP_Stride            __IZ,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_ctozD(
    const DSPDoubleComplex      *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Z,
    vDSP_Stride                  __IZ,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Map:

            Pseudocode:     Memory:
            C[n]            C[n*IC/2].real + i * C[n*IC/2].imag
            Z[n]            Z->realp[n*IZ] + i * Z->imagp[n*IZ]

        These compute:

            for (n = 0; n < N; ++n)
                Z[n] = C[n];
    */


//  Convert a complex-split array to a complex array.
extern void vDSP_ztoc(
    const DSPSplitComplex *__Z,
    vDSP_Stride            __IZ,
    DSPComplex            *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_ztocD(
    const DSPDoubleSplitComplex *__Z,
    vDSP_Stride                  __IZ,
    DSPDoubleComplex            *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Map:

            Pseudocode:     Memory:
            Z[n]            Z->realp[n*IZ] + i * Z->imagp[n*IZ]
            C[n]            C[n*IC/2].real + i * C[n*IC/2].imag

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = Z[n];
    */



/*  In-place complex Discrete Fourier Transform routines, with and without
    temporary memory.  We suggest you use the DFT routines instead of these.
*/
extern void vDSP_fft_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        scale = 0 < Direction ? 1 : 1./N;

        // Define a complex vector, h:
        for (j = 0; j < N; ++j)
            h[j] = C->realp[j*IC] + i * C->imagp[j*IC];

        // Perform Discrete Fourier Transform.
        for (k = 0; k < N; ++k)
            H[k] = scale * sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

        // Store result.
        for (k = 0; k < N; ++k)
        {
            C->realp[k*IC] = Re(H[k]);
            C->imagp[k*IC] = Im(H[k]);
        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain the lesser of 16,384
        bytes or N * sizeof *C->realp bytes and is preferably 16-byte aligned
        or better.
    */


/*  Out-of-place complex Discrete Fourier Transform routines, with and without
    temporary memory.  We suggest you use the DFT routines instead of these.
*/
extern void vDSP_fft_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        scale = 0 < Direction ? 1 : 1./N;

        // Define a complex vector, h:
        for (j = 0; j < N; ++j)
            h[j] = A->realp[j*IA] + i * A->imagp[j*IA];

        // Perform Discrete Fourier Transform.
        for (k = 0; k < N; ++k)
            H[k] = scale * sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

        // Store result.
        for (k = 0; k < N; ++k)
        {
            C->realp[k*IC] = Re(H[k]);
            C->imagp[k*IC] = Im(H[k]);
        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain the lesser of 16,384
        bytes or N * sizeof *C->realp bytes and is preferably 16-byte aligned
        or better.
    */


/*  In-place real-to-complex Discrete Fourier Transform routines, with and
    without temporary memory.  We suggest you use the DFT routines instead of
    these.
*/
extern void vDSP_fft_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        If Direction is +1, a real-to-complex transform is performed, taking
        input from a real vector that has been coerced into the complex
        structure:

            scale = 2;

            // Define a real vector, h:
            for (j = 0; j < N/2; ++j)
            {
                h[2*j + 0] = C->realp[j*IC];
                h[2*j + 1] = C->imagp[j*IC];
            }

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale *
                    sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Pack DC and Nyquist components into C->realp[0] and C->imagp[0].
            C->realp[0*IC] = Re(H[ 0 ]).
            C->imagp[0*IC] = Re(H[N/2]).

            // Store regular components:
            for (k = 1; k < N/2; ++k)
            {
                C->realp[k*IC] = Re(H[k]);
                C->imagp[k*IC] = Im(H[k]);
            }

            Note that, for N/2 < k < N, H[k] is not stored.  However, since
            the input is a real vector, the output has symmetry that allows the
            unstored elements to be derived from the stored elements:  H[k] =
            conj(H(N-k)).  This symmetry also implies the DC and Nyquist
            components are real, so their imaginary parts are zero.

        If Direction is -1, a complex-to-real inverse transform is performed,
        producing a real output vector coerced into the complex structure:

            scale = 1./N;

            // Define a complex vector, h:
            h[ 0 ] = C->realp[0*IC];
            h[N/2] = C->imagp[0*IC];
            for (j = 1; j < N/2; ++j)
            {
                h[ j ] = C->realp[j*IC] + i * C->imagp[j*IC];
                h[N-j] = conj(h[j]);
            }

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale *
                    sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Coerce real results into complex structure:
            for (k = 0; k < N/2; ++k)
            {
                C->realp[k*IC] = H[2*k+0];
                C->imagp[k*IC] = H[2*k+1];
            }

            Note that, mathematically, the symmetry in the input vector compels
            every H[k] to be real, so there are no imaginary components to be
            stored.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain N/2 * sizeof *C->realp
        bytes and is preferably 16-byte aligned or better.
    */


/*  Out-of-place real-to-complex Discrete Fourier Transform routines, with and
    without temporary memory.  We suggest you use the DFT routines instead of
    these.
*/
extern void vDSP_fft_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        If Direction is +1, a real-to-complex transform is performed, taking
        input from a real vector that has been coerced into the complex
        structure:

            scale = 2;

            // Define a real vector, h:
            for (j = 0; j < N/2; ++j)
            {
                h[2*j + 0] = A->realp[j*IA];
                h[2*j + 1] = A->imagp[j*IA];
            }

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale *
                    sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Pack DC and Nyquist components into C->realp[0] and C->imagp[0].
            C->realp[0*IC] = Re(H[ 0 ]).
            C->imagp[0*IC] = Re(H[N/2]).

            // Store regular components:
            for (k = 1; k < N/2; ++k)
            {
                C->realp[k*IC] = Re(H[k]);
                C->imagp[k*IC] = Im(H[k]);
            }

            Note that, for N/2 < k < N, H[k] is not stored.  However, since
            the input is a real vector, the output has symmetry that allows the
            unstored elements to be derived from the stored elements:  H[k] =
            conj(H(N-k)).  This symmetry also implies the DC and Nyquist
            components are real, so their imaginary parts are zero.

        If Direction is -1, a complex-to-real inverse transform is performed,
        producing a real output vector coerced into the complex structure:

            scale = 1./N;

            // Define a complex vector, h:
            h[ 0 ] = A->realp[0*IA];
            h[N/2] = A->imagp[0*IA];
            for (j = 1; j < N/2; ++j)
            {
                h[ j ] = A->realp[j*IA] + i * A->imagp[j*IA];
                h[N-j] = conj(h[j]);
            }

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale *
                    sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Coerce real results into complex structure:
            for (k = 0; k < N/2; ++k)
            {
                C->realp[k*IC] = H[2*k+0];
                C->imagp[k*IC] = H[2*k+1];
            }

            Note that, mathematically, the symmetry in the input vector compels
            every H[k] to be real, so there are no imaginary components to be
            stored.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain N/2 * sizeof *C->realp
        bytes and is preferably 16-byte aligned or better.
    */


/*  In-place two-dimensional complex Discrete Fourier Transform routines, with
    and without temporary memory.
*/
extern void vDSP_fft2d_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft2d_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC1,
    vDSP_Stride            __IC0,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N0 = 1 << Log2N0;
        N1 = 1 << Log2N1;

        if (IC1 == 0) IC1 = IC0*N0;

        scale = 0 < Direction ? 1 : 1. / (N1*N0);

        // Define a complex matrix, h:
        for (j1 = 0; j1 < N1; ++j1)
        for (j0 = 0; j0 < N0; ++j0)
            h[j1][j0] = C->realp[j1*IC1 + j0*IC0]
                  + i * C->imagp[j1*IC1 + j0*IC0];

        // Perform Discrete Fourier Transform.
        for (k1 = 0; k1 < N1; ++k1)
        for (k0 = 0; k0 < N0; ++k0)
            H[k1][k0] = scale * sum(sum(h[j1][j0]
                * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

        // Store result.
        for (k1 = 0; k1 < N1; ++k1)
        for (k0 = 0; k0 < N0; ++k0)
        {
            C->realp[k1*IC1 + k0*IC0] = Re(H[k1][k0]);
            C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][k0]);
        }

        Note that the 0 and 1 dimensions are separate and identical, except
        that IC1 is set to a default, IC0*N0, if it is zero.  If IC1 is not
        zero, then the IC0 and N0 arguments may be swapped with the IC1 and N1
        arguments without affecting the results.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain the lesser of 16,384
        bytes or N1*N0 * sizeof *C->realp bytes and is preferably 16-byte
        aligned or better.
    */


/*  Out-of-place two-dimensional complex Discrete Fourier Transform routines,
    with and without temporary memory.
*/
extern void vDSP_fft2d_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft2d_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N0 = 1 << Log2N0;
        N1 = 1 << Log2N1;

        if (IA1 == 0) IA1 = IA0*N0;
        if (IC1 == 0) IC1 = IC0*N0;

        scale = 0 < Direction ? 1 : 1. / (N1*N0);

        // Define a complex matrix, h:
        for (j1 = 0; j1 < N1; ++j1)
        for (j0 = 0; j0 < N0; ++j0)
            h[j1][j0] = A->realp[j1*IA1 + j0*IA0]
                  + i * A->imagp[j1*IA1 + j0*IA0];

        // Perform Discrete Fourier Transform.
        for (k1 = 0; k1 < N1; ++k1)
        for (k0 = 0; k0 < N0; ++k0)
            H[k1][k0] = scale * sum(sum(h[j1][j0]
                * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

        // Store result.
        for (k1 = 0; k1 < N1; ++k1)
        for (k0 = 0; k0 < N0; ++k0)
        {
            C->realp[k1*IC1 + k0*IC0] = Re(H[k1][k0]);
            C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][k0]);
        }

        Note that the 0 and 1 dimensions are separate and identical, except
        that IA1 or IC1 are set to defaults, IA0*N0 or IC0*N0, if either is
        zero.  If neither is zero, then the IA0, IC0, and N0 arguments may be
        swapped with the IA1, IC1 and N1 arguments without affecting the
        results.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain the lesser of 16,384
        bytes or N1*N0 * sizeof *C->realp bytes and is preferably 16-byte
        aligned or better.
    */


/*  In-place two-dimensional real-to-complex Discrete Fourier Transform
    routines, with and without temporary memory.
*/
extern void vDSP_fft2d_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __flag)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft2d_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __flag)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N0 = 1 << Log2N0;
        N1 = 1 << Log2N1;

        if (IC1 == 0) IC1 = IC0*N0/2;

        If Direction is +1, a real-to-complex transform is performed, taking
        input from a real vector that has been coerced into the complex
        structure:

            scale = 2;

            // Define a real matrix, h:
            for (j1 = 0; j1 < N1  ; ++j1)
            for (j0 = 0; j0 < N0/2; ++j0)
            {
                h[j1][2*j0+0] = C->realp[j1*IC1 + j0*IC0]
                          + i * C->imagp[j1*IC1 + j0*IC0];
                h[j1][2*j0+1] = C->realp[j1*IC1 + j0*IC0]
                          + i * C->imagp[j1*IC1 + j0*IC0];
            }

            // Perform Discrete Fourier Transform.
            for (k1 = 0; k1 < N1; ++k1)
            for (k0 = 0; k0 < N0; ++k0)
                H[k1][k0] = scale * sum(sum(h[j1][j0]
                    * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                    * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

            // Pack special pure-real elements into output matrix:
            C->realp[0*IC1][0*IC0] = H[0   ][0   ].
            C->imagp[0*IC1][0*IC0] = H[0   ][N0/2]
            C->realp[1*IC1][0*IC0] = H[N1/2][0   ].
            C->imagp[1*IC1][0*IC0] = H[N1/2][N0/2]

            // Pack two vectors into output matrix "vertically":
            // (This awkward format is due to a legacy implementation.)
            for (k1 = 1; k1 < N1/2; ++k1)
            {
                C->realp[(2*k1+0)*IC1][0*IC0] = Re(H[k1][0   ]);
                C->realp[(2*k1+1)*IC1][0*IC0] = Im(H[k1][0   ]);
                C->imagp[(2*k1+0)*IC1][0*IC0] = Re(H[k1][N0/2]);
                C->imagp[(2*k1+1)*IC1][0*IC0] = Im(H[k1][N0/2]);
            }

            // Store regular elements:
            for (k1 = 0; k1 < N1  ; ++k1)
            for (k0 = 1; k0 < N0/2; ++k0)
            {
                C->realp[k1*IC1 + k0*IC0] = Re(H[k1][k0]);
                C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][k0]);
            }

            Many elements of H are not stored.  However, since the input is a
            real matrix, H has symmetry that makes all the unstored elements of
            H functions of the stored elements of H.  So the data stored in C
            has complete information about the transform result.

        If Direction is -1, a complex-to-real inverse transform is performed,
        producing a real output vector coerced into the complex structure:

            scale = 1. / (N1*N0);

            // Define a complex matrix, h, in multiple steps:

            // Unpack the special elements:
            h[0   ][0   ] = C->realp[0*IC1][0*IC0];
            h[0   ][N0/2] = C->imagp[0*IC1][0*IC0];
            h[N1/2][0   ] = C->realp[1*IC1][0*IC0];
            h[N1/2][N0/2] = C->imagp[1*IC1][0*IC0];

            // Unpack the two vectors from "vertical" storage:
            for (j1 = 1; j1 < N1/2; ++j1)
            {
                h[j1][0   ] = C->realp[(2*j1+0)*IC1][0*IC0]
                        + i * C->realp[(2*j1+1)*IC1][0*IC0]
                h[j1][N0/2] = C->imagp[(2*j1+0)*IC1][0*IC0]
                        + i * C->imagp[(2*j1+1)*IC1][0*IC0]
            }

            // Take regular elements:
            for (j1 = 0; j1 < N1  ; ++j1)
            for (j0 = 1; j0 < N0/2; ++j0)
            {
                h[j1][j0   ] = C->realp[j1*IC1 + j0*IC0]
                         + i * C->imagp[j1*IC1 + j0*IC0];
                h[j1][N0-j0] = conj(h[j1][j0]);
            }

            // Perform Discrete Fourier Transform.
            for (k1 = 0; k1 < N1; ++k1)
            for (k0 = 0; k0 < N0; ++k0)
                H[k1][k0] = scale * sum(sum(h[j1][j0]
                    * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                    * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

            // Store result.
            for (k1 = 0; k1 < N1  ; ++k1)
            for (k0 = 0; k0 < N0/2; ++k0)
            {
                C->realp[k1*IC1 + k0*IC0] = Re(H[k1][2*k0+0]);
                C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][2*k0+1]);
            }

        Unlike the two-dimensional complex transform, the dimensions are not
        symmetric in this real-to-complex transform.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for the greater
        of N1 or N0/2 floating-point elements.  The addresses are preferably
        16-byte aligned or better.
    */


/*  Out-of-place two-dimensional real-to-complex Discrete Fourier Transform
    routines, with and without temporary memory.
*/
extern void vDSP_fft2d_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_fft2d_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fft2d_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N0 = 1 << Log2N0;
        N1 = 1 << Log2N1;

        if (IA1 == 0) IA1 = IA0*N0/2;
        if (IC1 == 0) IC1 = IC0*N0/2;

        If Direction is +1, a real-to-complex transform is performed, taking
        input from a real vector that has been coerced into the complex
        structure:

            scale = 2;

            // Define a real matrix, h:
            for (j1 = 0; j1 < N1  ; ++j1)
            for (j0 = 0; j0 < N0/2; ++j0)
            {
                h[j1][2*j0+0] = A->realp[j1*IA1 + j0*IA0]
                          + i * A->imagp[j1*IA1 + j0*IA0];
                h[j1][2*j0+1] = A->realp[j1*IA1 + j0*IA0]
                          + i * A->imagp[j1*IA1 + j0*IA0];
            }

            // Perform Discrete Fourier Transform.
            for (k1 = 0; k1 < N1; ++k1)
            for (k0 = 0; k0 < N0; ++k0)
                H[k1][k0] = scale * sum(sum(h[j1][j0]
                    * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                    * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

            // Pack special pure-real elements into output matrix:
            C->realp[0*IC1][0*IC0] = H[0   ][0   ].
            C->imagp[0*IC1][0*IC0] = H[0   ][N0/2]
            C->realp[1*IC1][0*IC0] = H[N1/2][0   ].
            C->imagp[1*IC1][0*IC0] = H[N1/2][N0/2]

            // Pack two vectors into output matrix "vertically":
            // (This awkward format is due to a legacy implementation.)
            for (k1 = 1; k1 < N1/2; ++k1)
            {
                C->realp[(2*k1+0)*IC1][0*IC0] = Re(H[k1][0   ]);
                C->realp[(2*k1+1)*IC1][0*IC0] = Im(H[k1][0   ]);
                C->imagp[(2*k1+0)*IC1][0*IC0] = Re(H[k1][N0/2]);
                C->imagp[(2*k1+1)*IC1][0*IC0] = Im(H[k1][N0/2]);
            }

            // Store regular elements:
            for (k1 = 0; k1 < N1  ; ++k1)
            for (k0 = 1; k0 < N0/2; ++k0)
            {
                C->realp[k1*IC1 + k0*IC0] = Re(H[k1][k0]);
                C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][k0]);
            }

            Many elements of H are not stored.  However, since the input is a
            real matrix, H has symmetry that makes all the unstored elements of
            H functions of the stored elements of H.  So the data stored in C
            has complete information about the transform result.

        If Direction is -1, a complex-to-real inverse transform is performed,
        producing a real output vector coerced into the complex structure:

            scale = 1. / (N1*N0);

            // Define a complex matrix, h, in multiple steps:

            // Unpack the special elements:
            h[0   ][0   ] = A->realp[0*IA1][0*IA0];
            h[0   ][N0/2] = A->imagp[0*IA1][0*IA0];
            h[N1/2][0   ] = A->realp[1*IA1][0*IA0];
            h[N1/2][N0/2] = A->imagp[1*IA1][0*IA0];

            // Unpack the two vectors from "vertical" storage:
            for (j1 = 1; j1 < N1/2; ++j1)
            {
                h[j1][0   ] = A->realp[(2*j1+0)*IA1][0*IA0]
                        + i * A->realp[(2*j1+1)*IA1][0*IA0]
                h[j1][N0/2] = A->imagp[(2*j1+0)*IA1][0*IA0]
                        + i * A->imagp[(2*j1+1)*IA1][0*IA0]
            }

            // Take regular elements:
            for (j1 = 0; j1 < N1  ; ++j1)
            for (j0 = 1; j0 < N0/2; ++j0)
            {
                h[j1][j0   ] = A->realp[j1*IA1 + j0*IA0]
                         + i * A->imagp[j1*IA1 + j0*IA0];
                h[j1][N0-j0] = conj(h[j1][j0]);
            }

            // Perform Discrete Fourier Transform.
            for (k1 = 0; k1 < N1; ++k1)
            for (k0 = 0; k0 < N0; ++k0)
                H[k1][k0] = scale * sum(sum(h[j1][j0]
                    * e**(-Direction*2*pi*i*j0*k0/N0), 0 <= j0 < N0)
                    * e**(-Direction*2*pi*i*j1*k1/N1), 0 <= j1 < N1);

            // Store result.
            for (k1 = 0; k1 < N1  ; ++k1)
            for (k0 = 0; k0 < N0/2; ++k0)
            {
                C->realp[k1*IC1 + k0*IC0] = Re(H[k1][2*k0+0]);
                C->imagp[k1*IC1 + k0*IC0] = Im(H[k1][2*k0+1]);
            }

        Unlike the two-dimensional complex transform, the dimensions are not
        symmetric in this real-to-complex transform.

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for the greater
        of N1 or N0/2 floating-point elements.  The addresses are preferably
        16-byte aligned or better.
    */


/*  In-place multiple complex Discrete Fourier Transform routines, with and
    without temporary memory.
*/
extern void vDSP_fftm_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        scale = 0 < Direction ? 1 : 1./N;

        // Repeat M times:
        for (m = 0; m < M; ++m)
        {

            // Define a complex vector, h:
            for (j = 0; j < N; ++j)
                h[j] = C->realp[m*IM + j*IC] + i * C->imagp[m*IM + j*IC];

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale * sum(h[j]
                    * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Store result.
            for (k = 0; k < N; ++k)
            {
                C->realp[m*IM + k*IC] = Re(H[k]);
                C->imagp[m*IM + k*IC] = Im(H[k]);
            }

        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for N
        floating-point elements and is preferably 16-byte aligned or better.
    */


/*  Out-of-place multiple complex Discrete Fourier Transform routines, with and
    without temporary memory.
*/
extern void vDSP_fftm_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        scale = 0 < Direction ? 1 : 1./N;

        // Repeat M times:
        for (m = 0; m < M; ++m)
        {

            // Define a complex vector, h:
            for (j = 0; j < N; ++j)
                h[j] = A->realp[m*IMA + j*IA] + i * A->imagp[m*IMA + j*IA];

            // Perform Discrete Fourier Transform.
            for (k = 0; k < N; ++k)
                H[k] = scale * sum(h[j]
                    * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

            // Store result.
            for (k = 0; k < N; ++k)
            {
                C->realp[m*IM + k*IC] = Re(H[k]);
                C->imagp[m*IM + k*IC] = Im(H[k]);
            }

        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for N
        floating-point elements and is preferably 16-byte aligned or better.
    */


/*  In-place multiple real-to-complex Discrete Fourier Transform routines, with
    and without temporary memory.  We suggest you use the DFT routines instead
    of these.
*/
extern void vDSP_fftm_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        // Repeat M times:
        for (m = 0; m < M; ++m)
        {

            If Direction is +1, a real-to-complex transform is performed,
            taking input from a real vector that has been coerced into the
            complex structure:

                scale = 2;

                // Define a real vector, h:
                for (j = 0; j < N/2; ++j)
                {
                    h[2*j + 0] = C->realp[m*IM + j*IC];
                    h[2*j + 1] = C->imagp[m*IM + j*IC];
                }

                // Perform Discrete Fourier Transform.
                for (k = 0; k < N; ++k)
                    H[k] = scale *
                        sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

                // Pack DC and Nyquist components into initial elements.
                C->realp[m*IM + 0*IC] = Re(H[ 0 ]).
                C->imagp[m*IM + 0*IC] = Re(H[N/2]).

                // Store regular components:
                for (k = 1; k < N/2; ++k)
                {
                    C->realp[m*IM + k*IC] = Re(H[k]);
                    C->imagp[m*IM + k*IC] = Im(H[k]);
                }

                Note that, for N/2 < k < N, H[k] is not stored.  However, since
                the input is a real vector, the output has symmetry that allows
                the unstored elements to be derived from the stored elements:
                H[k] = conj(H(N-k)).  This symmetry also implies the DC and
                Nyquist components are real, so their imaginary parts are zero.

            If Direction is -1, a complex-to-real inverse transform is
            performed, producing a real output vector coerced into the complex
            structure:

                scale = 1./N;

                // Define a complex vector, h:
                h[ 0 ] = C->realp[m*IM + 0*IC];
                h[N/2] = C->imagp[m*IM + 0*IC];
                for (j = 1; j < N/2; ++j)
                {
                    h[ j ] = C->realp[m*IM + j*IC] + i * C->imagp[m*IM + j*IC];
                    h[N-j] = conj(h[j]);
                }

                // Perform Discrete Fourier Transform.
                for (k = 0; k < N; ++k)
                    H[k] = scale *
                        sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

                // Coerce real results into complex structure:
                for (k = 0; k < N/2; ++k)
                {
                    C->realp[m*IM + k*IC] = H[2*k+0];
                    C->imagp[m*IM + k*IC] = H[2*k+1];
                }

                Note that, mathematically, the symmetry in the input vector
                compels every H[k] to be real, so there are no imaginary
                components to be stored.

        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for N/2
        floating-point elements and is preferably 16-byte aligned or better.
    */



/*  Out-of-place multiple real-to-complex Discrete Fourier Transform routines,
    with and without temporary memory.  We suggest you use the DFT routines
    instead of these.
*/
extern void vDSP_fftm_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_fftm_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        N = 1 << Log2N;

        // Repeat M times:
        for (m = 0; m < M; ++m)
        {

            If Direction is +1, a real-to-complex transform is performed,
            taking input from a real vector that has been coerced into the
            complex structure:

                scale = 2;

                // Define a real vector, h:
                for (j = 0; j < N/2; ++j)
                {
                    h[2*j + 0] = A->realp[m*IMA + j*IA];
                    h[2*j + 1] = A->imagp[m*IMA + j*IA];
                }

                // Perform Discrete Fourier Transform.
                for (k = 0; k < N; ++k)
                    H[k] = scale *
                        sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

                // Pack DC and Nyquist components into initial elements.
                C->realp[m*IMC + 0*IC] = Re(H[ 0 ]).
                C->imagp[m*IMC + 0*IC] = Re(H[N/2]).

                // Store regular components:
                for (k = 1; k < N/2; ++k)
                {
                    C->realp[m*IMC + k*IC] = Re(H[k]);
                    C->imagp[m*IMC + k*IC] = Im(H[k]);
                }

                Note that, for N/2 < k < N, H[k] is not stored.  However, since
                the input is a real vector, the output has symmetry that allows
                the unstored elements to be derived from the stored elements:
                H[k] = conj(H(N-k)).  This symmetry also implies the DC and
                Nyquist components are real, so their imaginary parts are zero.

            If Direction is -1, a complex-to-real inverse transform is
            performed, producing a real output vector coerced into the complex
            structure:

                scale = 1./N;

                // Define a complex vector, h:
                h[ 0 ] = A->realp[m*IMA + 0*IA];
                h[N/2] = A->imagp[m*IMA + 0*IA];
                for (j = 1; j < N/2; ++j)
                {
                    h[ j ] = A->realp[m*IMA + j*IA]
                       + i * A->imagp[m*IMA + j*IA];
                    h[N-j] = conj(h[j]);
                }

                // Perform Discrete Fourier Transform.
                for (k = 0; k < N; ++k)
                    H[k] = scale *
                        sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

                // Coerce real results into complex structure:
                for (k = 0; k < N/2; ++k)
                {
                    C->realp[m*IMC + k*IC] = H[2*k+0];
                    C->imagp[m*IMC + k*IC] = H[2*k+1];
                }

                Note that, mathematically, the symmetry in the input vector
                compels every H[k] to be real, so there are no imaginary
                components to be stored.

        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.

        The temporary buffer versions perform the same operation but are
        permitted to use the temporary buffer for improved performance.  Each
        of Buffer->realp and Buffer->imagp must contain space for N/2
        floating-point elements and is preferably 16-byte aligned or better.
    */


/*  Non-power-of-two out-of-place complex Discrete Fourier Transform routines.
    We suggest you use the DFT routines instead of these.
*/
extern void vDSP_fft3_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED_WITH_REPLACEMENT("use DFT routines", macos(10.2, 10.11), ios(4.0, 9.0));

extern void vDSP_fft3_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED_WITH_REPLACEMENT("use DFT routines", macos(10.2, 10.11), ios(4.0, 9.0));
extern void vDSP_fft5_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED_WITH_REPLACEMENT("use DFT routines", macos(10.2, 10.11), ios(4.0, 9.0));
extern void vDSP_fft5_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED_WITH_REPLACEMENT("use DFT routines", macos(10.2, 10.11), ios(4.0, 9.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        p = 3 or 5, as shown in the routine name.

        N = p << Log2N;

        scale = 0 < Direction ? 1 : 1./N;

        // Define a complex vector, h:
        for (j = 0; j < N; ++j)
            h[j] = A->realp[j*IA] + i * A->imagp[j*IA];

        // Perform Discrete Fourier Transform.
        for (k = 0; k < N; ++k)
            H[k] = scale * sum(h[j] * e**(-Direction*2*pi*i*j*k/N), 0 <= j < N);

        // Store result.
        for (k = 0; k < N; ++k)
        {
            C->realp[k*IC] = Re(H[k]);
            C->imagp[k*IC] = Im(H[k]);
        }

        Setup must have been properly created by a call to vDSP_create_fftsetup
        (for single precision) or vDSP_create_fftsetupD (for double precision)
        and not subsequently destroyed.

        Direction must be +1 or -1.
    */


/*  Cascade biquadratic IIR filters.
*/
extern void vDSP_biquad(
    const struct vDSP_biquad_SetupStruct *__Setup,
    float       *__Delay,
    const float *__X, vDSP_Stride __IX,
    float       *__Y, vDSP_Stride __IY,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.9), ios(6.0));
extern void vDSP_biquadD(
    const struct vDSP_biquad_SetupStructD *__Setup,
    double       *__Delay,
    const double *__X, vDSP_Stride __IX,
    double       *__Y, vDSP_Stride __IY,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.9), ios(6.0));
    /*  Maps:

            For this routine, strides are shown explicitly; the default maps
            are not used.

        These compute:

        S, B0, B1, B2, A1, and A2 are determined by Setup.
        S is the number of sections.

        X provides the bulk of the input signal.  Delay provides prior state
        data for S biquadratic filters.  The filters are applied to the data in
        turn.  The output of the final filter is stored in Y, and the final
        state data of the filters are stored in Delay.

        // Initialize the first row of a matrix x with data from X:
        for (n = 0; n < N; ++n)
            x[0][n ] = X[n*IX];

        // Initialize the "past" information, elements -2 and -1, from Delay:
        for (s = 0; s <= S; ++s)
        {
            x[s][-2] = Delay[2*s+0];
            x[s][-1] = Delay[2*s+1];
        }

        // Apply each filter:
        for (s = 1; s <= S; ++s)
            for (n = 0; n < N; ++n)
                x[s][n] =
                    + B0[s] * x[s-1][n-0]
                    + B1[s] * x[s-1][n-1]
                    + B2[s] * x[s-1][n-2]
                    - A1[s] * x[s  ][n-1]
                    - A2[s] * x[s  ][n-2];

        // Save the updated state data from the end of each row:
        for (s = 0; s <= S; ++s)
        {
            Delay[2*s+0] = x[s][N-2];
            Delay[2*s+1] = x[s][N-1];
        }

        // Store the results of the final filter:
        for (n = 0; n < N; ++n)
            Y[n*IY] = x[S][n];
    */


/*  vDSP_biquadm (for float) or vDSP_biquadmD (for double) applies a
    multi-channel biquadm IIR filter created with vDSP_biquadm_CreateSetup or
    vDSP_biquadm_CreateSetupD, respectively.
 */
extern void vDSP_biquadm(
    vDSP_biquadm_Setup       __Setup,
    const float * __nonnull * __nonnull __X, vDSP_Stride __IX,
    float       * __nonnull * __nonnull __Y, vDSP_Stride __IY,
    vDSP_Length              __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_biquadmD(
     vDSP_biquadm_SetupD       __Setup,
     const double * __nonnull * __nonnull __X, vDSP_Stride __IX,
     double       * __nonnull * __nonnull __Y, vDSP_Stride __IY,
     vDSP_Length               __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  These routines perform the same function as M calls to vDSP_biquad or
        vDSP_biquadD, where M, the delay values, and the biquad setups are
        derived from the biquadm setup:

            for (m = 0; m < M; ++M)
                vDSP_biquad(
                    setup derived from vDSP_biquadm setup,
                    delays derived from vDSP_biquadm setup,
                    X[m], IX,
                    Y[m], IY,
                    N);
    */


/*  Convolution and correlation.
*/
extern void vDSP_conv(
    const float *__A,  // Input signal.
    vDSP_Stride  __IA,
    const float *__F,  // Filter.
    vDSP_Stride  __IF,
    float       *__C,  // Output signal.
    vDSP_Stride  __IC,
    vDSP_Length  __N,  // Output length.
    vDSP_Length  __P)  // Filter length.
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_convD(
    const double *__A, // Input signal.
    vDSP_Stride   __IA,
    const double *__F, // Filter
    vDSP_Stride   __IF,
    double       *__C, // Output signal.
    vDSP_Stride   __IC,
    vDSP_Length   __N, // Output length.
    vDSP_Length   __P) // Filter length.
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zconv(
    const DSPSplitComplex *__A,  // Input signal.
    vDSP_Stride            __IA,
    const DSPSplitComplex *__F,  // Filter.
    vDSP_Stride            __IF,
    const DSPSplitComplex *__C,  // Output signal.
    vDSP_Stride            __IC,
    vDSP_Length            __N,  // Output length.
    vDSP_Length            __P)  // Filter length.
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zconvD(
    const DSPDoubleSplitComplex *__A,    // Input signal.
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__F,    // Filter.
    vDSP_Stride                  __IF,
    const DSPDoubleSplitComplex *__C,    // Output signal.
    vDSP_Stride                  __IC,
    vDSP_Length                  __N,    // Output length.
    vDSP_Length                  __P)    // Filter length.
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = sum(A[n+p] * F[p], 0 <= p < P);

        Commonly, this is called correlation if IF is positive and convolution
        if IF is negative.
    */


/*  3*3 and 5*5 convolutions.
*/
extern void vDSP_f3x3(
    const float *__A,
    vDSP_Length  __NR,
    vDSP_Length  __NC,
    const float *__F,
    float       *__C)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_f3x3D(
    const double *__A,
    vDSP_Length   __NR,
    vDSP_Length   __NC,
    const double *__F,
    double       *__C)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_f5x5(
    const float *__A,
    vDSP_Length  __NR,
    vDSP_Length  __NC,
    const float *__F,
    float       *__C)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_f5x5D(
    const double *__A,
    vDSP_Length   __NR,
    vDSP_Length   __NC,
    const double *__F,
    double       *__C)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            This routine does not have strides.

            A and C are regarded as two-dimensional matrices with dimensions
            [NR][NC].  F is regarded as a two-dimensional matrix with
            dimensions [P][P]:

            Pseudocode:     Memory:
            A[j][k]         A[j*NC + k]
            C[j][k]         C[j*NC + k]
            F[j][k]         F[j*P  + k]

        These compute:

            P = 3 or 5, according to the routine name.

            Below, "P/2" is evaluated using integer arithmetic, so it is 1 or 2
            (not 1.5 or 2.5).

            for (r = P/2; r < NR-P/2; ++r)
            for (c = P/2; c < NC-P/2; ++c)
                C[r][c] = sum(A[r+j][c+k] * F[j+P/2][k+P/2],
                    -P/2 <= j < P/2, -P/2 <= k < P/2);

            All other elements of C (a border of P/2 elements around all four
            sides) are set to zero.
    */


/*  Two-dimensional (image) convolution.
*/
extern void vDSP_imgfir(
    const float *__A,  // Input.
    vDSP_Length  __NR, // Number of image rows.
    vDSP_Length  __NC, // Number of image columns.
    const float *__F,  // Filter.
    float       *__C,  // Output.
    vDSP_Length  __P,  // Number of filter rows.
    vDSP_Length  __Q)  // Number of filter columns.
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_imgfirD(
    const double *__A,  // Input.
    vDSP_Length   __NR, // Number of image rows.
    vDSP_Length   __NC, // Number of image columns.
    const double *__F,  // Filter.
    double       *__C,  // Output.
    vDSP_Length   __P,  // Number of filter rows.
    vDSP_Length   __Q)  // Number of filter columns.
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            This routine does not have strides.

            A and C are regarded as two-dimensional matrices with dimensions
            [NR][NC].  F is regarded as a two-dimensional matrix with
            dimensions [P][Q].

            A and C are regarded as two-dimensional matrices with dimensions
            [NR][NC].  F is regarded as a two-dimensional matrix with
            dimensions [P][P]:

            Pseudocode:     Memory:
            A[j][k]         A[j*NC + k]
            C[j][k]         C[j*NC + k]
            F[j][k]         F[j*Q  + k]

        These compute:

            P and Q must be odd.  "P/2" and "Q/2" are evaluated with integer
            arithmetic, so, if P is 3, P/2 is 1, not 1.5.

            for (r = P/2; r < NR-P/2; ++r)
            for (c = Q/2; c < NC-Q/2; ++c)
                C[r][c] = sum(A[r+j][c+k] * F[j+P/2][k+Q/2],
                    -P/2 <= j < P/2, -Q/2 <= k < Q/2);

            All other elements of C (borders of P/2 elements at the top and
            bottom and Q/2 elements at the left and right) are set to zero.
    */


extern void vDSP_mtrans(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __M,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_mtransD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __M,
    vDSP_Length   __N)
            API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            A is regarded as a two-dimensional matrix with dimemnsions
            [N][M] and stride IA.  C is regarded as a two-dimensional matrix
            with dimemnsions [M][N] and stride IC:

            Pseudocode:     Memory:
            A[n][m]         A[(n*M + m)*IA]
            C[m][n]         C[(m*N + n)*IC]

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                C[m][n] = A[n][m];
    */


/*  Matrix multiply.
*/
extern void vDSP_mmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __M,
    vDSP_Length  __N,
    vDSP_Length  __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_mmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __M,
    vDSP_Length   __N,
    vDSP_Length   __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            A is regarded as a two-dimensional matrix with dimemnsions [M][P]
            and stride IA.  B is regarded as a two-dimensional matrix with
            dimemnsions [P][N] and stride IB.  C is regarded as a
            two-dimensional matrix with dimemnsions [M][N] and stride IC.

            Pseudocode:     Memory:
            A[m][p]         A[(m*P+p)*IA]
            B[p][n]         B[(p*N+n)*IB]
            C[m][n]         C[(m*N+n)*IC]

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                C[m][n] = sum(A[m][p] * B[p][n], 0 <= p < P);
    */


/*  Split-complex matrix multiply and add.
*/
extern void vDSP_zmma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zmmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            Pseudocode:     Memory:
            A[m][p]         A->realp[(m*P+p)*IA] + i * A->imagp[(m*P+p)*IA].
            B[p][n]         B->realp[(p*N+n)*IB] + i * B->imagp[(p*N+n)*IB].
            C[m][n]         C->realp[(m*N+n)*IC] + i * C->imagp[(m*N+n)*IC].
            D[m][n]         D->realp[(m*N+n)*ID] + i * D->imagp[(m*N+n)*ID].

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                D[m][n] = sum(A[m][p] * B[p][n], 0 <= p < P) + C[m][n];
    */


/*  Split-complex matrix multiply and subtract.
*/
extern void vDSP_zmms(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zmmsD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            Pseudocode:     Memory:
            A[m][p]         A->realp[(m*P+p)*IA] + i * A->imagp[(m*P+p)*IA].
            B[p][n]         B->realp[(p*N+n)*IB] + i * B->imagp[(p*N+n)*IB].
            C[m][n]         C->realp[(m*N+n)*IC] + i * C->imagp[(m*N+n)*IC].
            D[m][n]         D->realp[(m*N+n)*ID] + i * D->imagp[(m*N+n)*ID].

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                D[m][n] = sum(A[m][p] * B[p][n], 0 <= p < P) - C[m][n];
    */


// Vector multiply, multiply, add, and add.
extern void vDSP_zvmmaa(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    const DSPSplitComplex *__E,
    vDSP_Stride            __IE,
    const DSPSplitComplex *__F,
    vDSP_Stride            __IF,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_zvmmaaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    const DSPDoubleSplitComplex *__E,
    vDSP_Stride                  __IE,
    const DSPDoubleSplitComplex *__F,
    vDSP_Stride                  __IF,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                F[n] = A[n] * B[n] + C[n] * D[n] + E[n];
    */


/*  Split-complex matrix multiply and reverse subtract.
*/
extern void vDSP_zmsm(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zmsmD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            Pseudocode:     Memory:
            A[m][p]         A->realp[(m*P+p)*IA] + i * A->imagp[(m*P+p)*IA].
            B[p][n]         B->realp[(p*N+n)*IB] + i * B->imagp[(p*N+n)*IB].
            C[m][n]         C->realp[(m*N+n)*IC] + i * C->imagp[(m*N+n)*IC].
            D[m][n]         D->realp[(m*N+n)*ID] + i * D->imagp[(m*N+n)*ID].

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                D[m][n] = C[m][n] - sum(A[m][p] * B[p][n], 0 <= p < P);
    */


/*  Split-complex matrix multiply.
*/
extern void vDSP_zmmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zmmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:

            Pseudocode:     Memory:
            A[m][p]         A->realp[(m*P+p)*IA] + i * A->imagp[(m*P+p)*IA].
            B[p][n]         B->realp[(p*N+n)*IB] + i * B->imagp[(p*N+n)*IB].
            C[m][n]         C->realp[(m*N+n)*IC] + i * C->imagp[(m*N+n)*IC].

        These compute:

            for (m = 0; m < M; ++m)
            for (n = 0; n < N; ++n)
                C[m][n] = sum(A[m][p] * B[p][n], 0 <= p < P);
    */


// Vector add.
extern void vDSP_vadd(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vaddD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_vaddi(
    const int   *__A,
    vDSP_Stride  __IA,
    const int   *__B,
    vDSP_Stride  __IB,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_zvadd(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zvaddD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zrvadd(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zrvaddD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] + B[n];
    */


// Vector subtract.
extern void vDSP_vsub(
    const float *__B,  // Caution:  A and B are swapped!
    vDSP_Stride  __IB,
    const float *__A,  // Caution:  A and B are swapped!
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vsubD(
    const double *__B, // Caution:  A and B are swapped!
    vDSP_Stride   __IB,
    const double *__A, // Caution:  A and B are swapped!
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zvsub(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zvsubD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] - B[n];
    */


// Vector multiply.
extern void vDSP_vmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zrvmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zrvmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] * B[n];
    */


// Vector divide.
extern void vDSP_vdiv(
    const float *__B,  // Caution:  A and B are swapped!
    vDSP_Stride  __IB,
    const float *__A,  // Caution:  A and B are swapped!
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vdivD(
    const double *__B, // Caution:  A and B are swapped!
    vDSP_Stride   __IB,
    const double *__A, // Caution:  A and B are swapped!
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vdivi(
    const int   *__B,  // Caution:  A and B are swapped!
    vDSP_Stride  __IB,
    const int   *__A,  // Caution:  A and B are swapped!
    vDSP_Stride  __IA,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvdiv(
    const DSPSplitComplex *__B,    // Caution:  A and B are swapped!
    vDSP_Stride            __IB,
    const DSPSplitComplex *__A,    // Caution:  A and B are swapped!
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvdivD(
    const DSPDoubleSplitComplex *__B,  // Caution:  A and B are swapped!
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__A,  // Caution:  A and B are swapped!
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zrvdiv(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zrvdivD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] / B[n];
    */


// Vector-scalar multiply.
extern void vDSP_vsmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vsmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] * B[0];
    */


// Vector square.
extern void vDSP_vsq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vsqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n]**2;
    */



// Vector signed square.
extern void vDSP_vssq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vssqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] * |A[n]|;
    */


// Euclidean distance, squared.
extern void vDSP_distancesq(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.8), ios(5.0));
extern void vDSP_distancesqD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum((A[n] - B[n]) ** 2, 0 <= n < N);
    */


// Dot product.
extern void vDSP_dotpr(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_dotprD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zdotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zdotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
extern void vDSP_zrdotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zrdotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n] * B[n], 0 <= n < N);
    */


// Vector add and multiply.
extern void vDSP_vam(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_vamD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __IDD,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = (A[n] + B[n]) * C[n];
    */


// Vector multiply and add.
extern void vDSP_vma(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_zvmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n] * B[n] + C[n];
    */


// Complex multiplication with optional conjugation.
extern void vDSP_zvmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N,
    int                    __Conjugate)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zvmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N,
    int                          __Conjugate)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            If Conjugate is +1:

                for (n = 0; n < N; ++n)
                    C[n] = A[n] * B[n];

            If Conjugate is -1:

                for (n = 0; n < N; ++n)
                    C[n] = conj(A[n]) * B[n];
    */


// Complex-split inner (conjugate) dot product.
extern void vDSP_zidotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zidotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(conj(A[n]) * B[n], 0 <= n < N);
    */


// Complex-split conjugate multiply and add.
extern void vDSP_zvcma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zvcmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = conj(A[n]) * B[n] + C[n];
    */


// Subtract real from complex-split.
extern void vDSP_zrvsub(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.0), ios(4.0));
extern void vDSP_zrvsubD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.2), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] - B[n];
    */


// Vector convert between double precision and single precision.
extern void vDSP_vdpsp(
    const double *__A,
    vDSP_Stride   __IA,
    float        *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vspdp(
    const float *__A,
    vDSP_Stride  __IA,
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n];
    */


// Vector absolute value.
extern void vDSP_vabs(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vabsD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vabsi(
    const int   *__A,
    vDSP_Stride  __IA,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvabs(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    float                 *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvabsD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    double                      *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = |A[n]|;
    */


// Vector bit-wise equivalence, NOT (A XOR B).
extern void vDSP_veqvi(
    const int   *__A,
    vDSP_Stride  __IA,
    const int   *__B,
    vDSP_Stride  __IB,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = ~(A[n] ^ B[n]);
    */


// Vector fill.
extern void vDSP_vfill(
    const float *__A,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfillD(
    const double *__A,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfilli(
    const int   *__A,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvfill(
    const DSPSplitComplex *__A,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvfillD(
    const DSPDoubleSplitComplex *__A,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[0];
    */


// Vector-scalar add.
extern void vDSP_vsadd(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsaddD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsaddi(
    const int   *__A,
    vDSP_Stride  __IA,
    const int   *__B,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] + B[0];
    */


// Vector-scalar divide.
extern void vDSP_vsdiv(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsdivD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsdivi(
    const int   *__A,
    vDSP_Stride  __IA,
    const int   *__B,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] / B[0];
    */


// Complex-split accumulating autospectrum.
extern void vDSP_zaspec(
    const DSPSplitComplex *__A,
    float                 *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zaspecD(
    const DSPDoubleSplitComplex *__A,
    double                      *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; arrays map directly to memory.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] += |A[n]| ** 2;
    */


// Create Blackman window.
extern void vDSP_blkman_window(
    float       *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_blkman_windowD(
    double      *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; the array maps directly to memory.

        These compute:

            If Flag & vDSP_HALF_WINDOW:
                Length = (N+1)/2;
            Else
                Length = N;

            for (n = 0; n < Length; ++n)
            {
                angle = 2*pi*n/N;
                C[n] = .42 - .5 * cos(angle) + .08 * cos(2*angle);
            }
    */


// Coherence function.
extern void vDSP_zcoher(
    const float           *__A,
    const float           *__B,
    const DSPSplitComplex *__C,
    float                 *__D,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zcoherD(
    const double                *__A,
    const double                *__B,
    const DSPDoubleSplitComplex *__C,
    double                      *__D,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; arrays map directly to memory.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = |C[n]| ** 2 / (A[n] * B[n]);
    */


// Anti-aliasing down-sample with real filter.
extern void vDSP_desamp(
    const float *__A,   // Input signal.
    vDSP_Stride  __DF,  // Decimation Factor.
    const float *__F,   // Filter.
    float       *__C,   // Output.
    vDSP_Length  __N,   // Output length.
    vDSP_Length  __P)   // Filter length.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_desampD(
    const double *__A,  // Input signal.
    vDSP_Stride   __DF, // Decimation Factor.
    const double *__F,  // Filter.
    double       *__C,  // Output.
    vDSP_Length   __N,  // Output length.
    vDSP_Length   __P)  // Filter length.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zrdesamp(
    const DSPSplitComplex *__A,  // Input signal.
    vDSP_Stride            __DF, // Decimation Factor.
    const float           *__F,  // Filter.
    const DSPSplitComplex *__C,  // Output.
    vDSP_Length            __N,  // Output length.
    vDSP_Length            __P)  // Filter length.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zrdesampD(
    const DSPDoubleSplitComplex *__A,    // Input signal.
    vDSP_Stride                  __DF,   // Decimation Factor.
    const double                *__F,    // Filter.
    const DSPDoubleSplitComplex *__C,    // Output.
    vDSP_Length                  __N,    // Output length.
    vDSP_Length                  __P)    // Filter length.
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; arrays map directly to memory.  DF specifies
            the decimation factor.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = sum(A[n*DF+p] * F[p], 0 <= p < P);
    */


// Transfer function, B/A.
extern void vDSP_ztrans(
    const float           *__A,
    const DSPSplitComplex *__B,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_ztransD(
    const double                *__A,
    const DSPDoubleSplitComplex *__B,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; arrays map directly to memory.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = B[n] / A[n];
    */


// Accumulating cross-spectrum.
extern void vDSP_zcspec(
    const DSPSplitComplex *__A,
    const DSPSplitComplex *__B,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zcspecD(
    const DSPDoubleSplitComplex *__A,
    const DSPDoubleSplitComplex *__B,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; arrays map directly to memory.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] += conj(A[n]) * B[n];
    */


// Vector conjugate and multiply.
extern void vDSP_zvcmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvcmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __iC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = conj(A[n]) * B[n];
    */


// Vector conjugate.
extern void vDSP_zvconj(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvconjD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = conj(A[n]);
    */


// Vector multiply with scalar.
extern void vDSP_zvzsml(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvzsmlD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] * B[0];
    */


// Vector magnitudes squared.
extern void vDSP_zvmags(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    float                 *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvmagsD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    double                      *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = |A[n]| ** 2;
    */


// Vector magnitudes square and add.
extern void vDSP_zvmgsa(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    float                 *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvmgsaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    double                      *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = |A[n]| ** 2 + B[n];
    */


// Complex-split vector move.
extern void vDSP_zvmov(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvmovD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n];
    */


// Vector negate.
extern void vDSP_zvneg(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvnegD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = -A[n];
    */


// Vector phasea.
extern void vDSP_zvphas(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    float                 *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvphasD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    double                      *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = atan2(Im(A[n]), Re(A[n]));
    */


// Vector multiply by scalar and add.
extern void vDSP_zvsma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_zvsmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n] * B[0] + C[n];
    */


// Difference equation, 2 poles, 2 zeros.
extern void vDSP_deq22(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_deq22D(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 2; n < N+2; ++n)   // Note outputs start with C[2].
                C[n] =
                    + A[n-0]*B[0]
                    + A[n-1]*B[1]
                    + A[n-2]*B[2]
                    - C[n-1]*B[3]
                    - C[n-2]*B[4];
    */


// Create Hamming window.
extern void vDSP_hamm_window(
    float       *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_hamm_windowD(
    double      *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; the array maps directly to memory.

        These compute:

            If Flag & vDSP_HALF_WINDOW:
                Length = (N+1)/2;
            Else
                Length = N;

            for (n = 0; n < Length; ++n)
                C[n] = .54 - .46 * cos(2*pi*n/N);
    */


// Create Hanning window.
extern void vDSP_hann_window(
    float       *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_hann_windowD(
    double      *__C,
    vDSP_Length  __N,
    int          __Flag)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            No strides are used; the array maps directly to memory.

        These compute:

            If Flag & vDSP_HALF_WINDOW:
                Length = (N+1)/2;
            Else
                Length = N;

            If Flag & vDSP_HANN_NORM:
                W = .8165;
            Else
                W = .5;

            for (n = 0; n < Length; ++n)
                C[n] = W * (1 - cos(2*pi*n/N));
    */


// Maximum magnitude of vector.
extern void vDSP_maxmgv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_maxmgvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the greatest value of |A[n]| for 0 <= n < N.
    */


// Maximum magnitude of vector.
extern void vDSP_maxmgvi(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length *__I,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_maxmgviD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length  *__I,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the greatest value of |A[n]| for 0 <= n < N.
        I[0] is set to the least i*IA such that |A[i]| has the value in C[0].
    */


// Maximum value of vector.
extern void vDSP_maxv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_maxvD(
    const double *__A,
    vDSP_Stride   __I,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the greatest value of A[n] for 0 <= n < N.
    */


// Maximum value of vector, with index.
extern void vDSP_maxvi(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length *__I,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_maxviD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length  *__I,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the greatest value of A[n] for 0 <= n < N.
        I[0] is set to the least i*IA such that A[i] has the value in C[0].
    */


// Mean magnitude of vector.
extern void vDSP_meamgv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_meamgvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(|A[n]|, 0 <= n < N) / N;
    */


// Mean of vector.
extern void vDSP_meanv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_meanvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n], 0 <= n < N) / N;
    */


// Mean square of vector.
extern void vDSP_measqv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_measqvD(
    const double *__A,
    vDSP_Stride   __I,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n]**2, 0 <= n < N) / N;
    */


// Minimum magnitude of vector.
extern void vDSP_minmgv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_minmgvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the least value of |A[n]| for 0 <= n < N.
    */


// Minimum magnitude of vector, with index.
extern void vDSP_minmgvi(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length *__I,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_minmgviD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length  *__I,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the least value of |A[n]| for 0 <= n < N.
        I[0] is set to the least i*IA such that |A[i]| has the value in C[0].
    */


// Minimum value of vector.
extern void vDSP_minv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_minvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the least value of A[n] for 0 <= n < N.
    */


// Minimum value of vector, with index.
extern void vDSP_minvi(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length *__I,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_minviD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length  *__I,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        C[0] is set to the least value of A[n] for 0 <= n < N.
        I[0] is set to the least i*IA such that A[i] has the value in C[0].
    */


// Matrix move.
extern void vDSP_mmov(
    const float *__A,
    float       *__C,
    vDSP_Length  __M,
    vDSP_Length  __N,
    vDSP_Length  __TA,
    vDSP_Length  __TC)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_mmovD(
    const double *__A,
    double       *__C,
    vDSP_Length   __M,
    vDSP_Length   __N,
    vDSP_Length   __TA,
    vDSP_Length   __TC)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:

            This routine does not have strides.

            A is regarded as a two-dimensional matrix with dimensions [N][TA].
            C is regarded as a two-dimensional matrix with dimensions [N][TC].

        These compute:

            for (n = 0; n < N; ++n)
            for (m = 0; m < M; ++m)
                C[n][m] = A[n][m];
    */


// Mean of signed squares of vector.
extern void vDSP_mvessq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_mvessqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n] * |A[n]|, 0 <= n < N) / N;
    */


// Find zero crossing.
extern void vDSP_nzcros(
    const float *__A,
    vDSP_Stride  __IA,
    vDSP_Length  __B,
    vDSP_Length *__C,
    vDSP_Length *__D,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_nzcrosD(
    const double *__A,
    vDSP_Stride   __IA,
    vDSP_Length   __B,
    vDSP_Length  *__C,
    vDSP_Length  *__D,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        Let S be the number of times the sign bit changes in the sequence A[0],
        A[1],... A[N-1].

        If B <= S:
            D[0] is set to B.
            C[0] is set to n*IA, where the B-th sign bit change occurs between
            elements A[n-1] and A[n].
        Else:
            D[0] is set to S.
            C[0] is set to 0.
    */


// Convert rectangular to polar.
extern void vDSP_polar(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_polarD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  Strides are shown explicitly in pseudocode.

        These compute:

            for (n = 0; n < N; ++n)
            {
                x = A[n*IA+0];
                y = A[n*IA+1];
                C[n*IC+0] = sqrt(x**2 + y**2);
                C[n*IC+1] = atan2(y, x);
            }
    */


// Convert polar to rectangular.
extern void vDSP_rect(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_rectD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  Strides are shown explicitly in pseudocode.

        These compute:

            for (n = 0; n < N; ++n)
            {
                r     = A[n*IA+0];
                theta = A[n*IA+1];
                C[n*IC+0] = r * cos(theta);
                C[n*IC+1] = r * sin(theta);
            }
    */


// Root-mean-square of vector.
extern void vDSP_rmsqv(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_rmsqvD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sqrt(sum(A[n] ** 2, 0 <= n < N) / N);
    */


// Scalar-vector divide.
extern void vDSP_svdiv(
    const float *__A,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_svdivD(
    const double *__A,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[0] / B[n];

        When A[0] is not zero or NaN and B[n] is zero, C[n] is set to an
        infinity.
    */


// Sum of vector elements.
extern void vDSP_sve(
    const float *__A,
    vDSP_Stride  __I,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_sveD(
    const double *__A,
    vDSP_Stride   __I,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n], 0 <= n < N);
    */


// Sum of vector elements magnitudes.
extern void vDSP_svemg(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_svemgD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(|A[n]|, 0 <= n < N);
    */


// Sum of vector elements' squares.
extern void vDSP_svesq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_svesqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n] ** 2, 0 <= n < N);
    */


// Sum of vector elements and sum of vector elements' squares.
extern void vDSP_sve_svesq(
    const float  *__A,
    vDSP_Stride   __IA,
    float        *__Sum,
    float        *__SumOfSquares,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.8), ios(6.0));
extern void vDSP_sve_svesqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__Sum,
    double       *__SumOfSquares,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.8), ios(6.0));
    /*  Maps:  The default maps are used.

        These compute:

            Sum[0]          = sum(A[n],      0 <= n < N);
            SumOfSquares[0] = sum(A[n] ** 2, 0 <= n < N);
    */


/*  Compute mean and standard deviation and then calculate new elements to have
    a zero mean and a unit standard deviation.

    For iOS 9.0 and later or OS X 10.11 and later, the production of new
    elements may be omitted by passing NULL for C.
*/
#if (defined __IPHONE_OS_VERSION_MIN_REQUIRED && __IPHONE_OS_VERSION_MIN_REQUIRED < 90000) || \
     (defined __MAC_OS_X_VERSION_MIN_REQUIRED && __MAC_OS_X_VERSION_MIN_REQUIRED < 101100)
    extern void vDSP_normalize(
        const float  *__A,
        vDSP_Stride   __IA,
        float        *__C,
        vDSP_Stride   __IC,
        float        *__Mean,
        float        *__StandardDeviation,
        vDSP_Length   __N)
            API_AVAILABLE(macos(10.8), ios(6.0));
    extern void vDSP_normalizeD(
        const double *__A,
        vDSP_Stride   __IA,
        double       *__C,
        vDSP_Stride   __IC,
        double       *__Mean,
        double       *__StandardDeviation,
        vDSP_Length   __N)
            API_AVAILABLE(macos(10.8), ios(6.0));
#else
    extern void vDSP_normalize(
        const float       *__A,
        vDSP_Stride        __IA,
        float * __nullable __C,
        vDSP_Stride        __IC,
        float             *__Mean,
        float             *__StandardDeviation,
        vDSP_Length        __N)
            API_AVAILABLE(macos(10.8), ios(6.0));
    extern void vDSP_normalizeD(
        const double       *__A,
        vDSP_Stride         __IA,
        double * __nullable __C,
        vDSP_Stride         __IC,
        double             *__Mean,
        double             *__StandardDeviation,
        vDSP_Length         __N)
            API_AVAILABLE(macos(10.8), ios(6.0));
#endif
    /*  Maps:  The default maps are used.

        These compute:

            // Calculate mean and standard deviation.
            m = sum(A[n], 0 <= n < N) / N;
            d = sqrt(sum(A[n]**2, 0 <= n < N) / N - m**2);

            if (C)
            {
                // Normalize.
                for (n = 0; n < N; ++n)
                    C[n] = (A[n] - m) / d;
            }
    */


// Sum of vector elements' signed squares.
extern void vDSP_svs(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_svsD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            C[0] = sum(A[n] * |A[n]|, 0 <= n < N);
    */


// Vector add, add, and multiply.
extern void vDSP_vaam(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vaamD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                E[n] = (A[n] + B[n]) * (C[n] + D[n]);
    */


// Vector add, subtract, and multiply.
extern void vDSP_vasbm(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vasbmD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                E[n] = (A[n] + B[n]) * (C[n] - D[n]);
    */


// Vector add and scalar multiply.
extern void vDSP_vasm(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vasmD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = (A[n] + B[n]) * C[0];
    */


// Vector linear average.
extern void vDSP_vavlin(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vavlinD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = (C[n]*B[0] + A[n]) / (B[0] + 1);
    */


// Vector clip.
extern void vDSP_vclip(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vclipD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
            {
                D[n] = A[n];
                if (D[n] < B[0]) D[n] = B[0];
                if (C[0] < D[n]) D[n] = C[0];
            }
    */


// Vector clip and count.
extern void vDSP_vclipc(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N,
    vDSP_Length *__NLow,
    vDSP_Length *__NHigh)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vclipcD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N,
    vDSP_Length  *__NLow,
    vDSP_Length  *__NHigh)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            NLow[0]  = 0;
            NHigh[0] = 0;
            for (n = 0; n < N; ++n)
            {
                D[n] = A[n];
                if (D[n] < B[0]) { D[n] = B[0]; ++NLow[0];  }
                if (C[0] < D[n]) { D[n] = C[0]; ++NHigh[0]; }
            }
    */


// Vector clear.
extern void vDSP_vclr(
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vclrD(
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = 0;
    */


// Vector compress.
extern void vDSP_vcmprs(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vcmprsD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            p = 0;
            for (n = 0; n < N; ++n)
                if (B[n] != 0)
                    C[p++] = A[n];
    */


// Vector convert to decibels, power, or amplitude.
extern void vDSP_vdbcon(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    unsigned int __F)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vdbconD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    unsigned int  __F)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            If Flag is 1:
                alpha = 20;
            If Flag is 0:
                alpha = 10;

            for (n = 0; n < N; ++n)
                C[n] = alpha * log10(A[n] / B[0]);
    */


// Vector distance.
extern void vDSP_vdist(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vdistD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = sqrt(A[n]**2 + B[n]**2);
    */


// Vector envelope.
extern void vDSP_venvlp(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_venvlpD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
            {
                if (D[n] < B[n] || A[n] < D[n]) D[n] = C[n];
                else D[n] = 0;
            }
    */


// Vector convert to integer, round toward zero.
extern void vDSP_vfix8(
    const float *__A,
    vDSP_Stride  __IA,
    char        *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfix8D(
    const double *__A,
    vDSP_Stride   __IA,
    char         *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfix16(
    const float *__A,
    vDSP_Stride  __IA,
    short       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfix16D(
    const double *__A,
    vDSP_Stride   __IA,
    short        *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfix32(
    const float *__A,
    vDSP_Stride  __IA,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfix32D(
    const double *__A,
    vDSP_Stride   __IA,
    int          *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu8(
    const float   *__A,
    vDSP_Stride    __IA,
    unsigned char *__C,
    vDSP_Stride    __IC,
    vDSP_Length    __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu8D(
    const double  *__A,
    vDSP_Stride    __IA,
    unsigned char *__C,
    vDSP_Stride    __IC,
    vDSP_Length    __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu16(
    const float    *__A,
    vDSP_Stride     __IA,
    unsigned short *__C,
    vDSP_Stride     __IC,
    vDSP_Length     __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu16D(
    const double   *__A,
    vDSP_Stride     __IA,
    unsigned short *__C,
    vDSP_Stride     __IC,
    vDSP_Length     __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu32(
    const float  *__A,
    vDSP_Stride   __IA,
    unsigned int *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixu32D(
    const double *__A,
    vDSP_Stride   __IA,
    unsigned int *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = trunc(A[n]);
    */


/*  Vector convert single precision to 24-bit integer with pre-scaling.
    The scaled value is rounded toward zero.
*/
extern void vDSP_vsmfixu24(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_uint24 *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
    
/*  Vector convert single precision to 24-bit unsigned integer with pre-scaling.
    The scaled value is rounded toward zero.
*/
extern void vDSP_vsmfix24(
   const float *__A,
   vDSP_Stride  __IA,
   const float *__B,
   vDSP_int24  *__C,
   vDSP_Stride  __IC,
   vDSP_Length  __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
    /*  Maps:  The default maps are used.
    
        These compute:
    
            for (n = 0; n < N; ++n)
                C[n] = trunc(A[n] * B[0]);
    
        Note: Values outside the representable range are clamped to the largest
        or smallest representable values of the destination type.
    */

    
// Vector convert 24-bit integer to single-precision float.
extern void vDSP_vfltu24(
   const vDSP_uint24 *__A,
   vDSP_Stride        __IA,
   float             *__C,
   vDSP_Stride        __IC,
   vDSP_Length        __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_vflt24(
  const vDSP_int24 *__A,
  vDSP_Stride       __IA,
  float            *__C,
  vDSP_Stride       __IC,
  vDSP_Length       __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
    /*  Maps:  The default maps are used.
    
        These compute:
    
            for (n = 0; n < N; ++n)
                C[n] = A[n];
    */

    
// Vector convert 24-bit integer to single-precision float and scale.
extern void vDSP_vfltsmu24(
     const vDSP_uint24 *__A,
     vDSP_Stride        __IA,
     const float       *__B,
     float             *__C,
     vDSP_Stride        __IC,
     vDSP_Length        __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
extern void vDSP_vfltsm24(
    const vDSP_int24 *__A,
    vDSP_Stride       __IA,
    const float      *__B,
    float            *__C,
    vDSP_Stride       __IC,
    vDSP_Length       __N)
        API_AVAILABLE(macos(10.9), ios(7.0));
    /*  Maps:  The default maps are used.
    
        These compute:
    
            for (n = 0; n < N; ++n)
                C[n] = B[0] * (float)A[n];
    */
        

// Vector convert to integer, round to nearest.
extern void vDSP_vfixr8(
    const float *__A,
    vDSP_Stride  __IA,
    char        *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixr8D(
    const double *__A,
    vDSP_Stride   __IA,
    char         *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixr16(
    const float *__A,
    vDSP_Stride  __IA,
    short       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixr16D(
    const double *__A,
    vDSP_Stride   __IA,
    short        *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixr32(
    const float *__A,
    vDSP_Stride  __IA,
    int         *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixr32D(
    const double *__A,
    vDSP_Stride   __IA,
    int          *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru8(
    const float   *__A,
    vDSP_Stride    __IA,
    unsigned char *__C,
    vDSP_Stride    __IC,
    vDSP_Length    __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru8D(
    const double  *__A,
    vDSP_Stride    __IA,
    unsigned char *__C,
    vDSP_Stride    __IC,
    vDSP_Length    __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru16(
    const float    *__A,
    vDSP_Stride     __IA,
    unsigned short *__C,
    vDSP_Stride     __IC,
    vDSP_Length     __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru16D(
    const double   *__A,
    vDSP_Stride     __IA,
    unsigned short *__C,
    vDSP_Stride     __IC,
    vDSP_Length     __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru32(
    const float  *__A,
    vDSP_Stride   __IA,
    unsigned int *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfixru32D(
    const double *__A,
    vDSP_Stride   __IA,
    unsigned int *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = rint(A[n]);

        Note:  It is expected that the global rounding mode be the default,
        round-to-nearest.  It is unspecified whether ties round up or down.
    */


// Vector convert to floating-point from integer.
extern void vDSP_vflt8(
    const char  *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vflt8D(
    const char  *__A,
    vDSP_Stride  __IA,
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vflt16(
    const short *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vflt16D(
    const short *__A,
    vDSP_Stride  __IA,
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vflt32(
    const int   *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vflt32D(
    const int   *__A,
    vDSP_Stride  __IA,
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu8(
    const unsigned char *__A,
    vDSP_Stride          __IA,
    float               *__C,
    vDSP_Stride          __IC,
    vDSP_Length          __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu8D(
    const unsigned char *__A,
    vDSP_Stride          __IA,
    double              *__C,
    vDSP_Stride          __IC,
    vDSP_Length          __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu16(
    const unsigned short *__A,
    vDSP_Stride           __IA,
    float                *__C,
    vDSP_Stride           __IC,
    vDSP_Length           __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu16D(
    const unsigned short *__A,
    vDSP_Stride           __IA,
    double               *__C,
    vDSP_Stride           __IC,
    vDSP_Length           __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu32(
    const unsigned int *__A,
    vDSP_Stride         __IA,
    float              *__C,
    vDSP_Stride         __IC,
    vDSP_Length         __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfltu32D(
    const unsigned int *__A,
    vDSP_Stride         __IA,
    double             *__C,
    vDSP_Stride         __IC,
    vDSP_Length         __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n];
    */


// Vector fraction part (subtract integer toward zero).
extern void vDSP_vfrac(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vfracD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] - trunc(A[n]);
    */


// Vector gather.
extern void vDSP_vgathr(
    const float       *__A,
    const vDSP_Length *__B,
    vDSP_Stride        __IB,
    float             *__C,
    vDSP_Stride        __IC,
    vDSP_Length        __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vgathrD(
    const double      *__A,
    const vDSP_Length *__B,
    vDSP_Stride        __IB,
    double            *__C,
    vDSP_Stride        __IC,
    vDSP_Length        __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.  Note that A has unit stride.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[B[n] - 1];
    */


// Vector gather, absolute pointers.
extern void vDSP_vgathra(
    const float * __nonnull * __nonnull __A,
    vDSP_Stride                         __IA,
    float                              *__C,
    vDSP_Stride                         __IC,
    vDSP_Length                         __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vgathraD(
    const double * __nonnull * __nonnull __A,
    vDSP_Stride                          __IA,
    double                              *__C,
    vDSP_Stride                          __IC,
    vDSP_Length                          __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = *A[n];
    */


// Vector generate tapered ramp.
extern void vDSP_vgen(
    const float *__A,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vgenD(
    const double *__A,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[0] + (B[0] - A[0]) * n/(N-1);
    */


// Vector generate by extrapolation and interpolation.
extern void vDSP_vgenp(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __M)  // Length of A and of B.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vgenpD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    vDSP_Length   __M)  // Length of A and of B.
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                If n <= B[0],  then C[n] = A[0].
                If B[M-1] < n, then C[n] = A[M-1].
                Otherwise:
                    Let m be such that B[m] < n <= B[m+1].
                    C[n] = A[m] + (A[m+1]-A[m]) * (n-B[m]) / (B[m+1]-B[m]).

         The elements of B are expected to be in increasing order.
    */


// Vector inverted clip.
extern void vDSP_viclip(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_viclipD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
            {
                if (A[n] <= B[0] || C[0] <= A[n])
                    D[n] = A[n];
                else
                    if (A[n] < 0)
                        D[n] = B[0];
                    else
                        D[n] = C[0];
            }

        It is expected that B[0] <= 0 <= C[0].
    */


// Vector index, C[i] = A[truncate[B[i]].
extern void vDSP_vindex(
    const float *__A,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vindexD(
    const double *__A,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[trunc(B[n])];
    */


// Vector interpolation between vectors.
extern void vDSP_vintb(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vintbD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n] + C[0] * (B[n] - A[n]);
    */


// Vector test limit.
extern void vDSP_vlim(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vlimD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                if (B[0] <= A[n])
                    D[n] = +C[0];
                else
                    D[n] = -C[0];
    */


// Vector linear interpolation.
extern void vDSP_vlint(
    const float *__A,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __M)  // Nominal length of A, but not used.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vlintD(
    const double *__A,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    vDSP_Length   __M)  // Nominal length of A, but not used.
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
            {
                b = trunc(B[n]);
                a = B[n] - b;
                C[n] = A[b] + a * (A[b+1] - A[b]);
            }
    */


// Vector maxima.
extern void vDSP_vmax(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmaxD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = B[n] <= A[n] ? A[n] : B[n];
    */


// Vector maximum magnitude.
extern void vDSP_vmaxmg(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmaxmgD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = |B[n]| <= |A[n]| ? |A[n]| : |B[n]|;
    */


// Vector sliding window maxima.
extern void vDSP_vswmax(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __WindowLength)
        API_AVAILABLE(macos(10.10), ios(8.0));
extern void vDSP_vswmaxD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length  __N,
    vDSP_Length  __WindowLength)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  Maps:  The default maps are used.

        These compute the maximum value within a window to the input vector.
        A maximum is calculated for each window position:

            for (n = 0; n < N; ++n)
                C[n] = the greatest value of A[w] for n <= w < n+WindowLength.

        A must contain N+WindowLength-1 elements, and C must contain space for
        N+WindowLength-1 elements.  Although only N outputs are provided in C,
        the additional elements may be used for intermediate computation.

        A and C may not overlap.

        WindowLength must be positive (zero is not supported).
    */


// Vector minima.
extern void vDSP_vmin(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vminD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] <= B[n] ? A[n] : B[n];
    */


// Vector minimum magnitude.
extern void vDSP_vminmg(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vminmgD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.

        These compute:

            for (n = 0; n < N; ++n)
                C[n] = |A[n]| <= |B[n]| ? |A[n]| : |B[n]|;
    */


// Vector multiply, multiply, and add.
extern void vDSP_vmma(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmmaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                E[n] = A[n]*B[n] + C[n]*D[n];
    */


// Vector multiply, multiply, and subtract.
extern void vDSP_vmmsb(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmmsbD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                E[n] = A[n]*B[n] - C[n]*D[n];
    */


// Vector multiply and scalar add.
extern void vDSP_vmsa(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmsaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n]*B[n] + C[0];
    */


// Vector multiply and subtract.
extern void vDSP_vmsb(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vmsbD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n]*B[n] - C[n];
    */


// Vector negative absolute value.
extern void vDSP_vnabs(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vnabsD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = -|A[n]|;
    */


// Vector negate.
extern void vDSP_vneg(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vnegD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = -A[n];
    */


// Vector polynomial.
extern void vDSP_vpoly(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __P)  // P is the polynomial degree.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vpolyD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    vDSP_Length   __P)  // P is the polynomial degree.
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = sum(A[P-p] * B[n]**p, 0 <= p <= P);
    */


// Vector Pythagoras.
extern void vDSP_vpythg(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vpythgD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                E[n] = sqrt((A[n]-C[n])**2 + (B[n]-D[n])**2);
    */


// Vector quadratic interpolation.
extern void vDSP_vqint(
    const float *__A,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __M)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vqintD(
    const double *__A,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    vDSP_Length   __M)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
            {
                b = max(trunc(B[n]), 1);
                a = B[n] - b;
                C[n] = (A[b-1]*(a**2-a) + A[b]*(2-2*a**2) + A[b+1]*(a**2+a))
                    / 2;
            }
    */


// Vector build ramp.
extern void vDSP_vramp(
    const float *__A,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vrampD(
    const double *__A,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[0] + n*B[0];
    */


// Vector running sum integration.
extern void vDSP_vrsum(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__S,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vrsumD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__S,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = S[0] * sum(A[j], 0 < j <= n);

        Observe that C[0] is set to 0, and A[0] is not used.
    */


// Vector reverse order, in-place.
extern void vDSP_vrvrs(
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vrvrsD(
    double      *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            Let A contain a copy of C.
            for (n = 0; n < N; ++n)
                C[n] = A[N-1-n];
    */


// Vector subtract and multiply.
extern void vDSP_vsbm(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsbmD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = (A[n] - B[n]) * C[n];
    */


// Vector subtract, subtract, and multiply.
extern void vDSP_vsbsbm(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    vDSP_Stride  __ID,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsbsbmD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    vDSP_Stride   __ID,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                E[n] = (A[n] - B[n]) * (C[n] - D[n]);
    */


// Vector subtract and scalar multiply.
extern void vDSP_vsbsm(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsbsmD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = (A[n] - B[n]) * C[0];
    */


// Vector Simpson integration.
extern void vDSP_vsimps(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsimpsD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            C[0] = 0;
            C[1] = B[0] * (A[0] + A[1])/2;
            for (n = 2; n < N; ++n)
                C[n] = C[n-2] + B[0] * (A[n-2] + 4*A[n-1] + A[n])/3;
    */


// Vector-scalar multiply and vector add.
extern void vDSP_vsma(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsmaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n]*B[0] + C[n];
    */


// Vector-scalar multiply and scalar add.
extern void vDSP_vsmsa(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsmsaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n]*B[0] + C[0];
    */


// Vector scalar multiply and vector subtract.
extern void vDSP_vsmsb(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsmsbD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                D[n] = A[n]*B[0] - C[n];
    */


// Vector-scalar multiply, vector-scalar multiply and vector add.
extern void vDSP_vsmsma(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    vDSP_Stride  __IC,
    const float *__D,
    float       *__E,
    vDSP_Stride  __IE,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.9), ios(6.0));
extern void vDSP_vsmsmaD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    vDSP_Stride   __IC,
    const double *__D,
    double       *__E,
    vDSP_Stride   __IE,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.10), ios(8.0));
    /*  Maps:  The default maps are used.
        
        This computes:

            for (n = 0; n < N; ++n)
                E[n] = A[n]*B[0] + C[n]*D[0];
    */


// Vector sort, in-place.
extern void vDSP_vsort(
    float       *__C,
    vDSP_Length  __N,
    int          __Order)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsortD(
    double      *__C,
    vDSP_Length  __N,
    int          __Order)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  If Order is +1, C is sorted in ascending order.
        If Order is -1, C is sorted in descending order.
    */


// Vector sort indices, in-place.
extern void vDSP_vsorti(
    const float *__C,
    vDSP_Length *__I,
    vDSP_Length * __nullable __Temporary,
    vDSP_Length  __N,
    int          __Order)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vsortiD(
    const double *__C,
    vDSP_Length  *__I,
    vDSP_Length  * __nullable __Temporary,
    vDSP_Length   __N,
    int           __Order)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  No strides are used; arrays map directly to memory.

        I contains indices into C.
        
        If Order is +1, I is sorted so that C[I[n]] increases, for 0 <= n < N.
        If Order is -1, I is sorted so that C[I[n]] decreases, for 0 <= n < N.

        Temporary is not used.  NULL should be passed for it.
    */


// Vector swap.
extern void vDSP_vswap(
    float       *__A,
    vDSP_Stride  __IA,
    float       *__B,
    vDSP_Stride  __IB,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vswapD(
    double      *__A,
    vDSP_Stride  __IA,
    double      *__B,
    vDSP_Stride  __IB,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                A[n] is swapped with B[n].
    */


// Vector sliding window sum.
extern void vDSP_vswsum(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N,
    vDSP_Length  __P) // Length of window.
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vswsumD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N,
    vDSP_Length   __P) // Length of window.
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = sum(A[n+p], 0 <= p < P);

        Note that A must contain N+P-1 elements.
    */


// Vector table lookup and interpolation.
extern void vDSP_vtabi(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__S1,
    const float *__S2,
    const float *__C,
    vDSP_Length  __M,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vtabiD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__S1,
    const double *__S2,
    const double *__C,
    vDSP_Length   __M,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
            {
                p = S1[0] * A[n] + S2[0];
                if (p < 0)
                    D[n] = C[0];
                else if (p < M-1)
                {
                    q = trunc(p);
                    r = p-q;
                    D[n] = (1-r)*C[q] + r*C[q+1];
                }
                else
                    D[n] = C[M-1];
            }
    */


// Vector threshold.
extern void vDSP_vthr(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vthrD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                if (B[0] <= A[n])
                    C[n] = A[n];
                else
                    C[n] = B[0];
    */


// Vector threshold with zero fill.
extern void vDSP_vthres(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vthresD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                if (B[0] <= A[n])
                    C[n] = A[n];
                else
                    C[n] = 0;
    */


// Vector threshold with signed constant.
extern void vDSP_vthrsc(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    const float *__C,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vthrscD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    const double *__C,
    double       *__D,
    vDSP_Stride   __ID,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                if (B[0] <= A[n])
                    D[n] = +C[0];
                else
                    D[n] = -C[0];
    */


// Vector tapered merge.
extern void vDSP_vtmerg(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vtmergD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            for (n = 0; n < N; ++n)
                C[n] = A[n] + (B[n] - A[n]) * n/(N-1);
    */


// Vector trapezoidal integration.
extern void vDSP_vtrapz(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_vtrapzD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.4), ios(4.0));
    /*  Maps:  The default maps are used.
        
        These compute:

            C[0] = 0;
            for (n = 1; n < N; ++n)
                C[n] = C[n-1] + B[0] * (A[n-1] + A[n])/2;
    */


// Wiener Levinson.
extern void vDSP_wiener(
    vDSP_Length  __L,
    const float *__A,
    const float *__C,
    float       *__F,
    float       *__P,
    int          __Flag,
    int         *__Error)
        API_AVAILABLE(macos(10.4), ios(4.0));
extern void vDSP_wienerD(
    vDSP_Length   __L,
    const double *__A,
    const double *__C,
    double       *__F,
    double       *__P,
    int           __Flag,
    int          *__Error)
        API_AVAILABLE(macos(10.4), ios(4.0));


/*  vDSP_FFT16_copv and vDSP_FFT32_copv perform 16- and 32-element FFTs on
    interleaved complex unit-stride vector-block-aligned data.

    Parameters:

        float *Output

            Pointer to space for output data (interleaved complex).  This
            address must be vector-block aligned.

        const float *Input

            Pointer to input data (interleaved complex).  This address must be
            vector-block aligned.

        FFT_Direction Direction

            Transform direction, FFT_FORWARD or FFT_INVERSE.

    These routines calculate:

        For 0 <= k < N,

            H[k] = sum(1**(S * j*k/N) * h[j], 0 <= j < N),

    where:

        N is 16 or 32, as specified by the routine name,

        h[j] is Input[2*j+0] + i * Input[2*j+1] at routine entry,

        H[j] is Output[2*j+0] + i * Output[2*j+1] at routine exit,

        S is -1 if Direction is FFT_FORWARD and +1 if Direction is FFT_INVERSE,
        and

        1**x is e**(2*pi*i*x).

    Input and Output may be equal but may not otherwise overlap.
*/
void vDSP_FFT16_copv(float *__Output, const float *__Input,
    FFTDirection __Direction)
        API_AVAILABLE(macos(10.6), ios(4.0));
void vDSP_FFT32_copv(float *__Output, const float *__Input,
    FFTDirection __Direction)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_FFT16_zopv and vDSP_FFT32_zopv perform 16- and 32-element FFTs on
    separated complex unit-stride vector-block-aligned data.

    Parameters:

        float *Or, float *Oi

            Pointers to space for real and imaginary output data.  These
            addresses must be vector-block aligned.

        const float *Ir, *Ii

            Pointers to real and imaginary input data.  These addresses must be
            vector-block aligned.

        FFT_Direction Direction

            Transform direction, FFT_FORWARD or FFT_INVERSE.

    These routines calculate:

        For 0 <= k < N,

            H[k] = sum(1**(S * j*k/N) * h[j], 0 <= j < N),

    where:

        N is 16 or 32, as specified by the routine name,

        h[j] is Ir[j] + i * Ii[j] at routine entry,

        H[j] is Or[j] + i * Oi[j] at routine exit,

        S is -1 if Direction is FFT_FORWARD and +1 if Direction is FFT_INVERSE,
        and

        1**x is e**(2*pi*i*x).

    Or may equal Ir or Ii, and Oi may equal Ii or Ir, but the ararys may not
    otherwise overlap.
*/
void vDSP_FFT16_zopv(
          float *__Or,       float *__Oi,
    const float *__Ir, const float *__Ii,
    FFTDirection __Direction)
        API_AVAILABLE(macos(10.6), ios(4.0));
void vDSP_FFT32_zopv(
          float *__Or,       float *__Oi,
    const float *__Ir, const float *__Ii,
    FFTDirection __Direction)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  How to use the Discrete Fourier Transform (DFT) and Discrete Cosine
    Transform (DCT) interfaces.

    There are three steps to performing a DFT or DCT:

        Call a setup routine (e.g., vDSP_DFT_zop_CreateSetup) to get a setup
        object.

            This is a preparation step to be done when a program is starting or
            is starting some new phase (e.g., when a communication channel is
            opened).  It should never be done during real-time processing.  The
            setup routine is slow and is called only once to prepare data that
            can be used many times.

        Call an execution routine (e.g., vDSP_DFT_Execute or vDSP_DCT_Execute)
        to perform a DFT or DCT, and pass it the setup object.

            The execution routine is fast (for selected cases) and is generally
            called many times.

        Call a destroy routine (e.g., vDSP_DFT_DestroySetup) to release the
        memory held by the setup object.

            This is done when a program is ending or is ending some phase.
            After calling a destroy routine, the setup data is no longer valid
            and should not be used.

    Discussion:

        The current sequences of setup, execution, destroy routines are:

            For single-precision (float):

                vDSP_DFT_zop_CreateSetup,
                vDSP_DFT_Execute,
                vDSP_DFT_DestroySetup.

                vDSP_DFT_zrop_CreateSetup,
                vDSP_DFT_Execute,
                vDSP_DFT_DestroySetup.

                vDSP_DCT_CreateSetup,
                vDSP_DCT_Execute,
                vDSP_DFT_DestroySetup.

                vDSP_DFT_CreateSetup,
                vDSP_DFT_zop,
                vDSP_DFT_DestroySetup.

            For double-precision (double):

                vDSP_DFT_zop_CreateSetupD,
                vDSP_DFT_ExecuteD,
                vDSP_DFT_DestroySetupD.

                vDSP_DFT_zrop_CreateSetupD,
                vDSP_DFT_ExecuteD,
                vDSP_DFT_DestroySetupD.

        Sharing DFT and DCT setups:

            Any setup returned by a DFT or DCT setup routine may be passed as
            input to any DFT or DCT setup routine for the same precision (float
            or double), in the parameter named Previous.  (This allows the
            setups to share data, avoiding unnecessary duplication of some
            setup data.)  Setup routines may be executed in any order.  Passing
            any setup of a group of setups sharing data will result in a new
            setup sharing data with all of the group.

            When calling an execution routine, each setup can be used only with
            its intended execution routine.  Thus the setup returned by
            vDSP_DFT_CreateSetup can only be used with vDSP_DFT_zop and not
            with vDSP_DFT_Execute.

            vDSP_DFT_DestroySetup is used to destroy any single-precision DFT
            or DCT setup.  vDSP_DFT_DestroySetupD is used to destroy any
            double-precision DFT or DCT setup.

        History:

            vDSP_DFT_CreateSetup and vDSP_DFT_zop are the original vDSP DFT
            routines.  vDSP_DFT_zop_CreateSetup, vDSP_DFT_zrop_CreateSetup, and
            vDSP_DFT_Execute are newer, more specialized DFT routines.  These
            newer routines do not have stride parameters (stride is one) and
            incorporate the direction parameter into the setup.  This reduces
            the number of arguments passed to the execution routine, which
            receives only the setup and four address parameters.  Additionally,
            the complex-to-complex DFT (zop) and real-to-complex DFT (zrop) use
            the same execution routine (the setup indicates which function to
            perform).

            We recommend you use vDSP_DFT_zop_CreateSetup,
            vDPS_DFT_zrop_CreateSetup, and vDSP_DFT_Execute, and that you not
            use vDSP_DFT_CreateSetup and vDSP_DFT_zop.

    Multithreading:

        Never call a setup or destroy routine in a thread when any DFT or DCT
        routine (setup, execution, or destroy) that shares setup data may be
        executing.  (This applies not just to multiple threads but also to
        calling DFT or DCT routines in signal handlers.)

        Multiple DFT or DCT execution routines may be called simultaneously.
        (Their access to the setup data is read-only.)

        If you need to call setup and/or destroy routines while other DFT or
        DCT routines might be executing, you can either use Grand Central
        Dispatch or locks (costs time) to avoid simultaneous execution or you
        can create separate setup objects for them (costs memory).
*/


/*  A vDSP_DFT_Setup object is a pointer to a structure whose definition is
    unpubilshed.
*/
typedef struct vDSP_DFT_SetupStruct  *vDSP_DFT_Setup;
typedef struct vDSP_DFT_SetupStructD *vDSP_DFT_SetupD;


// DFT direction may be specified as vDSP_DFT_FORWARD or vDSP_DFT_INVERSE.
typedef vDSP_ENUM(int, vDSP_DFT_Direction)
    { vDSP_DFT_FORWARD = +1, vDSP_DFT_INVERSE = -1 };


/*  vDSP_DFT_CreateSetup is a DFT setup routine.  It creates a setup object
    for use with the vDSP_DFT_zop execution routine.  We recommend you use
    vDSP_DFT_zop_CreateSetup instead of this routine.

    Parameters:

        vDSP_DFT_Setup Previous

            Previous is either zero or a previous DFT or DCT setup.  If a
            previous setup is passed, the new setup will share data with the
            previous setup, if feasible (and with any other setups the previous
            setup shares with).  If zero is passed, the routine will allocate
            and initialize new memory.

        vDSP_Length Length

            Length is the number of complex elements to be transformed.

    Return value:

        Zero is returned if memory is unavailable.

    The returned setup object may be used only with vDSP_DFT_zop for the length
    given during setup.  Unlike previous vDSP FFT routines, the setup may not
    be used to execute transforms with shorter lengths.

    Do not call this routine while any DFT routine sharing setup data might be
    executing.
*/
__nullable vDSP_DFT_Setup vDSP_DFT_CreateSetup(
    __nullable vDSP_DFT_Setup __Previous,
    vDSP_Length               __Length)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_DFT_zop_CreateSetup is a DFT setup routine.  It creates a setup object
    for use with the vDSP_DFT_Execute execution routine, to perform a
    complex-to-complex DFT.

    Parameters:

        vDSP_DFT_Setup Previous

            Previous is either zero or a previous DFT or DCT setup.  If a
            previous setup is passed, the new setup will share data with the
            previous setup, if feasible (and with any other setups the previous
            setup shares with).  If zero is passed, the routine will allocate
            and initialize new memory.

        vDSP_Length Length

            Length is the number of complex elements to be transformed.

        vDSP_DFT_Direction Direction

            Transform direction, vDSP_DFT_FORWARD or vDSP_DFT_INVERSE.

    Return value:

        Zero is returned if memory is unavailable or if there is no
        implementation for the requested case.  Currently, the implemented
        cases are:

            Length = 2**n.

            Length = f * 2**n, where f is 3, 5, or 15 and 3 <= n.

        Additionally, it is recommended that the array addresses (passed to
        vDSP_DFT_Execute) be 16-byte aligned.  For other cases, performance may
        be slightly or greatly worse, depending on transform length and
        processor model.

    Function:

        When vDSP_DFT_Execute is called with a setup returned from this
        routine, it calculates:

            For 0 <= k < N,

                H[k] = sum(1**(S * j*k/N) * h[j], 0 <= j < N),

        where:

            N is the length given in the setup;

            h is the array of complex numbers specified by Ir and Ii when
            vDSP_DFT_Execute is called:

                for 0 <= j < N,
                    h[j] = Ir[j] + i * Ii[j];

            H is the array of complex numbers specified by Or and Oi when
            vDSP_DFT_Execute returns:

                for 0 <= k < N,
                    H[k] = Or[k] + i * Oi[k];

            S is -1 if Direction is vDSP_DFT_FORWARD and +1 if Direction is
            vDSP_DFT_INVERSE; and

            1**x is e**(2*pi*i*x).

    Performance:

        Performance is good when the array addresses (passed to
        vDSP_DFT_Execute) are 16-byte aligned.  Other alignments are supported,
        but performance may be significantly worse in some cases, depending on
        the processor model or the transform length (because different
        algorithms are used for different forms of transform length).

    In-Place Operation:

        Or may equal Ir and Oi may equal Ii (in the call to vDSP_DFT_Execute).
        Otherwise, no overlap of Or, Oi, Ir, and Ii is supported.

    The returned setup object may be used only with vDSP_DFT_Execute for the
    length given during setup.  Unlike previous vDSP FFT routines, the setup
    may not be used to execute transforms with shorter lengths.

    Do not call this routine while any DFT or DCT routine sharing setup data
    might be executing.
*/
__nullable vDSP_DFT_Setup vDSP_DFT_zop_CreateSetup(
    __nullable vDSP_DFT_Setup __Previous,
    vDSP_Length               __Length,
    vDSP_DFT_Direction        __Direction)
        API_AVAILABLE(macos(10.7), ios(4.0));
__nullable vDSP_DFT_SetupD vDSP_DFT_zop_CreateSetupD(
    __nullable vDSP_DFT_SetupD __Previous,
    vDSP_Length                __Length,
    vDSP_DFT_Direction         __Direction)
        API_AVAILABLE(macos(10.9), ios(7.0));

/*  vDSP_DFT_zrop_CreateSetup and vDSP_DFT_zrop_CreateSetupD are DFT setup
    routines.  Each creates a setup object for use with the corresponding
    execution routine, vDSP_DFT_Execute or vDSP_DFT_ExecuteD, to perform a
    real-to-complex DFT or a complex-to-real DFT.  Documentation below is
    written for vDSP_DFT_zrop_CreateSetup.  vDSP_DFT_CreateSetupD behaves the
    same way, with corresponding changes of the types, objects, and routines to
    the double-precision versions.

    Parameters:

        vDSP_DFT_Setup Previous

            Previous is either zero or a previous DFT or DCT setup.  If a
            previous setup is passed, the new setup will share data with the
            previous setup, if feasible (and with any other setups the previous
            setup shares with).  If zero is passed, the routine will allocate
            and initialize new memory.

        vDSP_Length Length

            Length is the number of real elements to be transformed (in a a
            forward, real-to-complex transform) or produced (in a reverse,
            complex-to-real transform).  Length must be even.

        vDSP_DFT_Direction Direction

            Transform direction, vDSP_DFT_FORWARD or vDSP_DFT_INVERSE.

    Return value:

        Zero is returned if memory is unavailable or if there is no
        implementation for the requested case.  Currently, the implemented
        cases are:

            Length = 2**n.

            Length = f * 2**n, where f is 3, 5, or 15 and 4 <= n.

        Additionally, it is recommended that the array addresses (passed to
        vDSP_DFT_Execute) be 16-byte aligned.  For other cases, performance may
        be slightly or greatly worse, depending on transform length and
        processor model.

    Function:

        When vDSP_DFT_Execute is called with a setup returned from this
        routine, it calculates:

            For 0 <= k < N,

                H[k] = C * sum(1**(S * j*k/N) * h[j], 0 <= j < N),

        where:

            N is the Length given in the setup;

            h is the array of numbers specified by Ir and Ii when
            vDSP_DFT_Execute is called (see "Data Layout" below);

            H is the array of numbers specified by Or and Oi when
            vDSP_DFT_Execute returns (see "Data Layout" below);

            C is 2 if Direction is vDSP_DFT_FORWARD and 1 if Direction is
            vDSP_DFT_INVERSE;

            S is -1 if Direction is vDSP_DFT_FORWARD and +1 if Direction is
            vDSP_DFT_INVERSE; and

            1**x is e**(2*pi*i*x).

        Data Layout:

            If Direction is vDSP_DFT_FORWARD, then:

                h is an array of real numbers, with its even-index elements
                stored in Ir and its odd-index elements stored in Ii:

                    For 0 <= j < N/2,
                        h[2*j+0] = Ir[j], and
                        h[2*j+1] = Ii[j].

                H is an array of complex numbers, stored in Or and Oi:

                    H[0  ] = Or[0].  (H[0  ] is pure real.)
                    H[N/2] = Oi[0].  (H[N/2] is pure real.)
                    For 1 < k < N/2,
                        H[k] = Or[k] + i * Oi[k].

                For N/2 < k < N, H[k] is not explicitly stored in memory but is
                known because it necessarily equals the conjugate of H[N-k],
                which is stored as described above.

            If Direction is vDSP_DFT_INVERSE, then the layouts of the input and
            output arrays are swapped.  Ir and Ii describe an input array with
            complex elements laid out as described above for Or and Oi.  When
            vDSP_DFT_Execute returns, Or and Oi contain a pure real array, with
            its even-index elements stored in Or and its odd-index elements in
            Oi.

    Performance:

        Performance is good when the array addresses (passed to
        vDSP_DFT_Execute) are 16-byte aligned.  Other alignments are supported,
        but performance may be significantly worse in some cases, depending on
        the processor model or the transform length (because different
        algorithms are used for different forms of transform length).

    In-Place Operation:

        Or may equal Ir and Oi may equal Ii (in the call to vDSP_DFT_Execute).
        Otherwise, no overlap of Or, Oi, Ir, and Ii is supported.

    The returned setup object may be used only with vDSP_DFT_Execute for the
    length given during setup.  Unlike previous vDSP FFT routines, the setup
    may not be used to execute transforms with shorter lengths.

    Do not call this routine while any DFT routine sharing setup data might be
    executing.
*/
__nullable vDSP_DFT_Setup vDSP_DFT_zrop_CreateSetup(
    __nullable vDSP_DFT_Setup __Previous,
    vDSP_Length __Length, vDSP_DFT_Direction __Direction)
        API_AVAILABLE(macos(10.7), ios(4.0));
__nullable vDSP_DFT_SetupD vDSP_DFT_zrop_CreateSetupD(
    __nullable vDSP_DFT_SetupD __Previous,
    vDSP_Length __Length, vDSP_DFT_Direction __Direction)
        API_AVAILABLE(macos(10.9), ios(7.0));


/*  vDSP_DFT_DestroySetup and vDSP_DFT_DestroySetupD are DFT destroy routines.
    They release the memory used by a setup object.  Documentation below is
    written for vDSP_DFT_DestroySetup.  vDSP_DFT_DestroySetupD behaves the same
    way, with corresponding changes of the types, objects, and routines to the
    double-precision versions.

    Parameters:

        vDSP_DFT_Setup Setup

            Setup is the setup object to be released.  The object may have
            been previously allocated with any DFT or DCT setup routine, such
            as vDSP_DFT_zop_CreateSetup, vDSP_DFT_zrop_CreateSetup, or
            vDSP_DCT_CreateSetup.

            Setup may be a null pointer, in which case the call has no effect.

    Destroying a setup with shared data is safe; it will release only memory
    not needed by other undestroyed setups.  Memory (and the data it contains)
    is freed only when all setup objects using it have been destroyed.

    Do not call this routine while any DFT or DCT routine sharing setup data
    might be executing.
*/
void vDSP_DFT_DestroySetup(__nullable vDSP_DFT_Setup __Setup)
        API_AVAILABLE(macos(10.6), ios(4.0));
void vDSP_DFT_DestroySetupD(__nullable vDSP_DFT_SetupD __Setup)
        API_AVAILABLE(macos(10.9), ios(7.0));


/*  vDSP_DFT_zop is a DFT execution routine.  It performs a DFT, with the aid
    of previously created setup data.

    Parameters:

        vDSP_DFT_Setup Setup

            A setup object returned by a previous call to
            vDSP_DFT_zop_CreateSetup.

        const float *Ir
        const float *Ii

            Pointers to real and imaginary components of input data.

        vDSP_Stride Is

            The number of physical elements from one logical input element to
            the next.

        float *Or
        float *Oi

            Pointers to space for real and imaginary components of output
            data.

            The input and output arrays may not overlap except as specified
            in "In-Place Operation", below.

        vDSP_Stride Os

            The number of physical elements from one logical output element to
            the next.

        vDSP_DFT_Direction Direction

            Transform direction, vDSP_DFT_FORWARD or vDSP_DFT_INVERSE.

    Observe there is no separate length parameter.  The length is passed via
    the setup object.

    Performance:

        Performance is good for these cases:

            All addresses are 16-byte aligned, all strides are one, and the
            length is f * 2**n, where f is 3, 5, or 15 and 3 <= n.

        Performance is extremely slow for all other cases.

    In-Place Operation:

        For cases where the length is f * 2**n, where f is 3, 5, or 15 and 3 <=
        n, Or may equal Ir and Oi may equal Ii.  Otherwise, no overlap of Or,
        Oi, Ir, and Ii is supported.

    This routine calculates:

        For 0 <= k < N,

            H[k] = sum(1**(S * j*k/N) * h[j], 0 <= j < N),

    where:

        N is the length given in the setup,

        h is the array of complex numbers specified by Ir, Ii, and Is at
        routine entry:

            h[j] = Ir[j*Is] + i * Ii[j*Is],
            for 0 <= j < N,

        H is the array of complex numbers stored as specified by Or, Oi, and Os
        at routine exit:

            H[k] = Or[k*Os] + i * Oi[k*Os],
            for 0 <= k < N,

        S is -1 if Direction is vDSP_DFT_FORWARD and +1 if Direction is
        vDSP_DFT_INVERSE, and

        1**x is e**(2*pi*i*x).

    Do not call this routine while any DFT setup or destroy routine sharing
    setup data might be executing.
*/
void vDSP_DFT_zop(
    const struct vDSP_DFT_SetupStruct *__Setup,
    const float *__Ir, const float *__Ii, vDSP_Stride __Is,
          float *__Or,       float *__Oi, vDSP_Stride __Os,
    vDSP_DFT_Direction __Direction)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_DFT_Execute and vDSP_DFT_ExecuteD are DFT execution routines.  They
    perform a DFT, with the aid of previously created setup data.
    Documentation below is written for vDSP_DFT_Execute.  vDSP_DFT_ExecuteD
    behaves the same way, with corresponding changes of the types, objects, and
    routines to the double-precision versions.

    Parameters:

        vDSP_DFT_Setup Setup

            A setup object returned by a previous call to
            vDSP_DFT_zop_CreateSetup or vDSP_DFT_zrop_CreateSetup.

        const float *Ir
        const float *Ii

            Pointers to input data.

        float *Or
        float *Oi

            Pointers to output data.

            The input and output arrays may not overlap except as specified
            in "In-Place Operation", below.

    Performance and In-Place Operation:

        See notes for the setup routine for the operation being executed.

    Function:

        The function performed by this routine is determined by the setup
        passed to it.  The documentation for the routine used to create the
        setup describes the function.

        Note that different numbers of elements are required when this routine
        is called, depending on the setup used:

            When the setup is from vDSP_zop_CreateSetup, each array (Ir, Ii,
            Or, and Oi) must have Length elements.

            When the setup is from vDSP_zrop_CreateSetup, each array (Ir, Ii,
            Or, and Oi) must have Length/2 elements.

    Do not call this routine while any DFT setup or destroy routine sharing
    setup data might be executing.
*/
void vDSP_DFT_Execute(
    const struct vDSP_DFT_SetupStruct *__Setup,
    const float *__Ir,  const float *__Ii,
          float *__Or,        float *__Oi)
        API_AVAILABLE(macos(10.7), ios(4.0));
void vDSP_DFT_ExecuteD(
    const struct vDSP_DFT_SetupStructD *__Setup,
    const double *__Ir,  const double *__Ii,
          double *__Or,        double *__Oi)
        API_AVAILABLE(macos(10.9), ios(7.0));


/*  vDSP_DCT_CreateSetup is a DCT setup routine.  It creates a setup object
    for use with the vDSP_DCT_Execute routine.  See additional information
    above, at "How to use the Discrete Fourier Transform (DFT) and Discrete
    Cosine Transform (DCT) interfaces."

    Parameters:

        vDSP_DFT_Setup Previous

            Previous is either zero or a previous DFT or DCT setup.  If a
            previous setup is passed, the new setup will share data with the
            previous setup, if feasible (and with any other setups the
            previous setup shares with).  If zero is passed, the routine
            will allocate and initialize new memory.

        vDSP_Length Length

            Length is the number of real elements to be transformed.

        vDSP_DCT_Type Type

            Type specifies which DCT variant to perform.  At present, the
            supported DCT types are II and III (which are mutual inverses, up
            to scaling) and IV (which is its own inverse).  These are specified
            with symbol names vDSP_DCT_II, vDSP_DCT_III, and vDSP_DCT_IV.

    Return value:

        Zero is returned if memory is unavailable or if there is no
        implementation for the requested case.  Currently, the implemented
        cases are:

            Length = f * 2**n, where f is 1, 3, 5, or 15 and 4 <= n.

    Function:

        When vDSP_DCT_Execute is called with a setup returned from this
        routine, it calculates:

            If Type is vDSP_DCT_II:

                For 0 <= k < N,

                    Or[k] = sum(Ir[j] * cos(k * (j+1/2) * pi / N, 0 <= j < N).

            If Type is vDSP_DCT_III

                For 0 <= k < N,

                    Or[k] = Ir[0]/2
                        + sum(Ir[j] * cos((k+1/2) * j * pi / N), 1 <= j < N).

            If Type is vDSP_DCT_IV:

                For 0 <= k < N,

                    Or[k] = sum(Ir[j] * cos((k+1/2) * (j+1/2) * pi / N, 0 <= j < N).

            Where:

                N is the length given in the setup,

                h is the array of real numbers passed to vDSP_DCT_Execute in
                Input, and

                H is the array of real numbers stored by vDSP_DCT_Execute in
                the array passed to it in Output.

     Performance:

        Performance is good when the array addresses (passed to
        vDSP_DFT_Execute) are 16-byte aligned.  Other alignments are supported,
        but performance may be significantly worse in some cases, depending on
        the processor model or the transform length (because different
        algorithms are used for different forms of transform length).

    In-Place Operation:

        Output may equal Input (in the call the vDSP_DCT_Execute).  Otherwise,
        no overlap is permitted between the two buffers.

    The returned setup object may be used only with vDSP_DCT_Execute for the
    length given during setup.

    Do not call this routine while any DFT or DCT routine sharing setup data
    might be executing.
*/
typedef vDSP_ENUM(int, vDSP_DCT_Type)
{
    vDSP_DCT_II  = 2,
    vDSP_DCT_III = 3,
    vDSP_DCT_IV  = 4
};

__nullable vDSP_DFT_Setup vDSP_DCT_CreateSetup(
    __nullable vDSP_DFT_Setup __Previous,
    vDSP_Length               __Length,
    vDSP_DCT_Type             __Type)
        API_AVAILABLE(macos(10.9), ios(6.0));


/*  vDSP_DCT_Execute is a DCT execution routine.  It performs a DCT, with the
    aid of previously created setup data.  See additional information above, at
    "How to use the Discrete Fourier Transform (DFT) and Discrete Cosine
    Transform (DCT) interfaces."

    Parameters:

        vDSP_DFT_Setup Setup

            A setup object returned by a previous call to vDSP_DCT_CreateSetup.

        const float *Input

            Pointer to the input buffer.

        float *Output

            Pointer to the output buffer.

        Observe there are no separate length or type parameters.  They are
        specified at the time that the Setup is created.

        Because the DCT is real-to-real, the parameters for vDSP_DCT_Execute
        are different from those used for a DFT.
*/
void vDSP_DCT_Execute(
    const struct vDSP_DFT_SetupStruct *__Setup,
    const float                       *__Input,
    float                             *__Output)
        API_AVAILABLE(macos(10.9), ios(6.0));


/*  vDSP_dotpr2, vector single-precision stereo dot product.

    Function:

        This routine calculates the dot product of A0 with B and the dot
        product of A1 with B.  This is functionally equivalent to calculating
        two dot products but might execute faster.

        In pseudocode, the operation is:

            sum0 = 0;
            sum1 = 0;
            for (i = 0; i < Length; ++i)
            {
                sum0 += A0[i*A0Stride] * B[i*BStride];
                sum1 += A1[i*A1Stride] * B[i*BStride];
            }
            *C0 = sum0;
            *C1 = sum1;

    Input:

        const float *A0, vDSP_Stride A0Stride.

            Starting address and stride for input vector A0.

        const float *A1, vDSP_Stride A1Stride.

            Starting address and stride for input vector A1.

        const float *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        float *C0.

            Address for dot product of A0 and B.

        float *C1.

            Address for dot product of A1 and B.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to *C0 and *C1.
*/
void vDSP_dotpr2(
    const float *__A0, vDSP_Stride __IA0,
    const float *__A1, vDSP_Stride __IA1,
    const float *__B,  vDSP_Stride __IB,
    float       *__C0,
    float       *__C1,
    vDSP_Length  __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_dotpr2D, vector double-precision stereo dot product.

    Function:

        This routine calculates the dot product of A0 with B and the dot
        product of A1 with B.  This is functionally equivalent to calculating
        two dot products but might execute faster.

        In pseudocode, the operation is:

            sum0 = 0;
            sum1 = 0;
            for (i = 0; i < Length; ++i)
            {
                sum0 += A0[i*A0Stride] * B[i*BStride];
                sum1 += A1[i*A1Stride] * B[i*BStride];
            }
            *C0 = sum0;
            *C1 = sum1;

    Input:

        const double *A0, vDSP_Stride A0Stride.

            Starting address and stride for input vector A0.

        const double *A1, vDSP_Stride A1Stride.

            Starting address and stride for input vector A1.

        const double *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        double *C0.

            Address for dot product of A0 and B.

        double *C1.

            Address for dot product of A1 and B.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to *C0 and *C1.
*/
void vDSP_dotpr2D(
    const double *__A0, vDSP_Stride __IA0,
    const double *__A1, vDSP_Stride __IA1,
    const double *__B,  vDSP_Stride __IB,
    double       *__C0,
    double       *__C1,
    vDSP_Length   __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_dotpr_s1_15, vector integer 1.15 format dot product.

    Function:

        This routine calculates the dot product of A with B.

        In pseudocode, the operation is:

            sum = 0;
            for (i = 0; i < N; ++i)
            {
                sum0 += A[i*AStride] * B[i*BStride];
            }
            *C = sum;

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *A, vDSP_Stride AStride.

            Starting address and stride for input vector A.

        const short int *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        short int *C.

            Address for dot product of A and B.

        vDSP_Length N.

            Number of elements in each vector.

    Output:

        The result is written to *C.
*/
void vDSP_dotpr_s1_15(
    const short int *__A, vDSP_Stride __IA,
    const short int *__B, vDSP_Stride __IB,
    short int       *__C,
    vDSP_Length      __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_dotpr2_s1_15, vector integer 1.15 format stereo dot product.

    Function:

        This routine calculates the dot product of A0 with B and the dot
        product of A1 with B.  This is functionally equivalent to calculating
        two dot products but might execute faster.

        In pseudocode, the operation is:

            sum0 = 0;
            sum1 = 0;
            for (i = 0; i < N; ++i)
            {
                sum0 += A0[i*A0Stride] * B[i*BStride];
                sum1 += A1[i*A1Stride] * B[i*BStride];
            }
            *C0 = sum0;
            *C1 = sum1;

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *A0, vDSP_Stride A0Stride.

            Starting address and stride for input vector A0.

        const short int *A1, vDSP_Stride A1Stride.

            Starting address and stride for input vector A1.

        const short int *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        short int *C0.

            Address for dot product of A0 and B.

        short int *C1.

            Address for dot product of A1 and B.

        vDSP_Length N.

            Number of elements in each vector.

    Output:

        The results are written to *C0 and *C1.
*/
void vDSP_dotpr2_s1_15(
    const short int *__A0, vDSP_Stride __IA0,
    const short int *__A1, vDSP_Stride __IA1,
    const short int *__B,  vDSP_Stride __IB,
    short int       *__C0,
    short int       *__C1,
    vDSP_Length      __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_dotpr_s8_24, vector integer 8.24 format dot product.

    Function:

        This routine calculates the dot product of A with B.

        In pseudocode, the operation is:

            sum = 0;
            for (i = 0; i < N; ++i)
            {
                sum0 += A[i*AStride] * B[i*BStride];
            }
            *C = sum;

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *A, vDSP_Stride AStride.

            Starting address and stride for input vector A.

        const int *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        int *C.

            Address for dot product of A and B.

        vDSP_Length N.

            Number of elements in each vector.

    Output:

        The result is written to *C.
*/
void vDSP_dotpr_s8_24(
    const int  *__A, vDSP_Stride __IA,
    const int  *__B, vDSP_Stride __IB,
    int        *__C,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_dotpr2_s8_24, vector integer 8.24 format stereo dot product.

    Function:

        This routine calculates the dot product of A0 with B and the dot
        product of A1 with B.  This is functionally equivalent to calculating
        two dot products but might execute faster.

        In pseudocode, the operation is:

            sum0 = 0;
            sum1 = 0;
            for (i = 0; i < N; ++i)
            {
                sum0 += A0[i*A0Stride] * B[i*BStride];
                sum1 += A1[i*A1Stride] * B[i*BStride];
            }
            *C0 = sum0;
            *C1 = sum1;

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *A0, vDSP_Stride A0Stride.

            Starting address and stride for input vector A0.

        const int *A1, vDSP_Stride A1Stride.

            Starting address and stride for input vector A1.

        const int *B,  vDSP_Stride BStride.

            Starting address and stride for input vector B.

        int *C0.

            Address for dot product of A0 and B.

        int *C1.

            Address for dot product of A1 and B.

        vDSP_Length N.

            Number of elements in each vector.

    Output:

        The results are written to *C0 and *C1.
*/
void vDSP_dotpr2_s8_24(
    const int  *__A0, vDSP_Stride __IA0,
    const int  *__A1, vDSP_Stride __IA1,
    const int  *__B,  vDSP_Stride __IB,
    int        *__C0,
    int        *__C1,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vaddsub, vector single-precision add and subtract.

    Adds vector I0 to vector I1 and leaves the result in vector O0.
    Subtracts vector I0 from vector I1 and leaves the result in vector O1.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            float i1 = I1[i*I1S], i0 = I0[i*I0S];
            O0[i*O0S] = i1 + i0;
            O1[i*O1S] = i1 - i0;
        }

    Input:

        const float *I0, const float *I1, vDSP_Stride I0S, vDSP_Stride I1S.

            Starting addresses of both inputs and strides for the input vectors.

        float *O0, float *O1, vDSP_Stride O0S, vDSP_Stride O1S.

            Starting addresses of both outputs and strides for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

    In-Place Operation:

        Either of O0 and/or O1 may equal I0 and/or I1, but O0 may not equal
        O1.  Otherwise, no overlap is permitted between any of the buffers.
*/
void vDSP_vaddsub(
    const float *__I0, vDSP_Stride __I0S,
    const float *__I1, vDSP_Stride __I1S,
          float *__O0, vDSP_Stride __O0S,
          float *__O1, vDSP_Stride __O1S,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vaddsubD, vector double-precision add and subtract.

    Adds vector I0 to vector I1 and leaves the result in vector O0.
    Subtracts vector I0 from vector I1 and leaves the result in vector O1.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            double i1 = I1[i*I1S], i0 = I0[i*I0S];
            O0[i*O0S] = i1 + i0;
            O1[i*O1S] = i1 - i0;
        }

    Input:

        const double *I0, const double *I1, vDSP_Stride I0S, vDSP_Stride I1S.

            Starting addresses of both inputs and strides for the input vectors.

        double *O0, double *O1, vDSP_Stride O0S, vDSP_Stride O1S.

            Starting addresses of both outputs and strides for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

    In-Place Operation:

        Either of O0 and/or O1 may equal I0 and/or I1, but O0 may not equal
        O1.  Otherwise, no overlap is permitted between any of the buffers.
*/
void vDSP_vaddsubD(
    const double *__I0, vDSP_Stride __I0S,
    const double *__I1, vDSP_Stride __I1S,
          double *__O0, vDSP_Stride __O0S,
          double *__O1, vDSP_Stride __O1S,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vrampmul, vector single-precision vramp and multiply.

    This routine puts into O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp
    in a consecutive call.  To continue the ramp smoothly, the new value of
    *Step includes rounding errors accumulated during the routine rather than
    being calculated directly as *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] = *Start * I[i*IS];
            *Start += *Step;
        }

    Input:

        const float *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        float *Start.

            Starting value for the ramp.

        const float *Step.

            Value of the step for the ramp.

        float *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmul(
    const float *__I, vDSP_Stride __IS,
    float *__Start,
    const float *__Step,
    float *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmulD, vector double-precision vramp and multiply.

    This routine puts into O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp
    in a consecutive call.  To continue the ramp smoothly, the new value of
    *Step includes rounding errors accumulated during the routine rather than
    being calculated directly as *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] = *Start * I[i*IS];
            *Start += *Step;
        }

    Input:

        const double *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        double *Start.

            Starting value for the ramp.

        const double *Step.

            Value of the step for the ramp.

        double *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmulD(
    const double *__I, vDSP_Stride __IS,
    double *__Start,
    const double *__Step,
    double *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vrampmuladd, vector single-precision vramp, multiply and add.

    This routine adds to O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp in a
    consecutive call.  To continue the ramp smoothly, the new value of *Step
    includes rounding errors accumulated during the routine rather than being
    calculated directly as *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] += *Start * I[i*IS];
            *Start += *Step;
        }

    Input:

        const float *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        float *Start.

            Starting value for the ramp.

        const float *Step.

            Value of the step for the ramp.

        float *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are added to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladd(
    const float *__I, vDSP_Stride __IS,
    float *__Start,
    const float *__Step,
    float *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladdD, vector double-precision vramp, multiply and add.

    This routine adds to O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp in a
    consecutive call.  To continue the ramp smoothly, the new value of *Step
    includes rounding errors accumulated during the routine rather than being
    calculated directly as *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] += *Start * I[i*IS];
            *Start += *Step;
        }

    Input:

        const double *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        double *Start.

            Starting value for the ramp.

        const double *Step.

            Value of the step for the ramp.

        double *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are added to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladdD(
    const double *__I, vDSP_Stride __IS,
          double *__Start,
    const double *__Step,
          double *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vrampmul2, stereo vector single-precision vramp and multiply.

    This routine:

        Puts into O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Puts into O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.  To continue
    the ramp smoothly, the new value of *Step includes rounding errors
    accumulated during the routine rather than being calculated directly as
    *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] = *Start * I0[i*IS];
            O1[i*OS] = *Start * I1[i*IS];
            *Start += *Step;
        }

    Input:

        const float *I0, const float *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        float *Start.

            Starting value for the ramp.

        const float *Step.

            Value of the step for the ramp.

        float *O0, float *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmul2(
    const float *__I0, const float *__I1, vDSP_Stride __IS,
    float *__Start,
    const float *__Step,
    float *__O0, float *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmul2D, stereo vector double-precision vramp and multiply.

    This routine:

        Puts into O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Puts into O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.  To continue
    the ramp smoothly, the new value of *Step includes rounding errors
    accumulated during the routine rather than being calculated directly as
    *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] = *Start * I0[i*IS];
            O1[i*OS] = *Start * I1[i*IS];
            *Start += *Step;
        }

    Input:

        const double *I0, const double *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        double *Start.

            Starting value for the ramp.

        const double *Step.

            Value of the step for the ramp.

        double *O0, double *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmul2D(
    const double *__I0, const double *__I1, vDSP_Stride __IS,
          double *__Start,
    const double *__Step,
          double *__O0, double *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vrampmuladd2, stereo vector single-precision vramp, multiply and add.

    This routine:

        Adds to O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Adds to O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.  To continue
    the ramp smoothly, the new value of *Step includes rounding errors
    accumulated during the routine rather than being calculated directly as
    *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] += *Start * I0[i*IS];
            O1[i*OS] += *Start * I1[i*IS];
            *Start += *Step;
        }

    Input:

        const float *I0, const float *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        float *Start.

            Starting value for the ramp.

        const float *Step.

            Value of the step for the ramp.

        float *O0, float *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladd2(
    const float *__I0, const float *__I1, vDSP_Stride __IS,
    float *__Start,
    const float *__Step,
    float *__O0, float *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladd2D, stereo vector double-precision vramp, multiply and add.

    This routine:

        Adds to O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Adds to O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.  To continue
    the ramp smoothly, the new value of *Step includes rounding errors
    accumulated during the routine rather than being calculated directly as
    *Start + N * *Step.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] += *Start * I0[i*IS];
            O1[i*OS] += *Start * I1[i*IS];
            *Start += *Step;
        }

    Input:

        const double *I0, const double *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        double *Start.

            Starting value for the ramp.

        const double *Step.

            Value of the step for the ramp.

        double *O0, double *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladd2D(
    const double *__I0, const double *__I1, vDSP_Stride __IS,
    double *__Start,
    const double *__Step,
    double *__O0, double *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.10), ios(8.0));


/*  vDSP_vrampmul_s1_15, vector integer 1.15 format vramp and multiply.

    This routine puts into O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp
    in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] = *Start * I[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        short int *Start.

            Starting value for the ramp.

        const short int *Step.

            Value of the step for the ramp.

        short int *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmul_s1_15(
    const short int *__I, vDSP_Stride __IS,
    short int *__Start,
    const short int *__Step,
    short int *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladd_s1_15, vector integer 1.15 format vramp, multiply and add.

    This routine adds to O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp in a
    consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] += *Start * I[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        short int *Start.

            Starting value for the ramp.

        const short int *Step.

            Value of the step for the ramp.

        short int *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are added to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladd_s1_15(
    const short int *__I, vDSP_Stride __IS,
    short int *__Start,
    const short int *__Step,
    short int *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmul2_s1_15, stereo vector integer 1.15 format vramp and multiply.

    This routine:

        Puts into O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Puts into O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] = *Start * I0[i*IS];
            O1[i*OS] = *Start * I1[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *I0, const short int *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        short int *Start.

            Starting value for the ramp.

        const short int *Step.

            Value of the step for the ramp.

        short int *O0, short int *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output
            vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.

*/
void vDSP_vrampmul2_s1_15(
    const short int *__I0, const short int *__I1, vDSP_Stride __IS,
    short int *__Start,
    const short int *__Step,
    short int *__O0, short int *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladd2_s1_15, stereo vector integer 1.15 format vramp, multiply
    and add.

    This routine:

        Adds to O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Adds to O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] += *Start * I0[i*IS];
            O1[i*OS] += *Start * I1[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with one sign bit and 15
    fraction bits.  Where the value of the short int is normally x, it is
    x/32768 for the purposes of this routine.

    Input:

        const short int *I0, const short int *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        short int *Start.

            Starting value for the ramp.

        const short int *Step.

            Value of the step for the ramp.

        short int *O0, short int *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output
            vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are added to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.

*/
void vDSP_vrampmuladd2_s1_15(
    const short int *__I0, const short int *__I1, vDSP_Stride __IS,
    short int *__Start,
    const short int *__Step,
    short int *__O0, short int *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmul_s8_24, vector integer 8.24 format vramp and multiply.

    This routine puts into O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp
    in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] = *Start * I[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        int *Start.

            Starting value for the ramp.

        const int *Step.

            Value of the step for the ramp.

        int *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmul_s8_24(
    const int *__I, vDSP_Stride __IS,
    int *__Start,
    const int *__Step,
    int *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladd_s8_24, vector integer 8.24 format vramp, multiply and add.

    This routine adds to O the product of I and a ramp function with initial
    value *Start and slope *Step.  *Start is updated to continue the ramp in a
    consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O[i*OS] += *Start * I[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *I, vDSP_Stride IS.

            Starting address and stride for the input vector.

        int *Start.

            Starting value for the ramp.

        const int *Step.

            Value of the step for the ramp.

        int *O, vDSP_Stride OS.

            Starting address and stride for the output vector.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are added to O.

        On return, *Start contains initial *Start + N * *Step.
*/
void vDSP_vrampmuladd_s8_24(
    const int *__I, vDSP_Stride __IS,
    int *__Start,
    const int *__Step,
    int *__O, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmul2_s8_24, stereo vector integer 8.24 format vramp and multiply.

    This routine:

        Puts into O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Puts into O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] = *Start * I0[i*IS];
            O1[i*OS] = *Start * I1[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *I0, const int *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        int *Start.

            Starting value for the ramp.

        const int *Step.

            Value of the step for the ramp.

        int *O0, int *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output
            vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.

*/
void vDSP_vrampmul2_s8_24(
    const int *__I0, const int *__I1, vDSP_Stride __IS,
    int *__Start,
    const int *__Step,
    int *__O0, int *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  vDSP_vrampmuladd2_s8_24, stereo vector integer 8.24 format vramp, multiply
    and add.

    This routine:

        Adds to O0 the product of I0 and a ramp function with initial value
        *Start and slope *Step.

        Adds to O1 the product of I1 and a ramp function with initial value
        *Start and slope *Step.

    *Start is updated to continue the ramp in a consecutive call.

    This routine calculates:

        for (i = 0; i < N; ++i)
        {
            O0[i*OS] += *Start * I0[i*IS];
            O1[i*OS] += *Start * I1[i*IS];
            *Start += *Step;
        }

    The elements are fixed-point numbers, each with eight integer bits
    (including sign) and 24 fraction bits.  Where the value of the int is
    normally x, it is x/16777216 for the purposes of this routine.

    Input:

        const int *I0, const int *I1, vDSP_Stride IS.

            Starting addresses of both inputs and stride for the input vectors.

        int *Start.

            Starting value for the ramp.

        const int *Step.

            Value of the step for the ramp.

        int *O0, int *O1, vDSP_Stride OS.

            Starting addresses of both outputs and stride for the output
            vectors.

        vDSP_Length Length.

            Number of elements in each vector.

    Output:

        The results are written to O0 and O1.

        On return, *Start contains initial *Start + N * *Step.

*/
void vDSP_vrampmuladd2_s8_24(
    const int *__I0, const int *__I1, vDSP_Stride __IS,
    int *__Start,
    const int *__Step,
    int *__O0, int *__O1, vDSP_Stride __OS,
    vDSP_Length __N)
        API_AVAILABLE(macos(10.6), ios(4.0));


/*  When compiling for i386 on OS X 10.11 or later, the old vDSP routine names
    are deprecated.
*/
#if defined vDSP_DeprecateTranslations

extern FFTSetup create_fftsetup(
    vDSP_Length __Log2n,
    FFTRadix    __Radix)
		API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
    API_UNAVAILABLE(ios, watchos, tvos);

extern void destroy_fftsetup(FFTSetup __setup)
		API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
    API_UNAVAILABLE(ios, watchos, tvos);
extern void ctoz(
    const DSPComplex      *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Z,
    vDSP_Stride            __IZ,
    vDSP_Length            __N)
		API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
    API_UNAVAILABLE(ios, watchos, tvos);
extern void ztoc(
    const DSPSplitComplex *__Z,
    vDSP_Stride            __IZ,
    DSPComplex            *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC1,
    vDSP_Stride            __IC0,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA0,
    vDSP_Stride            __IA1,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC0,
    vDSP_Stride            __IC1,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N0,
    vDSP_Length            __Log2N1,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft3_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11), ios(4.0, 9.0))
        API_UNAVAILABLE(watchos, tvos);
extern void fft5_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __Log2N,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11), ios(4.0, 9.0))
        API_UNAVAILABLE(watchos, tvos);
extern void fftm_zop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zopt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zipt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zrop(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zropt(
    FFTSetup               __Setup,
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    vDSP_Stride            __IMA,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IMC,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zrip(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zript(
    FFTSetup               __Setup,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Stride            __IM,
    const DSPSplitComplex *__Buffer,
    vDSP_Length            __Log2N,
    vDSP_Length            __M,
    FFTDirection           __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void f3x3(
    const float *__A,
    vDSP_Length  __NR,
    vDSP_Length  __NC,
    const float *__F,
    float       *__C)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void f5x5(
    const float *__A,
    vDSP_Length  __NR,
    vDSP_Length  __NC,
    const float *__F,
    float       *__C)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void conv(
    const float *__A,  // Input signal.
    vDSP_Stride  __IA,
    const float *__F,  // Filter.
    vDSP_Stride  __IF,
    float       *__C,  // Output signal.
    vDSP_Stride  __IC,
    vDSP_Length  __N,  // Output length.
    vDSP_Length  __P)  // Filter length.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void dotpr(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void imgfir(
    const float *__A,  // Input.
    vDSP_Length  __NR, // Number of image rows.
    vDSP_Length  __NC, // Number of image columns.
    const float *__F,  // Filter.
    float       *__C,  // Output.
    vDSP_Length  __P,  // Number of filter rows.
    vDSP_Length  __Q)  // Number of filter columns.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void mtrans(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __M,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void mmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __M,
    vDSP_Length  __N,
    vDSP_Length  __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vadd(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsub(
    const float *__B,  // Caution:  A and B are swapped!
    vDSP_Stride  __IB,
    const float *__A,  // Caution:  A and B are swapped!
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsmul(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vam(
    const float *__A,
    vDSP_Stride  __IA,
    const float *__B,
    vDSP_Stride  __IB,
    const float *__C,
    vDSP_Stride  __IC,
    float       *__D,
    vDSP_Stride  __ID,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vssq(
    const float *__A,
    vDSP_Stride  __IA,
    float       *__C,
    vDSP_Stride  __IC,
    vDSP_Length  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvadd(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvsub(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zdotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zconv(
    const DSPSplitComplex *__A,  // Input signal.
    vDSP_Stride            __IA,
    const DSPSplitComplex *__F,  // Filter.
    vDSP_Stride            __IF,
    const DSPSplitComplex *__C,  // Output signal.
    vDSP_Stride            __IC,
    vDSP_Length            __N,  // Output length.
    vDSP_Length            __P)  // Filter length.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvcma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N,
    int                    __Conjugate)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zidotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmma(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmms(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmsm(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    const DSPSplitComplex *__D,
    vDSP_Stride            __ID,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const DSPSplitComplex *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __M,
    vDSP_Length            __N,
    vDSP_Length            __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvadd(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvmul(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvsub(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Stride            __IC,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrdotpr(
    const DSPSplitComplex *__A,
    vDSP_Stride            __IA,
    const float           *__B,
    vDSP_Stride            __IB,
    const DSPSplitComplex *__C,
    vDSP_Length            __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.0, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __flag)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __flag)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft2d_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA0,
    vDSP_Stride                  __IA1,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC0,
    vDSP_Stride                  __IC1,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N0,
    vDSP_Length                  __Log2N1,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zipD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_ziptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zoptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zripD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zriptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IM,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zropD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fftm_zroptD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    vDSP_Stride                  __IMA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Stride                  __IMC,
    const DSPDoubleSplitComplex *__Buffer,
    vDSP_Length                  __Log2N,
    vDSP_Length                  __M,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void fft3_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11), ios(4.0, 9.0))
        API_UNAVAILABLE(watchos, tvos);
extern void fft5_zopD(
    FFTSetupD                    __Setup,
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __Log2N,
    FFTDirection                 __Direction)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11), ios(4.0, 9.0))
        API_UNAVAILABLE(watchos, tvos);
extern void ctozD(
    const DSPDoubleComplex      *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__Z,
    vDSP_Stride                  __IZ,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void ztocD(
    const DSPDoubleSplitComplex *__Z,
    vDSP_Stride                  __IZ,
    DSPDoubleComplex            *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern FFTSetupD create_fftsetupD(
    vDSP_Length __Log2n,
    FFTRadix    __Radix)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void destroy_fftsetupD(FFTSetupD __setup)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void f3x3D(
    const double *__A,
    vDSP_Length   __NR,
    vDSP_Length   __NC,
    const double *__F,
    double       *__C)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void f5x5D(
    const double *__A,
    vDSP_Length   __NR,
    vDSP_Length   __NC,
    const double *__F,
    double       *__C)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void convD(
    const double *__A, // Input signal.
    vDSP_Stride   __IA,
    const double *__F, // Filter
    vDSP_Stride   __IF,
    double       *__C, // Output signal.
    vDSP_Stride   __IC,
    vDSP_Length   __N, // Output length.
    vDSP_Length   __P) // Filter length.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void dotprD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void imgfirD(
    const double *__A,  // Input.
    vDSP_Length   __NR, // Number of image rows.
    vDSP_Length   __NC, // Number of image columns.
    const double *__F,  // Filter.
    double       *__C,  // Output.
    vDSP_Length   __P,  // Number of filter rows.
    vDSP_Length   __Q)  // Number of filter columns.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void mtransD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __M,
    vDSP_Length   __N)
            API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
            API_UNAVAILABLE(ios, watchos, tvos);
extern void mmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __M,
    vDSP_Length   __N,
    vDSP_Length   __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vaddD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsubD(
    const double *__B, // Caution:  A and B are swapped!
    vDSP_Stride   __IB,
    const double *__A, // Caution:  A and B are swapped!
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vmulD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vamD(
    const double *__A,
    vDSP_Stride   __IA,
    const double *__B,
    vDSP_Stride   __IB,
    const double *__C,
    vDSP_Stride   __IC,
    double       *__D,
    vDSP_Stride   __IDD,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vsqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void vssqD(
    const double *__A,
    vDSP_Stride   __IA,
    double       *__C,
    vDSP_Stride   __IC,
    vDSP_Length   __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvaddD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvsubD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zdotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zconvD(
    const DSPDoubleSplitComplex *__A,    // Input signal.
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__F,    // Filter.
    vDSP_Stride                  __IF,
    const DSPDoubleSplitComplex *__C,    // Output signal.
    vDSP_Stride                  __IC,
    vDSP_Length                  __N,    // Output length.
    vDSP_Length                  __P)    // Filter length.
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvcmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zvmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N,
    int                          __Conjugate)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zidotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmmaD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmmsD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmsmD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    const DSPDoubleSplitComplex *__D,
    vDSP_Stride                  __ID,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zmmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const DSPDoubleSplitComplex *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __M,
    vDSP_Length                  __N,
    vDSP_Length                  __P)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvaddD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvmulD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrvsubD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Stride                  __IC,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);
extern void zrdotprD(
    const DSPDoubleSplitComplex *__A,
    vDSP_Stride                  __IA,
    const double                *__B,
    vDSP_Stride                  __IB,
    const DSPDoubleSplitComplex *__C,
    vDSP_Length                  __N)
        API_DEPRECATED("No longer supported, use vDSP_ prefixed version instead.", macos(10.2, 10.11))
        API_UNAVAILABLE(ios, watchos, tvos);

#endif  //  #if defined vDSP_DeprecateTranslations


#ifndef USE_NON_APPLE_STANDARD_DATATYPES
#define USE_NON_APPLE_STANDARD_DATATYPES 1
#endif  /* !defined(USE_NON_APPLE_STANDARD_DATATYPES) */

#if USE_NON_APPLE_STANDARD_DATATYPES
enum {
    FFT_FORWARD = kFFTDirection_Forward,
    FFT_INVERSE = kFFTDirection_Inverse
};

enum {
    FFT_RADIX2  = kFFTRadix2,
    FFT_RADIX3  = kFFTRadix3,
    FFT_RADIX5  = kFFTRadix5
};

typedef DSPComplex                      COMPLEX;
typedef DSPSplitComplex                 COMPLEX_SPLIT;
typedef DSPDoubleComplex                DOUBLE_COMPLEX;
typedef DSPDoubleSplitComplex           DOUBLE_COMPLEX_SPLIT;
#endif  /* USE_NON_APPLE_STANDARD_DATATYPES */


#pragma options align=reset


#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif


#ifdef __cplusplus
}
#endif


#endif // __VDSP__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vBasicOps.h
/*
     File:       vecLib/vBasicOps.h
 
     Contains:   Basic Algebraic Operations for AltiVec
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 1999-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
*/
#ifndef __VBASICOPS__
#define __VBASICOPS__

#include <stdint.h>

#include "vecLibTypes.h"

#include <os/availability.h>

#if PRAGMA_ONCE
#pragma once
#endif

#ifdef __cplusplus
extern "C" {
#endif


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#else
    #define __nullable
    #define __nonnull
#endif


#if defined(__ppc__) || defined(__ppc64__) || defined(__i386__) || defined(__x86_64__)
#if defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE2__

/*                                                                                  
  This section is a collection of algebraic functions that uses the AltiVec       
  instruction set, and is designed to facilitate vector processing in             
  mathematical programming. Following table indicates which functions are covered
  by AltiVec instruction set and which ones are performed by vBasicOps library:

Legend:
    H/W   = Hardware
    LIB  = vBasicOps Library
    NRel  = Next Release of vBasicOps Library
    N/A   = Not Applicable

+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
| Data Type/    | U8  | S8  | U16 | S16 | U32 | S32 | U64 | S64 | U128 | S128 |
| Function      |     |     |     |     |     |     |     |     |      |      |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    Add        | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  |  LIB |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    AddS       | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    Sub        | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    SubS       | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|  Mul(Half)    | LIB | LIB | LIB | LIB | LIB | LIB | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|Mul Even (Full)| H/W | H/W | H/W | H/W | LIB | LIB | LIB | LIB |  N/A |  N/A |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|Mul Odd  (Full)| H/W | H/W | H/W | H/W | LIB | LIB | LIB | LIB |  N/A |  N/A |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    Divide     | LIB | LIB | LIB | LIB | LIB | LIB | LIB |NRel | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    Shift      | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+
|    Rotate     | H/W | H/W | H/W | H/W | H/W | H/W | LIB | LIB | LIB  | LIB  |
+---------------+-----+-----+-----+-----+-----+-----+-----+-----+------+------+



Following is a short description of functions in this section:
                                                                         
      Add:      It takes two vectors of data elements and adds each element         
                of the second vector to the corresponding element of the first      
                vector and puts the result in the associated data element of the    
                destination register.

      Subtract: It takes two vectors of data elements and subtracts each element    
                of the second vector from the corresponding element of the first    
                vector and puts the result in the associated data element of the    
                destination register.

      Multiply: It takes two vectors of data elements and multiplies each element   
                of the first vector by the corresponding element of the second      
                vector and puts the result in the associated data element of the    
                destination register. 

      Divide:   It takes two vectors of data elements and divides each element      
                of the first vector by the corresponding element of the second      
                vector and puts the result in the associated data element of the    
                destination register. A pointer is passed to the function to get   
                the remainder.

      Shift:    It takes a vector of two 64-bit data elements or one 128-bit
                data element and shifts it to right or left, in a logical or 
                algebraic manner, using a shift factor that is passed as an
                arguement to the function.

      Rotate:   It takes a vector of two 64-bit data elements or one 128-bit
                data element and rotates it to right or left, using a shift 
               factor that is passed as an arguement to the function.


   Following abbreviations are used in the names of functions in this section:   
                                                                                 
      v            Vector                                                        
      U            Unsigned                                                      
      S            Signed                                                        
      8            8-bit                                                         
      16           16-bit                                                        
      32           32-bit                                                        
      64           64-bit                                                        
      128          128-bit                                                       
      Add          Addition                                                      
      AddS         Addition with Saturation                                      
      Sub          Subtraction                                                   
      SubS         Subtraction with Saturation                                   
      Mul          Multiplication                                                
      Divide       Division                                                      
      Half         Half (multiplication, width of result is the same as width of 
                      operands)                                                  
      Full         Full (multiplication, width of result is twice width of each  
                      operand)                                                   
      Even         Multiplication is performed on EVEN data elements of vector   
                      (Please note that Big endian is used. So the left-most     
                      data element is labled as element 0)                       
      Odd          Multiplication is performed on ODD  data elements of vector.  
      A            Algebraic      
      LL           Logical Left     
      LR           Logical Right     
      Shift        Shift by one factor     
      Shift2       Shift by two factors( only apply to 64 bit operation )     
      Rotate       Rotate by one factor     
      Rotate2      Rotate by two factors( only apply to 64 bit operation )     
                                                                                 
*/

// There are certain routines (namely vS64Add and vU64Add) with 1
// instruction implementations. There is no point in having a function
// call occur and then return after executing 1 instruction. Thus we
// introduce this conditional define to allow for certain inline
// attributes to be defined.
// However, as we still include these symbols in they dylib for
// backwards compatability, they must not be inline for the tapi
// installapi pass.
#if defined __SSE2__

#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
	#include <immintrin.h>
    _Pragma("clang assume_nonnull begin")
#else
	#include <immintrin.h>
#endif

#define __VBASICOPS_INLINE_ATTR__ __attribute__((__always_inline__, __nodebug__))
#endif // defined __SSE2__


/*
 *  vU8Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt8 
vU8Divide(
  vUInt8    vN,
  vUInt8    vD,
  vUInt8 * __nullable vRemainder)
	API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);

/*
 *  vS8Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt8 
vS8Divide(
  vSInt8    vN,
  vSInt8    vD,
  vSInt8 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU16Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt16 
vU16Divide(
  vUInt16    vN,
  vUInt16    vD,
  vUInt16 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS16Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt16 
vS16Divide(
  vSInt16    vN,
  vSInt16    vD,
  vSInt16 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU32Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU32Divide(
  vUInt32    vN,
  vUInt32    vD,
  vUInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS32Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS32Divide(
  vSInt32    vN,
  vSInt32    vD,
  vSInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU64Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64Divide(
  vUInt32    vN,
  vUInt32    vD,
  vUInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64Divide(
  vSInt32    vN,
  vSInt32    vD,
  vSInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128Divide(
  vUInt32    vN,
  vUInt32    vD,
  vUInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS128Divide()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128Divide(
  vSInt32    vN,
  vSInt32    vD,
  vSInt32 * __nullable vRemainder)
    API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vU8HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt8 
vU8HalfMultiply(
  vUInt8   vA,
  vUInt8   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS8HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt8 
vS8HalfMultiply(
  vSInt8   vA,
  vSInt8   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU16HalfMultiply()
 *  
 *  Currently on Intel, we inline a one instruction implementation of vU16HalfMultiply.
 *  An implementation is also exported in the library for legacy applications.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vUInt16 __VBASICOPS_INLINE_ATTR__
vU16HalfMultiply(
  vUInt16   __vbasicops_vA,
  vUInt16   __vbasicops_vB) { return _mm_mullo_epi16(__vbasicops_vA, __vbasicops_vB); }
#else // defined __SSE2__ && !defined __clang_tapi__
extern vUInt16 
vU16HalfMultiply(
  vUInt16   vA,
  vUInt16   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif // defined __SSE2__ && !defined __clang_tapi__


/*
 *  vS16HalfMultiply()
 *  
 *  On Intel, this function has a one instruction implementation that we inline. An
 *  implementation is also available via an exported symbol in the library for legacy
 *  applications.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vSInt16 __VBASICOPS_INLINE_ATTR__
vS16HalfMultiply(
  vSInt16   __vbasicops_vA,
  vSInt16   __vbasicops_vB) { return _mm_mullo_epi16(__vbasicops_vA, __vbasicops_vB); }
#else // defined __SSE2__ && !defined __clang_tapi__
extern vSInt16 
vS16HalfMultiply(
  vSInt16   vA,
  vSInt16   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif // defined __SSE2__ && !defined __clang_tapi__


/*
 *  vU32HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU32HalfMultiply(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS32HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS32HalfMultiply(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU32FullMulEven()
 *  
 *  Currently we use a 3 instruction inlined implementation for vU32FullMulEven
 *  on Intel. Note that for legacy applications, there is still a compiled 
 *  implementation available in the library via an exported symbol.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined (__SSE2__) && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vU32FullMulEven(
  vUInt32   __vbasicops_vA,
  vUInt32   __vbasicops_vB)
{
    __vbasicops_vA = _mm_srli_epi64(__vbasicops_vA, 32);
    __vbasicops_vB = _mm_srli_epi64(__vbasicops_vB, 32);
    return _mm_mul_epu32(__vbasicops_vA, __vbasicops_vB);
}
#else // defined (__SSE2__) && !defined __clang_tapi__
extern vUInt32 
vU32FullMulEven(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif // defined (__SSE2__) && !defined __clang_tapi__


/*
 *  vU32FullMulOdd()
 *  
 *  Currently on Intel, we inline a one instruction implementation of vU32FullMulOdd.
 *  An implementation is also exported in the library for legacy applications.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vU32FullMulOdd(
  vUInt32   __vbasicops_vA,
  vUInt32   __vbasicops_vB) { return _mm_mul_epu32(__vbasicops_vA, __vbasicops_vB); }
#else // defined __SSE2__ && !defined __clang_tapi__
extern vUInt32 
vU32FullMulOdd(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif // defined __SSE2__ && !defined __clang_tapi__


/*
 *  vS32FullMulEven()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS32FullMulEven(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS32FullMulOdd()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS32FullMulOdd(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU64FullMulEven()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64FullMulEven(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU64FullMulOdd()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64FullMulOdd(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU64HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64HalfMultiply(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64HalfMultiply(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64FullMulEven()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64FullMulEven(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64FullMulOdd()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64FullMulOdd(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128HalfMultiply(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS128HalfMultiply()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128HalfMultiply(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vU64Sub()
 *  
 *  When SSE2 code generation is enabled on Intel architectures,
 *  vU64Sub has a single instruction implementation provided by an
 *  inlined function. On other architectures, the extern declaration
 *  will be provided and on all architectures, an exported vU64Sub
 *  routine is provided. This ensures that legacy applications are
 *  supported on Intel.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vU64Sub(
  vUInt32   __vbasicops_vA,
  vUInt32   __vbasicops_vB) { return _mm_sub_epi64(__vbasicops_vA, __vbasicops_vB); }
#else	//	defined __SSE2__ && !defined __clang_tapi__
extern vUInt32 
vU64Sub(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif	//	defined __SSE2__ && !defined __clang_tapi__


/*
 *  vU64SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64SubS(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128Sub(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128SubS(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64Sub()
 *
 *  When SSE2 code generation is enabled on Intel architectures,
 *  vS64Sub has a single instruction implementation provided by an
 *  inlined function. On other architectures, the extern declaration
 *  will be provided and on all architectures, an exported vS64Sub
 *  routine is provided. This ensures that legacy applications are
 *  supported on Intel.
 *
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vSInt32 __VBASICOPS_INLINE_ATTR__
vS64Sub(
  vSInt32   __vbasicops_vA,
  vSInt32   __vbasicops_vB) { return _mm_sub_epi64(__vbasicops_vA, __vbasicops_vB); }
#else	//	defined __SSE2__ && !defined __clang_tapi__
extern vSInt32 
vS64Sub(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif	//	defined __SSE2__ && !defined __clang_tapi__


/*
 *  vS128Sub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128Sub(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64SubS(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS128SubS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128SubS(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vU64Add()
 *  
 *  When SSE2 code generation is enabled on Intel architectures, single-instruction
 *  implementations of vU64Add is inlined instead of making an external function call. 
 * 
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined (__SSE2__) && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vU64Add(
  vUInt32   __vbasicops_vA,
  vUInt32   __vbasicops_vB) { return _mm_add_epi64(__vbasicops_vA, __vbasicops_vB); }
#else	//	defined __SSE2__ && !defined __clang_tapi__
extern vUInt32
vU64Add(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif	//	defined __SSE2__ && !defined __clang_tapi__

/*
 *  vU64AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU64AddS(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128Add(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU128AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vU128AddS(
  vUInt32   vA,
  vUInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64Add()
 *
 *  When SSE2 code generation is enabled on Intel architectures, single-instruction
 *  implementations of vS64Add is inlined instead of making an external function call. 
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined __SSE2__ && !defined __clang_tapi__
static __inline__ vSInt32 __VBASICOPS_INLINE_ATTR__
vS64Add(
  vSInt32   __vbasicops_vA,
  vSInt32   __vbasicops_vB) { return _mm_add_epi64(__vbasicops_vA, __vbasicops_vB); }
#else // defined __SSE2__ && !defined __clang_tapi__
extern vSInt32 
vS64Add(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif // defined __SSE2__ && !defined __clang_tapi__


/*
 *  vS64AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS64AddS(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS128Add()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128Add(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS128AddS()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vSInt32 
vS128AddS(
  vSInt32   vA,
  vSInt32   vB) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vU64Neg()
 */  
extern vUInt32 
vU64Neg (
  vUInt32   vA) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS64Neg()
 */  
extern vSInt32 
vS64Neg (
  vSInt32   vA) API_AVAILABLE(macos(10.5)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vU1284Neg()
 */  
extern vUInt32 
vU128Neg (
  vUInt32   vA) API_AVAILABLE(macos(10.5)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vS1284Neg()
 */  
extern vSInt32 
vS128Neg (
  vSInt32   vA) API_AVAILABLE(macos(10.5)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vLL64Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined (__SSE2__) && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vLL64Shift(
  vUInt32   __vbasicops_vA,
  vUInt8    __vbasicops_vShiftFactor)
{
    return _mm_sll_epi64(__vbasicops_vA,
                         _mm_and_si128(__vbasicops_vShiftFactor, _mm_cvtsi32_si128( 0x3F )));
}
#else	//	defined __SSE2__ && !defined __clang_tapi__
extern vUInt32 
vLL64Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif	//	defined __SSE2__ && !defined __clang_tapi__

/*
 *  vA64Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vA64Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vLR64Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
#if defined (__SSE2__) && !defined __clang_tapi__
static __inline__ vUInt32 __VBASICOPS_INLINE_ATTR__
vLR64Shift(
    vUInt32   __vbasicops_vA,
    vUInt8    __vbasicops_vShiftFactor)
{
    return _mm_srl_epi64(__vbasicops_vA,
                         _mm_and_si128(__vbasicops_vShiftFactor, _mm_cvtsi32_si128( 0x3F )));
}
#else	//	defined __SSE2__ && !defined __clang_tapi__
extern vUInt32 
vLR64Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);
#endif	//	defined __SSE2__ && !defined __clang_tapi__

/*
 *  vLL64Shift2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vLL64Shift2(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vA64Shift2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vA64Shift2(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vLR64Shift2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vLR64Shift2(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vLL128Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vLL128Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.5)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vLR128Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vLR128Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.5)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vA128Shift()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vA128Shift(
  vUInt32   vA,
  vUInt8    vShiftFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);



/*
 *  vL64Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vL64Rotate(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vR64Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vR64Rotate(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vL64Rotate2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vL64Rotate2(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vR64Rotate2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vR64Rotate2(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vL128Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vL128Rotate(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


/*
 *  vR128Rotate()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern vUInt32 
vR128Rotate(
  vUInt32   vA,
  vUInt8    vRotateFactor) API_AVAILABLE(macos(10.0)) API_UNAVAILABLE(ios, watchos, tvos);


#endif  // defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE2__

#endif  /* defined(__ppc__) || defined(__ppc64__) || defined(__i386__) || defined(__x86_64__) */


#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif


#ifdef __cplusplus
}
#endif

#endif /* __VBASICOPS__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/LinearAlgebra.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LINEAR_ALGEBRA_PUBLIC_HEADER__
#define __LINEAR_ALGEBRA_PUBLIC_HEADER__

#ifdef __cplusplus
extern "C" {
#endif

#include <vecLib/LinearAlgebra/base.h>
#include <vecLib/LinearAlgebra/object.h>
#include <vecLib/LinearAlgebra/matrix.h>
#include <vecLib/LinearAlgebra/vector.h>
#include <vecLib/LinearAlgebra/splat.h>
#include <vecLib/LinearAlgebra/arithmetic.h>
#include <vecLib/LinearAlgebra/linear_systems.h>
#include <vecLib/LinearAlgebra/norms.h>

#ifdef __cplusplus
}
#endif

#endif // __LINEAR_ALGEBRA_PUBLIC_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/norms.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_NORMS_HEADER__
#define __LA_NORMS_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

#define LA_L1_NORM 1
#define LA_L2_NORM 2
#define LA_LINF_NORM 3
typedef unsigned long la_norm_t;

/*!
 @abstract
 Compute a norm of a vector or matrix.

 @discussion
 "vector" refers to the fact that this function computes the norm of its
 argument considered as a vector, and not an operator norm.  The actual
 argument may be either a vector or a matrix.  If it is not a vector or
 matrix, or if the vector_norm parameter is not a supported value, NAN is
 returned.
 */
LA_FUNCTION LA_AVAILABILITY
float la_norm_as_float(la_object_t vector, la_norm_t vector_norm);

LA_FUNCTION LA_AVAILABILITY
double la_norm_as_double(la_object_t vector, la_norm_t vector_norm);

/*!
 @abstract
 "Normalizes" a vector or matrix.
 
 @discussion
 The returned object has the same direction as the first operand, and has
 norm 1 in the specified vector norm.  If the input vector is zero, it cannot
 be meaningfully normalized and the returned object is also zero.  If the
 parameter vector is not a vector or matrix, or if the vector_norm parameter
 is not a supported value, the returned object has status
 LA_INVALID_PARAMETER_ERROR.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_normalized_vector(la_object_t vector, la_norm_t vector_norm);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/matrix.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_MATRIX_HEADER__
#define __LA_MATRIX_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*!
 @brief When creating a matrix object from existing data, these hints allow the
 user to pass useful information about the structure of the matrix.

 @discussion When creating a matrix container from an existing buffer, these
 hints allow the user to pass useful information about the traits of the
 matrix.  Increasing the amount of information known about a matrix can in some
 cases improve performance.  Consider the extreme where a matrix is a diagonal
 matrix.  When multiplying this matrix with another, if it was not known to be
 diagonal the cost would be O(N^3) but the extra information allows the cost
 to drop to O(N^2).

 In many cases, the framework can inspect the data to determine this information
 when it is not made available by hint, but for certain traits, such as
 symmetric positive definiteness, this can be a costly operation, especially if
 the user already knows the trait to be true.

 These are however hints, an incorrect hint will not break the intended
 operation, in the worst case it adds overhead.  When in doubt, it is
 recommended that rather than guessing, if no information is known, use
 LA_NO_HINT.
 */
#define LA_NO_HINT                       (0U)
#define LA_SHAPE_DIAGONAL                (1U << 0)
#define LA_SHAPE_LOWER_TRIANGULAR        (1U << 1)
#define LA_SHAPE_UPPER_TRIANGULAR        (1U << 2)
#define LA_FEATURE_SYMMETRIC             (1U << 16)
#define LA_FEATURE_POSITIVE_DEFINITE     (1U << 17)
#define LA_FEATURE_DIAGONALLY_DOMINANT   (1U << 18)
typedef unsigned long la_hint_t;

/*!
 @abstract
 Create a matrix using data from a buffer of floats.  Ownership of the buffer
 remains in control of the caller.

 @param buffer
 Pointer to float data providing the elements of the matrix.

 @param matrix_rows
 The number of rows in the matrix.

 @param matrix_cols
 The number of columns in the matrix.

 @param matrix_row_stride
 The offset in the buffer (measured in floats) between corresponding elements
 in consecutive rows of the matrix.  Must be positive.

 @param matrix_hint
 Flags describing special matrix structures.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the matrix.

 @discussion
 This function creates an object representing a matrix whose entries are
 copied out of the supplied buffer of floats.  Negative or zero strides
 are not supported by this function (but note that you can reverse the
 rows or columns using the la_matrix_slice function defined below).

 This routine assumes that the elements of the matrix are stored in the buffer
 in row-major order.  If you need to work with data that is in column-major
 order, you can do that as follows:

 1. Use this routine to create a matrix object, but pass the number of
 columns in your matrix for the matrix_rows parameter and vice-versa.  For
 the matrix_row_stride parameter, pass the column stride of your matrix.

 2. Make a new matrix transpose object from the object created in step 1.  The
 resulting object represents the matrix that you want to work with.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_from_float_buffer(const float *buffer,
                                           la_count_t matrix_rows,
                                           la_count_t matrix_cols,
                                           la_count_t matrix_row_stride,
                                           la_hint_t matrix_hint,
                                           la_attribute_t attributes);

/*!
 @abstract
 Create a matrix using data from a buffer of doubles.  Ownership of the buffer
 remains in control of the caller.

 @param buffer
 Pointer to double data providing the elements of the matrix.

 @param matrix_rows
 The number of rows in the matrix.

 @param matrix_cols
 The number of columns in the matrix.

 @param matrix_row_stride
 The offset in the buffer (measured in doubles) between corresponding elements
 in consecutive rows of the matrix.  Must be positive.

 @param matrix_hint
 Flags describing special matrix structures.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the matrix.

 @discussion
 This function creates an object representing a matrix whose entries are
 copied out of the supplied buffer of doubles.  Negative or zero strides
 are not supported by this function (but note that you can reverse the
 rows or columns using the la_matrix_slice function defined below).

 This routine assumes that the elements of the matrix are stored in the buffer
 in row-major order.  If you need to work with data that is in column-major
 order, you can do that as follows:

 1. Use this routine to create a matrix object, but pass the number of
 columns in your matrix for the matrix_rows parameter and vice-versa.  For
 the matrix_row_stride parameter, pass the column stride of your matrix.

 2. Make a new matrix transpose object from the object created in step 1.  The
 resulting object represents the matrix that you want to work with.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_from_double_buffer(const double *buffer,
                                            la_count_t matrix_rows,
                                            la_count_t matrix_cols,
                                            la_count_t matrix_row_stride,
                                            la_hint_t matrix_hint,
                                            la_attribute_t attributes);

/*!
 @abstract
 Create a matrix using data from a buffer of floats.  Ownership of the buffer
 is transferred from the caller to the returned object.

 @param buffer
 Pointer to float data providing the elements of the matrix.

 @param matrix_rows
 The number of rows in the matrix.

 @param matrix_cols
 The number of columns in the matrix.

 @param matrix_row_stride
 The offset in the buffer (measured in floats) between corresponding elements
 in consecutive rows of the matrix.  Must be positive.

 @param matrix_hint
 Flags describing special matrix structures.

 @param deallocator
 Callback to be used to deallocate the buffer when the returned matrix object
 is destroyed.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the matrix.

 @discussion
 This function creates an object representing a matrix whose entries are
 copied out of the supplied buffer of floats.  Negative or zero strides
 are not supported by this function (but note that you can reverse the
 rows or columns using the la_matrix_slice function defined below).

 This routine assumes that the elements of the matrix are stored in the buffer
 in row-major order.  If you need to work with data that is in column-major
 order, you can do that as follows:

 1. Use this routine to create a matrix object, but pass the number of
 columns in your matrix for the matrix_rows parameter and vice-versa.  For
 the matrix_row_stride parameter, pass the column stride of your matrix.

 2. Make a new matrix transpose object from the object created in step 1.  The
 resulting object represents the matrix that you want to work with.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_from_float_buffer_nocopy(float *buffer,
                                                  la_count_t matrix_rows,
                                                  la_count_t matrix_cols,
                                                  la_count_t matrix_row_stride,
                                                  la_hint_t matrix_hint,
                                                  __nullable la_deallocator_t deallocator,
                                                  la_attribute_t attributes);

/*!
 @abstract
 Create a matrix using data from a buffer of doubles.  Ownership of the buffer
 is transferred from the caller to the returned object.

 @param buffer
 Pointer to double data providing the elements of the matrix.

 @param matrix_rows
 The number of rows in the matrix.

 @param matrix_cols
 The number of columns in the matrix.

 @param matrix_row_stride
 The offset in the buffer (measured in doubles) between corresponding elements
 in consecutive rows of the matrix.  Must be positive.

 @param matrix_hint
 Flags describing special matrix structures.

 @param deallocator
 Callback to be used to deallocate the buffer when the returned matrix object
 is destroyed.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the matrix.

 @discussion
 This function creates an object representing a matrix whose entries are
 copied out of the supplied buffer of doubles.  Negative or zero strides
 are not supported by this function (but note that you can reverse
 the rows or columns using the la_matrix_slice function defined below).

 This routine assumes that the elements of the matrix are stored in the buffer
 in row-major order.  If you need to work with data that is in column-major
 order, you can do that as follows:

 1. Use this routine to create a matrix object, but pass the number of
 columns in your matrix for the matrix_rows parameter and vice-versa.  For
 the matrix_row_stride parameter, pass the column stride of your matrix.

 2. Make a new matrix transpose object from the object created in step 1.  The
 resulting object represents the matrix that you want to work with.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_from_double_buffer_nocopy(double *buffer,
                                                   la_count_t matrix_rows,
                                                   la_count_t matrix_cols,
                                                   la_count_t matrix_row_stride,
                                                   la_hint_t matrix_hint,
                                                   __nullable la_deallocator_t deallocator,
                                                   la_attribute_t attributes);

/*!
 @abstract
 Stores the elements of a matrix to a buffer.
 
 @param buffer
 Pointer to the destination buffer.
 
 @param buffer_row_stride
 Offset (measured in floats) between the destinations of corresponding elements
 of consecutive rows of the matrix.  Must be positive.
 
 @param matrix
 The matrix to store.
 
 @discussion
 The buffer must be large enough to accomodate the matrix being stored.
 Specifically, it must have sufficient space to hold
 
    buffer_row_stride*(la_matrix_rows(matrix)-1) + la_matrix_cols(matrix)
 
 float elements.
 
 This function supports storing the contents of a vector as well as a matrix.
 A vector of length n will be interpreted as a rows(matrix) x cols(matrix) by 
 this function.  If the object is a vector or matrix and does not have an error 
 status, its contents are stored to the buffer.  If it has an error status, NaNs 
 are stored to the buffer.
 
 If the object is not a matrix or vector, nothing is written to the buffer and
 LA_INVALID_PARAMETER_ERROR is returned.
 */
LA_FUNCTION LA_AVAILABILITY
la_status_t la_matrix_to_float_buffer(float *buffer,
                                            la_count_t buffer_row_stride,
                                            la_object_t matrix);

/*!
 @abstract
 Stores the elements of a matrix to a buffer.

 @param buffer
 Pointer to the destination buffer.

 @param buffer_row_stride
 Offset (measured in doubles) between the destinations of corresponding elements
 of consecutive rows of the matrix.  Must be positive.

 @param matrix
 The matrix to store.

 @discussion
 The buffer must be large enough to accomodate the matrix being stored.
 Specifically, it must have sufficient space to hold

    buffer_row_stride*(la_matrix_rows(matrix)-1) + la_matrix_cols(matrix)

 double elements.
 
 This function supports storing the contents of a vector as well as a matrix.
 A vector of length n will be interpreted as a rows(matrix) x cols(matrix) by
 this function.  If the object is a vector or matrix and does not have an error
 status, its contents are stored to the buffer.  If it has an error status, NaNs
 are stored to the buffer.

 If the object is not a matrix or vector, nothing is written to the buffer and
 LA_INVALID_PARAMETER_ERROR is returned.
 */
LA_FUNCTION LA_AVAILABILITY
la_status_t la_matrix_to_double_buffer(double *buffer,
                                             la_count_t buffer_row_stride,
                                             la_object_t matrix);

/*!
 @abstract
 Get the number of rows in a matrix.
 
 @discussion
 If the argument has an error status, zero is returned.
 If the argument is a vector, the number of rows may be 1 or length(vector) 
 depending on the orientation of the vector.
 If the argument is a matrix, the number of rows is returned.
 Otherwise, zero is returned.
 */
LA_FUNCTION LA_CONST LA_AVAILABILITY
la_count_t la_matrix_rows(la_object_t matrix);

/*!
 @abstract
 Get the number of columns in a matrix.

 @discussion
 If the argument has an error status, zero is returned.
 If the argument is a vector, the number of columns may be 1 or length(vector)
 depending on the orientation of the vector.
 If the argument is a matrix, the number of columns is returned.
 Otherwise, zero is returned.
 */
LA_FUNCTION LA_CONST LA_AVAILABILITY
la_count_t la_matrix_cols(la_object_t matrix);

/*!
 @abstract
 Create a slice of a matrix.
 
 @param matrix
 The matrix to be sliced.
 
 @param matrix_first_row
 The index of the row of the source matrix containing the first element of
 new slice matrix.
 
 @param matrix_first_col
 The index of the column of the source matrix containing the first element
 of the slice matrix.
 
 @param matrix_row_stride
 The offset in the source matrix between rows that will be consecutive in
 the slice matrix.
 
 @param matrix_col_stride
 The offset in the source matrix between columns that will be consecutve in
 the slice matrix.
 
 @param slice_rows
 The number of rows in the slice matrix.
 
 @param slice_cols
 The number of columns in the slice matrix.
 
 @return
 A new matrix with size slice_rows x slice_cols whose elements are taken
 from the source matrix.
 
 @discussion
 The result object is the slice_rows x slice_cols matrix whose i,jth entry is:
 
    matrix[matrix_first_row + i*matrix_row_stride,
           matrix_first_col + j*matrix_col_stride]
 
 Slices provide an efficient means to operate on tiles and strides.  These are
 lightweight objects that reference the storage of the matrix from which they
 originate.  In most cases, creating a slice does not require any allocation
 beyond the object representing the slice, nor require copying.  In some 
 less common cases, a copy may be required.
 
 This function supports slicing a vector (interpreted as 
 rows(matrix) x cols(matrix)) or a matrix.  If the object is not a matrix or 
 vector, the returned object will have status LA_INVALID_PARAMETER_ERROR.

 If the slice references indices that are less than zero or greater than or
 equal to the dimensions of the matrix, LA_SLICE_OUT_OF_BOUNDS_ERROR is 
 returned.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_slice(la_object_t matrix,
                                 la_index_t matrix_first_row,
                                 la_index_t matrix_first_col,
                                 la_index_t matrix_row_stride,
                                 la_index_t matrix_col_stride,
                                 la_count_t slice_rows,
                                 la_count_t slice_cols);

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_identity_matrix(la_count_t matrix_size,
                                    la_scalar_type_t scalar_type,
                                    la_attribute_t attributes);

/*!
 @abstract
 Create a matrix with a specified diagonal provided by a vector, and zeros in
 all the other entries.
 
 @param vector
 Vector providing the data for the non-zero diagonal.
 
 @param matrix_diagonal
 The index of the non-zero diagonal.
 
 @discussion
 Creates a new matrix with entries on the specified diagonal taken from the
 vector argument, and zeros in the other entries.  The matrix is square, and
 has size length(vector) + abs(matrix_diagonal).
 
 If matrix_diagonal is zero, the main diagonal is set.  If matrix_diagonal is
 +1, the first superdiagonal is set.  If matrix_diagonal is -2, the second
 subdiagonal is set.
 
 The diagonal may be specified by a vector or by a matrix that has only one
 row or only one column.  If the provided object is not a vector or matrix,
 or is a matrix with both dimensions larger than one, the returned object
 will have status LA_INVALID_PARAMETER_ERROR.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_diagonal_matrix_from_vector(la_object_t vector,
                                            la_index_t matrix_diagonal);

/*!
 @abstract
 Creates a vector from the specified row of the matrix.
 
 @param matrix
 Matrix from which to create the row vector.
 
 @param matrix_row.
 The zero-based index of the row to create the vector from.  
 
 @return
 The resulting vector is a 1 x cols(matrix) vector.
 
 @discussion
 Creates a vector from the specified row of the matrix.  If the value for 
 matrix_row is less than zero or greater than rows(matrix)-1,
 LA_INVALID_PARAMETER_ERROR is returned.  
 
 If matrix is a splat, LA_INVALID_PARAMETER_ERROR is returned.

 Always returns a 1 x vector_length vector.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_vector_from_matrix_row(la_object_t matrix,
                                           la_count_t matrix_row);

/*!
 @abstract
 Creates a vector from the specified column of the matrix.
 
 @param matrix
 Matrix from which to create the column vector.
 
 @param matrix_col.
 The zero-based index of the column to create the vector from.
 
 @return
 The resulting vector is a rows(matrix) x 1 vector.
 
 @discussion
 Creates a vector from the specified column of the matrix.  If the value for
 matrix_col is less than zero or greater than cols(matrix)-1,
 LA_INVALID_PARAMETER_ERROR is returned.
 
 If matrix is a splat, LA_INVALID_PARAMETER_ERROR is returned.

 Always returns a vector_length x 1 vector.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_vector_from_matrix_col(la_object_t matrix,
                                           la_count_t matrix_col);

/*!
 @abstract
 Creates a vector from the specified diagonal of the matrix.
 
 @param matrix
 Matrix from which to create the vector.
 
 @param matrix_diagonal.
 The index of the diagonal to create the vector from.
 
 @return
 The resulting vector is a length x 1 vector where length is 
 min(rows(matrix),cols(matrix)-abs(matrix_diagonal)
 
 @discussion
 Creates a new vector with entries on the specified diagonal taken from the
 vector argument, and zeros in the other entries.  The matrix is square, and
 has size length(vector) + abs(matrix_diagonal).
 
 If matrix_diagonal is zero, the main diagonal is set.  If matrix_diagonal is
 +1, the first superdiagonal is set.  If matrix_diagonal is -2, the second
 subdiagonal is set.
 
 Creates a vector from the specified diagonal of the matrix.  If the value for
 matrix_diagonal is less than zero and abs(matrix_diagonal) > rows(matrix)-1,
 or if matrix_diagonal is greater than zero and matrix_diagonal > 
 cols(matrix)-1, LA_INVALID_PARAMETER_ERROR is returned.
 
 If matrix is a splat, LA_INVALID_PARAMETER_ERROR is returned.

 Always returns a vector_length x 1 vector.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_vector_from_matrix_diagonal(la_object_t matrix,
                                              la_index_t matrix_diagonal);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_MATRIX_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/splat.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_SPLAT_HEADER__
#define __LA_SPLAT_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*
 Splat objects are a feature that is relatively unique to the LinearAlgebra
 library.

 A splat object represents a matrix or vector whose entries are all equal.
 It may be used in the place of a matrix or vector in many computational
 operations in the library.

 Splat objects are effectively dimensionless matrices.  Computational
 operations that accept them interpret them as having dimensions necessary
 to be compatibile with the other operand.  Because the dimensions of a
 splat are inferred from the other operands, a single operation cannot
 have multiple splat operands.

 A partial list of operations accepting splat operands, and the rules for the
 inferred size:

 Operation                           Inferred Dimensions
 sum(A,splat)                        (rows(A), cols(A))
 sum(splat,A)                        (rows(A), cols(A))
 difference(A,splat)                 (rows(A), cols(A))
 difference(splat,A)                 (rows(A), cols(A))
 elementwise_product(A,splat)        (rows(A), cols(A))
 elementwise_product(splat,A)        (rows(A), cols(A))
 inner_product(A,splat)              (length(A), 1)
 inner_product(splat,A)              (1, length(A))
 matrix_product(A,splat)             (cols(A), 1)
 matrix_product(splat,A)             (1, rows(A))
 */

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_splat_from_float(float scalar_value,
                                la_attribute_t attributes);

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_splat_from_double(double scalar_value,
                                 la_attribute_t attributes);

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_splat_from_vector_element(la_object_t vector,
                                         la_index_t vector_index);

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_splat_from_matrix_element(la_object_t matrix,
                                         la_index_t matrix_row,
                                         la_index_t matrix_col);

/*
 It is sometimes useful to be able to just generate a matrix or vector with
 fixed dimensions from a splat object, either because the operation you want
 to perform doesn't infer the same dimensions as you would like to use, or
 if the operation you want to perform doesn't support inferring dimensions
 at all.  For this purpose, the following functions are provided.
 
 The vector returned from la_vector_from_splat is always vector_length x 1.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_vector_from_splat(la_object_t splat,
                                 la_count_t vector_length);

LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_from_splat(la_object_t splat,
                                 la_count_t matrix_rows,
                                 la_count_t matrix_cols);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_SPLAT_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/linear_systems.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_LINEAR_SYSTEMS_HEADER__
#define __LA_LINEAR_SYSTEMS_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*!
 @abstract
 Solves a system of linear equations
 
 @param matrix_system
 A matrix describing the left-hand side of the system.
 
 @param obj_rhs
 A vector or matrix describing one or more right-hand sides for which the
 equations are to be solved.
 
 @return
 A matrix of the solution(s) of the system of equations.

 @discussion
 If matrix_system represents a matrix A, and obj_rhs represents a vector
 B, la_solve returns a vector X representing a solution to the equation
 AX = B, if such a solution exists.  If obj_rhs represents a matrix, then
 la_solve returns a matrix representing the solution of the same equation.
 
 There are several different cases, and different algorithms are chosen
 depending on the specifics:
 
 If the matrix has a special structure that allows us to solve the system
 without factoring (e.g. if the matrix is diagonal or triangular), we may
 use that structure to compute the solution.
 
 If the matrix is symmetric and all diagonal entries are positive, or if we
 know, either via a hint parameter or by how earlier computations were
 structured, that the matrix is positive definite, we attempt a Cholesky
 factorization.  If this succeeds, it is used to compute the solution via
 forward- and back-substitution.
 
 If the matrix is square, we try to perform Gaussian elimination to construct
 a triangular factorization with pivoting.  If this factorization succeeds,
 we use it to solve the system.  If it fails, the returned object has the
 error status LA_SINGULAR_ERROR.
 
 If the matrix is not square, we return a least-squares solution computed by
 performing a QR factorization of the matrix.
 
 If the number of rows of the matrix does not match the number of rows of
 the right hand side object, the returned object has status 
 LA_DIMENSION_MISMATCH_ERROR.
 
 If the object describing the matrix is not a matrix, or if the right hand
 side is not a matrix or vector, the returned object has status
 LA_INVALID_PARAMETER_ERROR.
 
 If you want to solve the system XA = B, which is less common (but still
 occurs fairly frequently), you may accomplish this by transposing A and B,
 solving, and then transposing the result of the solve.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_solve(la_object_t matrix_system, la_object_t obj_rhs);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_ARITHMETIC_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/object.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_OBJECT_HEADER__
#define __LA_OBJECT_HEADER__

#include <vecLib/LinearAlgebra/base.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*!
 @class la_object_t

 @abstract
 Type used by all functions in the LinearAlgebra library.

 @discussion LinearAlgebra objects behave rather differently depending on 
 whether you are writing Objective-C or C.  For details of semantics and memory 
 management consult the appropriate section below.  For further information, 
 consult the comments in <os/object.h>.

 Objective-C with ARC:
 LinearAlgebra objects are objective-c objects.  This means that
 they participate in ARC, may be used with collections, etc.  Thus, if you are 
 using ARC, you must not retain or release LinearAlgebra objects; the compiler 
 will take care of it for you.

 Objective-C without ARC:
 If you are not using ARC, you send -[retain] and -[release] messages to
 LinearAlgebra objects, just like you would with objects of any other 
 Objective-C type.

 C:
 LinearAlgebra objects are reference counted and must be retained and released
 via the la_retain( ) and la_release( ) functions.  Do not attempt to
 call free( ) on LinearAlgebra objects; this will typically result in leaks.
 */
#if OS_OBJECT_USE_OBJC
    OS_OBJECT_DECL(la_object);
#   define LA_RETURNS_RETAINED OS_OBJECT_RETURNS_RETAINED
#else
typedef struct la_s *la_object_t;
#   define LA_RETURNS_RETAINED
#endif

/*!
 @abstract
 Increment the reference count of a la_object_t object.

 @param object
 The object to retain.

 @result
 The retained object.

 @discussion
 On a platform with the modern Objective-C runtime this is exactly equivalent
 to sending the object the -[retain] message.
 */
LA_FUNCTION LA_AVAILABILITY
la_object_t la_retain(la_object_t object);
#if OS_OBJECT_USE_OBJC
#   undef la_retain
#   define la_retain(object) [object retain]
#endif

/*!
 @abstract
 Decrement the reference count of an la_object_t object.

 @param object
 The object to release.

 @discussion
 On a platform with the modern Objective-C runtime this is exactly equivalent
 to sending the object the -[release] message.
 */
LA_FUNCTION LA_AVAILABILITY
void la_release(la_object_t object);
#if OS_OBJECT_USE_OBJC
#   undef la_release
#   define la_release(object) [object release]
#endif

/*!
 @abstract
 Add attributes to an la_object_t object.

 @param object
 The object that will have its attributes modified.

 @param attributes
 Attributes which are to be added to the object's existing attributes
 to create its new set of attributes.  This value should be constructed by
 or'ing together LA_ATTRIBUTE_* constants.

 @discussion
 This operation does not remove any existing attributes from the LinearAlgebra 
 object, though it is possible that some attributes will override others (if so, 
 this will be documented in the discussion of those attributes above).  
 Following this function call, the specified object has all of the attributes it 
 had before the call, plus any new attributes specified by the second parameter.
 
 Adding an attribute that the object already has does not change the object.
 
 This function is not reentrant or thread-safe.  Attempting to add or remove
 attributes from multiple threads will have unpredictable results.
 */
LA_FUNCTION LA_AVAILABILITY
void la_add_attributes(la_object_t object, la_attribute_t attributes);

/*!
 @abstract
 Remove attributes from an la_object_t object.

 @param object
 The object that will have its attributes modified.

 @param attributes
 Attributes which are to be removed from the object's existing attributes
 to create its new set of attributes.  This value should be constructed by
 or'ing together LA_ATTRIBUTE_* constants.
 
 @discussion
 This function removes the specified attributes from the LinearAlgebra object.
 Removing an attribute that the object does not have is harmless and does not
 change the object.

 This function is not reentrant or thread-safe.  Attempting to add or remove
 attributes from multiple threads will have unpredictable results.
 */
LA_FUNCTION LA_AVAILABILITY
void la_remove_attributes(la_object_t object, la_attribute_t attributes);

/*!
 @abstract Query the status of an la_object.
 
 @param object
 The object whose status is being requested.
 
 @return
 The status of the supplied object.

 @discussion Returns the status of a LinearAlgebra object.  The status will be 
 one of the codes defined in LinearAlgebra/base.h.  New status codes may be
 added in the future, but the following basic principle will continue to hold: 
 zero indicates success, status codes greater than zero are warnings, and
 status codes less than zero are errors.  Thus, careful error handling
 might look like the following:

 <pre>
 @textblock
 la_status_t status = la_status(result_object);
 if (status == LA_SUCCESS) {
    // Everything is copacetic.  Get your data from result_object.
 } else if (status > 0) {
    // No errors occured, but the result does not have full accuracy due to
    // numerical considerations.  Here, you might re-compute the result using
    // a more careful or stable algorithm.
 } else {
    // An error occured.  Something is seriously amiss and you will need
    // to handle it however makes sense for your application.
 }
 @/textblock
 </pre>

 Note that errors and warnings are propagated.  In general, there is no need
 to check the status of each subcomputation.  Rather, the preferred idiom is
 to do a complete computation, then check to see if anything went wrong.
 Querying status may force evaluation of parts of your computation that might
 otherwise be deferred until their results were actually needed.
 */
LA_FUNCTION LA_AVAILABILITY
la_status_t la_status(la_object_t object);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_OBJECTS_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/arithmetic.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_ARITHMETIC_HEADER__
#define __LA_ARITHMETIC_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*!
 @abstract
 Transpose a vector or matrix.

 @discussion
 Returns a matrix that is the transpose of the source vector or matrix.  If the
 source object is not a vector or matrix, the returned object will have status
 LA_INVALID_PARAMETER_ERROR.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_transpose(la_object_t matrix);

/*!
 @abstract
 Multiply a matrix or vector by a scalar given by a float.
 
 @discussion
 Returns a matrix whose entries are the product of the scalar and the 
 corresponding element of the source matrix.  If the source object is not
 a vector or matrix, the returned object will have status 
 LA_INVALID_PARAMETER_ERROR.
 
 If the scalar type of matrix is not float LA_PRECISION_MISMATCH_ERROR is 
 returned.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_scale_with_float(la_object_t matrix, float scalar);

/*!
 @abstract
 Multiply a matrix or vector by a scalar given by a double.

 @discussion
 Returns a matrix whose entries are the product of the scalar and the
 corresponding element of the source matrix.  If the source object is not
 a vector or matrix, the returned object will have status
 LA_INVALID_PARAMETER_ERROR.
 
 If the scalar type of matrix is not double LA_PRECISION_MISMATCH_ERROR is
 returned.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_scale_with_double(la_object_t matrix, double scalar);

/*!
 @abstract
 Compute the element-wise sum of two vectors or matrices.
 
 @discussion
 If either source operand is not a vector or matrix or splat, or if both
 operands are splats, the result has status LA_INVALID_PARAMETER_ERROR.
 
 The two operands must have the same dimensions.  If they do not, the result
 will have status LA_DIMENSION_MISMATCH_ERROR.  For simplicity, a vector
 of length n, a 1xn matrix, and an nx1 matrix are all treated as having the
 same dimensions.  If 1xn and nx1 or nx1 and 1xn vectors are passed, an nx1
 vector will be created, otherwise orientation matches input.
 
 The result has the same dimensions as the operands, and each element in
 the result is the sum of the corresponding elements in the source operands.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_sum(la_object_t obj_left, la_object_t obj_right);

/*!
 @abstract
 Compute the element-wise difference of two vectors or matrices.

 @discussion
 If either source operand is not a vector or matrix or splat, or if both
 operands are splats, the result has status LA_INVALID_PARAMETER_ERROR.

 The two operands must have the same dimensions.  If they do not, the result
 will have status LA_DIMENSION_MISMATCH_ERROR.  For simplicity, a vector
 of length n, a 1xn matrix, and an nx1 matrix are all treated as having the
 same dimensions.  If 1xn and nx1 or nx1 and 1xn vectors are passed, an nx1
 vector will be created, otherwise orientation matches input.

 The result has the same dimensions as the operands, and each element in
 the result is given by subtracting the corresponding element of obj_right
 from the corresponding element of obj_left.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_difference(la_object_t obj_left, la_object_t obj_right);

/*!
 @abstract
 Compute the element-wise product of two vectors or matrices.

 @discussion
 If either source operand is not a vector or matrix or splat, or if both
 operands are splats, the result has status LA_INVALID_PARAMETER_ERROR.

 The two operands must have the same dimensions.  If they do not, the result
 will have status LA_DIMENSION_MISMATCH_ERROR.  For simplicity, a vector
 of length n, a 1xn matrix, and an nx1 matrix are all treated as having the
 same dimensions.  If 1xn and nx1 or nx1 and 1xn vectors are passed, an nx1
 vector will be created, otherwise orientation matches input.

 The result has the same dimensions as the operands, and each element in
 the result is the product of the corresponding elements in the source operands.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_elementwise_product(la_object_t obj_left, la_object_t obj_right);

/*!
 @abstract
 Compute the inner product of two vectors.
 
 @discussion
 If either operand is a matrix that is not 1xn or nx1, the result has the
 status LA_INVALID_PARAMETER_ERROR.
 
 If either operand is not a vector or matrix or splat, or if both operands
 are splats, the result has the status LA_INVALID_PARAMETER_ERROR.

 If the lengths of the two operands do not match, the result has the status
 LA_DIMENSION_MISMATCH_ERROR.
 
 Otherwise the result is a 1x1 matrix containing the inner product:
 
        sum_{i=0...length} vector_left[i] * vector_right[i]
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_inner_product(la_object_t vector_left, la_object_t vector_right);

/*!
 @abstract
 Compute the outer product of two vectors.

 @discussion
 Splats are not supported by this function.  If either operand
 is a splat, the result has status LA_INVALID_PARAMETER_ERROR.

 If either operand is a matrix that is not 1xn or nx1, the result has the
 status LA_INVALID_PARAMETER_ERROR.

 If either operand is not a vector or matrix, the result has the status
 LA_INVALID_PARAMETER_ERROR.
 
 Otherwise the result is a matrix containg the outer product.  It has
 length(vector_left) rows and length(vector_right) columns.  The i,jth
 element of the matrix is vector_left[i] * vector_right[j].
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_outer_product(la_object_t vector_left, la_object_t vector_right);

/*!
 @abstract
 Compute a matrix product.

 @discussion
 Left splat operands are treated as 1 x rows(matrix_right) vectors and right 
 splat operands are treated as cols(matrix_left) x 1 vectors.
 
 For convenience, in certain situations vector operands may be implicitly
 transposed.  Specifically,
 
 If cols(matrix_left) == rows(matrix_right)
 	rows(matrix_left) x cols(matrix_right) = matrix_left * matrix_right
 Else if cols(matrix_left) == 1 and rows(matrix_left) == rows(matrix_right)
 	1 x cols(matrix_right) = transpose(matrix_left) * matrix_right
 Else if rows(matrix_right) == 1 and cols(matrix_left) == cols(matrix_right)
 	rows(matrix_left) x 1 = matrix_left * transpose(matrix_right)
 Else
  	result has the status LA_DIMENSION_MISMATCH_ERROR.

 If either operand is not a vector or matrix or splat, or if both operands
 are splats, the result has the status LA_INVALID_PARAMETER_ERROR.

 Otherwise the result is a matrix with 1 row if matrix_left is vector or splat 
 and rows(matrix_left) otherwise, and 1 column if matrix_right is vector or
 splat and cols(matrix_right) otherwise.
 
 If cols(matrix_left) == rows(matrix_right), the i,jth element of the matrix is:
 		sum_{k=0...cols(matrix_left)} matrix_left[i,k] * matrix_right[k,j]
 If cols(matrix_left) == 1 and rows(matrix_left) == rows(matrix_right), the 
    0,jth element of matrix is:
 		sum_{k=0...rows(matrix_right)} matrix_left[k,0] * matrix_right[k,j]
 If rows(matrix_right) == 1 and cols(matrix_left) == cols(matrix_right), the
 	i,0th element of matrix is:
 	sum_{k=0...cols(matrix_left)} matrix_left[i,k] * matrix_right[0,k]
 
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_matrix_product(la_object_t matrix_left,
                              la_object_t matrix_right);

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_ARITHMETIC_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/vector.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_VECTOR_HEADER__
#define __LA_VECTOR_HEADER__

#include <vecLib/LinearAlgebra/object.h>

#if __has_feature(assume_nonnull)
////  If assume_nonnull is available, use it and use nullability qualifiers.
_Pragma("clang assume_nonnull begin")
#else
////  Otherwise, neuter the nullability qualifiers.
#define __nullable
#define __nonnull
#endif

/*!
 @abstract
 Create a vector using data from a buffer of floats.  Ownership of the buffer
 remains in control of the caller.

 @param buffer
 Pointer to float data providing the elements of the vector.  

 @param vector_length
 The length of the vector.

 @param buffer_stride
 The offset in the buffer (measured in floats) between consecutive elements
 of the vector.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the vector.

 @discussion
 This function creates an object representing a vector whose entries are copied
 out of the supplied buffer of floats.  Negative or zero strides are not
 supported by this function (but note that you can reverse the elements in a
 vector using the la_vector_reverse macro defined below).

 The stride determines the offset (measured in floats) between the values
 providing consecutive vector elements in the buffer.  For example, if
 buffer_stride is 1, then the vector is:

 { buffer[0], buffer[1], buffer[2], ... , buffer[vector_length-1] }

 If buffer_stride is 3, the vector is:

 { buffer[0], buffer[3], buffer[6], ... , buffer[3*(vector_length-1)] }

 Thus, the size of the buffer (in floats) should be at least

 buffer_stride*(vector_length-1) + 1.
 
 Always returns a vector_length x 1 vector.
 */
#define la_vector_from_float_buffer(buffer, vector_length, buffer_stride, attributes) \
    la_matrix_from_float_buffer(buffer, vector_length, 1, buffer_stride, LA_NO_HINT, attributes)

/*!
 @abstract
 Create a vector using data from a buffer of doubles.  Ownership of the buffer
 remains in control of the caller.

 @param buffer
 Pointer to double data providing the elements of the vector.

 @param vector_length
 The length of the vector.

 @param buffer_stride
 The offset in the buffer (measured in doubles) between consecutive elements
 of the vector.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the vector.

 @discussion
 This function creates an object representing a vector whose entries are copied
 out of the supplied buffer of doubles.  Negative or zero strides are not
 supported by this function (but note that you can reverse the elements in a
 vector using the la_vector_reverse macro defined below).

 The stride determines the offset (measured in doubles) between the values
 providing consecutive vector elements in the buffer.  For example, if
 buffer_stride is 1, then the vector is:

 { buffer[0], buffer[1], buffer[2], ... , buffer[vector_length-1] }

 If buffer_stride is 3, the vector is:

 { buffer[0], buffer[3], buffer[6], ... , buffer[3*(vector_length-1)] }

 Thus, the size of the buffer (in doubles) should be at least

 buffer_stride*(vector_length-1) + 1.
 
 Always returns a vector_length x 1 vector.
 */
#define la_vector_from_double_buffer(buffer, vector_length, buffer_stride, attributes) \
    la_matrix_from_double_buffer(buffer, vector_length, 1, buffer_stride, LA_NO_HINT, attributes)

/*!
 @abstract
 Create a vector using data from a buffer of floats.  Ownership of the buffer
 is transferred from the caller to the returned object.

 @param buffer
 Pointer to float data providing the elements of the vector.

 @param vector_length
 The length of the vector.

 @param deallocator
 Callback to be used to deallocate the buffer when the returned vector object
 is destroyed.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the vector.

 @discussion
 This function creates an object representing a vector whose entries are the
 supplied buffer of floats.  Ownership of the buffer is transferred to the new
 vector object, meaning that you must not read, modify, or destroy it after
 calling this function.

 Strides are not supported with this function, as there is no way to take
 ownership of (say) every third element in a region of memory.  Instead,
 you would create a vector with this function, then create a slice using
 la_vector_slice to achieve the same effect without copying.
 
 Always returns a vector_length x 1 vector.
 */
#define la_vector_from_float_buffer_nocopy(buffer, vector_length, deallocator, attributes) \
    la_matrix_from_float_buffer_nocopy(buffer, vector_length, 1, 1, LA_NO_HINT, deallocator, attributes)

/*!
 @abstract
 Create a vector using data from a buffer of doubles.  Ownership of the buffer
 is transferred from the caller to the returned object.

 @param buffer
 Pointer to double data providing the elements of the vector.

 @param vector_length
 The length of the vector.

 @param deallocator
 Callback to be used to deallocate the buffer when the returned vector object
 is destroyed.

 @param attributes
 Attributes to attach to the new la_object_t object.  Pass LA_DEFAULT_ATTRIBUTES
 to create a normal object.

 @return a new la_object_t object representing the vector.

 @discussion
 This function creates an object representing a vector whose entries are the
 supplied buffer of doubles.  Ownership of the buffer is transferred to the new
 vector object, meaning that you must not read, modify, or destroy it after
 calling this function.

 Strides are not supported with this function, as there is no way to take
 ownership of (say) every third element in a region of memory.  Instead,
 you would create a vector with this function, then create a slice using
 la_vector_slice to achieve the same effect without copying.
 
 Always returns a vector_length x 1 vector.
 */
#define la_vector_from_double_buffer_nocopy(buffer, vector_length, deallocator, attributes) \
    la_matrix_from_double_buffer_nocopy(buffer, vector_length, 1, 1, LA_NO_HINT, deallocator, attributes)

/*!
 @abstract
 Stores the elements of a vector to a buffer.

 @param buffer
 Pointer to the destination buffer.

 @param buffer_stride
 Offset (in floats) between the destinations of consecutive vector elements
 in the buffer.  Negative strides are not supported (you can get the same
 effect by reversing the vector before calling this function).

 @param vector
 The vector to store.

 @discussion
 The buffer must be large enough to accomodate the vector being stored.
 Specifically, it must have have sufficient space to hold

 buffer_stride*(la_vector_length(vector)-1) + 1

 float elements.  Real usage in the most common case (stride = 1) will
 look roughly like this:

 <pre>
 @textblock
 la_count_t length = la_vector_length(vector);
 if (!length) {
 	// an error occured.
 }
 float *buffer = malloc(length * sizeof buffer[0]);
 la_vector_to_float_buffer(buffer, 1, vector);
 @/textblock
 </pre>

 This function supports storing the contents of a vector, or a matrix that
 has only one row or only one column.  If the object satisfies those
 requirements, and it does not have an error status, its contents are stored
 to the buffer.  If it has an error status, NaNs are stored to the buffer.

 If the object is not a matrix or vector, or if it is a matrix with both
 dimensions larger than one, nothing is written to the buffer and
 LA_INVALID_PARAMETER_ERROR is returned.
 */
LA_FUNCTION LA_AVAILABILITY
la_status_t la_vector_to_float_buffer(float *buffer,
                                            la_index_t buffer_stride,
                                            la_object_t vector);

/*!
 @abstract
 Stores the elements of a vector to a buffer.

 @param buffer
 Pointer to the destination buffer.

 @param buffer_stride
 Offset (in doubles) between the destinations of consecutive vector elements
 in the buffer.  Negative strides are not supported (you can get the same
 effect by reversing the vector before calling this function).

 @param vector
 The vector to store.

 @return
 If vector is a valid vector object, its status is returned.  Otherwise
 the return value is LA_INVALID_PARAMETER_ERROR.

 @discussion
 The buffer must be large enough to accomodate the vector being stored.
 Specifically, it must have have sufficient space to hold

 buffer_stride*(la_vector_length(vector)-1) + 1

 double elements.  Real usage in the most common case (stride = 1) will
 look roughly like this:

 <pre>
 @textblock
 la_count_t length = la_vector_length(vector);
 if (!length) {
 	// an error occured.
 }
 double *buffer = malloc(length * sizeof buffer[0]);
 la_vector_to_double_buffer(buffer, 1, vector);
 @/textblock
 </pre>

 This function supports storing the contents of a vector, or a matrix that
 has only one row or only one column.  If the object satisfies those
 requirements, and it does not have an error status, its contents are stored
 to the buffer.  If it has an error status, NaNs are stored to the buffer.

 If the object is not a matrix or vector, or if it is a matrix with both
 dimensions larger than one, nothing is written to the buffer and
 LA_INVALID_PARAMETER_ERROR is returned.
 */
LA_FUNCTION LA_AVAILABILITY
la_status_t la_vector_to_double_buffer(double *buffer,
                                             la_index_t buffer_stride,
                                             la_object_t vector);

/*!
 @abstract
 Get the length of a vector.

 @discussion
 If the argument has an error status, zero is returned.
 If the argument is a vector, its length is returned.
 If the argument is a matrix with only one row or only one column, the other
 dimension is returned.
 Otherwise, zero is returned.
 */
LA_FUNCTION LA_CONST LA_AVAILABILITY
la_count_t la_vector_length(la_object_t vector);

/*!
 @abstract
 Create a slice of a vector.

 @param vector
 The vector to be sliced.

 @param vector_first
 The index of the source vector element that will become the first element
 of the new slice vector.

 @param vector_stride
 The offset in the source vector between elements that will be consecutive in
 the new slice vector.

 @param slice_length
 The length of the resulting slice vector.

 @return
 A new vector with length slice_length whose elements are taken from vector.

 @discussion
 The result object is the vector:

 [ vector[vector_first], vector[vector_first+vector_stride], ...
 ... , vector[vector_first + (slice_length-1)*vector_stride] ]
 
 Slices provide an efficient means to operate on subvectors and strides.
 These are lightweight objects that reference the storage of the vector from
 which they originate.  Creating a vector slice does not require any allocation
 beyond the object representing the slice, nor does it require copying.

 This function supports slicing a vector, or a matrix that has only one row
 or only one column.  If the object is not a matrix or vector, or if it is
 a matrix with both dimensions larger than one, the returned object will have
 status LA_INVALID_PARAMETER_ERROR.

 If the slice references indices that are less than zero or greater than or
 equal to the length of the vector, LA_SLICE_OUT_OF_BOUNDS_ERROR is returned.
 
 Always return a vector with the same orientation as the input.  If input is
 vector_length x 1, output is vector_length x 1 and if input is
 1 x vector_length, output is 1 x vector_length.
 */
LA_FUNCTION LA_AVAILABILITY LA_RETURNS_RETAINED
la_object_t la_vector_slice(la_object_t vector,
                               la_index_t vector_first,
                               la_index_t vector_stride,
                               la_count_t slice_length);

/*!
 @abstract
 Generate a new vector that is the reverse of the supplied vector.
 
 Always return a vector with the same orientation as the input.  If input is
 vector_length x 1, output is vector_length x 1 and if input is 
 1 x vector_length, output is 1 x vector_length.
 */
#define la_vector_reverse(vector) \
la_vector_slice(vector, la_vector_length(vector)-1, -1, la_vector_length(vector))

#if __has_feature(assume_nonnull)
_Pragma("clang assume_nonnull end")
#endif

#endif // __LA_VECTOR_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/LinearAlgebra/base.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __LA_BASE_HEADER__
#define __LA_BASE_HEADER__

#include <os/object.h>
#include <os/availability.h>
#include <stddef.h>

/*  Define abstractions for a number of attributes that we wish to be able to
    consisely attach to functions in the LinearAlgebra library.             */
#define LA_AVAILABILITY  API_AVAILABLE(macos(10.10), ios(8.0))
#define LA_NONNULL1      OS_NONNULL1
#define LA_NONNULL       OS_NONNULL_ALL
#define LA_EXPORT        OS_EXPORT
#define LA_NOTHROW       OS_NOTHROW
#define LA_FUNCTION      OS_EXPORT OS_NOTHROW
#define LA_CONST         OS_CONST

/*!
 @typedef la_attribute_t

 @abstract
 Attributes to control the behavior of operations in the LinearAlgebra library.

 @discussion
 These attributes are "sticky"; created objects inherit the inclusive-or of
 the attributes set on their parents at the point that they are created.

 @constant LA_ENABLE_LOGGING
 Computations performed using objects with this attribute print debugging
 information to stderr.
 */
#define LA_DEFAULT_ATTRIBUTES           (0)
#define LA_ATTRIBUTE_ENABLE_LOGGING     (1U << 0)
typedef unsigned long la_attribute_t;

/*!
 @typedef la_status_t

 @abstract
 Status codes for la_object_t objects.

 @discussion
 Every la_object_t object has a status.  If everything has gone well in a
 computation, the status will be LA_STATUS_SUCCESS, which is defined to
 be zero.  Status codes greater than zero indicate warnings, and codes less
 than zero indicate errors.

 Errors and warnings propagate with a computation to all descendants of the
 la_object_t object that produced them.  If a la_object_t object receives an 
 error or warning from both parents, one of them will take precedence over the
 other.  The exact precedence ranking is not defined, but errors always
 supercede warnings.

 @constant LA_SUCCESS
 There are no errors or warnings associated with the la_object_t object.

 @constant LA_WARNING_POORLY_CONDITIONED
 One or more parts of the computation was poorly conditioned numerically, and
 the results are signficantly less accurate than might otherwise expected.

 @constant LA_INTERNAL_ERROR
 An error was encountered internal to the LinearAlgebra library.  For example, 
 an allocation failed, or memory was corrupted.

 @constant LA_INVALID_PARAMETER_ERROR
 An invalid parameter was passed to the function that created this object or
 one of it's ancestors.

 @constant LA_DIMENSION_MISMATCH_ERROR
 The function that created this object or one of its ancestors was passed
 arguments whose dimensions were not compatibile.  For example, this error
 might be generated by attempting to add a 63-element vector to a 135-element
 vector, or to multiply a 10x4 matrix by a 7x128 matrix.

 @constant LA_PRECISION_MISMATCH_ERROR
 The function that created this object or one of its ancestors was passed
 arguments whose underlying scalar types did not match.

 @constant LA_SINGULAR_ERROR
 One or more parts of the computation attempted to solve a singular system,
 and no correct result is possible.
 */
#define LA_SUCCESS                       (0)
#define LA_WARNING_POORLY_CONDITIONED    (1000)
#define LA_INTERNAL_ERROR                (-1000)
#define LA_INVALID_PARAMETER_ERROR       (-1001)
#define LA_DIMENSION_MISMATCH_ERROR      (-1002)
#define LA_PRECISION_MISMATCH_ERROR      (-1003)
#define LA_SINGULAR_ERROR                (-1004)
#define LA_SLICE_OUT_OF_BOUNDS_ERROR     (-1005)
typedef long la_status_t;

#define LA_SCALAR_TYPE_FLOAT  (0x8000)
#define LA_SCALAR_TYPE_DOUBLE (0x4000)
typedef unsigned int la_scalar_type_t;

typedef unsigned long la_count_t;
typedef long la_index_t;

typedef void (*la_deallocator_t)(void *ptr);

#endif // defined __LA_BASE_HEADER__
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vectorOps.h
/*
     File:       vecLib/vectorOps.h
 
     Contains:   vector and matrix functions for AltiVec
 
     Version:    vecLib-728.0
 
     Copyright:  Copyright (c) 1999-2019 by Apple Inc. All rights reserved.
 
     Bugs:       For bug reports, consult the following page on
                 the World Wide Web:
 
                     http://developer.apple.com/bugreporter/
 
*/
#ifndef __VECTOROPS__
#define __VECTOROPS__

#include <stdint.h>
/*
#ifndef __VECLIBTYPES__
#include <vecLib/vecLibTypes.h>
#endif

*/
#include "vecLibTypes.h"

#include <os/availability.h>

#if PRAGMA_ONCE
#pragma once
#endif

#ifdef __cplusplus
extern "C" {
#endif


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#else
    #define __nullable
    #define __nonnull
#endif


/*
-------------------------------------------------------------------------------------
                                                                                                                                                                  
 This section is a collection of Basic Linear Algebra Subprograms (BLAS), which   
 use AltiVec technology for their implementations. The functions are grouped into 
 three categories (called levels), as follows:                                    
                                                                                  
    1) Vector-scalar linear algebra subprograms                                   
    2) Matrix-vector linear algebra subprograms                                   
    3) Matrix operations                                                          
                                                                                  
 Following is a list of subprograms and a short description of each one.          
-------------------------------------------------------------------------------------
*/
#if defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE__
/*
-------------------------------------------------------------------------------------
     Level 1
-------------------------------------------------------------------------------------
*/
/**************************************************
  vIsamax finds the position of the first vector
  element having the largest magnitude.         
     count  length of vector x (count is a      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vIsamax()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern int32_t 
vIsamax(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use cblas_isamax or vDSP_maxmgvi instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vIsamin finds the position of the first vector
  element having minimum absolute value.        
     count  length of vector x (count is a      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vIsamin()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern int32_t 
vIsamin(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use vDSP_minmgvi instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vIsmax finds the position of the first vector 
  element having maximum value.                 
     count  length of vector x (count is a      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vIsmax()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern int32_t 
vIsmax(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use vDSP_maxvi instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vIsmin finds the position of the first vector 
  element having minimum value.                 
     count  length of vector x (count is a      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vIsmin()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern int32_t 
vIsmin(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use vDSP_minvi instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vSasum finds the sum of the magnitudes of the 
  elements in a vector.                         
     count  length of vector x (count is a      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vSasum()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern float 
vSasum(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use cblas_sasum or vDSP_svemg instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vSsum is the vector version of sasum but without  
  the absolute value. It takes the value of each
  element of the array and adds them together.      
            multiple of 4)                      
     x      array of floats                     
**************************************************/
/*
 *  vSsum()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern float 
vSsum(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use vDSP_sve instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/**************************************************
  vSaxpy multiplies a vector x, by a scalar and 
  adds it to a vector y and stores the result in y
     n      number of floats in x (n is a       
            multiple of 4)                      
     alpha  scalar number is single-precision   
            floating-point                      
     x      array of vFloats              
     y      array of vFloats, where the   
            the result is stored                
**************************************************/
/*
 *  vSaxpy()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSaxpy(
  int32_t        n,
  float          alpha,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use cblas_saxpy instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
  vScopy copies a vector x, into another vector y.  
     n      mumber of floats in x and y (n is a 
            multiple of 4)                      
     x      array of vFloats              
     y      array of vFloats              
*************************************************************/
/*
 *  vScopy()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vScopy(
  int32_t        n,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use cblas_scopy or memcpy instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);



/*************************************************************
 vSdot finds the dot product of two vectors.      
    n       mumber of floats in x and y (n is a 
                multiple of 4)                      
    x       array of vFloats              
    y       array of vFloats              
*************************************************************/
/*
 *  vSdot()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern float 
vSdot(
  int32_t        n,
  const vFloat   *x,
  const vFloat   *y)
  API_DEPRECATED("Use cblas_sdot or vDSP_dotpr instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);



/*************************************************************
 vSnaxpy computes saxpy "n" times.               
   n            number of saxpyV computations to be 
                performed and the number of elements
                in vector A (n is a multiple of 4)  
   m            number of floats in each vector x(i)
                or y(i)                             
   a            array of vFloats containing   
                scalars a(i)                        
   x            matrix containing arrays of vector- 
                floats x(i)                         
   y            matrix containing vectors y(i)      
*************************************************************/
/*
 *  vSnaxpy()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSnaxpy(
  int32_t        n,
  int32_t        m,
  const vFloat   *a,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use cblas_saxpy in a loop instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSndot computes the dot products "n" times.     
    n       number of dot product computations  
            to be performed and the number of   
                elements in vector S                
    m       number of elements in vectors x(i)  
                and y(i) for each dot product       
                computation (m is a multiple of 4)  
    s       array of floats. Depending on the   
                value of "isw" different computations/
                are performed and the results are   
                stored in the array S               
    isw     indicates the type of computation   
                to perform.                         
                if isw=1, S(i) <--   x(i)   y(i)    
                if isw=2, S(i) <-- - x(i)   y(i)    
                if isw=3, S(i) <-- S(i) + x(i)   y(i)/
                if isw=4, S(i) <-- S(i) - x(i)   y(i)/
    x       matrix containing arrays x(i)       
    y       matrix containing arrays y(i)       
*************************************************************/
/*
 *  vSndot()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSndot(
  int32_t        n,
  int32_t        m,
  float          *s,
  int32_t        isw,
  const vFloat   *x,
  const vFloat   *y)
  API_DEPRECATED("Use cblas_sdot or vDSP_dotpr in a loop instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSnrm2 finds the Euclidean length of a vector   
 with scaling of input to avoid destructive      
 underflow and overflow.                         
    count   length of vector (multiple of 4)    
    x       array of vFloats              
*************************************************************/
/*
 *  vSnrm2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern float 
vSnrm2(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use cblas_snrm2 instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSnorm2 finds the Euclidean length of a vector  
 with no scaling of input.                       
    count   length of vector (multiple of 4)    
    x       array of vFloats              
*************************************************************/
/*
 *  vSnorm2()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern float 
vSnorm2(
  int32_t        count,
  const vFloat   *x)
  API_DEPRECATED("Use cblas_snrm2 instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSrot applies a plane rotation.                 
    n       number of points to be rotated, also
                number of elements in x and y (n is 
                a multiple of 4)                    
    x       array of vFloats. It is a     
                vector of length n, containing x(i) 
                coordinates of points to be rotated 
    y       array of vFloats. It is a     
                vector of length n, containing y(i) 
                coordinates of points to be rotated 
    c       cosine of angle of rotation         
    s       sine of angle of rotation           
*************************************************************/
/*
 *  vSrot()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSrot(
  int32_t   n,
  vFloat    *x,
  vFloat    *y,
  float     c,
  float     s)
  API_DEPRECATED("Use cblas_srot instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSscal multiplies a vector x, by a scalar and   
 stores the result in the vector x.              
    n       number of floats in x (n is a       
                multiple of 4)                      
    alpha   scalar number is single-precision   
                floating-point                      
    x       array of vFloats              
*************************************************************/
/*
 *  vSscal()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSscal(
  int32_t   n,
  float     alpha,
  vFloat    *x)
  API_DEPRECATED("Use cblas_sscal instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSswap interchanges the elements of vectors x   
 and y                                           
    n       number of floats in x and y (n is a 
                multiple of 4)                      
    x       array of vFloats              
    y       array of vFloats              
*************************************************************/
/*
 *  vSswap()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSswap(
  int32_t   n,
  vFloat    *x,
  vFloat    *y)
  API_DEPRECATED("Use cblas_sswap instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSyax multiplies a vector x, by a scalar and    
 stores the result in a vector y.                
    n       number of floats in x (n is a       
                multiple of 4)                      
    alpha   scalar number is single-precision   
                floating-point                      
    x       array of vFloats              
    y       array of vFloats              
*************************************************************/
/*
 *  vSyax()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSyax(
  int32_t        n,
  float          alpha,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use vDSP_vsmul instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSzaxpy multiplies a vector x, by a scalar and  
 adds it to a vector y and stores the result in  
 vector Z.                                       
    n       number of floats in x (n is a       
                multiple of 4)                      
    alpha   scalar number is single-precision   
                floating-point                      
    x       array of vFloats              
    y       array of vFloats              
    Z       array of vFloats, where the   
                is stored                           
*************************************************************/
/*
 *  vSzaxpy()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSzaxpy(
  int32_t        n,
  float          alpha,
  const vFloat   *x,
  const vFloat   *y,
  vFloat         *z)
  API_DEPRECATED("Use vDSP_vsma instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);





/*
-------------------------------------------------------------------------------------
     Level 2
-------------------------------------------------------------------------------------
*/
/*************************************************************
 vSgemv multiplies an array of vFloats y by
 a  scalar beta, and takes the result and adds it
 to the product of a scalar alpha multiplied by  
 a matrix A multiplied by a vector x. The above  
 result is stored in array y. Futhermore, the    
 same function also performs the above calculation/
 with the transpose of matrix A, instead of      
 matrix A. In this function argument "forma"     
 distinguishes between the above two cases.      
    forma   indicates the form of matrix A to   
                use in the computation, where:      
                If forma = "n", Matrix A is used    
                If forma = "T", Transpose of Matrix 
                 A is used                          
    m       number of rows in matrix A and      
                depending on value of forma         
                if forma = "n", it is the length of 
                 vector y                           
                if forma = "T", it is the length of 
                 vector x. m is a multiple of 4     
    n       number of columns in matrix A and   
                depending on value of forma         
                if forma = "n", it is the length of 
                 vector x                           
                if forma = "T", it is the length of 
                 vector y. m is a multiple of 4     
    alpha   is a scaling constant               
    A       is an m by n matrix. Its elements   
                are vFloats                   
    x       is an array of vFloats        
    beta        is a scaling constant               
    y       is an array of vFloats        
*************************************************************/
/*
 *  vSgemv()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgemv(
  char           forma,
  int32_t        m,
  int32_t        n,
  float          alpha,
  const vFloat   *a,
  const vFloat   *x,
  float          beta,
  vFloat         *y)
  API_DEPRECATED("Use cblas_sgemv instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);





/*************************************************************
 vSgemx adds an array of vFloats y to the  
 product of an scalar alpha by a mtrix A         
 multiplied by an array of vFloats x. It   
 then stores the result in the vector y.         
    m       number of rows in matrix A and      
                the length of vector y. m is a      
            multiple of 4                       
    n       number of columns in matrix A and   
                the length of vector x. m is a      
            multiple of 4                       
    alpha   is a scaling constant               
    a       is an m by n matrix. Its elements   
                are vFloats                   
    x       is an array of vFloats        
    y       is an array of vFloats        
*************************************************************/
/*
 *  vSgemx()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgemx(
  int32_t        m,
  int32_t        n,
  float          alpha,
  const vFloat   *a,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use cblas_sgemv instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSgemtx takes the transpose of a mtrix A and    
 multiplies it by an array x. It then multiplies 
 the result by a scalar alpha. Finally adds the  
 above result to an array y and stores the result
 in array y.                                     
    m       number of rows in matrix A and      
                the length of vector x. m is a      
            multiple of 4                       
    n       number of columns in matrix A and   
                the length of vector y. m is a      
            multiple of 4                       
    alpha   is a scaling constant               
    a       is an m by n matrix. Its elements   
                are vFloats                   
    x       is an array of vFloats        
    y       is an array of vFloats        
*************************************************************/
/*
 *  vSgemtx()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgemtx(
  int32_t        m,
  int32_t        n,
  float          alpha,
  const vFloat   *a,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use cblas_sgemv instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*
-------------------------------------------------------------------------------------
     Level 3
-------------------------------------------------------------------------------------
*/


/*************************************************************
 vSgeadd performs matrix addition for general    
 matrices or their transposes.                   
    height  height of the matrix (it is multiple
                of 4)                               
    width   width of the matrix (it is multiple 
                of 4)                               
    A       matrix A, and depending on forma:   
                if forma='N', A is used in  the     
                computation, and A has m rows and   
                n columns                           
                if forma='T', A(T) is used in the   
                computation, and A has n rows and   
                m columns                           
    forma   indicates the form of matrix A to   
                use in the computation, where:      
                if forma='N', A is used in  the     
                computation                         
                if forma='T', A(T) is used in  the  
                computation                         
    b       matrix b, and depending on formb:   
                if formb='N', b is used in  the     
                computation, and b has m rows and   
                n columns                           
                if formb='T', b(T) is used in the   
                computation, and b has n rows and   
                m columns                           
    formb   indicates the form of matrix b to   
                use in the computation, where:      
                if forma='N', b is used in  the     
                computation                         
                if forma='T', b(T) is used in  the  
                computation                         
    c       is an m by n matrix c, containing   
                the reults of the computation       
*************************************************************/
/*
 *  vSgeadd()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgeadd(
  int32_t        height,
  int32_t        width,
  const vFloat   *a,
  char           forma,
  const vFloat   *b,
  char           formb,
  vFloat         *c)
  API_DEPRECATED("Use appleblas_sgeadd instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSgesub performs matrix subtraction for general 
 matrices or their transposes.                   
    height  height of the matrix (it is multiple
                of 4)                               
    width   width of the matrix (it is multiple 
                of 4)                               
    A       matrix A, and depending on forma:   
                if forma='N', A is used in  the     
                computation, and A has m rows and   
                n columns                           
                if forma='T', A(T) is used in the   
                computation, and A has n rows and   
                m columns                           
    forma   indicates the form of matrix A to   
                use in the computation, where:      
                if forma='N', A is used in  the     
                computation                         
                if forma='T', A(T) is used in  the  
                computation                         
    b       matrix b, and depending on formb:   
                if formb='N', b is used in  the     
                computation, and b has m rows and   
                n columns                           
                if formb='T', b(T) is used in the   
                computation, and b has n rows and   
                m columns                           
    formb   indicates the form of matrix b to   
                use in the computation, where:      
                if forma='N', b is used in  the     
                computation                         
                if forma='T', b(T) is used in  the  
                computation                         
    c       is an m by n matrix c, containing   
                the reults of the computation       
*************************************************************/
/*
 *  vSgesub()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgesub(
  int32_t        height,
  int32_t        width,
  const vFloat   *a,
  char           forma,
  const vFloat   *b,
  char           formb,
  vFloat         *c)
  API_DEPRECATED("Use appleblas_sgeadd instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSgemul performs matrix multiplication for      
 general matrices or their transposes.           
    l       height of the matrix A (it is       
                multiple of 4)                      
    m       width of  matrix A  (it is multiple 
                of 4)                               
    n       width of  matrix b  (it is multiple 
                of 4)                               
    A       matrix A, and depending on forma:   
                if forma='N', A is used in  the     
                computation, and A has l rows and   
                m columns                           
                if forma='T', A(T) is used in the   
                computation, and A has m rows and   
                l columns                           
    forma   indicates the form of matrix A to   
                use in the computation, where:      
                if forma='N', A is used in  the     
                computation                         
                if forma='T', A(T) is used in  the  
                computation                         
    b       matrix b, and depending on formb:   
                if formb='N', b is used in  the     
                computation, and b has m rows and   
                n columns                           
                if formb='T', b(T) is used in the   
                computation, and b has n rows and   
                m columns                           
    formb   indicates the form of matrix b to   
                use in the computation, where:      
                if forma='N', b is used in  the     
                computation                         
                if forma='T', b(T) is used in  the  
                computation                         
    matrix  is the matrix containing the     
                results of the computation           
*************************************************************/
/*
 *  vSgemul()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgemul(
  int32_t        l,
  int32_t        m,
  int32_t        n,
  const vFloat   *a,
  char           forma,
  const vFloat   *b,
  char           formb,
  vFloat         *matrix)
  API_DEPRECATED("Use cblas_sgemm instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);




/*************************************************************
 vSgemm performs combined matrix multiplication  
 and addition for general matrices or their transposes.                                     
    l       number of rows in matrix c (it is   
                multiple of 4)                      
    m       has the following meaning:          
                if forma='N', it is the number of   
                columns in matrix A                 
                if forma='T', it is the number of   
                rows in matrix A. In addition       
                if formb='N', it is the number of   
                rows in matrix b                    
                if formb='T', it is the number of   
                columns in matrix b                 
    n       columns in  matrix c                
    A       matrix A, and depending on forma:   
                if forma='N', A is used in  the     
                computation, and A has l rows and   
                m columns                           
                if forma='T', A(T) is used in the   
                computation, and A has m rows and   
                l columns                           
    forma   indicates the form of matrix A to   
                use in the computation, where:      
                if forma='N', A is used in  the     
                computation                         
                if forma='T', A(T) is used in  the  
                computation                         
    b       matrix b, and depending on formb:   
                if formb='N', b is used in  the     
                computation, and b has m rows and   
                n columns                           
                if formb='T', b(T) is used in the   
                computation, and b has n rows and   
                m columns                           
    formb   indicates the form of matrix b to   
                use in the computation, where:      
                if forma='N', b is used in  the     
                computation                         
                if forma='T', b(T) is used in  the  
                computation                         
    alpha   is a scalar                         
    beta        is a scalar                         
    matrix      is the l by n matrix          
*************************************************************/
/*
 *  vSgemm()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgemm(
  int32_t        l,
  int32_t        m,
  int32_t        n,
  const vFloat   *a,
  char           forma,
  const vFloat   *b,
  char           formb,
  vFloat         *c,
  float          alpha,
  float          beta,
  vFloat         *matrix)
  API_DEPRECATED("Use cblas_sgemm instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);





/*************************************************************
 vSgetmi performs general matrix transpose (in place).                                         
    size        is the number of rows and columns   
                in matrix x                         
*************************************************************/
/*
 *  vSgetmi()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgetmi(
  int32_t   size,
  vFloat    *x)
  API_DEPRECATED("Use appleblas_sgeadd instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);





/*************************************************************
 vSgetmo performs general matrix transpose (out-of-place).                                      
    height  is the height of the matrix         
    width   is the width of the matrix          
    x       array of vFloats              
    y       array of vFloats              
*************************************************************/
/*
 *  vSgetmo()
 *  
 *  Availability:
 *    Mac OS X:         in version 10.0 and later in vecLib.framework
 *    CarbonLib:        not in Carbon, but vecLib is compatible with CarbonLib
 *    Non-Carbon CFM:   in vecLib 1.0 and later
 */
extern void 
vSgetmo(
  int32_t        height,
  int32_t        width,
  const vFloat   *x,
  vFloat         *y)
  API_DEPRECATED("Use appleblas_sgeadd instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);





/*
	vsGevv puts the outer product of vectors A and B into matrix M.

	Input:

		l is the number of elements in A and the number of rows in M.
		n is the number of elements in B and the number of columns in N.
		A is a vector of L floating-point numbers.
		B is a vector of M floating-point numbers.
		M is an array with space for L*N floating-point numbers.

	Output:

		For 0 <= i < l and 0 <= j < n, C[i*n + j] = A[i] * B[j].

	Note:

		In the comments above, array elements are floating-point objects, in
		spite of the fact that the arrays are passed to the routine as pointers
		to vFloat.  For example, if A contains four floating-point numbers, l
		is 4, even though A contains only one vFloat object.

	Availability:

		Mac OS X:        In version 10.0 and later in vecLib.framework.
		CarbonLib:       Not in Carbon, but vecLib is compatible with CarbonLib.
		Non-Carbon CFM:  In vecLib 1.0 and later.
*/
extern void 
vSgevv(
  int32_t        l,
  int32_t        n,
  const vFloat   *A,
  const vFloat   *B,
  vFloat         *M)
  API_DEPRECATED("Use cblas_sger on a zero matrix instead", macos(10.0, 10.14))
  API_UNAVAILABLE(ios, tvos, watchos);


#endif	// defined _AltiVecPIMLanguageExtensionsAreEnabled || defined __SSE__


#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif
 

#ifdef __cplusplus
}
#endif

#endif /* __VECTOROPS__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/Solve.h
/*! @header
 *  This header provides sparse matrix solvers using "transparent" types
 *  (i.e. types where the matrix structure is exposed to the caller, in
 *  contrast to the opaque types used in the Sparse/BLAS.h header). A subset
 *  of the BLAS operations are also defined on these types.
 *
 *  @copyright 2016-2017 Apple, Inc.  All rights reserved.
 *  @unsorted
 */

/******************************************************************************
 *
 * We offer two different techniques for solving AX=B where A is sparse and
 * X, B are dense.
 *
 * DIRECT METHODS:    These perform a factorization of the matrix A, and are the
 *                    most reliable and easiest to use. It is recommended that
 *                    users try these first, and only resort to iterative methods
 *                    if time or memory constraints make these methods infeasible.
 *                    Once a factorization is performed additional solves with the
 *                    same A can be performed cheaply.
 *
 * ITERATIVE METHODS: These apply an iterative numerical algorithm to solve the
 *                    problem. They use less memory than direct methods, and can
 *                    be considerably faster if the problem is numerically well
 *                    conditioned or a good preconditioner is known. To safely
 *                    and correctly use these methods a greater degree of
 *                    mathematial knoweldge and effort is required of the user.
 *                    Subsequent solves require the same amount of computational
 *                    work, unless a good approximate solution is available.
 *                    For the best performance, a problem specific preconditioner
 *                    will need to be supplied by the user.
 *
 * ======================================
 * Direct Methods (Matrix Factorizations)
 * ======================================
 *
 * We offer the factorizations detailed below, but all use the same interface,
 * with the variant specified by the argument `type`. The most basic solution
 * sequence is:
 *   factors = SparseFactor(type, Matrix)
 *   SparseSolve(factors, rhs, soln)
 *   SparseCleanup(factors)
 *
 * It is sometimes required to perform repeated factorizations with the same
 * non-zero pattern but different numerical values. A SparseRefactor() entry
 * point is supplied that allows the reuse of an existing factor object and
 * its associated memory with different numerical values.
 *
 * If multiple different numeric factorizations with the same symbolic pattern
 * are required, or if the user wishes to perform ordering before numeric
 * values are known, a symbolic factorization object can be obtained by passing
 * a SparseMatrixStructure object to SparseFactor() in place of the full
 * SparseMatrix that also includes the numeric values. The underlying object
 * is reference counted, so this object may be safely destroyed by calling
 * SparseOpaqueDestroy() even if numeric factorizations that depend on it are
 * still in use. Due to this reference counting, if the user wishes to make a
 * shallow copy of the underlying object they should call SparseRetain().
 *
 * If the user wishes to apply matrix factors individually, they may obtain
 * opaque objects through the SparseCreateSubfactor() routine. These objects
 * may then be used through calls to SparseMultiply() and SparseSove().
 *
 * Cholesky
 * ========
 * A = PLL'P'
 * for real symmetric or complex Hermitian positive-definite matrices A.
 * If A is not positive-definite the factorization will detect this and fail,
 * potentially after significant computation.
 * P is a permutation matrix that is (by default) automatically calculated by
 * the solver (see options.orderMethod for further details).
 * L is the lower triangular factorization matrix.
 * By default no diagonal scaling matrix is applied to A, but one may be
 * enabled through options.scalingMethod.
 *
 * SparseSolve() will solve Ax = b.
 * SparseCreateSubfactor() allows the following subfactors to be extracted:
 * - SparseSubfactorL    returns an opaque object representing L. Both Multiply and Solve are valid.
 * - SparseSubfactorP    returns an opaque object representing P. Both Multiply and Solve are valid.
 * - SparseSubfactorPLPS returns an opaque object representing PLP'. Only Solve is valid, and
 *                       transpose solve followed by non-transpose solve is equivalent to a full
 *                       system solve with A.
 *
 * Symmetric Indefinite
 * ====================
 * SAS = PLDL'P'
 * for real symmetric or complex Hermitian matrices A.
 * P is a permutation matrix that is (by default) automatically calculated by
 * the solver (see options.orderMethod for further details).
 * S is a diagonal scaling matrix that is (by default) automatically calculated
 * by the solver (see options.scalingMethod for further details).
 * L is a unit lower triangular factorization matrix.
 * D is a block diagonal factorization matrix, with 1x1 and 2x2 diagonal blocks.
 * A variety of different pivoting options are offered:
 * - Unpivoted performs no numerical pivoting, and D only has 1x1 pivots. Only
 *   suitable for well behaved systems with full rank, otherwise very unstable.
 * - Supernode Bunch-Kaufmann (SBK) restricts pivoting to operations that do not
 *   alter the symbolic structure of the factors. Static pivoting (the addition
 *   (of sqrt(eps) to small diagonal entries) is used in the presence of small
 *   pivots. This method is often effective for well scaled matrices, but is
 *   not numerically stable for some systems.
 * - Threshold Partial Pivoting (TPP) is provably numerically stable, but at the
 *   cost of (potentially) increased factor size and number of operations.
 *
 * SparseSolve() will solve Ax = b.
 * SparseCreateSubfactor() allows the following sunfactors to be extracted:
 * - SparseSubfactorL    returns an opaque object representing L. Both Multiply and Solve are valid.
 * - SparseSubfactorD    returns an opaque object representing D. Both Multiply and Solve are valid.
 * - SparseSubfactorP    returns an opaque object representing P. Both Multiply and Solve are valid.
 * - SparseSubfactorS    returns an opaque object representing S. Both Multiply and Solve are valid.
 * - SparseSubfactorPLPS returns an opaque object representing PLP'S. When tranposed represents PLDP'S.
 *                    Only Solve is valid, and transpose solve followed by non-transpose solve is
 *                    equivalent to a full system solve with A.
 *
 * QR
 * ==
 * A = QRP      if m >= n so A is overdetermined or square
 * A = P'R'Q'   if m <  n so A is underdetermined
 * for real or complex matrices A of size m x n.
 * P is a column permutation that is (by default) automatically calculated by
 * the solver (see options.orderMethod for further details).
 * Q is an m x n (or n x m if underdetermined) orthagonal factor matrix.
 * R is an n x n (or m x m if underdetermined) upper triangular factor matrix.
 *
 * If a Cholesky factorization of A^T A is desired (being the factor R) consider
 * using the CholeskyAtA options below instead. This performs the same factorization but
 * without the overhead of storing the Q factor.
 *
 * We note that in many cases other methods of solving a given problem are normally faster
 * than the use of a Sparse QR factorization:
 * - For least squares, use a dedicated least squares solution method
 *   (e.g. Diagonally preconditioned LSMR).
 * - If a low rank approximation is required, multiply rank+5 random vectors by A and
 *   perform a dense QR of the result.
 *
 * SparseSolve() will solve either:
 * - x = arg min_x || Ax - b ||_2      if A is overdetermined.
 * - x = arg min_x || x ||_2 s.t. Ax=b if A is underdetermined.
 * SparseCreateSubfactor() allows the following sunfactors to be extracted:
 * - SparseSubfactorQ  returns an opaque object representing Q. Both Multiply and Solve are valid.
 * - SparseSubfactorR  returns an opaque object representing R. Both Multiply and Solve are valid.
 * - SparseSubfactorP  returns an opaque object representing P. Both Multiply and Solve are valid.
 * - SparseSubfactorRP returns an opaque object representing RP (or P'R'). Only Solve is valid.
 *
 * CholeskyAtA
 * ===========
 * A^TA = P'R'RP
 * for real matrices A.
 * This performs the same factorization as QR above, but avoids storing the Q factor resulting
 * in a significant storage saving. The number of rows in A must be greater than or equal to the
 * number of columns (otherwise A^T A is singular).
 *
 * SparseSolve() will solve A^TA x = b.
 * SparseCreateSubfactor() allows the following subfactors to be extracted:
 * - SparseSubfactorR  returns an opaque object representing R. Both Multiply and Solve are valid.
 * - SparseSubfactorP  returns an opaque object representing P. Both Multiply and Solve are valid.
 * - SparseSubfactorRP returns an opaque object representing RP (or P'R'). Only Solve is valid.
 *
 *
 * =====================================
 * Iterative Methods and Preconditioners
 * =====================================
 *
 * We offer the following iterative methods:
 * CG:    Conjugate Gradient method for symmetric positive-definite matrices.
 * GMRES: Generalised Minimum RESidual method and variants (FGMRES, DQGMRES) for
 *        symmetric indefinite and unsymmetric matrices.
 * LSMR:  Least Squares Minimum Residual method for solving least squares problems.
 *
 * The most basic solution sequence is:
 *
 *   SparseSolve( SparseCG(), X, B, A );
 *
 * However:
 * - Various method-specific options may be passed as the argument of SparseCG(), SparseGMRES()
 *   or SparseLSMR() in the first argument:
 *     SparseSolve( SparseCG( (SparseCGOptions) { .maxIterations=10 } ), X, B, A);
 * - The SparseMatrix argument A may be replaced by a block that applies the operator
 *   Y = Y + op(A) X where op(A) represents the application of the operator A or its transpose:
 *     SparseSolve( SparseLSMR(), X, B,
 *       ^void (enum CBLAS_TRANSPOSE trans, DenseMatrix_Double X, DenseMatrix_Double Y) {
 *         // Code to perform Y += op(A) X
 *       } );
 * - An optional Preconditioner may be supplied. If A is a SparseMatrix, this can be a predefined
 *   preconditioner supplied by the Accelerate library:
 *     SparseSolve( SparseCG(), X, B, SparsePreconditionerDiagonal, A );
 *   or be user specified (i.e. if a better preconditioner is available, or a routine for applying
 *   an operator is supplied by the user instead of a SparseMatrix A):
 *     SparseSolve( SparseCG(), X, B, (SparseOpaquePreconditioner) {
 *         .type  = SparsePreconditionerUser,
 *         .mem   = userDataPointer,
 *         .apply = userFunctionPointer
 *       }, A);
 *
 * If the user requires more control over convergence testing, or otherwise wishes to single-step
 * the method, they may instead perform a single iteration through a call of the form:
 *
 *  SparseIterate( SparseCG(), iteration, state, converged, X, B, R, Preconditioner, ApplyOperator);
 *
 * Users should note that solutions may not be available at all iterations, and a call with
 * iteration=-1 may be required to retrieve the current solution. Refer to the documentation for
 * individual methods before attempting to use this form.
 *
 * Further note that convience forms of this call using SparseMatrix data types and predefined
 * preconditioners are not offered.
 *
 * At present the following preconditioners are offered by this package:
 * SparsePreconditionerDiagonal     - Applies y=D^-1x, where D is the matrix containing only
 *                                    the diagonal elements of A. It provides a simple
 *                                    preconditioner for CG and GMRES methods.
 * SparsePreconditionerDiagScaling  - Applies y=D^-1x, where D is the diagonal matrix such that
 *                                    d_ii = || A_j ||_2 with A_j the j-th column of A. It provides
 *                                    a simple preconditioner for LSMR.
 *
 *****************************************************************************/


#ifndef SPARSE_SOLVE_HEADER
#define SPARSE_SOLVE_HEADER

#ifndef __has_include
# define __has_include(_) 0
#endif

#ifndef __has_feature
# define __has_feature(_) 0
#endif

#ifndef __has_attribute
# define __has_attribute(_) 0
#endif

/*  Standard attributes for public sparse interfaces.                         */
#if __has_attribute(overloadable)
#define SPARSE_PUBLIC_INTERFACE __attribute__((overloadable))

#include <limits.h>
#include <stdint.h>
#include <stdbool.h>
#include <stdlib.h>

#if defined __VECLIB__ /* Included via Accelerate */
# define SPARSE_INCLUDED_VIA_ACCELERATE
#endif

#if __has_include(<Accelerate/Accelerate.h>)
# include <Accelerate/Accelerate.h>
#else
# include <cblas.h>
#endif

// Due to changes in the implementation of OS_ENUM that would break ABI,
// we now define our own SPARSE_ENUM instead.
// Note that as specifying an enum_extensibility attribute causes Swift to import things
// differently compared to previous versions of Sparse, we disable it here to avoid
// breaking backwards compatability.
#if __has_attribute(enum_extensibility) && !defined(__swift__)
#  define __SPARSE_ENUM_ATTR __attribute__((enum_extensibility(open)))
#  define __SPARSE_ENUM_ATTR_CLOSED __attribute__((enum_extensibility(closed)))
#else
#  define __SPARSE_ENUM_ATTR
#  define __SPARSE_ENUM_ATTR_CLOSED
#endif // __has_attribute(enum_extensibility)

#if __has_feature(objc_fixed_enum) || __has_extension(cxx_strong_enums)
# define SPARSE_ENUM(_name, _type, ...) \
         typedef enum : _type { __VA_ARGS__ } _name##_t
# define SPARSE_CLOSED_ENUM(_name, _type, ...) \
         typedef enum : _type { __VA_ARGS__ } \
             __SPARSE_ENUM_ATTR_CLOSED _name##_t
#else
# define __SPARSE_ENUM_C_FALLBACK(_name, _type, ...) \
         typedef _type _name##_t; enum _name { __VA_ARGS__ }
# define SPARSE_ENUM(_name, _type, ...) \
         typedef _type _name##_t; enum { __VA_ARGS__ }
# define SPARSE_CLOSED_ENUM(_name, _type, ...) \
         __SPARSE_ENUM_C_FALLBACK(_name, _type, ## __VA_ARGS__) \
         __SPARSE_ENUM_ATTR_CLOSED
#endif

#if __has_include(<os/availability.h>)
# include <os/availability.h>
#else
# define API_AVAILABLE(_) /* nothing */
#endif

#if __has_feature(nullability)
#pragma clang assume_nonnull begin
#endif

/*******************************************************************************
 * @group Sparse Matrix Type Definitions
 ******************************************************************************/

/*! @abstract A flag to describe the type of matrix represented.
 *
 *  @discussion A SparseMatrixStructure object can represent several types of
 *  matrices:
 *
 *  @constant SparseOrdinary A "normal" sparse matrix without special structure.
 *
 *  @constant SparseTriangular A triangular sparse matrix with non-unit diagonal.
 *    The SparseTriangle_t field indicates which triangle (upper or lower)
 *    is used.
 *
 *  @constant SparseUnitTriangular A triangular sparse matrix with unit diagonal.
 *    The SparseTriangle_t field indicates which triangle (upper or lower)
 *    is used.
 *
 *  @constant SparseSymmetric A symmetric sparse matrix.  The SparseTriangle_t
 *    field indicates which triangle (upper or lower) is used to represent
 *    the matrix.                                                             */
SPARSE_ENUM(SparseKind, unsigned int,
  SparseOrdinary       = 0,
  SparseTriangular     = 1,
  SparseUnitTriangular = 2,
  SparseSymmetric      = 3,
);

/*! @abstract A flag to indicate which triangle of a matrix is used.
 *
 *  @constant SparseUpperTriangle
 *            For triangular and unit-triangular matrices, indicates that the
 *            upper triangle is to be used, and the lower triangle is implicitly
 *            zero.
 *            For symmetric matrices, indicates that the upper triangle is to
 *            be used; the lower triangle is implicitly defined by reflection.
 *
 *  @constant SparseLowerTriangle
 *            For triangular matrices, indicates that the lower triangle is to
 *            be used, and the upper triangle is implicitly zero.
 *            For symmetric matrices, indicates that the lower triangle is to
 *            be used; the upper triangle is implicitly defined by reflection.*/
SPARSE_CLOSED_ENUM(SparseTriangle, unsigned char,
  SparseUpperTriangle = 0,
  SparseLowerTriangle = 1
);

/*! @abstract A type representing the attributes of a matrix.
 *
 *  @field transpose If `true`, the matrix is implicitly transposed when used
 *  in any functions.
 *
 *  @field triangle If `kind` is `SparseOrdinary`, this field is ignored.
 *  Otherwise it indicates which triangle (upper or lower) represents the
 *  matrix.
 *
 *  @field kind Identifies the matrix as being full (`SparseOrdinary`), [unit-]
 *  triangular (`SparseTriangular`, `SparseUnitTriangular`), or symmetric
 *  (`SparseSymmetric`).
 *
 *  @field _reserved for future expansion. Must be zero.
 *
 *  @field _allocatedBySparse an implementation detail. Should be zero for any
 *  matrix you allocate.                                                      */
typedef struct {
  bool            transpose: 1;
  SparseTriangle_t triangle: 1;
  SparseKind_t         kind: 2;
  unsigned int    _reserved: 11;
  bool   _allocatedBySparse: 1;
} SparseAttributes_t;

/*! @abstract A type representing the sparsity structure of a sparse matrix.
 *
 *  @discussion The sparsity structure is represented in *block compressed
 *  sparse column* (block CSC) format. The matrix is divided into a regular
 *  grid of rowCount x columnCount blocks each of size `blockSize x blockSize`,
 *  and only blocks containing a non-zero entry are stored. CSC format is
 *  used to store the locations of these blocks. For each block column, a list
 *  of block row indices for non-zero blocks are stored, and the lists for each
 *  column are stored contigously one after the other. Hence the row indices
 *  for column j are given by rowIndices[columnStarts[j]:columnStarts[j+1]],
 *  where columnStarts[] is storing the location of the first index in each
 *  column.
 *  If the blockSize is 1, then this format is exactly equivalent to standard
 *  CSC format.
 *
 *  @field rowCount Number of (block) rows in matrix.
 *
 *  @field columnCount Number of (block) columns in matrix.
 *
 *  @field columnStarts Specifies where each (block) column starts in rowIndices
 *         array.
 *
 *  @field rowIndices Specifies the (block) row indices of the matrix.
 *
 *  @field attributes The attribute meta-data for the matrix, for example
 *         whether the matrix is symmetric and only half the entries are stored.
 *
 *  @field blockSize The block size of the matrix.                            */
typedef struct {
  int rowCount;
  int columnCount;
  long *columnStarts;
  int *rowIndices;
  SparseAttributes_t attributes;
  uint8_t blockSize;
} SparseMatrixStructure;

/*! @abstract A type representing a sparse matrix.
 *
 *  @discussion
 *  `data` is the array of values in the non-zero blocks of the matrix stored
 *  contiguously, each block in column-major order. If there are N structural
 *  non-zero blocks in the matrix, `data` holds `blockSize`*`blockSize`*`N`
 *  doubles.
 *
 * @field structure The symbolic structure of the matrix.
 *
 * @field data The numerical values of the matrix. If structure.blockSize > 1,
 *        blocks are stored contigously in column-major format.               */
typedef struct {
  SparseMatrixStructure structure;
  double *data;
} SparseMatrix_Double;

/*! @abstract A type representing a sparse matrix.
 *
 *  @discussion
 *  `data` is the array of values in the non-zero blocks of the matrix stored
 *  contiguously, each block in column-major order. If there are N structural
 *  non-zero blocks in the matrix, `data` holds `blockSize`*`blockSize`*`N`
 *  doubles.
 *
 * @field structure The symbolic structure of the matrix.
 *
 * @field data The numerical values of the matrix. If structure.blockSize > 1,
 *        blocks are stored contigously in column-major format.               */
typedef struct {
  SparseMatrixStructure structure;
  float *data;
} SparseMatrix_Float;

/******************************************************************************
 *  @group Conversion From Other Formats
 ******************************************************************************
 *  @discussion In the conversion functions below, the variables `rowCount`,
 *  `columnCount`, `blockCount`, `row[]` and `col[]` describe a sparse matrix
 *  structure with `blockCount` structurally non-zero entries, each of which
 *  is a "block".
 *
 *  The matrix described has `rowCount*blockSize` rows and
 *  `columnCount*blockSize` columns. For each `i` in `0..<blockCount`, there
 *  is a a structurally non-zero block at block position `(row[i], column[i])`
 *  with numerical values `data[i*blockSize*blockSize:(i+1)*blockSize*blockSize-1]`
 *  interpreted as the elements of a dense column-major matrix with `blockSize`
 *  rows and columns.
 *
 *  If the coordinates `(row[i], column[i])` are invalid (meaning that they
 *  lie outside the ranges `0..<rowCount` or `0..<columnCount`, respectively),
 *  or `attributes.kind` is `SparseTriangular` or `SparseUnitTriangular` and
 *  they lie in the wrong triangle, then that block element is ignored and
 *  not included in the returned matrix.
 *
 *  If `attributes.kind` is `SparseSymmetric`, any entries in the wrong
 *  triangle are transposed and summed into the block at `(column[i], row[i])`
 *  if one is present.
 *
 *  In all cases, if any duplicate coordinates are present, the elements are
 *  summed and replaced with a single entry.
 *
 *  There are two variants of each converter; one which allocates its own
 *  workspace internally, and also allocates space for the resulting sparse
 *  matrix, and one which requires you to pass storage for the new matrix and
 *  a separate workspace for precise control over allocations.                */

/*! @abstract Convert from coordinate format arrays to a SparseMatrix_Double
 *  object, dropping out-of-range entries and summing duplicates.
 *
 *  @discussion For symmetric matrices, entries are accepted in either triangle
 *  (if they are in the "wrong" triangle as specified by attributes.triangle,
 *  they are transposed, and if an entry is already present, are treated as
 *  duplicates and summed).
 *  For triangular matrices, entries in the "wrong" triangle as specified by
 *  attributes.triangle are treated as out-of-range and dropped.
 *
 *  @param rowCount (input) Number of rows in structure.
 *
 *  @param columnCount (input) Number of columns in structure.
 *
 *  @param blockCount (input) Number of blocks in matrix.
 *
 *  @param blockSize (input) Block size for data storage on both input and
 *  ouput.
 *
 *  @param attributes (input) Attributes of matrix to create. The matrix will
 *  be forced to conform to the specified attributes by copying or dropping
 *  elements as needed.
 *
 *  @param row (input) Row indices of matrix structure.
 *
 *  @param column (input) Column indices of matrix structure.
 *
 *  @param data (input) The contents of the structurally non-zero (block)
 *  matrix elements.
 *
 *  @return A new SparseMatrix_Double object. When you are done with this
 *  matrix, release the memory that has been allocated by calling
 *  SparseCleanup( ) on it.                                                   */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Double SparseConvertFromCoordinate(int rowCount, int columnCount,
  long blockCount, uint8_t blockSize, SparseAttributes_t attributes,
  const int *row, const int *column, const double *data)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Convert from coordinate format arrays to a SparseMatrix_Float
 *  object, dropping out-of-range entries and summing duplicates.
 *
 *  @discussion For symmetric matrices, entries are accepted in either triangle
 *  (if they are in the "wrong" triangle as specified by attributes.triangle,
 *  they are transposed, and if an entry is already present, are treated as
 *  duplicates and summed).
 *  For triangular matrices, entries in the "wrong" triangle as specified by
 *  attributes.triangle are treated as out-of-range and dropped.
 *
 *  @param rowCount (input) Number of rows in structure.
 *
 *  @param columnCount (input) Number of columns in structure.
 *
 *  @param blockCount (input) Number of blocks in matrix.
 *
 *  @param blockSize (input) Block size for data storage on both input and
 *  ouput.
 *
 *  @param attributes (input) Attributes of matrix to create. The matrix will
 *  be forced to conform to the specified attributes by copying or dropping
 *  elements as needed.
 *
 *  @param row (input) Row indices of matrix structure.
 *
 *  @param column (input) Column indices of matrix structure.
 *
 *  @param data (input) The contents of the structurally non-zero (block)
 *  matrix elements.
 *
 *  @return A new SparseMatrix_Float object. When you are done with this
 *  matrix, release the memory that has been allocated by calling
 *  SparseCleanup( ) on it.                                                   */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Float SparseConvertFromCoordinate(int rowCount, int columnCount, long blockCount,
  uint8_t blockSize, SparseAttributes_t attributes, const int *row,
  const int *column, const float *data)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Convert from coordinate format arrays to a SparseMatrix_Double
 *  object, dropping out-of-range entries and summing duplicates.
 *
 *  @discussion For symmetric matrices, entries are accepted in either triangle 
 *  (if they are in the "wrong" triangle as specified by attributes.triangle, 
 *  they are transposed, and if an entry is already present, are treated as 
 *  duplicates and summed).
 *  For triangular matrices, entries in the "wrong" triangle as specified by
 *  attributes.triangle are treated as out-of-range and dropped.
 *
 *  @param rowCount (input) Number of rows in structure.
 *
 *  @param columnCount (input) Number of columns in structure.
 *
 *  @param blockCount (input) Number of blocks in matrix.
 *
 *  @param blockSize (input) Block size for data storage on both input and
 *  ouput.
 *
 *  @param attributes (input) Attributes of matrix to create. The matrix will
 *  be forced to conform to the specified attributes by copying or dropping
 *  elements as needed.
 *
 *  @param row (input) Row indices of matrix structure.
 *
 *  @param column (input) Column indices of matrix structure.
 *
 *  @param data (input) The contents of the structurally non-zero (block)
 *  matrix elements.
 *
 *  @param storage (output) A block of memory of size at least:
 *
 *    48 + (columnCount+1)*sizeof(long) + blockCount*sizeof(int)
 *      + blockCount*blockSize*blockSize*sizeof(double)
 *
 *  The returned structures .structure.columnStarts, .structure.rowIndices,
 *  and .data will point into this storage. You are responsible for managing
 *  the allocation and cleanup of this memory.
 *
 *  @param workspace (scratch) Workspace of size rowCount*sizeof(int).
 *
 *  @return A new SparseMatrix_Double object, using the memory you provided in
 *  the `storage` parameter.                                                  */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Double SparseConvertFromCoordinate(int rowCount, int columnCount, long blockCount,
  uint8_t blockSize, SparseAttributes_t attributes, const int *row,
  const int *column, const double *data, void *storage, void *workspace)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Convert from coordinate format arrays to a SparseMatrix_Float
 *  object, dropping out-of-range entries and summing duplicates.
 *
 *  @discussion For symmetric matrices, entries are accepted in either triangle
 *  (if they are in the "wrong" triangle as specified by attributes.triangle,
 *  they are transposed, and if an entry is already present, are treated as
 *  duplicates and summed).
 *  For triangular matrices, entries in the "wrong" triangle as specified by
 *  attributes.triangle are treated as out-of-range and dropped.
 *
 *  @param rowCount (input) Number of rows in structure.
 *
 *  @param columnCount (input) Number of columns in structure.
 *
 *  @param blockCount (input) Number of blocks in matrix.
 *
 *  @param blockSize (input) Block size for data storage on both input and
 *  ouput.
 *
 *  @param attributes (input) Attributes of matrix to create. The matrix will
 *  be forced to conform to the specified attributes by copying or dropping
 *  elements as needed.
 *
 *  @param row (input) Row indices of matrix structure.
 *
 *  @param column (input) Column indices of matrix structure.
 *
 *  @param data (input) The contents of the structurally non-zero (block)
 *  matrix elements.
 *
 *  @param storage (output) A block of memory of size at least:
 *
 *    48 + (columnCount+1)*sizeof(long) + blockCount*sizeof(int)
 *      + blockCount*blockSize*blockSize*sizeof(float)
 *
 *  The returned structures .structure.columnStarts, .structure.rowIndices,
 *  and .data will point into this storage. You are responsible for managing
 *  the allocation and cleanup of this memory.
 *
 *  @param workspace (scratch) Workspace of size rowCount*sizeof(int).
 *
 *  @return A new SparseMatrix_Float object, using the memory you provided in
 *  the `storage` parameter.                                                  */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Float SparseConvertFromCoordinate(int rowCount, int columnCount, long blockCount,
  uint8_t blockSize, SparseAttributes_t attributes, const int *row,
  const int *column, const float *data, void *storage, void *workspace)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

#if defined __SPARSE_TYPES_H
/*! @group Conversions from opaque sparse matrix types
 *
 *  @discussion The older sparse headers on iOS and OS X (Sparse/BLAS.h) use
 *  a separate set of types that do not expose the internal storage of the
 *  sparse matrix. To assist in interoperation with the newer interfaces here,
 *  we provide a set of conversion functions from those types.                */

/*! @abstract Converts an opaque sparse_matrix_double object to a transparent
 *  SparseMatrix_Double object. When you are done with this matrix, release
 *  the memory that has been allocated by calling SparseCleanup( ) on it.
 *
 *  @param matrix The matrix to be converted.                                 */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Double SparseConvertFromOpaque(sparse_matrix_double matrix)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Converts an opaque sparse_matrix_float object to a transparent
 *  SparseMatrix_Float object. When you are done with this matrix, release
 *  the memory that has been allocated by calling SparseCleanup( ) on it.
 *
 *  @param matrix The matrix to be converted.                                 */
static inline SPARSE_PUBLIC_INTERFACE
SparseMatrix_Float SparseConvertFromOpaque(sparse_matrix_float matrix)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );
#endif /* defined __SPARSE_TYPES_H */

/******************************************************************************
 *  @group Dense Matrices and Vectors
 ******************************************************************************/

/*! @abstract Contains a dense vector of double values.
 *
 *  @field count    Number of entries in the vector.
 *  @field data     The array of double values.                               */
typedef struct {
  int count;
  double *data;
} DenseVector_Double;

/*! @abstract Contains a dense vector of float values.
 *
 *  @field count    Number of entries in the vector.
 *  @field data     The array of float values.                                */
typedef struct {
  int count;
  float *data;
} DenseVector_Float;

/*! @abstract Contains a dense `rowCount` x `columnCount` matrix of double
 *            values stored in column-major order.
 *
 *  @field rowCount     Number of rows in the matrix.
 *  @field columnCount  Number of columns in the matrix.
 *  @field columnStride The column stride of the matrix.
 *  @field attributes   The attributes of the matrix, for example whether the
 *                      matrix is symmetrical or triangular.
 *  @field data         The array of double values in column-major order.     */
typedef struct {
  int rowCount;
  int columnCount;
  int columnStride;
  SparseAttributes_t attributes;
  double *data;
} DenseMatrix_Double;

/*! @abstract Contains a dense `rowCount` x `columnCount` matrix of float
 *            values stored in column-major order.
 *
 *  @field rowCount     Number of rows in the matrix.
 *  @field columnCount  Number of columns in the matrix.
 *  @field columnStride The column stride of the matrix.
 *  @field attributes   The attributes of the matrix, for example whether the
 *                      matrix is symmetrical or triangular.
 *  @field data         The array of float values in column-major order.      */
typedef struct {
  int rowCount;
  int columnCount;
  int columnStride;
  SparseAttributes_t attributes;
  float *data;
} DenseMatrix_Float;

/******************************************************************************
 *  @group Sparse Factorization Related Types
 ******************************************************************************/

/*! @abstract Status field for a factorization.
 *
 *  @constant SparseStatusOK              Factorization was successful.
 *  @constant SparseFactorizationFailed   Factorization failed due to a
 *                                        numerical issue.
 *  @constant SparseMatrixIsSingular      Factorization aborted as matrix is
 *                                        singular.
 *  @constant SparseInternalError         Factorization encountered an internal
 *                                        error (e.g. failed to allocate memory).
 *  @constant SparseParameterError        Error in user-supplied parameter.
 *  @constant SparseStatusReleased        Factorization object has been freed.
 */
SPARSE_ENUM(SparseStatus, int,
  SparseStatusOK            =  0,
  SparseFactorizationFailed = -1,
  SparseMatrixIsSingular    = -2,
  SparseInternalError       = -3,
  SparseParameterError      = -4,
  SparseStatusReleased      = -INT_MAX,
);

/*! @abstract Types of factorization than can be performed.
 *
 *  @constant SparseFactorizationCholesky
 *              Cholesky (LL^T) factorization.
 *  @constant SparseFactorizationLDLT
 *              Default LDL^T factorization (currently LDL^T with TPP).
 *  @constant SparseFactorizationLDLTUnpivoted
 *              Cholesky-like LDL^T with only 1x1 pivots and no pivoting.
 *  @constant SparseFactorizationLDLTSBK
 *              LDL^T with Supernode Bunch-Kaufman and static pivoting.
 *  @constant SparseFactorizationLDLTTPP
 *              LDL^T with full threshold partial pivoting.
 *  @constant SparseFactorizationQR
 *              QR factorization.
 *  @constant SparseFactorizationCholeskyAtA
 *              QR factorization without storing Q (equivalent to A^TA = R^T R).
 */
SPARSE_ENUM(SparseFactorization, uint8_t,
  SparseFactorizationCholesky = 0,
  SparseFactorizationLDLT = 1,
  SparseFactorizationLDLTUnpivoted = 2,
  SparseFactorizationLDLTSBK = 3,
  SparseFactorizationLDLTTPP = 4,
  SparseFactorizationQR = 40,
  SparseFactorizationCholeskyAtA = 41
);

/*! @abstract Control flags for matrix factorization.
 *
 *  @constant SparseDefaultControl  Use default values.
 */
SPARSE_ENUM(SparseControl, uint32_t,
  SparseDefaultControl = 0
);

/*! @abstract Specifies type of fill-reducing ordering.
 *
 *  @discussion The order in which variables are eliminated (i.e. the column/row ordering) in a
 *  sparse factorization makes a big difference to the size of the resulting factors and amount of
 *  work required to calculate them (it is probably the biggest single factor). Minimizing the size
 *  or work required is an NP-complete problem, so only heuristics are implemented in this library.
 *
 *  We note that AMD-based orderings tend to be fast and provide good quality for small matrices,
 *  whilst nested dissection based orderings (such as metis) are normally considerably slower to
 *  compute, but provide better quality orderings for larger problems, and expose more parallelism
 *  during the factorization. We recommend AMD is used unless the problem is very large (millions
 *  of rows and columns) or you will perform many repeated factorizations. If you are uncertain,
 *  try both and see which gives better performance for your usage.
 *
 *  The AMD and MeTiS orderings provide good orderings for symmetric matrices. They can be used
 *  for the QR factorizations, but this involves forming A^TA explicitly, which is expensive.
 *  COLAMD on the other hand finds an ordering for A^T A whilst only working with A. For this
 *  reason, COLAMD cannot be used for symmetric factorizations.
 *
 *  @constant SparseOrderDefault
 *              Default ordering (AMD for symmetric and COLAMD for unsymmetric
 *              factorizations, but this may change if better algorithms become
 *              available).
 *  @constant SparseOrderUser
 *              User-supplied ordering, or identity if options->order is NULL
 *  @constant SparseOrderAMD
 *              AMD ordering. Large overhead cost if used for QR-based
 *              factorization due to explicit formation of A^T A.
 *  @constant SparseOrderMetis
 *              MeTiS Nested Dissection ordering. Large overhead cost if used
 *              for QR-based factorization due to explicit formation of A^T A.
 *  @constant SparseOrderCOLAMD
 *              Column AMD ordering for A^T A. Not valid for symmetric
 *              factorizations (use AMD instead).                             */
SPARSE_ENUM(SparseOrder, uint8_t,
  SparseOrderDefault = 0,
  SparseOrderUser = 1,
  SparseOrderAMD = 2,
  SparseOrderMetis = 3,
  SparseOrderCOLAMD = 4,
);

/*! @abstract Specifies type of scaling to be performed.
 *
 *  @constant SparseScalingDefault
 *              Default scaling (at present EquilibriationInf if LDL^T, or no
 *              scaling if Cholesky).
 *  @constant SparseScalingUser
 *              User scaling if options.scaling is non-NULL, otherwise no
 *              scaling.
 *  @constant SparseScalingEquilibriationInf
 *              Norm equilibriation scaling using inf norm.                   */
SPARSE_ENUM(SparseScaling, uint8_t,
  SparseScalingDefault = 0,
  SparseScalingUser = 1,
  SparseScalingEquilibriationInf = 2,
);

/*! @typedef SparseSymbolicFactorOptions
 *  @abstract Options that affect the symbolic stage of a sparse factorization.
 *
 *  @field control
 *    Flags controlling the computation.
 *
 *  @field orderMethod
 *    Ordering algorithm to use.
 *
 *  @field order    User-supplied array for ordering.
 *    Either NULL or a pointer to a row permutation that reduces fill in the
 *    matrix being factored.
 *    If `orderMethod` is `SparseOrderUser`, and this pointer is NULL, the
 *      original matrix ordering is used.
 *    If `orderMethod` is `SparseOrderUser`, and this pointer is non-NULL,
 *      the user-provided permutation is used to order the matrix before
 *      factorization.
 *    If `orderMethod` is not `SparseOrderUser`, the factor function will
 *      compute its own fill reducing ordering.
 *    If this pointer is non-NULL, the computed permutation will be returned in
 *      the array.
 *
 *  @field ignoreRowsAndColumns  Ignore rows and columns listed in this array.
 *    In some cases it is useful to ignore specified rows and columns,
 *    if this array is not NULL, it provides a list of rows and columns to
 *    ignore, terminated by a negative index.
 *    Note that this the row and column indices are for the actual matrix, not
 *    of its block structure, so 0 indicates the first row, not the first
 *    blockSize rows.
 *    In the symmetric case (Cholesky, LDL^T) each entry indicates that the
 *    matching row AND column should be ignored.
 *    In the unsymmetric case (QR, Cholesky A^TA) an index i<m indicates that
 *    row m should be ignored and an index i>=m indicates that column (i-m)
 *    should be ignored (where m is the number of rows in A,
 *    i.e. m=A.structure.rowCount*A.blockSize if A is not transposed, or
 *    m=A.structure.columnCount*A.blockSize if A is transposed).
 *
 *  \@callback malloc Function to use for allocation of internal memory
 *    \@discussion Memory will be freed through the free() callback. If this
 *      function pointer is NULL, the system malloc() will be used.
 *    \@param size Size of space to allocate in bytes.
 *    \@result Pointer to newly allocated memory, or NULL if allocation failed.
 *      The returned pointer must be 16-byte aligned (this requirement is
 *      satisfied by the system malloc()).
 *
 *  \@callback free Function to use to free memory allocated by malloc() callback.
 *    @discussion If this function pointer is NULL, the system free() will be
 *      used.
 *    \@param pointer Pointer to memory to be freed.
 *
 *  \@callback reportError Function to use to report parameter errors.
 *    \@param message
 *    \@discussion If NULL, errors are logged via <os/log.h> and execution is
 *      halted via __builtin_trap().  If non-NULL, the provided function is
 *      called with a human-readable string describing the error condition.
 *      If the callback returns, control will be returned to the caller with
 *      any outputs in a safe but undefined state (i.e. they may hold partial
 *      results or garbage, but all sizes and pointers are valid).            */
typedef struct {
  SparseControl_t control;
  SparseOrder_t orderMethod;
  int * _Nullable order;
  int *_Nullable ignoreRowsAndColumns;
  void * _Nullable(* _Nonnull malloc)(size_t size);
  void (* _Nonnull free)(void * _Nullable pointer);
  void (* _Nullable reportError)(const char *message);
} SparseSymbolicFactorOptions;

/*! @abstract Options that affect the numerical stage of a sparse factorization.
 *
 *  @field control
 *    Flags controlling the computation.
 *
 *  @field scalingMethod
 *    Scaling method to use.
 *
 *  @field scaling      User-supplied array for scaling.
 *    Either NULL or a pointer to an array of real values with length greater
 *    than or equal to the size of the matrix being factored. The type of the
 *    array values is the element type of the matrix.
 *    If `scalingMethod` is `SparseScalingUser`, and this pointer is NULL, no
 *      scaling is applied.
 *    If `scalingMethod` is `SparseScalingUser`, and this pointer is non-NULL,
 *      the user-provided array is used to scale the matrix before factorization.
 *    If `scalingMethod` is not `SparseScalingUser`, the factor function will
 *      compute its own scaling.
 *    If this pointer is non-NULL, the computed scaling will be returned in the
 *      array.
 *
 *  @field pivotTolerance
 *    Pivot tolerance used by threshold partial pivoting.
 *    Clamped to range [0,0.5].
 *
 *  @field zeroTolerance
 *    Zero tolerance used by some pivoting modes.
 *    Values less than zeroTolerance in absolute value will be treated as zero.
 */
typedef struct {
  SparseControl_t control;
  SparseScaling_t scalingMethod;
  void * _Nullable scaling;
  double pivotTolerance;
  double zeroTolerance;
} SparseNumericFactorOptions;

/*! @abstract A semi-opaque type representing symbolic matrix factorization.
 *
 *  @discussion Represents a symbolic matrix factorization (i.e. the pattern of
 *              the factors without the values). A single symbolic factorization
 *              may be the basis for multiple numerical factorizations of
 *              matrices with the same pattern but different values non-zero
 *              values.
 *
 *              Use the `SparseCleanup` function to free resources held by these
 *              objects. The internal factorization pointer is refence counted,
 *              so it is safe to destroy this object even if numeric
 *              factorizations exist that still depend on it.
 *
 *  @field status
 *    Indicates status of factorization object.
 *
 *  @field type
 *    Type fo factorization this represents.
 *
 *  @field rowCount
 *    Copy of field from SparseMatrixStructure passed to SparseFactor() call
 *    used to construct this symbolic factorization.
 *
 *  @field columnCount
 *    Copy of field from SparseMatrixStructure passed to SparseFactor() call
 *    used to construct this symbolic factorization.
 *
 *  @field attributes
 *    Copy of field from SparseMatrixStructure passed to SparseFactor() call
 *    used to construct this symbolic factorization.
 *
 *  @field blockSize
 *    Copy of field from SparseMatrixStructure passed to SparseFactor() call
 *    used to construct this symbolic factorization.
 *
 *  @field factorization
 *    Pointer to private internal representation of symbolic factor.
 *
 *  @field workspaceSize_Float
 *    Size, in bytes, of workspace required to perform numerical factorization
 *    in float.
 *
 *  @field workspaceSize_Double
 *    Size, in bytes, of workspace required to perform numerical factorization
 *    in double.
 *
 *  @field factorSize_Float
 *    Minimum size, in bytes, required to store numerical factors in float.
 *    If numerical pivoting requires a pivot to be delayed, the actual size
 *    required may be larger.
 *
 *  @field factorSize_Double
 *    Minimum size, in bytes, required to store numerical factors in double.
 *    If numerical pivoting requires a pivot to be delayed, the actual size
 *    required may be larger.                                                 */
typedef struct {
  SparseStatus_t status;
  int rowCount;
  int columnCount;
  SparseAttributes_t attributes;
  uint8_t blockSize;
  SparseFactorization_t type;
  void *_Nullable factorization;
  size_t workspaceSize_Float;
  size_t workspaceSize_Double;
  size_t factorSize_Float;
  size_t factorSize_Double;
} SparseOpaqueSymbolicFactorization;

/*! @abstract A semi-opaque type representing a matrix factorization in double.
 *
 *  @discussion Use the `SparseCleanup` function to free resources held
 *  by these objects.
 *
 *  The object can be in one of the following states:
 *  1) Something went wrong with symbolic factorization, nothing is valid.
 *     - indicated by .symbolicFactorization.status < 0
 *  2) Symbolic factorization was good, but failed in numeric factorization
 *     initialization.
 *     - indicated by .symbolicFactorization.status >= 0 && .status < 0 &&
 *                    .numericFactorization == NULL
 *     - symbolic factorization may be used for future calls.
 *  3) Symbolic factorization was good, factor allocated/initialized correctly,
 *     but numeric factorization failed
 *     e.g. a Cholesky factorization of an indefinite matrix was attempted.
 *     - indicated by .symbolicFactorization.status >= 0 && .status < 0 &&
 *                    .numericFactorization not NULL
 *     - user may pass this object to SparseRefactor_Double() with a modified
 *       matrix
 *  4) Symbolic and numeric factorizations are both good
 *     - indicated by .status >= 0
 *
 * @field status
 *    Indicates status of factorization object.
 *
 * @field attributes
 *    Flags associated with this factorization object. In particular, transpose
 *    field indicates whether object is considered to be factorization of A or
 *    A^T.
 *
 * @field symbolicFactorization
 *    Symbolic Factorization upon which this Numeric Factorization depends.
 *
 * @field userFactorStorage
 *    Flag that indicates if user provided storage backing this object. If
 *    true, then factor storage must be freed by the user once all references
 *    are finished with (though any additional storage allocated due to pivoting
 *    will still be freed by SparseCleanup()).
 *
 * @field numericFactorization
 *    Pointer to private internal representation of numeric factor.
 *
 * @field solveWorkspaceRequiredStatic
 *    The required size of workspace, in bytes) for a call to SparseSolve is
 *    solveWorkspaceRequiredStatic + nrhs * solveWorkspaceRequiredPerRHS
 *    where nrhs is the number of right-hand side vectors.
 *
 * @field solveWorkspaceRequiredPerRHS
 *    The required size of workspace, in bytes) for a call to SparseSolve is
 *    solveWorkspaceRequiredStatic + nrhs * solveWorkspaceRequiredPerRHS
 *    where nrhs is the number of right-hand side vectors.                    */
typedef struct {
  SparseStatus_t status;
  SparseAttributes_t attributes;
  SparseOpaqueSymbolicFactorization symbolicFactorization;
  bool userFactorStorage;
  void *_Nullable numericFactorization;
  size_t solveWorkspaceRequiredStatic;
  size_t solveWorkspaceRequiredPerRHS;
} SparseOpaqueFactorization_Double;

/*! @abstract A semi-opaque type representing a matrix factorization in float.
 *
 *  @discussion Use the `SparseCleanup` function to free resources held
 *  by these objects.
 *
 *  The object can be in one of the following states:
 *  1) Something went wrong with symbolic factorization, nothing is valid.
 *     - indicated by .symbolicFactorization.status < 0
 *  2) Symbolic factorization was good, but failed in numeric factorization
 *     initialization.
 *     - indicated by .symbolicFactorization.status >= 0 && .status < 0 &&
 *                    .numericFactorization == NULL
 *     - symbolic factorization may be used for future calls.
 *  3) Symbolic factorization was good, factor allocated/initialized correctly,
 *     but numeric factorization failed
 *     e.g. a Cholesky factorization of an indefinite matrix was attempted.
 *     - indicated by .symbolicFactorization.status >= 0 && .status < 0 &&
 *                    .numericFactorization not NULL
 *     - user may pass this object to SparseRefactor_Double() with a modified
 *       matrix
 *  4) Symbolic and numeric factorizations are both good
 *     - indicated by .status >= 0
 *
 * @field status
 *    Indicates status of factorization object.
 *
 * @field attributes
 *    Flags associated with this factorization object. In particular, transpose
 *    field indicates whether object is considered to be factorization of A or
 *    A^T.
 *
 * @field symbolicFactorization
 *    Symbolic Factorization upon which this Numeric Factorization depends.
 *
 * @field userFactorStorage
 *    Flag that indicates if user provided storage backing this object. If
 *    true, then factor storage must be freed by the user once all references
 *    are finished with (though any additional storage allocated due to pivoting
 *    will still be freed by SparseCleanup()).
 *
 * @field numericFactorization
 *    Pointer to private internal representation of numeric factor.
 *
 * @field solveWorkspaceRequiredStatic
 *    The required size of workspace, in bytes) for a call to SparseSolve is
 *    solveWorkspaceRequiredStatic + nrhs * solveWorkspaceRequiredPerRHS
 *    where nrhs is the number of right-hand side vectors.
 *
 * @field solveWorkspaceRequiredPerRHS
 *    The required size of workspace, in bytes) for a call to SparseSolve is
 *    solveWorkspaceRequiredStatic + nrhs * solveWorkspaceRequiredPerRHS
 *    where nrhs is the number of right-hand side vectors.                    */
typedef struct {
  SparseStatus_t status;
  SparseAttributes_t attributes;
  SparseOpaqueSymbolicFactorization symbolicFactorization;
  bool userFactorStorage;
  void *_Nullable numericFactorization;
  size_t solveWorkspaceRequiredStatic;
  size_t solveWorkspaceRequiredPerRHS;
} SparseOpaqueFactorization_Float;

/*******************************************************************************
 * @group Sub-factor Types
 ******************************************************************************/

/*! @abstract Types of sub-factor object.
 *
 *  @constant SparseSubfactorInvalid
 *    Invalid subfactor (requested type not compatible with supplied factorization
 *    or already destroyed).
 *  @constant SparseSubfactorP
 *    Permutation subfactor, valid for all factorization types.
 *  @constant SparseSubfactorS
 *    Diagonal scaling subfactor, valid for Cholesky and LDL^T only.
 *  @constant SparseSubfactorL
 *    L factor subfactor, valid for Cholesky and LDL^T only.
 *  @constant SparseSubfactorD
 *    D factor subfactor, valid for LDL^T only.
 *  @constant SparseSubfactorPLPS
 *    Half-solve subfactor, valid for Cholesky and LDL^T only.
 *    Corresponds to PLP' on forward (non-transpose) solve, and
 *    corresponds to PLDP' on backward (transpose) solve (D=I for Chokesky).
 *  @constant SparseSubfactorQ
 *    Q factor subfactor, valid for QR only.
 *  @constant SparseSubfactorR
 *    R factor subfactor, valid for QR and CholeskyAtA only.
 *  @constant SparseSubfactorRP
 *    Half-solve subfactor, valid for QR and CholeskyAtA only.                   */
SPARSE_ENUM(SparseSubfactor, uint8_t,
        SparseSubfactorInvalid = 0,
        SparseSubfactorP = 1,
        SparseSubfactorS = 2,
        SparseSubfactorL = 3,
        SparseSubfactorD = 4,
        SparseSubfactorPLPS = 5,
        SparseSubfactorQ = 6,
        SparseSubfactorR = 7,
        SparseSubfactorRP = 8
        );

/*! @abstract Represents a sub-factor of the factorization (eg  L from LDL^T).
 *
 *  @field attributes
 *    Attributes of subfactor. Notably transpose indicates whether it should be
 *    considered as the transpose of its underlying contents (e.g. should it
 *    count as L or L^T if .contents=SparseSubfactorL).
 *  @field contents
 *    Subfactor this represents, e.g. L or Q.
 *  @field factor
 *    Underlying factorization this subfactor is part of.
 *  @field workspaceRequiredStatic
 *    The size of the workspace, in bytes, required to perform SparseMultiply()
 *    or SparseSolve() with this subfactor is given by the expression:
 *    workspaceRequiredStatic + nrhs*workspaceRequiredPerRhs
 *    where nrhs is the number of right-hand side vectors.
 *  @field workspaceRequiredPerRHS
 *    The size of the workspace, in bytes, required to perform SparseMultiply()
 *    or SparseSolve() with this subfactor is given by the expression:
 *    workspaceRequiredStatic + nrhs*workspaceRequiredPerRhs
 *    where nrhs is the number of right-hand side vectors.
 */
typedef struct {
  SparseAttributes_t attributes;
  SparseSubfactor_t contents;
  SparseOpaqueFactorization_Double factor;
  size_t workspaceRequiredStatic;
  size_t workspaceRequiredPerRHS;
} SparseOpaqueSubfactor_Double;

/*! @abstract Represents a sub-factor of the factorization (eg  L from LDL^T).
 *
 *  @field attributes
 *    Attributes of subfactor. Notably transpose indicates whether it should be
 *    considered as the transpose of its underlying contents (e.g. should it
 *    count as L or L^T if .contents=SparseSubfactorL).
 *  @field contents
 *    Subfactor this represents, e.g. L or Q.
 *  @field factor
 *    Underlying factorization this subfactor is part of.
 *  @field workspaceRequiredStatic
 *    The size of the workspace, in bytes, required to perform SparseMultiply()
 *    or SparseSolve() with this subfactor is given by the expression:
 *    workspaceRequiredStatic + nrhs*workspaceRequiredPerRhs
 *    where nrhs is the number of right-hand side vectors.
 *  @field workspaceRequiredPerRHS
 *    The size of the workspace, in bytes, required to perform SparseMultiply()
 *    or SparseSolve() with this subfactor is given by the expression:
 *    workspaceRequiredStatic + nrhs*workspaceRequiredPerRhs
 *    where nrhs is the number of right-hand side vectors.
 */
typedef struct {
  SparseAttributes_t attributes;
  SparseSubfactor_t contents;
  SparseOpaqueFactorization_Float factor;
  size_t workspaceRequiredStatic;
  size_t workspaceRequiredPerRHS;
} SparseOpaqueSubfactor_Float;

/******************************************************************************
 *  @group Matrix and Vector Operations (Sparse BLAS Wrappers)
 ******************************************************************************/

/**** Multiplication **********************************************************/

/*! @abstract Performs the multiplication Y = AX for double values
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with their product.                               */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseMatrix_Double A, DenseMatrix_Double X, DenseMatrix_Double Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication Y = AX for float values.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with their product.                               */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseMatrix_Float A, DenseMatrix_Float X, DenseMatrix_Float Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication Y = alpha * AX for double values
 *
 *  @param alpha (input) scale to apply to the result.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with alpha * AX.                                  */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(double alpha, SparseMatrix_Double A, DenseMatrix_Double X, DenseMatrix_Double Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication Y = alpha * AX for float values.
 *
 *  @param alpha (input) scale to apply to the result.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with alpha * AX.                                  */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(float alpha, SparseMatrix_Float A, DenseMatrix_Float X, DenseMatrix_Float Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication y = Ax for double values
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseMatrix_Double A, DenseVector_Double x, DenseVector_Double y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication y = Ax for float values
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseMatrix_Float A, DenseVector_Float x, DenseVector_Float y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication y = alpha * Ax for double values
 *
 *  @param alpha (input) scale to apply to the result.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(double alpha, SparseMatrix_Double A, DenseVector_Double x, DenseVector_Double y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Performs the multiplication y = alpha * Ax for float values.
 *
 *  @param alpha (input) scale to apply to the result.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(float alpha, SparseMatrix_Float A, DenseVector_Float x, DenseVector_Float y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/**** Multipy-Add *************************************************************/

/*! @abstract Y += AX for double values
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with their product.                               */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(SparseMatrix_Double A, DenseMatrix_Double X, DenseMatrix_Double Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Y += AX for float values.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with their product.                               */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(SparseMatrix_Float A, DenseMatrix_Float X, DenseMatrix_Float Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Y += alpha * AX for double values
 *
 *  @param alpha (input) scale to apply to the product of A and X.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with alpha * AX.                                  */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(double alpha, SparseMatrix_Double A, DenseMatrix_Double X, DenseMatrix_Double Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Y += alpha * AX for float values.
 *
 *  @param alpha (input) scale to apply to the product of A and X.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param X (input) dense matrix. Inner dimensions of A and X must match.
 *
 *  @param Y (output) dense matrix. Dimensions must match the outer dimensions
 *  of A and X. Overwritten with alpha * AX.                                  */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(float alpha, SparseMatrix_Float A, DenseMatrix_Float X, DenseMatrix_Float Y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract y += Ax for double values
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(SparseMatrix_Double A, DenseVector_Double x, DenseVector_Double y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract y += Ax for float values.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(SparseMatrix_Float A, DenseVector_Float x, DenseVector_Float y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract y += alpha * Ax for double values
 *
 *  @param alpha (input) scale to apply to the product of A and x.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(double alpha, SparseMatrix_Double A, DenseVector_Double x, DenseVector_Double y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract y += alpha * Ax for float values.
 *
 *  @param alpha (input) scale to apply to the product of A and x.
 *
 *  @param A (input) sparse matrix.
 *
 *  @param x (input) dense vector.
 *
 *  @param y (output) dense vector.                                           */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(float alpha, SparseMatrix_Float A, DenseVector_Float x, DenseVector_Float y)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/******************************************************************************
 *  @group Transposition
 ******************************************************************************/

/*! @abstract Returns a transposed copy of the specified SparseMatrix_Double.
 *
 *  @discussion Note that the underlying storage is *not* reference counted,
 *  so users must ensure the original matrix (or at least its underlying
 *  storage) is not destroyed before they are finished with the matrix returned
 *  by this routine.
 *
 *  @param Matrix The matrix to transpose.
 *
 *  @returns A copy of matrix with A.structure.attributes.transpose flipped.  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseMatrix_Double SparseGetTranspose(SparseMatrix_Double Matrix);

/*! @abstract Returns a transposed copy of the specified SparseMatrix_Float.
 *
 *  @discussion Note that the underlying storage is *not* reference counted,
 *  so users must ensure the original matrix (or at least its underlying
 *  storage) is not destroyed before they are finished with the matrix returned
 *  by this routine.
 *
 *  @param Matrix The matrix to transpose.
 *
 *  @returns A copy of matrix with matrix.structure.attributes.transpose bit
 *           flipped.                                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseMatrix_Float SparseGetTranspose(SparseMatrix_Float Matrix);

/*! @abstract Returns a transposed, reference-counted copy of a factorization.
 *
 *  @param Factor The factorization to transpose.
 *
 *  @returns A matrix factorization of A^T, where the original was of A. As
 *           this is reference counted, it must be freed through a call to
 *           SparseCleanup() once it is no longer required.                   */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseGetTranspose(SparseOpaqueFactorization_Double Factor);

/*! @abstract Returns a transposed, reference-counted copy of a factorization.
 *
 *  @param Factor The factorization to transpose.
 *
 *  @returns A matrix factorization of A^T, where the original was of A. As
 *           this is reference counted, it must be freed through a call to
 *           SparseCleanup() once it is no longer required.                   */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseGetTranspose(SparseOpaqueFactorization_Float Factor);

/*! @abstract Returns a transposed, reference-counted copy of a subfactor.
 *
 *  @param Subfactor The object to transpose.
 *
 *  @returns A subfactor equivalent to the transpose of the one provided. As
 *           this is reference counted, it must be freed through a call to
 *           SparseCleanup() once it is no longer required.                   */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Double SparseGetTranspose(SparseOpaqueSubfactor_Double Subfactor);

/*! @abstract Returns a transposed, reference-counted copy of a subfactor.
 *
 *  @param Subfactor The object to transpose.
 *
 *  @returns A subfactor equivalent to the transpose of the one provided. As
 *           this is reference counted, it must be freed through a call to
 *           SparseCleanup() once it is no longer required.                   */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Float SparseGetTranspose(SparseOpaqueSubfactor_Float Subfactor);

/******************************************************************************
 *  @group Sparse Factor Functions
 ******************************************************************************/

/**** All-in-one Sparse Factor Functions **************************************/

/*! @abstract Returns the specified factorization of a sparse matrix of double
 *            values.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseFactor(SparseFactorization_t type, SparseMatrix_Double Matrix);

/*! @abstract Returns the specified factorization of a sparse matrix of float
 *            values.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The matrix to factorize.                                    */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseFactor(SparseFactorization_t type, SparseMatrix_Float Matrix);

/*! @abstract Returns the specified factorization of a sparse matrix of double
 *            values, using the specified options.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param sfoptions Symbolic factor options, for example the ordering algorithm
 *         to use.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseFactor(SparseFactorization_t type, SparseMatrix_Double Matrix, SparseSymbolicFactorOptions sfoptions, SparseNumericFactorOptions nfoptions);

/*! @abstract Returns the specified factorization of a sparse matrix of float
 *            values, using the specified options.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param sfoptions Symbolic factor options, for example the ordering algorithm
 *         to use.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseFactor(SparseFactorization_t type, SparseMatrix_Float Matrix, SparseSymbolicFactorOptions sfoptions, SparseNumericFactorOptions nfoptions);

/**** Sparse Factor Functions using pre-calculated symbolic factors ***********/

/*! @abstract Returns the factorization of a sparse matrix of double values
 *            corresponding to the supplied symbolic factorization.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Double Matrix);

/*! @abstract Returns the factorization of a sparse matrix of float values
 *            corresponding to the supplied symbolic factorization.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Float Matrix);

/*! @abstract Returns the factorization of a sparse matrix of double values
 *            corresponding to the supplied symbolic factorization, using the
 *            specified options.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Double Matrix, SparseNumericFactorOptions nfoptions);

/*! @abstract Returns the factorization of a sparse matrix of float values
 *            corresponding to the supplied symbolic factorization, using the
 *            specified options.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Float Matrix, SparseNumericFactorOptions nfoptions);

/**** Sparse Factor Functions with user-defined workspace *********************/

/*! @abstract Returns the factorization of a sparse matrix of double values
 *            corresponding to the supplied symbolic factorization, using the
 *            specified options and without any internal memory allocations.
 *
 *  @discussion Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @param factorStorage A pointer to space used to store the factorization
 *         of size at least SymbolicFactor.factorSize_Double bytes. This storage
 *         should not be altered by the user during the lifetime of the return
 *         value. This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *
 *  @param workspace A pointer to a workspace of size at least
 *         SymbolicFactor.workspaceSize_Double bytes. This workspace may be
 *         reused or destroyed by the user as soon as the function returns.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseFactor(
  SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Double Matrix,
  SparseNumericFactorOptions nfoptions, void *_Nullable factorStorage,
  void *_Nullable workspace);

/*! @abstract Returns the factorization of a sparse matrix of float values
 *            corresponding to the supplied symbolic factorization, using the
 *            specified options and without any internal memory allocations.
 *
 *  @discussion Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *
 *  @param SymbolicFactor A symbolic factorization, as returned by a call of the
 *         form SymbolicFactor = SparseFactor(Matrix.structure).
 *
 *  @param Matrix The matrix to factorize.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @param factorStorage A pointer to space used to store the factorization
 *         of size at least SymbolicFactor.factorSize_Float bytes. This storage
 *         should not be altered by the user during the lifetime of the return
 *         value. This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *
 *  @param workspace A pointer to a workspace of size at least
 *         SymbolicFactor.workspaceSize_Float bytes. This workspace may be
 *         reused or destroyed by the user as soon as the function returns.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *
 *  @returns Factorization of Matrix.                                         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseFactor(
  SparseOpaqueSymbolicFactorization SymbolicFactor, SparseMatrix_Float Matrix,
  SparseNumericFactorOptions nfoptions, void *_Nullable factorStorage,
  void *_Nullable workspace);

/******************************************************************************
 *  @group Sparse Direct Solve Functions (DenseMatrix)
 ******************************************************************************/

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param XB On entry, the right-hand sides B. On return, the solution vectors
 *         X. If A has dimension m x n, then XB must have dimension k x nrhs,
 *         where k=max(m,n) and nrhs is the number of right-hand sides to find
 *         solutions for.                                                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored, DenseMatrix_Double XB);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param XB On entry, the right-hand sides B. On return, the solution vectors
 *         X. If A has dimension m x n, then XB must have dimension k x nrhs,
 *         where k=max(m,n) and nrhs is the number of right-hand sides to find
 *         solutions for.                                                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseMatrix_Float XB);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X Matrix in which to return solutions. If A has dimension m x n, and
 *         B has dimension m x nrhs, then X must have dimension n x nrhs.     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored, DenseMatrix_Double B, DenseMatrix_Double X);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X Matrix in which to return solutions. If A has dimension m x n, and
 *         B has dimension m x nrhs, then X must have dimension n x nrhs.     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseMatrix_Float B, DenseMatrix_Float X);

/**** Solving Systems with User Defined Workspace *****************************/

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place, and without any internal memory allocations.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param XB On entry, the right-hand sides B. On return, the solution vectors
 *         X. If A has dimension m x n, then XB must have dimension k x nrhs,
 *         where k=max(m,n) and nrhs is the number of right-hand sides to find
 *         solutions for.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + nrhs * Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseMatrix_Double XB, void *workspace);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, in place, and without any internal memory allocations.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param XB On entry, the right-hand sides B. On return, the solution vectors
 *         X. If A has dimension m x n, then XB must have dimension k x nrhs,
 *         where k=max(m,n) and nrhs is the number of right-hand sides to find
 *         solutions for.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + nrhs * Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseMatrix_Float XB,
  void *workspace);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, and without any internal memory allocations.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X Matrix in which to return solutions. If A has dimension m x n, and
 *         B has dimension m x nrhs, then X must have dimension n x nrhs.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + nrhs * Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseMatrix_Double X, DenseMatrix_Double B, void *workspace);

/*! @abstract Solves the system AX=B for X, using the supplied factorization
 *            of A, and without any internal memory allocations.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || X ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_X || AX - B ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAX=B.
 *
 *  @param Factored A factorization of A.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X Matrix in which to return solutions. If A has dimension m x n, and
 *         B has dimension m x nrhs, then X must have dimension n x nrhs.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + nrhs * Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseMatrix_Float X,
  DenseMatrix_Float B, void *workspace);

/******************************************************************************
 *  @group Sparse Direct Solve Functions (DenseVector)
 ******************************************************************************/

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param xb On entry, the right-hand side b. On return, the solution vector
 *         x. If A has dimension m x n, then xb must have length k, where
 *         k=max(m,n).                                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseVector_Double xb);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param xb On entry, the right-hand side b. On return, the solution vector
 *         x. If A has dimension m x n, then xb must have length k, where
 *         k=max(m,n).                                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseVector_Float xb);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x Vector in which to return solution. If A has dimension m x n, then
 *         x must have length n.                                              */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseVector_Double b, DenseVector_Double x);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x Vector in which to return solution. If A has dimension m x n, then
 *         x must have length n.                                              */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseVector_Float b,
  DenseVector_Float x);

/**** Solving Systems with User Defined Workspace *****************************/

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param xb On entry, the right-hand side b. On return, the solution vector
 *         x. If A has dimension m x n, then xb must have length k, where
 *         k=max(m,n).
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseVector_Double xb, void *workspace);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param xb On entry, the right-hand side b. On return, the solution vector
 *         x. If A has dimension m x n, then xb must have length k, where
 *         k=max(m,n).
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseVector_Float xb,
                 void *workspace);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x Vector in which to return solution. If A has dimension m x n, then
 *         x must have length n.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Double Factored,
  DenseVector_Double x, DenseVector_Double b, void *workspace);

/*! @abstract Solves the system Ax=b for x, using the supplied factorization
 *            of A, in place.
 *
 *  @discussion If the factorization is A=QR and the system is underdetermined,
 *  the solution of minimum norm || x ||_2 is returned.
 *  If the factorization is A=QR and the system is overdetermined, the least
 *  squares solution arg min_x || Ax - b ||_2 is returned.
 *  In the case of a factorization of type=SparseCholeskyAtA, the factorization
 *  is in fact of A^T A, so the solution returned is for the system A^TAx=b.
 *
 *  @param Factored A factorization of A.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x Vector in which to return solution. If A has dimension m x n, then
 *         x must have length n.
 *
 *  @param workspace Scratch space of size
 *         Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueFactorization_Float Factored, DenseVector_Float x,
  DenseVector_Float b, void *workspace);

/******************************************************************************
 *  @group Advanced Solving Functions
 ******************************************************************************/

/**** Symbolic Factorization Functions ****************************************/

/*! @abstract Returns a symbolic factorization of the requested type for a
 *            matrix with the given structure.
 *
 *  @discussion The resulting symbolic factorization may be used for multiple
 *  numerical factorizations with different numerical values but the same
 *  non-zero structure.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The structure of the sparse matrix to be factorized.
 *
 *  @returns The requested symbolic factorization of Matrix.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSymbolicFactorization SparseFactor(SparseFactorization_t type,
  SparseMatrixStructure Matrix);

/*! @abstract Returns a symbolic factorization of the requested type for a
 *            matrix with the given structure, with the supplied options.
 *
 *  @discussion The resulting symbolic factorization may be used for multiple
 *  numerical factorizations with different numerical values but the same
 *  non-zero structure.
 *
 *  @param type The type of factorization to perform.
 *
 *  @param Matrix The structure of the sparse matrix to be factorized.
 *
 *  @param sfoptions Symbolic factor options, for example the ordering algorithm
 *         to use.
 *
 *  @returns The requested symbolic factorization of Matrix.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSymbolicFactorization SparseFactor(SparseFactorization_t type,
  SparseMatrixStructure Matrix, SparseSymbolicFactorOptions sfoptions);

/**** Symbolic Refactor Functions *********************************************/


/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  The same numerical factorization options will be used as in the original
 *  construction of Factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Double Matrix,
  SparseOpaqueFactorization_Double *Factorization);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  The same numerical factorization options will be used as in the original
 *  construction of Factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Float Matrix,
  SparseOpaqueFactorization_Float *Factorization);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, using different options.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters. */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Double Matrix,
  SparseOpaqueFactorization_Double *Factorization,
  SparseNumericFactorOptions nfoptions);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, using different options.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters. */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Float Matrix,
  SparseOpaqueFactorization_Float *Factorization,
  SparseNumericFactorOptions nfoptions);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, without any internal
 *            allocations.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  The same numerical factorization options will be used as in the original
 *  construction of Factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Double Matrix,
  SparseOpaqueFactorization_Double *Factorization, void *workspace);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, without any internal
 *            allocations.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.
 *  The same numerical factorization options will be used as in the original
 *  construction of Factorization.
 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Float Matrix,
  SparseOpaqueFactorization_Float *Factorization, void *workspace);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, using updated options and
 *            without any internal allocations.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.

 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @param workspace A pointer to a workspace of size at least
 *         Factorization->symbolicFactorization.workspaceSize_Double bytes.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *         This workspace may be reused or destroyed by the user as soon as the
 *         function returns.                                                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Double Matrix,
  SparseOpaqueFactorization_Double *Factorization,
  SparseNumericFactorOptions nfoptions, void *workspace);

/*! @abstract Reuses supplied factorization object's storage to compute a new
 *            factorization of the supplied matrix, using updated options and
 *            without any internal allocations.
 *
 *  @discussion Matrix must have the same non-zero structure as that used for
 *  the original factorization.

 *  This call provides very similar behavior to that which can be achieved by
 *  reusing explicit storage supplied to SparseFactor() as the argument
 *  factorStorage. However, in addition to providing a simplified call sequence,
 *  this call can also reuse any additional storage allocated to accomodate
 *  delayed pivots.
 *  Note that internal memory allocations may occur in the case of
 *  pivoted factorizations that result in delayed pivots. If you require closer
 *  control over memory allocations, supply a sfoptions.malloc() function that
 *  implements the required behaviour, or use an alternative non-pivoted
 *  factorization returns. Note that if sfoptions.malloc() returns NULL the
 *  factorization will abort immediately.
 *  Note that if the reference count of the underlying object is not
 *  exactly one (i.e. if there are any implict copies as a result of calls to
 *  SparseGetTranspose() or SparseCreateSubfactor() that have not been destroyed
 *  through a call to SparseCleanup()), then new storage will be allocated
 *  regardless.
 *
 *  @param Matrix The matrix to be factorized.
 *
 *  @param Factorization The factorization to be updated.
 *
 *  @param nfoptions Numeric factor options, for example pivoting parameters.
 *
 *  @param workspace A pointer to a workspace of size at least
 *         Factorization->symbolicFactorization.workspaceSize_Float bytes.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 *         This workspace may be reused or destroyed by the user as soon as the
 *         function returns.                                                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseRefactor(SparseMatrix_Float Matrix,
  SparseOpaqueFactorization_Float *Factorization,
  SparseNumericFactorOptions nfoptions, void *workspace);

/******************************************************************************
 *  @group Extracting Sub-factors of Factors
 ******************************************************************************/

/*! @abstract Returns an opaque object representing a sub-factor of a
 *            factorization.
 *
 *  @discussion Here the term "sub-factor" is used to mean one or more parts of
 *  the whole factorization. For example, just the Q factor from A=QRP.
 *  The returned object is a wrapper around the orignal factorization, and does
 *  not actually perform any extraction (sub-factors are stored in non-standard
 *  formats that exploit implicit structure for efficiency, and formation of the
 *  sub-factor explicitly can be expensive). It is intended to be used as an
 *  argument to SparseMultiply() and SparseSolve() functions only.
 *  As Factor is contained in the returned object, its underlying reference
 *  count in incremented, and the returned object must hence be destroyed
 *  through a call to SparseCleanup() to prevent a memory leak (however it is
 *  safe to call SparseCleanup() on the original factorization whilst this
 *  object is still being used).
 *
 *  @param subfactor The sub-factor the new object shuold represent.
 *
 *  @param Factor The factorization to extract the sub-factor from.
 *
 *  @returns Object representing the requested sub-factor. Must be cleaned up
 *  by a call to SparseCleanup() once it is no longer required.               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Double SparseCreateSubfactor(SparseSubfactor_t subfactor,
  SparseOpaqueFactorization_Double Factor);

/*! @abstract Returns an opaque object representing a sub-factor of a
 *            factorization.
 *
 *  @discussion Here the term "sub-factor" is used to mean one or more parts of
 *  the whole factorization. For example, just the Q factor from A=QRP.
 *  The returned object is a wrapper around the orignal factorization, and does
 *  not actually perform any extraction (sub-factors are stored in non-standard
 *  formats that exploit implicit structure for efficiency, and formation of the
 *  sub-factor explicitly can be expensive). It is intended to be used as an
 *  argument to SparseMultiply() and SparseSolve() functions only.
 *  As Factor is contained in the returned object, its underlying reference
 *  count in incremented, and the returned object must hence be destroyed
 *  through a call to SparseCleanup() to prevent a memory leak (however it is
 *  safe to call SparseCleanup() on the original factorization whilst this
 *  object is still being used).
 *
 *  @param subfactor The sub-factor the new object shuold represent.
 *
 *  @param Factor The factorization to extract the sub-factor from.
 *
 *  @returns Object representing the requested sub-factor. Must be cleaned up
 *  by a call to SparseCleanup() once it is no longer required.               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Float SparseCreateSubfactor(SparseSubfactor_t subfactor,
  SparseOpaqueFactorization_Float Factor);

/******************************************************************************
 *  @group Sub-factor Multiplication and Solve Functions
 ******************************************************************************/

/**** Matrix solve functions **************************************************/

/*! @abstract Solve the equation Subfactor * X = B for the matrix X, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XB (input/output) On input, the matrix B. On return it is overwritten
 *         with the matrix X. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         sides. If m != n, then only the first min(m,n) entries are used for
 *         input or output as approriate.                                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double XB);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XB (input/output) On input, the matrix B. On return it is overwritten
 *         with the matrix X. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         sides. If m != n, then only the first min(m,n) entries are used for
 *         input or output as approriate.                                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float XB);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by SparseCreateSubfactor().
 *
 *  @param B (input) The right-hand sides B. If Subfactor is m x n, then B must
 *         have dimension m x nrhs, where nrhs is the number of right-hand
 *         sides.
 *
 *  @param X (output) The solutions X. If Subfactor is m x n, and B is m x nrhs,
 *         then X must have dimension n x nrhs.                               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double B,
  DenseMatrix_Double X);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by SparseCreateSubfactor().
 *
 *  @param B (input) The right-hand sides B. If Subfactor is m x n, then B must
 *         have dimension m x nrhs, where nrhs is the number of right-hand
 *         sides.
 *
 *  @param X (output) The solutions X. If Subfactor is m x n, and B is m x nrhs,
 *         then X must have dimension n x nrhs.                               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float B,
  DenseMatrix_Float X);

/**** Matrix solve functions with user-supplied workspace *********************/

/*! @abstract Solve the equation Subfactor * X = B for the matrix X, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XB (input/output) On input, the matrix B. On return it is overwritten
 *         with the matrix X. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         sides. If m != n, then only the first min(m,n) entries are used for
 *         input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double XB,
  void *workspace);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XB (input/output) On input, the matrix B. On return it is overwritten
 *         with the matrix X. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         sides. If m != n, then only the first min(m,n) entries are used for
 *         input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float XB,
  void *workspace);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param B (input) The right-hand sides B. If Subfactor is m x n, then B must
 *         have dimension m x nrhs, where nrhs is the number of right-hand
 *         sides.
 *
 *  @param X (output) The solutions X. If Subfactor is m x n, and B is m x nrhs,
 *         then X must have dimension n x nrhs.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double B,
  DenseMatrix_Double X, void *workspace);

/*! @abstract Solve the equation Subfactor * X = B for the matrix X.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param B (input) The right-hand sides B. If Subfactor is m x n, then B must
 *         have dimension m x nrhs, where nrhs is the number of right-hand
 *         sides.
 *
 *  @param X (output) The solutions X. If Subfactor is m x n, and B is m x nrhs,
 *         then X must have dimension n x nrhs.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float B,
  DenseMatrix_Float X, void *workspace);

/**** Vector solve ************************************************************/

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xb (input/output) On input, the vector b. On return it is overwritten
 *         with the solution vector x. If Subfactor is m x n, then xb must have
 *         length k, where k = max(m, n). If m != n, then only the first
 *         min(m,n) entries are used for input or output as approriate.       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double xb);

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xb (input/output) On input, the vector b. On return it is overwritten
 *         with the solution vector x. If Subfactor is m x n, then xb must have
 *         length k, where k = max(m, n). If m != n, then only the first
 *         min(m,n) entries are used for input or output as approriate.       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float xb);

/*! @abstract Solve the equation Subfactor * x = b for the vector x.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param b (input) The right-hand side b. If Subfactor is m x n, then b must
 *         have length m.
 *
 *  @param x (output) The solution x. If Subfactor is m x n, then x must have
 *         length n.                                                          */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double b,
  DenseVector_Double x);

/*! @abstract Solve the equation Subfactor * x = b for the vector x.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param b (input) The right-hand side b. If Subfactor is m x n, then b must
 *         have length m.
 *
 *  @param x (output) The solution x. If Subfactor is m x n, then x must have
 *         length n.                                                          */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float b,
  DenseVector_Float x);

/**** Vector solve functions with user-supplied workspace *********************/

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xb (input/output) On input, the vector b. On return it is overwritten
 *         with the solution vector x. If Subfactor is m x n, then xb must have
 *         length k, where k = max(m, n). If m != n, then only the first
 *         min(m,n) entries are used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double xb,
  void *workspace);

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xb (input/output) On input, the vector b. On return it is overwritten
 *         with the solution vector x. If Subfactor is m x n, then xb must have
 *         length k, where k = max(m, n). If m != n, then only the first
 *         min(m,n) entries are used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double xb,
  void *workspace);

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param b (input) The right-hand side b. If Subfactor is m x n, then b must
 *         have length m.
 *
 *  @param x (output) The solution x. If Subfactor is m x n, then x must have
 *         length n.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double b,
  DenseVector_Double x, void *workspace);

/*! @abstract Solve the equation Subfactor * x = b for the vector x, in place.
 *
 *  @param Subfactor (input) The subfactor to solve a system involving, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param b (input) The right-hand side b. If Subfactor is m x n, then b must
 *         have length m.
 *
 *  @param x (output) The solution x. If Subfactor is m x n, then x must have
 *         length n.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseSolve(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float b,
  DenseVector_Float x, void *workspace);

/**** Matrix multiply functions ***********************************************/

/*! @abstract Perform the multiply operation Y = Subfactor * X in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XY (input/output) On input, the matrix X. On return it is overwritten
 *         with the matrix Y. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         side vectors. If m != n, then only the first min(m,n) entries are
 *         used for input or output as approriate.                            */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double XY);

/*! @abstract Perform the multiply operation Y = Subfactor * X in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XY (input/output) On input, the matrix X. On return it is overwritten
 *         with the matrix Y. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         side vectors. If m != n, then only the first min(m,n) entries are
 *         used for input or output as approriate.                            */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float XY);

/*! @abstract Perform the multiply operation Y = Subfactor * X.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param X (input) The right-hand side vectors X. If Subfactor is m x n, then X
 *         must have dimension n x nrhs, where nrhs is the number of right-hand
 *         side vectors.
 *
 *  @param Y (output) The result vectors Y. If Subfactor is m x n, and X is
 *         m x nrhs, then Y must have dimension m x nrhs.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double X,
  DenseMatrix_Double Y);

/*! @abstract Perform the multiply operation Y = Subfactor * X.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param X (input) The right-hand side vectors X. If Subfactor is m x n, then X
 *         must have dimension n x nrhs, where nrhs is the number of right-hand
 *         side vectors.
 *
 *  @param Y (output) The result vectors Y. If Subfactor is m x n, and X is
 *         m x nrhs, then Y must have dimension m x nrhs.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float X,
  DenseMatrix_Float Y);

/**** Matrix multiply functions with user-supplied workspace ******************/

/*! @abstract Perform the multiply operation Y = Subfactor * X in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XY (input/output) On input, the matrix X. On return it is overwritten
 *         with the matrix Y. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         side vectors. If m != n, then only the first min(m,n) entries are
 *         used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double XY,
  void *workspace);

/*! @abstract Perform the multiply operation Y = Subfactor * X in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param XY (input/output) On input, the matrix X. On return it is overwritten
 *         with the matrix Y. If Subfactor is m x n, then XB must have dimension
 *         k x nrhs, where k = max(m, n) and nrhs is the number of right-hand
 *         side vectors. If m != n, then only the first min(m,n) entries are
 *         used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float XY,
  void *workspace);

/*! @abstract Perform the multiply operation Y = Subfactor * X.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param X (input) The right-hand side vectors X. If Subfactor is m x n, then X
 *         must have dimension n x nrhs, where nrhs is the number of right-hand
 *         side vectors.
 *
 *  @param Y (output) The result vectors Y. If Subfactor is m x n, and X is
 *         m x nrhs, then Y must have dimension m x nrhs.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseMatrix_Double X,
  DenseMatrix_Double Y, void *workspace);

/*! @abstract Perform the multiply operation Y = Subfactor * X.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param X (input) The right-hand side vectors X. If Subfactor is m x n, then X
 *         must have dimension n x nrhs, where nrhs is the number of right-hand
 *         side vectors.
 *
 *  @param Y (output) The result vectors Y. If Subfactor is m x n, and X is
 *         m x nrhs, then Y must have dimension m x nrhs.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + nrhs * Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseMatrix_Float X,
  DenseMatrix_Float Y, void *workspace);

/**** Vector multiply functions ***********************************************/

/*! @abstract Perform the multiply operation y = Subfactor * x in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xy (input/output) On input, the vector x. On return it is overwritten
 *         with the vector y. If Subfactor is m x n, then xb must have length
 *         k, where k = max(m, n). If m != n, then only the first min(m,n)
 *         entries are used for input or output as approriate.                */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double xy)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Perform the multiply operation y = Subfactor * x in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xy (input/output) On input, the vector x. On return it is overwritten
 *         with the vector y. If Subfactor is m x n, then xb must have length
 *         k, where k = max(m, n). If m != n, then only the first min(m,n)
 *         entries are used for input or output as approriate.                */
static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float xy)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Perform the multiply operation y = Subfactor * x.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param x (input) The right-hand side vector x. If Subfactor is m x n, then x
 *         must have length n.
 *
 *  @param y (output) The result vector y. If Subfactor is m x n, then y must have
 *         length m.                                                          */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double x,
  DenseVector_Double y);

/*! @abstract Perform the multiply operation y = Subfactor * x.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param x (input) The right-hand side vector x. If Subfactor is m x n, then x
 *         must have length n.
 *
 *  @param y (output) The result vector y. If Subfactor is m x n, then y must have
 *         length m.                                                          */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float x,
  DenseVector_Float y);

/**** Vector multiply functions with user-supplied workspace ******************/

/*! @abstract Perform the multiply operation y = Subfactor * x in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xy (input/output) On input, the vector x. On return it is overwritten
 *         with the vector y. If Subfactor is m x n, then xb must have length
 *         k, where k = max(m, n). If m != n, then only the first min(m,n)
 *         entries are used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double xy,
  void *workspace);

/*! @abstract Perform the multiply operation y = Subfactor * x in place.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param xy (input/output) On input, the vector x. On return it is overwritten
 *         with the vector y. If Subfactor is m x n, then xb must have length
 *         k, where k = max(m, n). If m != n, then only the first min(m,n)
 *         entries are used for input or output as approriate.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float xy,
  void *workspace);

/*! @abstract Perform the multiply operation y = Subfactor * x.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param x (input) The right-hand side vector x. If Subfactor is m x n, then x
 *         must have length n.
 *
 *  @param y (output) The result vector y. If Subfactor is m x n, then y must have
 *         length m.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Double Subfactor, DenseVector_Double x,
  DenseVector_Double y, void *workspace);

/*! @abstract Perform the multiply operation y = Subfactor * x.
 *
 *  @param Subfactor (input) The subfactor to multiply by, as returned by
 *         SparseCreateSubfactor().
 *
 *  @param x (input) The right-hand side vector x. If Subfactor is m x n, then x
 *         must have length n.
 *
 *  @param y (output) The result vector y. If Subfactor is m x n, then y must have
 *         length m.
 *
 *  @param workspace (scratch) A workspace of size
 *         Subfactor.workspaceRequiredStatic + 1*Subfactor.workspaceRequiredPerRHS.
 *         This memory must be 16-byte aligned (any allocation returned
 *         by malloc() has this property).
 */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseMultiply(SparseOpaqueSubfactor_Float Subfactor, DenseVector_Float x,
  DenseVector_Float y, void *workspace);

/******************************************************************************
 *  @group Preconditioners
 ******************************************************************************/

/*! @abstract Types of preconditioner.
 *
 *  @const SparsePreconditionerNone
 *    No preconditioner, used to flag an empty type as required.
 *  @const SparsePreconditionerUser
 *    User-defined preconditioner.
 *  @const SparsePreconditionerDiagonal
 *    Diagonal (Jacobi) preconditioner D_ii = 1.0 / A_ii.
 *    Zero entries on the diagonal of A are replaced with 1.0.
 *  @cont SparsePreconditionerDiagScaling
 *    Diagonal scaling preconditioner D_ii = 1.0 / || A_i ||_2, where A_i is
 *    i-th column of A.
 */
SPARSE_ENUM(SparsePreconditioner, int,
  SparsePreconditionerNone = 0,
  SparsePreconditionerUser = 1,
  SparsePreconditionerDiagonal = 2,
  SparsePreconditionerDiagScaling = 3
);

/*! @abstract Represents a preconditioner.
 *
 *  @field type The type of preconditioner represented.
 *
 *  @field mem Block of memory that will be passed unaltered as the first
 *         argument of the apply() callback.
 *
 *  \@callback apply(mem, trans, X, Y) Function to call to apply the
 *             preconditioner as Y = PX (trans=false) or Y = P^TX (trans=true).
 *    \@param mem   The unaltered pointer mem from this struct.
 *    \@param trans Flags whether to apply the preconditioner or its transpose.
 *    \@param X     The right-hand side vectors X.
 *    \@param Y     The result vectors Y.                                     */
typedef struct {
  SparsePreconditioner_t type;
  void *mem;
  void (*apply) (void*, enum CBLAS_TRANSPOSE trans, DenseMatrix_Double X, DenseMatrix_Double Y);
} SparseOpaquePreconditioner_Double;

/*! @abstract Represents a preconditioner.
 *
 *  @field type The type of preconditioner represented.
 *
 *  @field mem Block of memory that will be passed unaltered as the first
 *         argument of the apply() callback.
 *
 *  \@callback apply(mem, trans, X, Y) Function to call to apply the
 *             preconditioner as Y = PX (trans=false) or Y = P^TX (trans=true).
 *    \@param mem   The unaltered pointer mem from this struct.
 *    \@param trans Flags whether to apply the preconditioner or its transpose.
 *    \@param X     The right-hand side vectors X.
 *    \@param Y     The result vectors Y.                                     */
typedef struct {
  SparsePreconditioner_t type;
  void *mem;
  void (*apply) (void*, enum CBLAS_TRANSPOSE trans, DenseMatrix_Float X, DenseMatrix_Float);
} SparseOpaquePreconditioner_Float;

/*! @abstract Create a preconditioner for the given matrix.
 *
 *  @param type (input) The type of preconditioner to create.
 *
 *  @param A (input) The matrix to construct a preconditioner for.
 *
 *  @returns The constructed preconditioner object. Resource must be freed
 *           through a call to SparseCleanup() once the user is finished with
 *           the preconditioner.                                              */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaquePreconditioner_Double SparseCreatePreconditioner(
  SparsePreconditioner_t type, SparseMatrix_Double A);

/*! @abstract Create a preconditioner for the given matrix.
 *
 *  @param type (input) The type of preconditioner to create.
 *
 *  @param A (input) The matrix to construct a preconditioner for.
 *
 *  @returns The constructed preconditioner object. Resource must be freed
 *           through a call to SparseCleanup() once the user is finished with
 *           the preconditioner.                                              */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaquePreconditioner_Float SparseCreatePreconditioner(
  SparsePreconditioner_t type, SparseMatrix_Float A);

/******************************************************************************
 *  @group Sparse Iterative Methods
 ******************************************************************************
 * @discussion
 * Sparse Iterative methods solve Ax=b through an iterative process that only
 * requires multiplication by A or A^T. However, if A is numerically difficult,
 * the iterative process may fail to converge to a solution. Further, even for
 * problems where the process converges, it may do so slowly. Both of these
 * issues can be fixed through the application of a problem-specific
 * preconditioner that cheaply approximates the inverse of A (though good
 * preconditioners are not known for all probelms).
 *
 * More information on iterative methods is available on wikipedia:
 * https://en.wikipedia.org/wiki/Iterative_method
 *
 * The following book provides a more in-depth treatment of the subject:
 * [1] Y. Saad (2003), "Iterative Methods for Sparse Linear Systems",
 *     2nd Edition, Published by SIAM.                                        */

/**** Type definitions ********************************************************/

/*! @abstract Exit status definitions for sparse iterative methods.
 *
 *  @const SparseIterativeConverged
 *    All solution vectors converged.
 *  @const SparseIterativeMaxIterations
 *    One or more solutions failed to converge in the maximum number of
 *    iterations.
 *  @const SparseIterativeParameterError
 *    There was an error with one or more user-supplied parameters.
 *  @const SparseIterativeIllConditioned
 *    Problem determined to be sufficiently ill conditioned convergence is
 *    unlikely.
 *  @const SparseIterativeInternalError
 *    Some internal failure occured (e.g. memory allocation failed).          */
SPARSE_ENUM(SparseIterativeStatus, int,
  SparseIterativeConverged = 0,
  SparseIterativeMaxIterations = 1,
  SparseIterativeParameterError = -1,
  SparseIterativeIllConditioned = -2,
  SparseIterativeInternalError = -99,
);

/*! @internal This type required for implementation use only.
 *
 *  @abstract Base type for iterative method options structures.
 *
 *  @discussion In the SparseIterativeMethod datatype, all possible options
 *  structures are held as a union. Defining this base type allows easy access
 *  to the reportError() method regardless of which particular method the
 *  structure represents, so long as reportError() is the first entry in each
 *  options structure.
 *
 *  \@callback reportError Function to use to report parameter errors.
 *    \@param message
 *    \@discussion If NULL, errors are logged via <os/log.h> and execution is
 *      halted via __builtin_trap().  If non-NULL, the provided function is
 *      called with a human-readable string describing the error condition.
 *      If the callback returns, control will be returned to the caller with
 *      any outputs in a safe but undefined state (i.e. they may hold partial
 *      results or garbage, but all sizes and pointers are valid).            */
struct _SparseIterativeMethodBaseOptions {
  // reportError should always be FIRST member in below structs
  void (* _Nullable reportError)(const char *message);
};

/*! @abstract Conjugate Gradient Options.
 *
 *  @discussion Use CG to solve Ax=b when A is symmetric positive-definite (the method may break
 *  down or fail to converge if A is not positive-definite).
 *
 *  For square, full rank unsymmetric or indefinite equations, use GMRES instead.
 *  For rectangular or singular systems, use LSMR instead.
 *
 *  More information on the CG algorithm can be found on wikipedia:
 *  https://en.wikipedia.org/wiki/Conjugate_gradient_method
 *
 *  A more detailed review of CG is available in the paper:
 *  [1] J. Shewchuk (1994), "An Introduction to the Conjugate Gradient Method Without the
 *      Agonizing Pain", Technical Report, Carnegie Mellon University.
 *
 *  \@callback reportError Function to use to report parameter errors.
 *    \@param message
 *    \@discussion If NULL, errors are logged via <os/log.h> and execution is
 *      halted via __builtin_trap().  If non-NULL, the provided function is
 *      called with a human-readable string describing the error condition.
 *      If the callback returns, control will be returned to the caller with
 *      any outputs in a safe but undefined state (i.e. they may hold partial
 *      results or garbage, but all sizes and pointers are valid).
 *
 *  \field maxIterations
 *    Maximum number of iterations to perform. If 0, the default value of 100
 *    is used.
 *
 *  \field atol
 *    Absolute convergence tolerance. Iterate is considered to have converged if
 *              || b-Ax ||_2 < rtol * || b-Ax_0 ||_2 + atol.
 *  \field rtol
 *    Relative convergence tolerance. Iterate is considered to have converged if
 *              || b-Ax ||_2 < rtol * || b-Ax_0 ||_2 + atol.
 *    If rtol = 0.0, default value of sqrt(epsilon) is used.
 *    If negative, rtol is treated as 0.0 (default is not used).
 *
 *  \@callback reportStatus Function to use to report status (iteration count
 *             and residual of first right-hand side) every few iterations.
 *    \@param message
 *    \@discussion If NULL, status is not reported.                           */
typedef struct {
  void (* _Nullable reportError)(const char *message);
  int maxIterations;
  double atol;
  double rtol;
  void (* _Nullable reportStatus)(const char *message);
} SparseCGOptions;

/*! @abstract Specify different variants of GMRES that can be used.
 *
 *  @discussion Basic GMRES requires restarting (as otherwise memory and
 *  calculation costs to keep vectors orthagonal become prohibitive). Upon
 *  restarting all information from previous iterations is discarded.
 *  DQGMRES implements a quasi-GMRES method that does not restart, but instead
 *  only performs orthagonalization against vectors from the most recent
 *  iterations.
 *  Basic GMRES also requires that the same preconditioner is used at every
 *  iteration. At the cost of some additional storage and work, it can be
 *  made flexible (able to cope with different preconditioners at each
 *  iteration) this variant is known as FGMRES.
 *  DGMRES is inherently flexible, so no additional flexible variant is
 *  required.
 *
 *  DQGMRES is the default variant, though for some problems, GMRES or FGMRES
 *  may converge faster and require less computation, particualrly if the
 *  number of orthagonalization vectors nvec is small (< 16).
 *
 *  @const SparseVariantDQGMRES
 *    Use DQGMRES variant. This method is flexible.
 *  @const SparseVariantGMRES
 *    Use standard restarted GMRES. This method is not flexible.
 *  @const SparseVariantFGMRES
 *    Use Flexible GMRES. This method is flexible.                            */
SPARSE_ENUM(SparseGMRESVariant, uint8_t,
  SparseVariantDQGMRES = 0,
  SparseVariantGMRES = 1,
  SparseVariantFGMRES = 2
);

/*! @abstract (right-preconditioned) (F/DQ)GMRES Parameters Options.
 *
 *  @discussion Use (F/DQ)GMRES to solve Ax=b when A is symmetric indefinite or unsymmetric.
 *
 *  For symmetric positive-definite systems, use CG instead.
 *  For rectangular or singular systems, use LSMR instead.
 *
 *  More information about GMRES can be found on wikipedia:
 *  https://en.wikipedia.org/wiki/Generalized_minimal_residual_method
 *
 *  A more detailed description of GMRES and its variants is available in the book:
 *  [1] Y. Saad (2003), "Iterative Methods for Sparse Linear Systems", 2nd Edition,
 *      Published by SIAM.
 *
 *  \@callback reportError Function to use to report parameter errors.
 *    \@param message
 *    \@discussion If NULL, errors are logged via <os/log.h> and execution is
 *      halted via __builtin_trap().  If non-NULL, the provided function is
 *      called with a human-readable string describing the error condition.
 *      If the callback returns, control will be returned to the caller with
 *      any outputs in a safe but undefined state (i.e. they may hold partial
 *      results or garbage, but all sizes and pointers are valid).
 *
 *  \field variant
 *    Variant of GMRES to use. See definition of SparseGMRESVariant_t for
 *    further information on the available variants.
 *
 *  \field nvec
 *    Number of orthagonal vectors maintained. For GMRES and FGMRES variants,
 *    this is the number of iterations between restarts. For DQGMRES it is the
 *    number of historical vectors maintained in memory.
 *    If nvec<=0, the default value of 16 is used.
 *
 *  \field maxIterations
 *    Maximum number of iterations to perform. If 0, the default value of 100
 *    is used.
 *
 *  \field atol
 *    Absolute convergence tolerance. Iterate is considered to have converged if
 *              || b-Ax ||_2 < rtol * || b-Ax_0 ||_2 + atol.
 *  \field rtol
 *    Relative convergence tolerance. Iterate is considered to have converged if
 *              || b-Ax ||_2 < rtol * || b-Ax_0 ||_2 + atol.
 *    If rtol = 0.0, default value of sqrt(epsilon) is used.
 *    If negative, rtol is treated as 0.0 (default is not used).
 *
 *  \@callback reportStatus Function to use to report status (iteration count
 *             and residual of first right-hand side) every few iterations.
 *    \@param message
 *    \@discussion If NULL, status is not reported.                           */
typedef struct {
  void (* _Nullable reportError)(const char *message);
  SparseGMRESVariant_t variant;
  int nvec;
  int maxIterations;
  double atol;
  double rtol;
  void (* _Nullable reportStatus)(const char *message);
} SparseGMRESOptions;

/*! @abstract Available types of convergence test for LSMR.
 *
 *  @discussion In addition to accelerate's default convergence test
 *  applied to the normal equations, the original more specializated
 *  convergence test of Fong and Saunders is offered.
 *
 *  @const SparseLSMRCTDefault
 *    Use accelerate's default convergence test:
 *      || A^Tb - A^TAx ||_2 < rtol * || A^Tb - A^TAx_0 ||_2 + atol.
 *
 *  @const SparseLSMRCTFongSaunders
 *    Use the convergence test of Fong and Saunders:
 *      Either || b-Ax ||_2 < btol * || b ||_2 + atol * || A ||_2 || x ||_2
 *      or     || A^T (b-Ax) ||_2 < atol * || A ||_2 * || A-bx ||_2
 *      or     Estimated condition of matrix >= conditionLimit                */
SPARSE_ENUM(SparseLSMRConvergenceTest, int,
  SparseLSMRCTDefault = 0,
  SparseLSMRCTFongSaunders = 1,
);

/*! @abstract LSMR is MINRES specialised for solving least squares.
 *
 *  @discussion Use LSMR to solve equations of the form Ax=b where an exact
 *  solution does not exist. The returned solution minimises || b-Ax ||_2.
 *
 *  Whilst LSMR is equivalent to MINRES applied to the normal equations
 *  A^TAx = A^Tb in exact arithmetic, it has superior numerical behaviour and
 *  should be used in preference.
 *
 *  We note that due to the implicit squaring of the condition of A in the
 *  normal equations, LSMR may struggle to converge in single precision, and
 *  double precision arithmetic is recommended.
 *
 *  For symmetric positive-definite systems, use CG instead.
 *  For square, full rank unsymmetric or indefinite equations, use GMRES instead.
 *
 *  LSMR is described in the following paper:
 *  [1] D.C.-L. Fong and M.A. Saunders (2011), "LSMR: An iterative algoirthm for
 *      sparse least-squares problems", SIAM J. Scientific Computing 33(5),
 *      pp 2950--2971.
 *
 *  \@callback reportError Function to use to report parameter errors.
 *    \@param message
 *    \@discussion If NULL, errors are logged via <os/log.h> and execution is
 *      halted via __builtin_trap().  If non-NULL, the provided function is
 *      called with a human-readable string describing the error condition.
 *      If the callback returns, control will be returned to the caller with
 *      any outputs in a safe but undefined state (i.e. they may hold partial
 *      results or garbage, but all sizes and pointers are valid).
 *
 *  \field lambda
 *    Damping parameter, if non-zero the actual problem solved is
 *          min_x || Ax-b ||_2 + lambda || x ||_2.
 *    Using damping can often allow the iteration to converge on ill-conditioned
 *    systems.
 *
 *  \field variant
 *    Variant of GMRES to use. See definition of SparseGMRESVariant_t for
 *    further information on the available variants.
 *
 *  \field nvec
 *    Number of vectors used for local orthagonalization.
 *    If nvec<=0, no orthagonalization is performed.
 *
 *  \field convergenceTest
 *    Which convergence test to use. See definition of
 *    SparseLSMRConvergenceTest_t for further information.
 *
 *  \field maxIterations
 *    Maximum number of iterations to perform. If 0, the default value of 4n
 *    is used.
 *    However, if a good preconditioner is available and/or the matrix is well
 *    conditioned such that singular values are clustered, a value of n/2 may
 *    be more approriate.
 *
 *  \field atol
 *    Either absolute tolerance (default test) or A tolerance (Fong-Saunders
 *    test). In the Fong and Saunders case, it should hold an estimate of the
 *    relative error in the data defining the matrix A. For example, if A is
 *    accurate to about 6 digits, set atol = 1.0e-6. In the Fong and Saunders
 *    case, if atol is 0.0, it is treated as machine epsilon. If using the
 *    default test, a value of 0.0 is treated as 0.0.
 *
 *  \field rtol
 *    Relative convergence tolerance (default test only).
 *    If rtol = 0.0, default value of sqrt(epsilon) is used.
 *    If negative, rtol is treated as 0.0 (default is not used).
 *
 *  \field btol
 *    b tolerance (Fong-Saunders test only). It should hold an estimate of the
 *    relative error in the data defining the rhs b. For example, if b is
 *    accurate to about 6 digits, set btol = 1.0e-6. If btol is zero, it
 *    is treated as machine epsilon.
 *
 *  \field conditionLimit
 *    Condition number limit (Fong-Saunders test). Iterations will be terminated
 *    if a computed estimate of cond(Abar) exceeds this value. This is intended
 *    to prevent certain small or zero singular values of A or Abar from coming
 *    into effect and causing unwanted growth in the computed solution.
 *    conditionLimit and lambda may be used separately or together to regularize
 *    ill-conditioned systems.
 *    Normally, conlim should be in the range 1000 to 1/eps.
 *    Suggested value:
 *    conlim = 1/(100*eps)  for compatible systems,
 *    conlim = 1/(10*sqrt(eps)) for least squares.
 *    If conditionLimit is 0.0, it is treated as 1/eps.
 *
 *  \@callback reportStatus Function to use to report status (iteration count
 *             and residual of first right-hand side) every few iterations.
 *    \@param message
 *    \@discussion If NULL, status is not reported.                           */
typedef struct {
  void (* _Nullable reportError)(const char *message);
  double lambda;
  int nvec;
  SparseLSMRConvergenceTest_t convergenceTest;
  double atol;
  double rtol;
  double btol;
  double conditionLimit;
  int maxIterations;
  void (* _Nullable reportStatus)(const char *message);
} SparseLSMROptions;

/*! @abstract General description object for all iterative methods.
 *
 *  @discussion This object is intended to be constructed through a call to an
 *  iterative method factory function, such as SparseConjugateGradient() or
 *  SparseLSMR().
 *
 *  @field method The type of method the object represents.
 *
 *  @field options The options to be used for the method.                     */
typedef struct {
  /*! @abstract Specify type of method described */
  int method;
  union {
    struct _SparseIterativeMethodBaseOptions base;
    SparseCGOptions cg;
    SparseGMRESOptions gmres;
    SparseLSMROptions lsmr;
    char padding[256]; /* Ensure union big enough for future method options */
  } options;
} SparseIterativeMethod;

/**** Iterative Method Factory Functions **************************************/

/*! @abstract Return a Conjugate Gradient Method with default options.        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseConjugateGradient(void);

/*! @abstract Return a Conjugate Gradient Method with specified options.
 *
 *  @param options Options for conjugate gradient method.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseConjugateGradient(SparseCGOptions options);

/*! @abstract Return a GMRES Method with default options.                     */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseGMRES(void);

/*! @abstract Return a GMRES Method with specified options.
 *
 *  @param options Options for GMRES method.                                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseGMRES(SparseGMRESOptions options);

/*! @abstract Return a LSMR Method with default options.                      */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseLSMR(void);

/*! @abstract Return a LSMR Method with specified options
 *
 *  @param options Options for LSMR method.                                   */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeMethod SparseLSMR(SparseLSMROptions options);

/******************************************************************************
 *  @group Iterative Sparse Solve Functions
 ******************************************************************************/

/**** Solve without preconditioner ********************************************/

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.                */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseMatrix_Double B, DenseMatrix_Double X);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.                */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseMatrix_Float B, DenseMatrix_Float X);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.                                                       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseVector_Double b, DenseVector_Double x);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.                                                       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseVector_Float b, DenseVector_Float x);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.                */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Double X, DenseMatrix_Double Y),
  DenseMatrix_Double B, DenseMatrix_Double X);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.                */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Float X, DenseMatrix_Float Y),
  DenseMatrix_Float B, DenseMatrix_Float X);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, x, y)
 *             should perform the operation y = op(A)x if accumulate is false,
 *             or y += op(A)x if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform y += op(A)x (if
 *            true) or y = op(A)x (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param x The vector to multiply.
 *    \@param y The vector in which to accumulate or store the result.
 *
 *  @param b The right-hand side b to solve for. If a has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.                                                       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseVector_Double x, DenseVector_Double y),
  DenseVector_Double b, DenseVector_Double x);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, x, y)
 *             should perform the operation y = op(A)x if accumulate is false,
 *             or y += op(A)x if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform y += op(A)x (if
 *            true) or y = op(A)x (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param x The vector to multiply.
 *    \@param y The vector in which to accumulate or store the result.
 *
 *  @param b The right-hand side b to solve for. If a has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.                                                       */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseVector_Float x, DenseVector_Float y),
  DenseVector_Float b, DenseVector_Float x);

/**** Solve with preconditioner ***********************************************/

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @param Preconditioner Type of preconditioner to create and apply.         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseMatrix_Double B, DenseMatrix_Double X,
  SparsePreconditioner_t Preconditioner);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @param Preconditioner Type of preconditioner to create and apply.         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseMatrix_Float B, DenseMatrix_Float X,
  SparsePreconditioner_t Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.
 *
 *  @param Preconditioner Type of preconditioner to create and apply.         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseVector_Double b, DenseVector_Double x,
  SparsePreconditioner_t Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.
 *
 *  @param Preconditioner Type of preconditioner to create and apply.         */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseVector_Float b, DenseVector_Float x,
  SparsePreconditioner_t Preconditioner);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @parameter Preconditioner The preconditioner to apply.                    */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseMatrix_Double B, DenseMatrix_Double X,
  SparseOpaquePreconditioner_Double Preconditioner);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @parameter Preconditioner The preconditioner to apply.                    */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseMatrix_Float B, DenseMatrix_Float X,
  SparseOpaquePreconditioner_Float Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.
 *
 *  @param Preconditioner The preconditioner to apply.                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Double A, DenseVector_Double b, DenseVector_Double x,
  SparseOpaquePreconditioner_Double Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  @param A (input) The matrix A to solve the system for. Only used for
 *         multiplication by A or A^T.
 *
 *  @param b The right-hand side b to solve for. If A has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.
 *
 *  @param Preconditioner The preconditioner to apply.                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  SparseMatrix_Float A, DenseVector_Float b, DenseVector_Float x,
  SparseOpaquePreconditioner_Float Preconditioner);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @param Preconditioner (input) The preconditioner to use.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Double X, DenseMatrix_Double Y),
  DenseMatrix_Double B, DenseMatrix_Double X,
  SparseOpaquePreconditioner_Double Preconditioner);

/*! @abstract Solve AX=B using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B The right-hand sides B to solve for. If A has dimension m x n, then
 *         B must have dimension m x nrhs, where nrhs is the number of
 *         right-hand sides to find solutions for.
 *
 *  @param X On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, and B has dimension m x nrhs, then X must have
 *         dimension n x nrhs. If no good initial estimate is available, user
 *         should set the initial guess to be the zero vector.
 *
 *  @param Preconditioner (input) The preconditioner to use.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Float X, DenseMatrix_Float Y),
  DenseMatrix_Float B, DenseMatrix_Float X,
  SparseOpaquePreconditioner_Float Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, x, y)
 *             should perform the operation y = op(A)x if accumulate is false,
 *             or y += op(A)x if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform y += op(A)x (if
 *            true) or y = op(A)x (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param x The vector to multiply.
 *    \@param y The vector in which to accumulate or store the result.
 *
 *  @param b The right-hand side b to solve for. If a has dimension m x n, then
 *         b must have length m.
 *
 *  @param x On entry, initial guess for solution, on return the solution. If A
 *         has dimension m x n, then x must have length n. If no good initial
 *         estimate is available, user should set the initial guess to be the
 *         zero vector.
 *
 *  @param Preconditioner (input) The preconditioner to use.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseVector_Double x, DenseVector_Double y),
  DenseVector_Double b, DenseVector_Double x,
  SparseOpaquePreconditioner_Double Preconditioner);

/*! @abstract Solve Ax=b using the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, x, y)
 *             should perform the operation y = op(A)x if accumulate is false,
 *             or y += op(A)x if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform y += op(A)x (if
 *            true) or y = op(A)x (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param x The vector to multiply.
 *    \@param y The vector in which to accumulate or store the result.
 *
 *  \@param b The right-hand side b to solve for. If a has dimension m x n, then
 *          b must have length m.
 *
 *  \@param x On entry, initial guess for solution, on return the solution. If A
 *          has dimension m x n, then x must have length n. If no good initial
 *          estimate is available, user should set the initial guess to be the
 *          zero vector.
 *
 *  @param Preconditioner (input) The preconditioner to use.                  */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseVector_Float x, DenseVector_Float y),
  DenseVector_Float B, DenseVector_Float X,
  SparseOpaquePreconditioner_Float Preconditioner);

/******************************************************************************
 *  @group Sparse Single-step Iteration  Functions
 ******************************************************************************
 *
 *  @discussion These functions are provided for expert uses who wish to execute
 *  iterative methods a single step at a time, for example to perform their own
 *  custom convergence tests.
 *
 *  A typical use of this routine might be as follows:
 *
 *  <pre>
 *  // Setup
 *  SparseIterativeMethod cg = SparseConjugateGradient();
 *  void *state = malloc( SparseGetStateSize_Double(cg, true, n, nrhs) );
 *  bool *converged = malloc(nrhs*sizeof(bool));
 *  memset(converged, false, nrhs*sizeof(bool)); // Initialize all as not converged
 *  SparseOpaquePreconditioner_Double Preconditioner = {
 *    .type  = SparsePreconditionerUser,
 *    .apply = myApplyFunction,
 *    .mem   = myDataBlock
 *  };
 *  for(long i=0; i<n*nrhs; i++) X->data[i] = 0.0; // Set X to initial guess of 0.0.
 *  memcpy(R->data, B->data, n*nrhs*sizeof(double)); // Set R = B   (X=0 => B-AX=B)
 *
 *  // Main iteration
 *  for(int iteration=0; iteration<maxIterations; iteration++) {
 *    SparseIterate(cg, iteration, converged, state, ApplyOperator, B, R, X, Preconditioner);
 *    bool all_converged = true;
 *    for(int j=0; j<nrhs; j++) {
 *      double residual2NormEstimate = R->data[j*R.columnStride];
 *      converged[j] = converged[j] || (residual2NormEstimate < convergenceTolerance);
 *      all_converged &= converged[j];
 *    }
 *    if(all_converged) break;
 *  }
 *
 *  // Finalise values
 *  SparseIterate(cg, -1, converged, state, ApplyOperator, B, R, X, Preconditioner);
 *
 *  // Free memory
 *  free(converged);
 *  free(state);
 *  </pre>
 */

/*! @abstract Returns size in bytes of state space required for call to
 *  SparseIterate().
 *
 *  @param method (input) Method to return required state space size for.
 *
 *  @param preconditioner (input) True if a preconditioner will be supplied,
 *         false otherwise.
 *
 *  @param m (input) Number of entries in right-hand side (rows in matrix A).
 *
 *  @param n (input) Number of variables to solve for (columns in matrix A).
 *
 *  @param nrhs (input) Number of right-hand sides to be solved for.
 *
 *  @returns Size of state space required in bytes.                           */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
size_t SparseGetStateSize_Double(SparseIterativeMethod method,
  bool preconditioner, int m, int n, int nrhs);

/*! @abstract Returns size in bytes of state space required for call to
 *  SparseIterate().
 *
 *  @param method (input) Method to return required state space size for.
 *
 *  @param preconditioner (input) True if a preconditioner will be supplied,
 *         false otherwise.
 *
 *  @param m (input) Number of entries in right-hand side (rows in matrix A).
 *
 *  @param n (input) Number of variables to solve for (columns in matrix A).
 *
 *  @param nrhs (input) Number of right-hand sides to be solved for.
 *
 *  @returns Size of state space required in bytes.                           */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
size_t SparseGetStateSize_Float(SparseIterativeMethod method,
  bool preconditioner, int m, int n, int nrhs);

/*! @abstract Perform a single iteration of the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *         Note that the options related to convergence testing (e.g.
 *         maxIterations, atol, rtol) are ignored as convergence tests must be
 *         performed by the user.
 *
 *  @param iteration (input) The current iteration number, starting from 0. If
 *         iteration<0, then the current iterate is finalised, and the value of
 *         X is updated (note that this may force some methods to restart,
 *         slowing convergence).
 *
 *  @param state (input/output) A pointer to a state-space of size returned by
 *         SparseGetStateSize_Double(). This memory must be 16-byte aligned
 *         (any allocation returned by malloc() has this property). It must not
 *         be altered by the user between iterations, but may be safely discarded
 *         after the final call to SparseIterate().
 *
 *  @param converged (input) Convergence status of each solution vector.
 *         converged[j]=true indicates that the vector stored as column j of X
 *         has converged, and it should be ignored in this iteration.
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B (input) The right-hand sides to solve for.
 *
 *  @param R (output) Residual estimate. On entry with iteration=0, it must hold
 *         the residuals b-Ax (equal to B if X=0). On return from each call with
 *         iteration>=0, the first entry(s) of each vector contain various
 *         estimates of norms to be used in convergence testing.
 *         For CG and GMRES:
 *           R(0,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *         For LSMR:
 *           R(0,j) holds an estimate of || A^T(b-Ax) ||_2 for the j-th rhs.
 *           R(1,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *           R(2,j) holds an estimate of || A ||_F, the Frobenius norm of A,
 *                  estimated using calculations related to the j-th rhs.
 *           R(3,j) holds an estimate of cond(A), the condition number of A,
 *                  estimated using calculations related to the j-th rhs.
 *         Other entries of R may be used by the routine as a workspace.
 *         On return from a call with iteration<0, the exact residual vector
 *         b-Ax is returned.
 *
 *  @param X (input/output) The current estimate of the solution vectors X.
 *         On entry with iteration=0, this should be an initial estimate for the
 *         solution. If no good estimate is available, use X = 0.0.
 *         Depending on the method used, X may not be updated at each iteration,
 *         or may be used to store some other vector.
 *         The user should make a call with iteration<0 once convergence has
 *         been achieved to bring X up to date.
 *
 *  @param Preconditioner (input) Preconditioner to apply.                    */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseIterate(SparseIterativeMethod method, int iteration,
  const bool *converged, void *state,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Double X, DenseMatrix_Double Y),
  DenseMatrix_Double B, DenseMatrix_Double R, DenseMatrix_Double X,
  SparseOpaquePreconditioner_Double Preconditioner);

/*! @abstract Perform a single iteration of the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *         Note that the options related to convergence testing (e.g.
 *         maxIterations, atol, rtol) are ignored as convergence tests must be
 *         performed by the user.
 *
 *  @param iteration (input) The current iteration number, starting from 0. If
 *         iteration<0, then the current iterate is finalised, and the value of
 *         X is updated (note that this may force some methods to restart,
 *         slowing convergence).
 *
 *  @param state (input/output) A pointer to a state-space of size returned by
 *         SparseGetStateSize_Double(). This memory must be 16-byte aligned
 *         (any allocation returned by malloc() has this property). It must not
 *         be altered by the user between iterations, but may be safely discarded
 *         after the final call to SparseIterate().
 *
 *  @param converged (input) Convergence status of each solution vector.
 *         converged[j]=true indicates that the vector stored as column j of X
 *         has converged, and it should be ignored in this iteration.
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B (input) The right-hand sides to solve for.
 *
 *  @param R (output) Residual estimate. On entry with iteration=0, it must hold
 *         the residuals b-Ax (equal to B if X=0). On return from each call with
 *         iteration>=0, the first entry(s) of each vector contain various
 *         estimates of norms to be used in convergence testing.
 *         For CG and GMRES:
 *           R(0,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *         For LSMR:
 *           R(0,j) holds an estimate of || A^T(b-Ax) ||_2 for the j-th rhs.
 *           R(1,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *           R(2,j) holds an estimate of || A ||_F, the Frobenius norm of A,
 *                  estimated using calculations related to the j-th rhs.
 *           R(3,j) holds an estimate of cond(A), the condition number of A,
 *                  estimated using calculations related to the j-th rhs.
 *         Other entries of R may be used by the routine as a workspace.
 *         On return from a call with iteration<0, the exact residual vector
 *         b-Ax is returned.
 *
 *  @param X (input/output) The current estimate of the solution vectors X.
 *         On entry with iteration=0, this should be an initial estimate for the
 *         solution. If no good estimate is available, use X = 0.0.
 *         Depending on the method used, X may not be updated at each iteration,
 *         or may be used to store some other vector.
 *         The user should make a call with iteration<0 once convergence has
 *         been achieved to bring X up to date.
 *
 *  @param Preconditioner (input) Preconditioner to apply.                    */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseIterate(SparseIterativeMethod method, int iteration,
  const bool *converged, void *state,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, DenseMatrix_Float X, DenseMatrix_Float Y),
  DenseMatrix_Float B, DenseMatrix_Float R, DenseMatrix_Float X,
  SparseOpaquePreconditioner_Float Preconditioner);

/*! @abstract Perform a single iteration of the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *         Note that the options related to convergence testing (e.g.
 *         maxIterations, atol, rtol) are ignored as convergence tests must be
 *         performed by the user.
 *
 *  @param iteration (input) The current iteration number, starting from 0. If
 *         iteration<0, then the current iterate is finalised, and the value of
 *         X is updated (note that this may force some methods to restart,
 *         slowing convergence).
 *
 *  @param state (input/output) A pointer to a state-space of size returned by
 *         SparseGetStateSize_Double(). This memory must be 16-byte aligned
 *         (any allocation returned by malloc() has this property). It must not
 *         be altered by the user between iterations, but may be safely discarded
 *         after the final call to SparseIterate().
 *
 *  @param converged (input) Convergence status of each solution vector.
 *         converged[j]=true indicates that the vector stored as column j of X
 *         has converged, and it should be ignored in this iteration.
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B (input) The right-hand sides to solve for.
 *
 *  @param R (output) Residual estimate. On entry with iteration=0, it must hold
 *         the residuals b-Ax (equal to B if X=0). On return from each call with
 *         iteration>=0, the first entry(s) of each vector contain various
 *         estimates of norms to be used in convergence testing.
 *         For CG and GMRES:
 *           R(0,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *         For LSMR:
 *           R(0,j) holds an estimate of || A^T(b-Ax) ||_2 for the j-th rhs.
 *           R(1,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *           R(2,j) holds an estimate of || A ||_F, the Frobenius norm of A,
 *                  estimated using calculations related to the j-th rhs.
 *           R(3,j) holds an estimate of cond(A), the condition number of A,
 *                  estimated using calculations related to the j-th rhs.
 *         Other entries of R may be used by the routine as a workspace.
 *         On return from a call with iteration<0, the exact residual vector
 *         b-Ax is returned.
 *
 *  @param X (input/output) The current estimate of the solution vectors X.
 *         On entry with iteration=0, this should be an initial estimate for the
 *         solution. If no good estimate is available, use X = 0.0.
 *         Depending on the method used, X may not be updated at each iteration,
 *         or may be used to store some other vector.
 *         The user should make a call with iteration<0 once convergence has
 *         been achieved to bring X up to date.                               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseIterate(SparseIterativeMethod method, int iteration,
  const bool *converged, void *state,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Double X, DenseMatrix_Double Y),
  DenseMatrix_Double B, DenseMatrix_Double R, DenseMatrix_Double X);

/*! @abstract Perform a single iteration of the specified iterative method.
 *
 *  @param method (input) Iterative method specification, eg return value of
 *         SparseConjugateGradient().
 *         Note that the options related to convergence testing (e.g.
 *         maxIterations, atol, rtol) are ignored as convergence tests must be
 *         performed by the user.
 *
 *  @param iteration (input) The current iteration number, starting from 0. If
 *         iteration<0, then the current iterate is finalised, and the value of
 *         X is updated (note that this may force some methods to restart,
 *         slowing convergence).
 *
 *  @param state (input/output) A pointer to a state-space of size returned by
 *         SparseGetStateSize_Double(). This memory must be 16-byte aligned
 *         (any allocation returned by malloc() has this property). It must not
 *         be altered by the user between iterations, but may be safely discarded
 *         after the final call to SparseIterate().
 *
 *  @param converged (input) Convergence status of each solution vector.
 *         converged[j]=true indicates that the vector stored as column j of X
 *         has converged, and it should be ignored in this iteration.
 *
 *  \@callback ApplyOperator (block) ApplyOperator(accumulate, trans, X, Y)
 *             should perform the operation Y = op(A)X if accumulate is false,
 *             or Y += op(A)X if accumulate is true.
 *    \@param accumulate (input) Indicates whether to perform Y += op(A)X (if
 *            true) or Y = op(A)X (if false).
 *    \@param trans (input) Indicates whether op(A) is the application of A
 *            (trans=CblasNoTrans) or A^T (trans=CblasTrans).
 *    \@param X The matrix to multiply.
 *    \@param Y The matrix in which to accumulate or store the result.
 *
 *  @param B (input) The right-hand sides to solve for.
 *
 *  @param R (output) Residual estimate. On entry with iteration=0, it must hold
 *         the residuals b-Ax (equal to B if X=0). On return from each call with
 *         iteration>=0, the first entry(s) of each vector contain various
 *         estimates of norms to be used in convergence testing.
 *         For CG and GMRES:
 *           R(0,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *         For LSMR:
 *           R(0,j) holds an estimate of || A^T(b-Ax) ||_2 for the j-th rhs.
 *           R(1,j) holds an estimate of || b-Ax ||_2 for the j-th rhs.
 *           R(2,j) holds an estimate of || A ||_F, the Frobenius norm of A,
 *                  estimated using calculations related to the j-th rhs.
 *           R(3,j) holds an estimate of cond(A), the condition number of A,
 *                  estimated using calculations related to the j-th rhs.
 *         Other entries of R may be used by the routine as a workspace.
 *         On return from a call with iteration<0, the exact residual vector
 *         b-Ax is returned.
 *
 *  @param X (input/output) The current estimate of the solution vectors X.
 *         On entry with iteration=0, this should be an initial estimate for the
 *         solution. If no good estimate is available, use X = 0.0.
 *         Depending on the method used, X may not be updated at each iteration,
 *         or may be used to store some other vector.
 *         The user should make a call with iteration<0 once convergence has
 *         been achieved to bring X up to date.                               */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
void SparseIterate(SparseIterativeMethod method, int iteration,
  const bool *converged, void *state,
  void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,
    DenseMatrix_Float X, DenseMatrix_Float Y),
  DenseMatrix_Float B, DenseMatrix_Float R, DenseMatrix_Float X);

/******************************************************************************
 *  @group Memory Management
 ******************************************************************************
 *  @discussion
 *  Underlying many object in sparse is a private data type references by a
 *  void* component on an opaque type. These are reference counted so that more
 *  than one opaque type can reference the same private object. For example,
 *  SparseCreateSubfactor() increments the refence count of the underlying type
 *  so that the returned SparseOpaqueSubfactor will still be valid even if the
 *  original factorization is destroyed.
 *
 *  Most users do not need to worry about the details of these objects, and
 *  should just ensure that they call SparseCleanup() on any object created
 *  by accelerate once they are done with it.
 *
 *  For advanced users, a SparseRetain() function is provided so they can create
 *  their own copies using the underlying reference counting. Whilst these
 *  functions return a copy, this can be discarded so long as a SparseCleanup()
 *  is invoked in some other fashion to release the reference.                */

/**** Retaining resources *****************************************************/

/*! @abstract Increase reference count on a symbolic factorization object,
 *            returning a copy.
 *
 *  @param SymbolicFactor The symbolic factorization to increase the underlying
 *         reference count of.
 *
 *  @returns A copy of SymbolicFactor.                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSymbolicFactorization SparseRetain(
  SparseOpaqueSymbolicFactorization SymbolicFactor);

/*! @abstract Increase reference count on a numeric factorization object,
 *            returning a copy.
 *
 *  @param NumericFactor The symbolic factorization to increase the underlying
 *         reference count of.
 *
 *  @returns A copy of NumericFactor.                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Double SparseRetain(
  SparseOpaqueFactorization_Double NumericFactor);

/*! @abstract Increase reference count on a numeric factorization object,
 *            returning a copy.
 *
 *  @param NumericFactor The symbolic factorization to increase the underlying
 *         reference count of.
 *
 *  @returns A copy of NumericFactor.                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueFactorization_Float SparseRetain(
  SparseOpaqueFactorization_Float NumericFactor);

/*! @abstract Increase reference count on a numeric factorization object,
 *            returning a copy.
 *
 *  @param NumericFactor The symbolic factorization to increase the underlying
 *         reference count of.
 *
 *  @returns A copy of NumericFactor.                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Double SparseRetain(SparseOpaqueSubfactor_Double NumericFactor);

/*! @abstract Increase reference count on a numeric factorization object,
 *            returning a copy.
 *
 *  @param NumericFactor The symbolic factorization to increase the underlying
 *         reference count of.
 *
 *  @returns A copy of NumericFactor.                                        */
static inline SPARSE_PUBLIC_INTERFACE API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
SparseOpaqueSubfactor_Float SparseRetain(SparseOpaqueSubfactor_Float NumericFactor);

/**** Cleaning up resources ***************************************************/

/*! @abstract Release a Sparse Object's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The resource to be freed.                                   */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueSymbolicFactorization Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Object's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The resource to be freed.                                   */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueFactorization_Double Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Object's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The resource to be freed.                                   */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueFactorization_Float Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Object's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The resource to be freed.                                   */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueSubfactor_Double Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Object's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The resource to be freed.                                   */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueSubfactor_Float Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse matrix's references to any memory allocated
 *  by the Sparse library.
 *
 *  @discussion Reports an error if the matrix was not allocated by Sparse.
 *
 *  @param Matrix The matrix to be freed.                                     */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseMatrix_Double Matrix)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse matrix's references to any memory allocated
 *  by the Sparse library.
 *
 *  @discussion Reports an error if the matrix was not allocated by Sparse.
 *
 *  @param Matrix The matrix to be freed.                                     */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseMatrix_Float Matrix)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Preconditioner's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The Preconditioner to be freed.                             */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaquePreconditioner_Double Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*! @abstract Release a Sparse Preconditioner's references to any memory allocated
 *  by the sparse library.
 *
 *  @param Opaque The Preconditioner to be freed.                             */
static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaquePreconditioner_Float Opaque)
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) );

/*********************************** END **************************************/

#if __has_feature(nullability)
# pragma clang assume_nonnull end
#endif

#if defined SPARSE_INCLUDED_VIA_ACCELERATE /* Included via Accelerate */
# include <vecLib/Sparse/SolveImplementation.h>
#else /* Standalone environments */
# include "SolveImplementation.h"
#endif

#endif /* __has_attribute(overloadable) */
#endif /* SPARSE_SOLVE_HEADER */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/SolveImplementationTyped.h
// This file expects the below preprocessor symbols to be defined prior to
// inclusion, which specifies which precision to generate implementations for.
#ifndef _SPARSE_IMPLEMENTATION_TYPE
# error "_SPARSE_IMPLEMENTATION_TYPE must be defined prior to inclusion"
#endif /* _SPARSE_IMPLEMENTATION_TYPE */
#ifndef _SPARSE_VARIANT
# error "_SPARSE_VARIANT must be defined prior to inclusion"
#endif /* _SPARSE_VARIANT */

/******************************************************************************
 *  External functions used to implement public API
 ******************************************************************************/
#if defined __cplusplus
extern "C" {
#endif

#if __has_feature(nullability)
#pragma clang assume_nonnull begin
#endif

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseMatrix) _SPARSE_VARIANT(_SparseConvertFromCoordinate)
(int m, int n, long nBlock, uint8_t blockSize, SparseAttributes_t attributes, const int *row,
 const int *col, const _SPARSE_IMPLEMENTATION_TYPE *val, char *storage, int *workspace);

#if defined __SPARSE_TYPES_H
API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
  extern _SPARSE_VARIANT(SparseMatrix) _SPARSE_VARIANT(_SparseConvertFromOpaque)(_SPARSE_OLDSTYLE(sparse_matrix) matrix);
#endif /* defined __SPARSE_TYPES_H */

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseOpaqueFactorization) _SPARSE_VARIANT(_SparseNumericFactorSymmetric)
(SparseOpaqueSymbolicFactorization *symbolicFactor,
 const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 const SparseNumericFactorOptions *options,
 void *factorStorage,
 void *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseOpaqueFactorization) _SPARSE_VARIANT(_SparseNumericFactorQR)
(SparseOpaqueSymbolicFactorization *symbolicFactor,
 const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 const SparseNumericFactorOptions *options,
 void *factorStorage,
 void *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseOpaqueFactorization) _SPARSE_VARIANT(_SparseFactorSymmetric)
(SparseFactorization_t factorType,
 const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 const SparseSymbolicFactorOptions *sfoptions,
 const SparseNumericFactorOptions *nfoptions);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseOpaqueFactorization) _SPARSE_VARIANT(_SparseFactorQR)
(SparseFactorization_t factorType,
 const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 const SparseSymbolicFactorOptions *sfoptions,
 const SparseNumericFactorOptions *nfoptions);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseRefactorSymmetric)
(const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 _SPARSE_VARIANT(SparseOpaqueFactorization) *Factorization,
 const SparseNumericFactorOptions *nfoptions,
 void *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseRefactorQR)
(const _SPARSE_VARIANT(SparseMatrix) *Matrix,
 _SPARSE_VARIANT(SparseOpaqueFactorization) *Factorization,
 const SparseNumericFactorOptions *nfoptions,
 void *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseMultiplySubfactor)
(const _SPARSE_VARIANT(SparseOpaqueSubfactor) *Subfactor,
 const _SPARSE_VARIANT(DenseMatrix) *_Nullable x,
 const _SPARSE_VARIANT(DenseMatrix) *y,
 char *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseSolveSubfactor)
(const _SPARSE_VARIANT(SparseOpaqueSubfactor) *Subfactor,
 const _SPARSE_VARIANT(DenseMatrix) *_Nullable b,
 const _SPARSE_VARIANT(DenseMatrix) *x,
 char *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseSolveOpaque)
(const _SPARSE_VARIANT(SparseOpaqueFactorization) *Factored,
 const _SPARSE_VARIANT(DenseMatrix) *_Nullable RHS,
 const _SPARSE_VARIANT(DenseMatrix) *Soln,
 void *workspace);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseDestroyOpaqueNumeric)(_SPARSE_VARIANT(SparseOpaqueFactorization) *toFree);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseRetainNumeric)
(_SPARSE_VARIANT(SparseOpaqueFactorization) *_Nonnull numericFactor);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseNumericFactorOptions _SPARSE_VARIANT(_SparseGetOptionsFromNumericFactor)(_SPARSE_VARIANT(SparseOpaqueFactorization) *factor);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseGetWorkspaceRequired)
(SparseSubfactor_t Subfactor, _SPARSE_VARIANT(SparseOpaqueFactorization) Factor, size_t *workStatic,
 size_t *workPerRHS);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern size_t _SPARSE_VARIANT(_SparseGetIterativeStateSize)
(const SparseIterativeMethod *method, bool preconditioner, int m, int n, int nrhs);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseCGIterate)
(const SparseCGOptions *options,
 int iteration,
 char *state,
 const bool *converged,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 _SPARSE_VARIANT(DenseMatrix) *R,
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)));

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseIterativeStatus_t _SPARSE_VARIANT(_SparseCGSolve)
(const SparseCGOptions *options,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseGMRESIterate)
(const SparseGMRESOptions *options,
 int iteration,
 char *state,
 const bool *converged,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 _SPARSE_VARIANT(DenseMatrix) *R,
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)));

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseIterativeStatus_t _SPARSE_VARIANT(_SparseGMRESSolve)
(SparseGMRESOptions *options,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseLSMRIterate)
(const SparseLSMROptions *options,
 int iteration,
 char *state,
 const bool *converged,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 _SPARSE_VARIANT(DenseMatrix) *R,
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)));

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseIterativeStatus_t _SPARSE_VARIANT(_SparseLSMRSolve)
(SparseLSMROptions *options,
 _SPARSE_VARIANT(DenseMatrix) *X,
 _SPARSE_VARIANT(DenseMatrix) *B,
 void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
 const _SPARSE_VARIANT(SparseOpaquePreconditioner) *_Nullable Preconditioner);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern _SPARSE_VARIANT(SparseOpaquePreconditioner) _SPARSE_VARIANT(_SparseCreatePreconditioner)
(SparsePreconditioner_t type,
 _SPARSE_VARIANT(SparseMatrix) *A);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseReleaseOpaquePreconditioner)
(_SPARSE_VARIANT(SparseOpaquePreconditioner) *toFree);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SPARSE_VARIANT(_SparseSpMV)
(_SPARSE_IMPLEMENTATION_TYPE alpha,
 _SPARSE_VARIANT(SparseMatrix) A,
 _SPARSE_VARIANT(DenseMatrix) x,
 bool accumulate,
 _SPARSE_VARIANT(DenseMatrix) y);

#if __has_feature(nullability)
#pragma clang assume_nonnull end
#endif

#if defined __cplusplus
} /* extern "C" */
#endif

/******************************************************************************
 *  Implementation helper definitions and types
 ******************************************************************************/
static inline __attribute__((__const__))
_SPARSE_VARIANT(DenseMatrix) _SPARSE_VARIANT(_DenseMatrixFromVector)(_SPARSE_VARIANT(DenseVector) x) {
  return (_SPARSE_VARIANT(DenseMatrix)){
    .rowCount = x.count,
    .columnCount = 1,
    .columnStride = x.count,
    .data = x.data,
  };
}

// This function just provides a shorthand way of delivering an invalid subfactor
static inline __attribute__((__const__))
_SPARSE_VARIANT(SparseOpaqueSubfactor) _SPARSE_VARIANT(_SparseInvalidSubfactor)() {
  return (_SPARSE_VARIANT(SparseOpaqueSubfactor)) {
    .contents = SparseSubfactorInvalid,
    .factor = {
      .symbolicFactorization = { .status = SparseInternalError },
      .status = SparseInternalError
    }
  };
}

// This function just provides a shorthand way of delivering a valid failed factor matrix
static inline __attribute__((__const__))
_SPARSE_VARIANT(SparseOpaqueFactorization) _SPARSE_VARIANT(_SparseFailedFactor)(SparseStatus_t status) {
  return (_SPARSE_VARIANT(SparseOpaqueFactorization)) {
    .symbolicFactorization = { .status = status },
    .status = status
  };
}

// Null matrix for return on bad function calls
static const _SPARSE_VARIANT(SparseMatrix) _SPARSE_VARIANT(_SparseNullMatrix) = {
  .structure = {
    .rowCount = -1,
    .columnCount = -1,
    .blockSize = 0,
  },
  .data = NULL
};


/*! @abstract Return dimensions of matrix for the part of the factorization represented by Subfactor */
static inline
void _SPARSE_VARIANT(_SparseSubFactorGetDimn)(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, int *_Nonnull m, int *_Nonnull n) {
  uint8_t blockSize = Subfactor.factor.symbolicFactorization.blockSize;
  *m = Subfactor.factor.symbolicFactorization.rowCount * blockSize;
  *n = Subfactor.factor.symbolicFactorization.columnCount * blockSize;

  // We always factor a matrix where m>=n. Swap them so this is so.
  if(*m < *n) {
    int temp = *m;
    *m = *n;
    *n = temp;
  }

  // All Subfactors are n x n, except Q in SparseFactorizationQR, which is m x n
  if(!(Subfactor.factor.symbolicFactorization.type == SparseFactorizationQR && Subfactor.contents == SparseSubfactorQ)) {
    // Not the Q factor of QR => n x n
    *m = *n;
  }

  // If Subfactor is transposed, swap m and n
  if(Subfactor.attributes.transpose) {
    int temp = *m;
    *m = *n;
    *n = temp;
  }
}

/******************************************************************************
 *  Conversion From Other Formats
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseMatrix) SparseConvertFromCoordinate(
    int rowCount, int columnCount, long blockCount, uint8_t blockSize, SparseAttributes_t attributes,
    const int *_Nonnull row, const int *_Nonnull column, const _SPARSE_IMPLEMENTATION_TYPE *_Nonnull data) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions;
  SPARSE_PARAMETER_CHECK(rowCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "rowCount (%d) must be non-negative.\n", rowCount);
  SPARSE_PARAMETER_CHECK(columnCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "columnCount (%d) must be non-negative.\n", columnCount);
  SPARSE_PARAMETER_CHECK(blockCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "blockCount (%ld) must be non-negative.\n", blockCount);
  SPARSE_PARAMETER_CHECK(attributes.kind==SparseOrdinary || rowCount==columnCount,
                         _SPARSE_VARIANT(_SparseNullMatrix),
                         "attributes.kind must be SparseOrdinary if matrix is not square.\n");
  // NB: Whilst we request 48+ actual space from user, this is for alignment
  //     as malloc() returns 16-byte aligned data anyway, 2*16-4=28 is actually sufficient
  char *storage = (char *)malloc(28 + (columnCount+1)*sizeof(long) + blockCount*sizeof(int) +
                                 blockCount*blockSize*blockSize*sizeof(data[0]));
  SPARSE_PARAMETER_CHECK(storage, _SPARSE_VARIANT(_SparseNullMatrix),
                         "Failed to allocate storage for result.\n");
  int *workspace = (int *)malloc(rowCount*sizeof(int));
  if(!workspace) free(storage);
  SPARSE_PARAMETER_CHECK(workspace, _SPARSE_VARIANT(_SparseNullMatrix),
                         "Failed to allocate workspace of size %ld.\n", rowCount*sizeof(int));
  _SPARSE_VARIANT(SparseMatrix) result =  _SPARSE_VARIANT(_SparseConvertFromCoordinate)(rowCount, columnCount, blockCount, blockSize, attributes, row, column, data, storage, workspace);
  result.structure.columnStarts = (long *)storage; // redundant to assignment in _SparseConvertFromCoordinate call, used to suppress static analyzer warning
  free(workspace);
  result.structure.attributes._allocatedBySparse = true;
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseMatrix) SparseConvertFromCoordinate(int rowCount,
    int columnCount, long blockCount, uint8_t blockSize, SparseAttributes_t attributes,
    const int *_Nonnull row, const int *_Nonnull column, const _SPARSE_IMPLEMENTATION_TYPE *_Nonnull data,
    void *_Nonnull storage, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions;
  SPARSE_PARAMETER_CHECK(rowCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "rowCount (%d) must be non-negative.\n", rowCount);
  SPARSE_PARAMETER_CHECK(columnCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "columnCount (%d) must be non-negative.\n", columnCount);
  SPARSE_PARAMETER_CHECK(blockCount>=0, _SPARSE_VARIANT(_SparseNullMatrix),
                         "blockCount (%ld) must be non-negative.\n", blockCount);
  SPARSE_PARAMETER_CHECK(attributes.kind==SparseOrdinary || rowCount==columnCount,
                         _SPARSE_VARIANT(_SparseNullMatrix),
                         "attributes.kind must be SparseOrdinary if matrix is not square.\n");
  return _SPARSE_VARIANT(_SparseConvertFromCoordinate)(rowCount, columnCount, blockCount, blockSize, attributes, row, column, data, (char*)storage, (int*)workspace);
}

#if defined __SPARSE_TYPES_H
static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseMatrix) SparseConvertFromOpaque(_Nonnull _SPARSE_OLDSTYLE(sparse_matrix) matrix) {
  return _SPARSE_VARIANT(_SparseConvertFromOpaque)(matrix);
}
#endif /* defined __SPARSE_TYPES_H */

/******************************************************************************
 *  Matrix and Vector Operations (Sparse BLAS Wrappers)
 ******************************************************************************/

/**** Multiplication **********************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_IMPLEMENTATION_TYPE alpha, _SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor

  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(Am, An, Y, X, /* no result */, "matrix A");
  _SPARSE_VARIANT(_SparseSpMV)(alpha, A, X, false, Y);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_IMPLEMENTATION_TYPE alpha, _SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseVector) x, _SPARSE_VARIANT(DenseVector) y) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor

  int m = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int n = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(n == x.count, /* No result */,
                         "Matrix dimensions (%dx%d) do not match x vector dimensions %dx%d\n",
                         m, n, x.count, 1);
  SPARSE_PARAMETER_CHECK(m == y.count, /* No result */,
                         "Matrix dimensions (%dx%d) do not match y vector dimensions %dx%d\n",
                         m, n, y.count, 1);
  _SPARSE_VARIANT(_SparseSpMV)(alpha, A, _SPARSE_VARIANT(_DenseMatrixFromVector)(x),
                               false, _SPARSE_VARIANT(_DenseMatrixFromVector)(y));
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y) {
  SparseMultiply(1, A, X, Y);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseVector) x, _SPARSE_VARIANT(DenseVector) y) {
  SparseMultiply(1, A, x, y);
}

/**** Multipy-Add *************************************************************/


static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(_SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y) {
  SparseMultiplyAdd(1, A, X, Y);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(_SPARSE_IMPLEMENTATION_TYPE alpha, _SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor

  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(Am, An, Y, X, /* no result */, "matrix A");
  _SPARSE_VARIANT(_SparseSpMV)(alpha, A, X, true, Y);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(_SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseVector) x, _SPARSE_VARIANT(DenseVector) y) {
  SparseMultiplyAdd(1, A, x, y);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiplyAdd(_SPARSE_IMPLEMENTATION_TYPE alpha, _SPARSE_VARIANT(SparseMatrix) A, _SPARSE_VARIANT(DenseVector) x, _SPARSE_VARIANT(DenseVector) y) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor

  int m = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int n = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(n == x.count, /* No result */,
                         "Matrix dimensions (%dx%d) do not match x vector dimensions %dx%d\n",
                         m, n, x.count, 1);
  SPARSE_PARAMETER_CHECK(m == y.count, /* No result */,
                         "Matrix dimensions (%dx%d) do not match y vector dimensions %dx%d\n",
                         m, n, y.count, 1);
  _SPARSE_VARIANT(_SparseSpMV)(alpha, A, _SPARSE_VARIANT(_DenseMatrixFromVector)(x),
                               true, _SPARSE_VARIANT(_DenseMatrixFromVector)(y));
}

/******************************************************************************
 *  Transposition
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE _SPARSE_VARIANT(SparseMatrix) SparseGetTranspose(_SPARSE_VARIANT(SparseMatrix) Matrix) {
  Matrix.structure.attributes.transpose = !Matrix.structure.attributes.transpose;
  return Matrix;
}

static inline SPARSE_PUBLIC_INTERFACE _SPARSE_VARIANT(SparseOpaqueFactorization) SparseGetTranspose(_SPARSE_VARIANT(SparseOpaqueFactorization) Factor) {
  Factor.attributes.transpose = !Factor.attributes.transpose;
  _SPARSE_VARIANT(_SparseRetainNumeric)(&Factor); // we're taking an additional reference
  return Factor;
}

static inline SPARSE_PUBLIC_INTERFACE _SPARSE_VARIANT(SparseOpaqueSubfactor) SparseGetTranspose(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor) {
  Subfactor.attributes.transpose = !Subfactor.attributes.transpose;
  _SPARSE_VARIANT(_SparseRetainNumeric)(&Subfactor.factor); // we're taking an additional reference
  return Subfactor;
}

/******************************************************************************
 *  Sparse Factor Functions
 ******************************************************************************/

/**** All-in-one Sparse Factor Functions **************************************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseFactor(SparseFactorization_t type,
    _SPARSE_VARIANT(SparseMatrix) Matrix) {
  return SparseFactor(type, Matrix, _SparseDefaultSymbolicFactorOptions,
                      _SPARSE_VARIANT(_SparseDefaultNumericFactorOptions));
}

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseFactor(SparseFactorization_t type,
    _SPARSE_VARIANT(SparseMatrix) Matrix, SparseSymbolicFactorOptions options,
    SparseNumericFactorOptions nfoptions) {
  SPARSE_CHECK_VALID_MATRIX_STRUCTURE(Matrix.structure, _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError));
  switch(type) {
    case SparseFactorizationCholeskyAtA:
    case SparseFactorizationQR:
      return _SPARSE_VARIANT(_SparseFactorQR)(type, &Matrix, &options, &nfoptions);
    default:
      SPARSE_PARAMETER_CHECK(Matrix.structure.attributes.kind == SparseSymmetric,
                             _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError),
                             "Cannot perform symmetric matrix factorization of non-square matrix.\n");
      return _SPARSE_VARIANT(_SparseFactorSymmetric)(type, &Matrix, &options, &nfoptions);
  }
}

/**** Sparse Factor Functions using pre-calculated symbolic factors ***********/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor,
    _SPARSE_VARIANT(SparseMatrix) Matrix) {
  return SparseFactor(SymbolicFactor, Matrix, _SPARSE_VARIANT(_SparseDefaultNumericFactorOptions), NULL, NULL);
}

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseFactor(SparseOpaqueSymbolicFactorization SymbolicFactor,
    _SPARSE_VARIANT(SparseMatrix) Matrix, SparseNumericFactorOptions nfoptions) {
  return SparseFactor(SymbolicFactor, Matrix, nfoptions, NULL, NULL);
}

/**** Sparse Factor Functions with user-defined workspace *********************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseFactor(SparseOpaqueSymbolicFactorization symbolicFactor,
    _SPARSE_VARIANT(SparseMatrix) Matrix, SparseNumericFactorOptions nfoptions,
    void *_Nullable factorStorage, void *_Nullable workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions;
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(symbolicFactor,
                                     _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError),
                                     "Bad symbolic factor.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&symbolicFactor);
  SPARSE_CHECK_MATCH_SYMB_FACTOR(Matrix, symbolicFactor,
                                 _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError));
  // User may or may not have supplied storage for factors and workspace. If they haven't, we
  // allocate our own, the following two variables track which case we're in.
  bool userFactorStorage = (factorStorage != NULL);
  bool userWorkspace     = (workspace != NULL);
  if(!userFactorStorage) {
    factorStorage = options.malloc(_SPARSE_VARIANT(symbolicFactor.factorSize));
    SPARSE_PARAMETER_CHECK(factorStorage != NULL,
                           _SPARSE_VARIANT(_SparseFailedFactor)(SparseInternalError),
                           "Failed to allocate factor storage of size %ld bytes.",
                           _SPARSE_VARIANT(symbolicFactor.factorSize));
  }
  if(!userWorkspace) {
    workspace = options.malloc(_SPARSE_VARIANT(symbolicFactor.workspaceSize));
    if(!workspace && !userFactorStorage) options.free(factorStorage);
    SPARSE_PARAMETER_CHECK(workspace != NULL,
                           _SPARSE_VARIANT(_SparseFailedFactor)(SparseInternalError),
                           "Failed to allocate workspace of size %ld bytes.",
                           _SPARSE_VARIANT(symbolicFactor.workspaceSize));
  }
  _SPARSE_VARIANT(SparseOpaqueFactorization) result;
  switch(symbolicFactor.type) {
    case SparseFactorizationCholeskyAtA:
    case SparseFactorizationQR:
      result = _SPARSE_VARIANT(_SparseNumericFactorQR)(&symbolicFactor, &Matrix, &nfoptions, factorStorage, workspace);
      break;

    default:
      result = _SPARSE_VARIANT(_SparseNumericFactorSymmetric)(&symbolicFactor, &Matrix, &nfoptions, factorStorage, workspace);
  }
  result.userFactorStorage = userFactorStorage; // Flag whether we should free memory on destruction or not
  if(!userWorkspace) options.free(workspace); // If we allocated workspace, free it
  return result;
}

/******************************************************************************
 *  Sparse Direct Solve Functions (DenseMatrix)
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseMatrix) XB) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                    "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_MAT_IN_PLACE(Factored, XB, /* no result */);
  int nrhs = (XB.attributes.transpose) ? XB.rowCount : XB.columnCount; // number of rhs
  size_t lworkspace = Factored.solveWorkspaceRequiredStatic + nrhs*Factored.solveWorkspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /*no result*/, "Failed to allocate workspace of size %ld for SparseSolve().\n", lworkspace);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, NULL, &XB, workspace);
  options.free(workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_MAT_OUT_PLACE(Factored, B, X, /* no result */);
  int nrhs = (B.attributes.transpose) ? B.rowCount : B.columnCount; // number of rhs
  size_t lworkspace = Factored.solveWorkspaceRequiredStatic + nrhs*Factored.solveWorkspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /*no result*/,
                         "Failed to allocate workspace of size %ld for SparseSolve().\n",
                         lworkspace);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, &B, &X, workspace);
  options.free(workspace);
}

/**** Solving Systems with User Defined Workspace *****************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseMatrix) XB, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_MAT_IN_PLACE(Factored, XB, /* no result */);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, NULL, &XB, workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_MAT_OUT_PLACE(Factored, B, X, /* no result */);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, &B, &X, workspace);
}

/******************************************************************************
 *  Sparse Direct Solve Functions (DenseVector)
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseVector) xb) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_VEC_IN_PLACE(Factored, xb, /* no result */);
  _SPARSE_VARIANT(DenseMatrix) XB = _SPARSE_VARIANT(_DenseMatrixFromVector)(xb);
  size_t lworkspace = Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /*no result*/, "Failed to allocate workspace of size %ld for SparseSolve().\n", lworkspace);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, NULL, &XB, workspace);
  options.free(workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE(Factored, b, x, /* no result */);
  _SPARSE_VARIANT(DenseMatrix) B = _SPARSE_VARIANT(_DenseMatrixFromVector)(b);
  _SPARSE_VARIANT(DenseMatrix) X = _SPARSE_VARIANT(_DenseMatrixFromVector)(x);
  size_t lworkspace = Factored.solveWorkspaceRequiredStatic + 1*Factored.solveWorkspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /*no result*/,
                         "Failed to allocate workspace of size %ld for SparseSolve().\n",
                         lworkspace);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, &B, &X, workspace);
  options.free(workspace);
}

/**** Solving Systems with User Defined Workspace *****************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseVector) xb, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_VEC_IN_PLACE(Factored, xb, /* no result */);
  _SPARSE_VARIANT(DenseMatrix) XB = _SPARSE_VARIANT(_DenseMatrixFromVector)(xb);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, NULL, &XB, workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueFactorization) Factored, _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored.symbolicFactorization, /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factored.symbolicFactorization);
  SPARSE_CHECK_VALID_NUMERIC_FACTOR(Factored, /* no result */);
  SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE(Factored, b, x, /* no result */);
  _SPARSE_VARIANT(DenseMatrix) B = _SPARSE_VARIANT(_DenseMatrixFromVector)(b);
  _SPARSE_VARIANT(DenseMatrix) X = _SPARSE_VARIANT(_DenseMatrixFromVector)(x);
  _SPARSE_VARIANT(_SparseSolveOpaque)(&Factored, &B, &X, workspace);
}

/******************************************************************************
 *  Advanced Solving Functions
 ******************************************************************************/

/**** Symbolic Refactor Functions *********************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseRefactor(_SPARSE_VARIANT(SparseMatrix) Matrix, _SPARSE_VARIANT(SparseOpaqueFactorization) *_Nonnull Factorization) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factorization->symbolicFactorization,
                                     /* no result */,
                                     "Factorization does not hold a completed matrix factorization.\n");
  SparseNumericFactorOptions nfoptions = _SPARSE_VARIANT(_SparseGetOptionsFromNumericFactor)(Factorization);
  SparseRefactor(Matrix, Factorization, nfoptions);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseRefactor(_SPARSE_VARIANT(SparseMatrix) Matrix, _SPARSE_VARIANT(SparseOpaqueFactorization) *_Nonnull Factorization, SparseNumericFactorOptions nfoptions) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factorization->symbolicFactorization,
                                     /* no result */,
                                     "Factorization does not hold a valid symbolic matrix factorization.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factorization->symbolicFactorization);
  void *workspace = options.malloc(_SPARSE_VARIANT(Factorization->symbolicFactorization.workspaceSize));
  if(!workspace) Factorization->status = SparseInternalError;
  SPARSE_PARAMETER_CHECK(workspace,
                         /* no return value */,
                         "Failed to allocate workspace of size %ld.",
                         _SPARSE_VARIANT(Factorization->symbolicFactorization.workspaceSize));

  SparseRefactor(Matrix, Factorization, nfoptions, workspace);
  options.free(workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseRefactor(_SPARSE_VARIANT(SparseMatrix) Matrix, _SPARSE_VARIANT(SparseOpaqueFactorization) *_Nonnull Factored, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(Factored->symbolicFactorization,
                                     /* no result */,
                                     "Factored does not hold a completed matrix factorization.\n");
  SparseNumericFactorOptions nfoptions = _SPARSE_VARIANT(_SparseGetOptionsFromNumericFactor)(Factored);
  return SparseRefactor(Matrix, Factored, nfoptions, workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseRefactor(_SPARSE_VARIANT(SparseMatrix) Matrix, _SPARSE_VARIANT(SparseOpaqueFactorization) *_Nonnull Factored, SparseNumericFactorOptions nfoptions, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_MATCH_SYMB_FACTOR(Matrix, Factored->symbolicFactorization, /* no result */);
  switch(Factored->symbolicFactorization.type) {
    case SparseFactorizationCholeskyAtA:
    case SparseFactorizationQR:
      _SPARSE_VARIANT(_SparseRefactorQR)(&Matrix, Factored, &nfoptions, workspace);
      break;
    default:
      _SPARSE_VARIANT(_SparseRefactorSymmetric)(&Matrix, Factored, &nfoptions, workspace);
  }
}

/******************************************************************************
 *  Extracting Sub-factors of Factors
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueSubfactor) SparseCreateSubfactor(SparseSubfactor_t subfactor,
                                                        _SPARSE_VARIANT(SparseOpaqueFactorization) Factor) {
  // Check user supplied a valid factorization object
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Factor.symbolicFactorization.status == SparseStatusOK &&
                         Factor.symbolicFactorization.factorization &&
                         Factor.status == SparseStatusOK &&
                         Factor.numericFactorization,
                         _SPARSE_VARIANT(_SparseInvalidSubfactor)(),
                         "Bad factor.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Factor.symbolicFactorization);
  // Check user is requesting a valid combination of subfactor and Factor
  SparseAttributes_t attributes = {
    .kind = SparseOrdinary,
    .transpose = false,
    .triangle = SparseLowerTriangle
  };
  SparseFactorization_t type = Factor.symbolicFactorization.type;
  switch(subfactor) {
    case SparseSubfactorP:
      break; // Valid for all factor types

    case SparseSubfactorL:
    case SparseSubfactorPLPS:
      // Valid for Cholesky and LDL^T only
      attributes.kind = (subfactor==SparseSubfactorPLPS) ? SparseOrdinary : SparseTriangular;
      SPARSE_PARAMETER_CHECK(type == SparseFactorizationCholesky ||
                             type == SparseFactorizationLDLTUnpivoted ||
                             type == SparseFactorizationLDLTSBK ||
                             type == SparseFactorizationLDLTTPP,
                             _SPARSE_VARIANT(_SparseInvalidSubfactor)(),
                             "Subfactor Type only valid for Cholesky and LDL^T factorizations.\n");
      break;

    case SparseSubfactorS:
    case SparseSubfactorD:
      // Valid for LDL^T only
      SPARSE_PARAMETER_CHECK(type == SparseFactorizationLDLTUnpivoted ||
                             type == SparseFactorizationLDLTSBK ||
                             type == SparseFactorizationLDLTTPP,
                             _SPARSE_VARIANT(_SparseInvalidSubfactor)(),
                             "Subfactor Type only valid for LDL^T factorizations.\n");
      break;

    case SparseSubfactorQ:
      // Valid for QR only
      SPARSE_PARAMETER_CHECK(type == SparseFactorizationQR,
                             _SPARSE_VARIANT(_SparseInvalidSubfactor)(),
                             "SparseSubfactorQ only valid for QR factorizations.\n");
      break;

    case SparseSubfactorR:
    case SparseSubfactorRP:
      // Valid for QR and CholeskyAtA only
      attributes.kind = SparseTriangular;
      attributes.triangle = SparseUpperTriangle;
      SPARSE_PARAMETER_CHECK(type == SparseFactorizationQR ||
                             type == SparseFactorizationCholeskyAtA,
                             _SPARSE_VARIANT(_SparseInvalidSubfactor)(),
                             "Subfactor Type only valid for QR and CholeskyAtA factorizations.\n");
      break;

    default:
      SPARSE_PARAMETER_CHECK(false, _SPARSE_VARIANT(_SparseInvalidSubfactor)(), "Invalid subfactor type.");
  }
  _SPARSE_VARIANT(_SparseRetainNumeric)(&Factor); // count new reference
  size_t workspaceRequiredStatic, workspaceRequiredPerRHS;
  _SPARSE_VARIANT(_SparseGetWorkspaceRequired)(subfactor, Factor, &workspaceRequiredStatic, &workspaceRequiredPerRHS);
  return (_SPARSE_VARIANT(SparseOpaqueSubfactor)) {
    .attributes = attributes,
    .contents = subfactor,
    .factor = Factor,
    .workspaceRequiredStatic = workspaceRequiredStatic,
    .workspaceRequiredPerRHS = workspaceRequiredPerRHS
  };
}

/******************************************************************************
 *  Sub-factor Multiplication and Solve Functions
 ******************************************************************************/

/**** Matrix solve functions **************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) XB) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factorization.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and b
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  // Check XY agrees
  int XBCount = (XB.attributes.transpose) ? XB.rowCount : XB.columnCount;
  int XBsize  = (XB.attributes.transpose) ? XB.columnCount : XB.rowCount;
  SPARSE_PARAMETER_CHECK(XBCount > 0, /* none */, "XB must have non-zero dimension.\n");
  int maxmn = (m > n) ? m : n;
  SPARSE_PARAMETER_CHECK(XBsize == maxmn, /* none */,
                         "XB dimension (%d) must match maximum subfactor dimension (%d).\n",
                         XBsize, maxmn);
  size_t lworkspace = Subfactor.workspaceRequiredStatic + XBCount*Subfactor.workspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /* none */,
                         "Failed to allocate workspace of size %ld.\n", lworkspace);
  _SPARSE_VARIANT(_SparseSolveSubfactor)(&Subfactor, NULL, &XB, (char*)workspace);
  options.free(workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factorization.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(m, n, B, X, /* no result */, "subfactor dimension");
  int nrhs = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  size_t lworkspace = Subfactor.workspaceRequiredStatic + nrhs*Subfactor.workspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /* none */,
                         "Failed to allocate workspace of size %ld.\n",
                         lworkspace);
  _SPARSE_VARIANT(_SparseSolveSubfactor)(&Subfactor, &B, &X, (char*)workspace);
  options.free(workspace);
}

/**** Matrix solve functions with user-supplied workspace *********************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) XB, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and b
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  // Check XY agrees
  int XBCount = (XB.attributes.transpose) ? XB.rowCount : XB.columnCount;
  int XBsize  = (XB.attributes.transpose) ? XB.columnCount : XB.rowCount;
  SPARSE_PARAMETER_CHECK(XBCount > 0, /* none */, "XB must have non-zero dimension.\n");
  int maxmn = (m > n) ? m : n;
  SPARSE_PARAMETER_CHECK(XBsize == maxmn, /* none */,
                         "XB dimension (%d) must match maximum subfactor dimension (%d).\n",
                         XBsize, maxmn);
  _SPARSE_VARIANT(_SparseSolveSubfactor)(&Subfactor, NULL, &XB, (char*)workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X,
                 void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(m, n, B, X, /* no result */, "subfactor dimension");
  _SPARSE_VARIANT(_SparseSolveSubfactor)(&Subfactor, &B, &X, (char*)workspace);
}

/**** Vector solve ************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) XB) {
  _SPARSE_VARIANT(DenseMatrix) XBMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(XB);
  SparseSolve(Subfactor, XBMatrix);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) B, _SPARSE_VARIANT(DenseVector) X) {
  _SPARSE_VARIANT(DenseMatrix) BMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(B);
  _SPARSE_VARIANT(DenseMatrix) XMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(X);
  SparseSolve(Subfactor, BMatrix, XMatrix);
}

/**** Vector solve functions with user-supplied workspace *********************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) XB, void *_Nonnull workspace) {
  _SPARSE_VARIANT(DenseMatrix) XBMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(XB);
  SparseSolve(Subfactor, XBMatrix, workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseSolve(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) B, _SPARSE_VARIANT(DenseVector) X,
                 void *_Nonnull workspace) {
  _SPARSE_VARIANT(DenseMatrix) BMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(B);
  _SPARSE_VARIANT(DenseMatrix) XMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(X);
  SparseSolve(Subfactor, BMatrix, XMatrix, workspace);
}

/**** Matrix multiply functions ***********************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) XY) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  // Check XY agrees
  int XYCount = (XY.attributes.transpose) ? XY.rowCount : XY.columnCount;
  int XYsize  = (XY.attributes.transpose) ? XY.columnCount : XY.rowCount;
  SPARSE_PARAMETER_CHECK(XYCount > 0, /* none */, "XY must have non-zero dimension.\n");
  int maxmn = (m > n) ? m : n;
  SPARSE_PARAMETER_CHECK(XYsize == maxmn, /* none */,
                         "XY dimension (%d) must match maximum subfactor dimension (%d).\n",
                         XYsize, maxmn);
  size_t lworkspace = Subfactor.workspaceRequiredStatic + XYCount*Subfactor.workspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /* none */,
                         "Failed to allocate workspace of size %ld.\n", lworkspace);
  _SPARSE_VARIANT(_SparseMultiplySubfactor)(&Subfactor, NULL, &XY, (char*)workspace);
  options.free(workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(n, m, X, Y, /* no result */, "subfactor dimension");

  int nrhs = (Y.attributes.transpose) ? Y.rowCount : Y.columnCount;
  size_t lworkspace = Subfactor.workspaceRequiredStatic + nrhs*Subfactor.workspaceRequiredPerRHS;
  void *workspace = options.malloc(lworkspace);
  SPARSE_PARAMETER_CHECK(workspace, /* none */,
                         "Failed to allocate workspace of size %ld.\n", lworkspace);
  _SPARSE_VARIANT(_SparseMultiplySubfactor)(&Subfactor, &X, &Y, (char*)workspace);
  options.free(workspace);
}

/**** Matrix multiply functions with user-supplied workspace ******************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) XY, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  // Check XY agrees
  int XYCount = (XY.attributes.transpose) ? XY.rowCount : XY.columnCount;
  int XYsize  = (XY.attributes.transpose) ? XY.columnCount : XY.rowCount;
  SPARSE_PARAMETER_CHECK(XYCount > 0, /* none */, "XY must have non-zero dimension.\n");
  int maxmn = (m > n) ? m : n;
  SPARSE_PARAMETER_CHECK(XYsize == maxmn, /* none */,
                         "XY dimension (%d) must match maximum subfactor dimension (%d).\n",
                         XYsize, maxmn);
  _SPARSE_VARIANT(_SparseMultiplySubfactor)(&Subfactor, NULL, &XY, (char*)workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseMatrix) X, _SPARSE_VARIANT(DenseMatrix) Y, void *_Nonnull workspace) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.contents != SparseSubfactorInvalid, /* none */,
                         "Subfactor does not hold a valid factor subobject.\n");
  // The only way to construct a valid Subfactor is to have a valid factorization.
  // We assume this from this point on.
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  // Figure out expected dimensions of x and y
  int m, n;
  _SPARSE_VARIANT(_SparseSubFactorGetDimn)(Subfactor, &m, &n);
  SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(m, n, Y, X, /* no result */, "subfactor dimension");
  _SPARSE_VARIANT(_SparseMultiplySubfactor)(&Subfactor, &X, &Y, (char*)workspace);
}

/**** Vector multiply functions ***********************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) XY) {
  _SPARSE_VARIANT(DenseMatrix) XYMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(XY);
  SparseMultiply(Subfactor, XYMatrix);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor,
                    _SPARSE_VARIANT(DenseVector) X, _SPARSE_VARIANT(DenseVector) Y) {
  _SPARSE_VARIANT(DenseMatrix) XMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(X);
  _SPARSE_VARIANT(DenseMatrix) YMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(Y);
  SparseMultiply(Subfactor, XMatrix, YMatrix);
}

/**** Vector multiply functions with user-supplied workspace ******************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor, _SPARSE_VARIANT(DenseVector) XY, void *_Nonnull workspace) {
  _SPARSE_VARIANT(DenseMatrix) XYMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(XY);
  SparseMultiply(Subfactor, XYMatrix, workspace);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseMultiply(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor,
                     _SPARSE_VARIANT(DenseVector) X, _SPARSE_VARIANT(DenseVector) Y,
                     void *_Nonnull workspace) {
  _SPARSE_VARIANT(DenseMatrix) XMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(X);
  _SPARSE_VARIANT(DenseMatrix) YMatrix = _SPARSE_VARIANT(_DenseMatrixFromVector)(Y);
  SparseMultiply(Subfactor, XMatrix, YMatrix, workspace);
}

/******************************************************************************
 *  Preconditioners
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaquePreconditioner) SparseCreatePreconditioner(
    SparsePreconditioner_t type, _SPARSE_VARIANT(SparseMatrix) A) {
  SparseIterativeMethod nullMethod = {};
  struct _SparseIterativeMethodBaseOptions options = nullMethod.options.base;
  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(Am>0 && An>0,
                         (_SPARSE_VARIANT(SparseOpaquePreconditioner)) { .type = SparsePreconditionerNone },
                         "Bad matrix dimensions %dx%d\n", Am, An);

  return _SPARSE_VARIANT(_SparseCreatePreconditioner) (type, &A);
}

/******************************************************************************
 *  Iterative Sparse Solve Functions
 ******************************************************************************/

/**** Solve without preconditioner ********************************************/

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(Am>0 && An>0, SparseIterativeParameterError,
                         "Bad matrix dimensions %dx%d\n", Am, An);

  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Xn == Bn, SparseIterativeParameterError,
                         "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
  SPARSE_PARAMETER_CHECK(Xm==An, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and X (%dx%d) do not match.", Am, An, Xm, Xn);
  SPARSE_PARAMETER_CHECK(Bm==Am, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and B (%dx%d) do not match.", Am, An, Bm, Bn);

  switch(method.method) {
    case _SparseMethodCG:
      // For CG A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For CG, trans is only ever false (and requires A==A^T anyway)
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);
        }, NULL);
    case _SparseMethodGMRES:
      // For GMRES A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For GMRES, trans is only ever false
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);        }, NULL);
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          if(accumulate) {
            if(trans==CblasNoTrans) SparseMultiplyAdd(A, X_, Y);
            else                    SparseMultiplyAdd(SparseGetTranspose(A), X_, Y);
          } else {
            if(trans==CblasNoTrans) SparseMultiply(A, X_, Y);
            else                    SparseMultiply(SparseGetTranspose(A), X_, Y);
          }
        }, NULL);
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x) {
  return SparseSolve(method, A, _SPARSE_VARIANT(_DenseMatrixFromVector)(b),
                     _SPARSE_VARIANT(_DenseMatrixFromVector)(x));
}


static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Xm > 0 && Xn > 0, SparseIterativeParameterError,
                         "Bad dimensions for X (%dx%d)\n", Xm, Xn);
  SPARSE_PARAMETER_CHECK(Bm > 0 && Bn > 0, SparseIterativeParameterError,
                         "Bad dimensions for B (%dx%d)\n", Bm, Bn);
  SPARSE_PARAMETER_CHECK(Xn == Bn, SparseIterativeParameterError,
                         "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);

  switch(method.method) {
    case _SparseMethodCG:
      // For CG A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B, ApplyOperator, NULL);
    case _SparseMethodGMRES:
      // For GMRES A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B, ApplyOperator, NULL);
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B, ApplyOperator, NULL);
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans,_SPARSE_VARIANT(DenseVector), _SPARSE_VARIANT(DenseVector)),
    _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = x.count;
  int Bm = b.count;
  SPARSE_PARAMETER_CHECK(Xm > 0, SparseIterativeParameterError,
                         "Bad dimension for x (%dx%d)\n", Xm, 1);
  SPARSE_PARAMETER_CHECK(Bm > 0, SparseIterativeParameterError,
                         "Bad dimensions for b (%dx%d)\n", Bm, 1);

  _SPARSE_VARIANT(DenseMatrix) X = _SPARSE_VARIANT(_DenseMatrixFromVector)(x);
  _SPARSE_VARIANT(DenseMatrix) B = _SPARSE_VARIANT(_DenseMatrixFromVector)(b);
  // Note: we rely on the fact that neither X nor Y is transpose in calls to ApplyOperator()
  // and Precondition()
  switch(method.method) {
    case _SparseMethodCG:
      // Must be symmetric for CG
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
         ApplyOperator(accumulate, trans,
           (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
           (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
           );
        }, NULL );
    case _SparseMethodGMRES:
      // Must be square for GMRES
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          ApplyOperator(accumulate, trans,
            (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
            (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
            );
        }, NULL );
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          ApplyOperator(accumulate, trans,
            (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
            (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
            );
        }, NULL );
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

/**** Solve with preconditioner ***********************************************/

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X, SparsePreconditioner_t Preconditioner) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(Am>0 && An>0, SparseIterativeParameterError,
                         "Bad matrix dimensions %dx%d\n", Am, An);

  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Xn == Bn, SparseIterativeParameterError,
                         "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
  SPARSE_PARAMETER_CHECK(Xm==An, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and X (%dx%d) do not match.", Am, An, Xm, Xn);
  SPARSE_PARAMETER_CHECK(Bm==Am, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and B (%dx%d) do not match.", Am, An, Bm, Bn);

  SPARSE_PARAMETER_CHECK(Preconditioner != SparsePreconditionerNone &&
                         Preconditioner != SparsePreconditionerUser, SparseIterativeParameterError,
                         "Invalid preconditioner type for this call: for no preconditioner, omit "
                         "the parameter. User-supplied preconditioners must supply apply() method.");

  // Create preconditioner
  _SPARSE_VARIANT(SparseOpaquePreconditioner) P =
    _SPARSE_VARIANT(_SparseCreatePreconditioner) (Preconditioner, &A);
  if(P.type == SparsePreconditionerNone) return SparseIterativeInternalError;

  // Call CG
  SparseIterativeStatus_t result;
  switch(method.method) {
    case _SparseMethodCG:
      result = _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For CG, trans is only ever false
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);
        }, &P);
      break;
    case _SparseMethodGMRES:
      result = _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For GMRES, trans is only ever false
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);
        }, &P);
      break;
    case _SparseMethodLSMR:
      result = _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B,
      ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
        if(accumulate) {
          if(trans==CblasNoTrans) SparseMultiplyAdd(A, X_, Y);
          else                    SparseMultiplyAdd(SparseGetTranspose(A), X_, Y);
        } else {
          if(trans==CblasNoTrans) SparseMultiply(A, X_, Y);
          else                    SparseMultiply(SparseGetTranspose(A), X_, Y);
        }
      }, &P);
      break;
    default:
      __builtin_unreachable(); // All options considered above.
  }

  // Release preconditioner
  _SPARSE_VARIANT(_SparseReleaseOpaquePreconditioner)(&P);

  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x,
    SparsePreconditioner_t Preconditioner) {
  return SparseSolve(method, A, _SPARSE_VARIANT(_DenseMatrixFromVector)(b),
                     _SPARSE_VARIANT(_DenseMatrixFromVector)(x));
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X,
    _SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;

  int Am = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.columnCount : A.structure.rowCount);
  int An = A.structure.blockSize * ((A.structure.attributes.transpose) ? A.structure.rowCount : A.structure.columnCount);
  SPARSE_PARAMETER_CHECK(Am>0 && An>0, SparseIterativeParameterError,
                         "Bad matrix dimensions %dx%d\n", Am, An);
  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Xn == Bn, SparseIterativeParameterError,
                         "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
  SPARSE_PARAMETER_CHECK(Xm==An, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and X (%dx%d) do not match.", Am, An, Xm, Xn);
  SPARSE_PARAMETER_CHECK(Bm==Am, SparseIterativeParameterError,
                         "Dimensions of A (%dx%d) and B (%dx%d) do not match.", Am, An, Bm, Bn);

  switch(method.method) {
    case _SparseMethodCG:
      // For CG A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For CG, trans is only ever false (and requires A==A^T anyway)
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);
        }, &Preconditioner);
    case _SparseMethodGMRES:
      // For GMRES A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          // For GMRES, trans is only ever false
          if(accumulate) SparseMultiplyAdd(A, X_, Y);
          else           SparseMultiply(A, X_, Y);
        }, &Preconditioner);
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          if(accumulate) {
            if(trans==CblasNoTrans) SparseMultiplyAdd(A, X_, Y);
            else                    SparseMultiplyAdd(SparseGetTranspose(A), X_, Y);
          } else {
            if(trans==CblasNoTrans) SparseMultiply(A, X_, Y);
            else                    SparseMultiply(SparseGetTranspose(A), X_, Y);
          }
        }, &Preconditioner);
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method, _SPARSE_VARIANT(SparseMatrix) A,
    _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x,
    _SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {
  return SparseSolve(method, A, _SPARSE_VARIANT(_DenseMatrixFromVector)(b),
                     _SPARSE_VARIANT(_DenseMatrixFromVector)(x), Preconditioner);
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)) ,
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) X,
    _SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Xm > 0 && Xn > 0, SparseIterativeParameterError,
                         "Bad dimensions for X (%dx%d)\n", Xm, Xn);
  SPARSE_PARAMETER_CHECK(Bm > 0 && Bn > 0, SparseIterativeParameterError,
                         "Bad dimensions for B (%dx%d)\n", Bm, Bn);
  SPARSE_PARAMETER_CHECK(Xn == Bn, SparseIterativeParameterError,
                         "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);

  switch(method.method) {
    case _SparseMethodCG:
      // For CG A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B, ApplyOperator, &Preconditioner);
    case _SparseMethodGMRES:
      // For GMRES A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Dimensions of X (%dx%d) and B (%dx%d) do not match.", Xm, Xn, Bm, Bn);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B, ApplyOperator, &Preconditioner);
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B, ApplyOperator, &Preconditioner);
    default:
      __builtin_unreachable(); // All options considered above.
  }
}


static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeStatus_t SparseSolve(SparseIterativeMethod method,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseVector), _SPARSE_VARIANT(DenseVector)),
    _SPARSE_VARIANT(DenseVector) b, _SPARSE_VARIANT(DenseVector) x,
    _SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {

  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = x.count;
  int Bm = b.count;

  SPARSE_PARAMETER_CHECK(Xm > 0, SparseIterativeParameterError,
                         "Bad dimension for x (%dx%d)\n", Xm, 1);
  SPARSE_PARAMETER_CHECK(Bm > 0, SparseIterativeParameterError,
                         "Bad dimensions for b (%dx%d)\n", Bm, 1);

  _SPARSE_VARIANT(DenseMatrix) X = _SPARSE_VARIANT(_DenseMatrixFromVector)(x);
  _SPARSE_VARIANT(DenseMatrix) B = _SPARSE_VARIANT(_DenseMatrixFromVector)(b);

  // Note: we rely on the fact that neither X nor Y is transpose in calls to ApplyOperator()
  // and Precondition() (true as input matrices are not-transpose)
  switch(method.method) {
    case _SparseMethodCG:
      // For CG A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseCGSolve)(&method.options.cg, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          ApplyOperator(accumulate, trans,
            (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
            (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
            );
        }, &Preconditioner );
    case _SparseMethodGMRES:
      // For GMRES A must be square, so X and B must have matching dimensions
      SPARSE_PARAMETER_CHECK(Xm == Bm, SparseIterativeParameterError,
                             "Counts of X (%d) and B (%d) do not match.", Xm, Bm);
      return _SPARSE_VARIANT(_SparseGMRESSolve)(&method.options.gmres, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          ApplyOperator(accumulate, trans,
            (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
            (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
            );
          }, &Preconditioner );
    case _SparseMethodLSMR:
      return _SPARSE_VARIANT(_SparseLSMRSolve)(&method.options.lsmr, &X, &B,
        ^void(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix) X_, _SPARSE_VARIANT(DenseMatrix) Y) {
          ApplyOperator(accumulate, trans,
            (_SPARSE_VARIANT(DenseVector)) { .count=X_.rowCount, .data=X_.data },
            (_SPARSE_VARIANT(DenseVector)) { .count=Y.rowCount, .data=Y.data }
            );
        }, &Preconditioner );
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

/******************************************************************************
 *  Sparse Single-step Iteration  Functions
 ******************************************************************************/

static inline SPARSE_PUBLIC_INTERFACE
size_t _SPARSE_VARIANT(SparseGetStateSize)(SparseIterativeMethod method, bool preconditioner, int m, int n, int nrhs) {
  return _SPARSE_VARIANT(_SparseGetIterativeStateSize)(&method, preconditioner, m, n, nrhs);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseIterate(SparseIterativeMethod method, int iteration, const bool *_Nonnull converged, void *_Nonnull state,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) R, _SPARSE_VARIANT(DenseMatrix) X,
    _SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {
  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  SPARSE_PARAMETER_CHECK(Xm>0 && Xn>0, /* no result */, "Invalid size for X (%dx%d).", Xm, Xn);
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Bm>0 && Bn>0, /* no result */, "Invalid size for B (%dx%d).", Bm, Bn);
  int Rm = (R.attributes.transpose) ? R.columnCount : R.rowCount;
  int Rn = (R.attributes.transpose) ? R.rowCount : R.columnCount;
  SPARSE_PARAMETER_CHECK(Rm>0 && Rn>0, /* no result */, "Invalid size for R (%dx%d).", Rm, Rn);
  SPARSE_PARAMETER_CHECK(Xn==Bn && Bn==Rn, /* no result */,
                         "Sizes of X (%dx%d), B (%dx%d) and R (%dx%d) are inconsistent.", Xm, Xn, Bm, Bn, Rm, Bn);
  SPARSE_PARAMETER_CHECK(Rm>=Bm, /* no result */,
                         "Sizes of residual matrix R(%dx%d) must be at least as large as right-hand side B (%dx%d).",
                         Rm, Rn, Bm, Rn);

  switch(method.method) {
    case _SparseMethodCG:
      _SPARSE_VARIANT(_SparseCGIterate) (&method.options.cg, iteration, (char*)state, converged,
                                         &X, &B, &R, &Preconditioner, ApplyOperator);
      break;
    case _SparseMethodGMRES:
      _SPARSE_VARIANT(_SparseGMRESIterate) (&method.options.gmres, iteration, (char*)state, converged,
                                            &X, &B, &R, &Preconditioner, ApplyOperator);
      break;
    case _SparseMethodLSMR:
      _SPARSE_VARIANT(_SparseLSMRIterate) (&method.options.lsmr, iteration, (char*)state, converged,
                                           &X, &B, &R, &Preconditioner, ApplyOperator);
      break;
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseIterate(SparseIterativeMethod method, int iteration, const bool *_Nonnull converged, void *_Nonnull state,
    void (^_Nonnull ApplyOperator)(bool accumulate, enum CBLAS_TRANSPOSE trans, _SPARSE_VARIANT(DenseMatrix), _SPARSE_VARIANT(DenseMatrix)),
    _SPARSE_VARIANT(DenseMatrix) B, _SPARSE_VARIANT(DenseMatrix) R, _SPARSE_VARIANT(DenseMatrix) X) {
  struct _SparseIterativeMethodBaseOptions options = method.options.base;
  int Xm = (X.attributes.transpose) ? X.columnCount : X.rowCount;
  int Xn = (X.attributes.transpose) ? X.rowCount : X.columnCount;
  SPARSE_PARAMETER_CHECK(Xm>0 && Xn>0, /* no result */, "Invalid size for X (%dx%d).", Xm, Xn);
  int Bm = (B.attributes.transpose) ? B.columnCount : B.rowCount;
  int Bn = (B.attributes.transpose) ? B.rowCount : B.columnCount;
  SPARSE_PARAMETER_CHECK(Bm>0 && Bn>0, /* no result */, "Invalid size for B (%dx%d).", Bm, Bn);
  int Rm = (R.attributes.transpose) ? R.columnCount : R.rowCount;
  int Rn = (R.attributes.transpose) ? R.rowCount : R.columnCount;
  SPARSE_PARAMETER_CHECK(Rm>0 && Rn>0, /* no result */, "Invalid size for R (%dx%d).", Rm, Rn);
  SPARSE_PARAMETER_CHECK(Xn==Bn && Bn==Rn, /* no result */,
                         "Sizes of X (%dx%d), B (%dx%d) and R (%dx%d) are inconsistent.", Xm, Xn, Bm, Bn, Rm, Bn);
  SPARSE_PARAMETER_CHECK(Rm>=Bm, /* no result */,
                         "Sizes of residual matrix R(%dx%d) must be at least as large as right-hand side B (%dx%d).",
                         Rm, Rn, Bm, Rn);

  switch(method.method) {
    case _SparseMethodCG:
      _SPARSE_VARIANT(_SparseCGIterate) (&method.options.cg, iteration, (char*)state, converged,
                                         &X, &B, &R, NULL, ApplyOperator);
      break;
    case _SparseMethodGMRES:
      _SPARSE_VARIANT(_SparseGMRESIterate) (&method.options.gmres, iteration, (char*)state, converged,
                                            &X, &B, &R, NULL, ApplyOperator);
      break;
    case _SparseMethodLSMR:
      _SPARSE_VARIANT(_SparseLSMRIterate) (&method.options.lsmr, iteration, (char*)state, converged,
                                           &X, &B, &R, NULL, ApplyOperator);
      break;
    default:
      __builtin_unreachable(); // All options considered above.
  }
}

/******************************************************************************
 *  Memory Management
 ******************************************************************************/

/**** Retaining resources *****************************************************/

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueFactorization) SparseRetain(_SPARSE_VARIANT(SparseOpaqueFactorization) NumericFactor) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(NumericFactor.symbolicFactorization,
                                     _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError),
                                     "Can only retain valid numeric factorizations.\n")
  options = _SparseGetOptionsFromSymbolicFactor(&NumericFactor.symbolicFactorization);
  SPARSE_PARAMETER_CHECK(NumericFactor.status == SparseStatusOK && NumericFactor.numericFactorization,
                         _SPARSE_VARIANT(_SparseFailedFactor)(SparseParameterError),
                         "Can only retain valid numeric factorizations.\n");
  _SPARSE_VARIANT(_SparseRetainNumeric)(&NumericFactor);
  return NumericFactor;
}

static inline SPARSE_PUBLIC_INTERFACE
_SPARSE_VARIANT(SparseOpaqueSubfactor) SparseRetain(_SPARSE_VARIANT(SparseOpaqueSubfactor) Subfactor) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // We need default error reporting until we verify valid factor
  SPARSE_PARAMETER_CHECK(Subfactor.factor.symbolicFactorization.status == SparseStatusOK &&
                         Subfactor.factor.symbolicFactorization.factorization,
                         _SPARSE_VARIANT(_SparseInvalidSubfactor)(), "Can only retain valid objects.\n");
  options = _SparseGetOptionsFromSymbolicFactor(&Subfactor.factor.symbolicFactorization);
  SPARSE_PARAMETER_CHECK(Subfactor.factor.status == SparseStatusOK && Subfactor.factor.numericFactorization,
                         _SPARSE_VARIANT(_SparseInvalidSubfactor)(), "Can only retain valid objects.\n");
  _SPARSE_VARIANT(_SparseRetainNumeric)(&Subfactor.factor);
  return Subfactor;
}

/**** Cleaning up resources ***************************************************/

static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(_SPARSE_VARIANT(SparseOpaqueFactorization) toFree) {
  _SPARSE_VARIANT(_SparseDestroyOpaqueNumeric)(&toFree);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(_SPARSE_VARIANT(SparseOpaqueSubfactor) toFree) {
  _SPARSE_VARIANT(_SparseDestroyOpaqueNumeric)(&toFree.factor);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(_SPARSE_VARIANT(SparseMatrix) toFree) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions; // Default error reporting.
  SPARSE_PARAMETER_CHECK(toFree.structure.attributes._allocatedBySparse == true, ,
                         "Attempting to call SparseCleanup on a matrix that was not allocated by the Sparse library.\n");
  free(toFree.structure.columnStarts);
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(_SPARSE_VARIANT(SparseOpaquePreconditioner) Preconditioner) {
  SparseIterativeMethod nullMethod = {};
  struct _SparseIterativeMethodBaseOptions options = nullMethod.options.base;
  SPARSE_PARAMETER_CHECK(Preconditioner.type!=SparsePreconditionerNone && Preconditioner.type!=SparsePreconditionerUser,
                         /* no result */, "Cannot cleanup this kind of Preconditioner.");

  _SPARSE_VARIANT(_SparseReleaseOpaquePreconditioner) (&Preconditioner);
}

/*********************************** END **************************************/
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/Types.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __SPARSE_TYPES_H
#define __SPARSE_TYPES_H

#pragma mark - Type defines -

/*!
 @header Sparse/Types.h
 @discussion
 Types supporting Sparse BLAS routines.
 
 @copyright Copyright (c) 2014 Apple Inc. All rights reserved.
*/

#include <stdint.h>
#include <stdbool.h>

/*!
 @abstract Sparse matrix type for float.  Opaque structure
 */
typedef struct sparse_m_float* sparse_matrix_float;
/*!
 @abstract Sparse matrix type for double.  Opaque structure
 */
typedef struct sparse_m_double* sparse_matrix_double;

/*!
 @abstract The dimension type. All dimensions are positive values.
 */
typedef uint64_t sparse_dimension;

/*!
 @abstract The stride type.  Supports negative strides for the dense vectors.
 */
typedef int64_t sparse_stride;

/*!
 @abstract The index type.  All indices including those in a sparse vectors 
 index array are positive values.
 */
typedef int64_t sparse_index;

/*!
 @abstract The type reflecting the status of an operations.
 
 @constant SPARSE_SUCCESS
 Operation was a success
 
 @constant SPARSE_ILLEGAL_PARAMETER
 Operation was not completed because one or more of the arguments had an illegal
 value.
 
 @constant SPARSE_CANNOT_SET_PROPERTY
 Matrix properties can only be set before any values are inserted into the
 matrix.  This error occurs if that order is not repsected.
 
 @constant SPARSE_SYSTEM_ERROR
 An internal error has occured, such as non enough memory.
 */
typedef enum {
    SPARSE_SUCCESS = 0,
    SPARSE_ILLEGAL_PARAMETER = -1000,
    SPARSE_CANNOT_SET_PROPERTY = -1001,
    SPARSE_SYSTEM_ERROR = -1002,
} sparse_status;

/*!
 @abstract The matrix property type
 */
typedef enum {
    SPARSE_UPPER_TRIANGULAR = 1,
    SPARSE_LOWER_TRIANGULAR = 2,
    
    SPARSE_UPPER_SYMMETRIC = 4,
    SPARSE_LOWER_SYMMETRIC = 8,
} sparse_matrix_property;


/*!
 @abstract The norm specifier
 @constant SPARSE_NORM_ONE
 Matrix element wise: sum over i,j ( | A[i,j] | )
 
 Matrix operator    : max over j ( sum over i ( | A[i,j] | )
 
 Vector element wise: sum over i ( | x[i] | )
 
 @constant SPARSE_NORM_TWO
 Matrix element wise: sqrt( sum over i,j (A[i,j])^2 )
 
 Matrix operator    : Largest singular value of matrix, note that the operator
 SPARSE_NORM_TWO is significantly more expensive than other norm operations.
 
 Vector element wise: sqrt( sum over i (x[i])^2 )
 
 @constant SPARSE_NORM_INF
 Matrix element wise: max over i,j ( | A[i,j] | )
 
 Matrix operator    : max over i ( sum over j ( | A[i,j] | )
 
 Vector element wise: max over i ( | x[i] | )
 
 @constant SPARSE_NORM_R1
 Matrix element wise: sum over j ( sqrt ( sum over i ( A[i,j]^2 ) ) )
 
 Matrix operator    : Not supported. Undefined
 
 Vector element wise: Not supported. Undefined
 */
typedef enum {
    SPARSE_NORM_ONE = 171,
    SPARSE_NORM_TWO = 173,
    SPARSE_NORM_INF = 175,
    SPARSE_NORM_R1 =  179
} sparse_norm;

#endif
   /* __SPARSE_TYPES_H */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/BLAS.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __SPARSE_BLAS_H
#define __SPARSE_BLAS_H

/*!
 @header Sparse/BLAS.h
 @discussion

 These sparse BLAS routines provide access to sparse computation while hiding
 the details of the sparse matrix storage formats.
 
 A brief note on the naming convention used.  When an operation has two 
 operands, if both operands are sparse then _sparse_ will be in the name, but if
 one operand is sparse and the other dense, then _dense_ will be in the name.  
 For example, sparse_inner_product_sparse_float is a sparse-sparse operation and 
 sparse_inner_product_dense_float is a sparse-dense operation.  Any operation that 
 has a single matrix or vector is going to be operating on sparse structures and 
 no _dense_ or _sparse_ is added to the name.
 
 This sparse BLAS package utilizes a compressed form vector where two storage 
 arrays of the same size are used, the first to store the nonzero values of the 
 vector, the other to store the index of the nonzero values.  For example, for
 the dense vector:
 
 d = [3, 0, 0, 7, 9, 0, 4, 0, 0]
 
 the sparse vector will be stored as two arrays:
 
 s =     [3, 7, 9, 4]
 
 sindx = [0, 3, 4, 6]
 
 These two arrays hold their values as consecutive elements and are referred to
 throughout the documentation as the dense storage of the sparse vector.
 
 Indices are always assumed to be stored in ascending order. Additionally, 
 indices are assumed to be unique.  Undefined behavior if either of these 
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 Due to the variety of sparse matrix representations, the details of the storage
 format are hidden from the developer.  There are however two general flavors;
 point wise and block wise storage.
 
 With a point wise format, individual elements are stored within the matrix 
 representation. Point wise formats allow for adding single (val,i,j) values,
 collections of (val,i,j) values, or sparse vectors as a row or column.
 
 With block wise storage formats, blocks of elements are stored within the 
 matrix. Block formats allow for adding blocks of values at specified block 
 indices. For example if block sizes are constant, adding the K x L block to the 
 matrix at block location bi,bj will add values to 
 matrix[bi*K:bi*K+K-1,bj*L:bj*L-1]. With block storage formats, block sizes can 
 either be uniform throughout the matrix, or variable based on row and column 
 location.
 
 Beyond selecting a point wise or block wise format, the matrix storage format 
 is hidden from the developer.
 
 Matrix objects must be memory managed.  They are created with a create routine,
 used to completion, and then must be destroyed to release any associated
 memory.  The typical work flow is as follows:
 
 <pre>
 // Create matrix object and check for error
 sparse_matrix_float A = sparse_matrix_create_float(40,40);
 if ( !A ) { // handle error }
 
 // Insert values from existing arrays values, iIndx, and jIndx
 // Then check for error
 if ( sparse_insert_entry_float( A, 12, values, iIndx, jIndx ) ) { // handle error }
 
 // Perform computation
 float nrm = sparse_matrix_norm_float( A, SPARSE_NORM_INF );
 
 // Clean up
 sparse_matrix_destroy( A );
 </pre>
 
 Since data insertion into the internal storage is very expensive, caused by data
 movement and possible memory allocation, data insertion functions do not update
 the internal storage immediately. Rather, all data insertions are put into a pending
 queue. The internal storage is then updated automatically when required by the BLAS
 operations, or explicitly triggered by calling sparse_commit().
 
 When commit is triggered automatically by the BLAS operation, expect the operation
 to take longer time. If this is undesirable, consider calling sparse_commit() in a
 less time-sensitive code segment.

 @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 */

#ifdef __cplusplus
extern "C" {
#endif
    
#ifndef CBLAS_H
#include <vecLib/cblas.h>
#endif

#ifndef __SPARSE_TYPES_H
#include <vecLib/Sparse/Types.h>
#endif

#include <os/availability.h>

#pragma mark - Level 1 Routines -

  /* Level 1 Computational Routines */
/*!
 @functiongroup Level 1
 @abstract Level 1 routines consisting of vector-vector operations
 */

/*!
 @abstract
 Compute the inner product of sparse vector x with dense vector y.
 
 @param nz
 The number of nonzero entries in the sparse vector x.
 
 @param x
 Pointer to the dense storage for the values of the sparse vector x.  The
 corresponding entry in indx holds the index of the value.  Contains nz values.
 
 @param indx
 Pointer to the dense storage for the index values of the sparse vector x.  The
 corresponding entry in x holds the values of the vector.  Contains nz values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param y
 Pointer to the dense vector y.  Accessed as y[indx[0..nz-1]*incy], so dimension 
 must be compatible with largest index value in indx.  Behavior undefined if 
 this is not met.  Negative strides are supported.  Note, unlike dense BLAS 
 routines, the pointer points to the last element when stride is negative.
 
 @param incy
 Increment between valid values in the dense vector y.  Negative strides are
 supported.
 
 @result
 On exit the result of the inner product is returned.
 
 @discussion
 Compute the inner product of sparse vector x with dense vector y.  Returns zero 
 if nz is less than or equal to zero.
 
 */
float sparse_inner_product_dense_float( sparse_dimension nz,
                                     const float * __restrict x,
                                     const sparse_index * __restrict indx,
                                     const float * __restrict y,
                                     sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_inner_product_dense_double( sparse_dimension nz,
                                       const double * __restrict x,
                                       const sparse_index * __restrict indx,
                                       const double * __restrict y,
                                       sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Compute the inner product of sparse vector x with sparse vector y.
 
 @param nzx
 The number of nonzero entries in the sparse vector x.
 
 @param nzy
 The number of nonzero entries in the sparse vector y.
 
 @param x
 Pointer to the dense storage for the values of the sparse vector x.  The
 corresponding entry in indx holds the index of the value.  Contains nzx values.
 
 @param indx
 Pointer to the dense storage for the index values of the sparse vector x.  The
 corresponding entry in x holds the values of the vector.  Contains nzx values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param y
 Pointer to the dense storage for the values of the sparse vector y.  The
 corresponding entry in indy holds the index of the value.  Contains nzy values.
 
 @param indy
 Pointer to the dense storage for the index values of the sparse vector y.  The
 corresponding entry in y holds the values of the vector.  Contains nzy values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @result
 On exit the result of the inner product is returned.
 
 @discussion
 Compute the inner product of sparse vector x with sparse vector y.  Returns
 zero if nzx or nzy is less than or equal to zero.
 
 */
float sparse_inner_product_sparse_float( sparse_dimension nzx, sparse_dimension nzy,
                                      const float * __restrict x,
                                      const sparse_index * __restrict indx,
                                      const float * __restrict y,
                                      const sparse_index * __restrict indy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_inner_product_sparse_double( sparse_dimension nzx, sparse_dimension nzy,
                                        const double * __restrict x,
                                        const sparse_index * __restrict indx,
                                        const double * __restrict y,
                                        const sparse_index * __restrict indy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Scales the sparse vector x by alpha and adds the result to the dense vector y.
 
 y = alpha * x + y
 
 @param nz
 The number of nonzero entries in the sparse vector x.
 
 @param alpha
 Scalar multiplier of x.
 
 @param x
 Pointer to the dense storage for the values of the sparse vector x.  The
 corresponding entry in indx holds the index of the value.  Contains nz values.
 
 @param indx
 Pointer to the dense storage for the index values of the sparse vector x.  The
 corresponding entry in x holds the values of the vector.  Contains nz values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param y
 Pointer to the dense vector y.  Accessed as y[indx[0..nz-1]*incy], so dimension
 must be compatible with largest index value in indx.  Behavior undefined if
 this is not met.  Negative strides are supported.  Note, unlike dense BLAS
 routines, the pointer points to the last element when stride is negative.
 
 @param incy
 Increment between valid values in the dense vector y.  Negative strides are
 supported.
 
 @result
 On exit y has been updated as y = alpha * x + y. If nz is less than or
 equal to zero or alpha is equal to zero, y is unchanged.
 
 @discussion
 Scales the sparse vector x by alpha and adds the result to the dense vector y.
 If alpha or nz is zero, y is unchanged.
 
 If the desired operation is y = alpha * x, then an efficient option is to 
 create the y buffer of zeros as y = calloc(sizeof(..)*ySize) and then perform
 the operation with the zero filled y.
 */
void sparse_vector_add_with_scale_dense_float( sparse_dimension nz, float alpha,
                                            const float * __restrict x,
                                            const sparse_index * __restrict indx,
                                            float * __restrict y,
                                            sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

void sparse_vector_add_with_scale_dense_double( sparse_dimension nz, double alpha,
                                             const double * __restrict x,
                                             const sparse_index * __restrict indx,
                                             double * __restrict y,
                                             sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Compute the specified norm of the sparse vector x.
 
 @param nz
 The number of nonzero values in the sparse vector x.
 
 @param x
 Pointer to the dense storage for the values of the sparse vector x.  The
 corresponding entry in indx holds the index of the value.  Contains nz values.
 
 @param indx
 Pointer to the dense storage for the index values of the sparse vector x.  The
 corresponding entry in x holds the values of the vector.  Contains nz values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param norm
 Specify the norm to be computed.  Must be one of SPARSE_NORM_ONE, SPARSE_NORM_TWO,
 or SPARSE_NORM_INF.  See discussion for further details.
 
 @result
 Upon success, return the requested norm.
 
 @discussion
 Compute the specified norm of the sparse vector x.  Specify one of:
 1) SPARSE_NORM_ONE : sum over i ( | x[i] | )
 2) SPARSE_NORM_TWO : sqrt( sum over i (x[i])^2 )
 3) SPARSE_NORM_INF : max over i ( | x[i] | )
 4) SPARSE_NORM_R1  : Not supported, undefined.
 
 If norm is not one of the enumerated norm types, the default value is
 SPARSE_NORM_INF.

*/
float sparse_vector_norm_float( sparse_dimension nz, const float * __restrict x,
                               const sparse_index * __restrict indx, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_vector_norm_double( sparse_dimension nz, const double * __restrict x,
                                 const sparse_index * __restrict indx, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));



#pragma mark - Level 2 Routines -

               /* Level 2 Computational Routines */
/*!
 @functiongroup Level 2
 @abstract Level 2 routines consisting of matrix-vector operations
 */

/*!
 @abstract
 Multiplies the dense vector x by the sparse matrix A and adds the result to
 the dense vector y.
 
 y = alpha * op(A) * x + y; where op(A) is either A or the transpose of A
 
 @param transa 
 Specifies whether to perform the operation with A or the transpose of A.
 Must be one of CblasNoTrans or CblasTrans.
 
 @param alpha
 Scalar multiplier of A.
 
 @param A
 The sparse matrix.
 
 @param x
 Pointer to the dense vector x. The dimension must be the number of columns of
 the matrix A when transa is no transpose or the number of rows of the matrix A
 when transa is transpose.  Behavior undefined if this is not met.  Negative 
 strides are supported.  Note, unlike dense BLAS routines, the pointer points to 
 the last element when stride is negative. 
 
 @param incx
 Increment between valid values in the dense vector x. Negative strides are
 supported.
 
 @param y
 Pointer to the dense vector y. The dimension must be the number of rows of
 the matrix A when transa is no transpose or the number of columns of the matrix 
 A when transa is transpose.  Behavior undefined if this is not met.  Negative
 strides are supported.  Note, unlike dense BLAS routines, the pointer points to
 the last element when stride is negative.
 
 @param incy
 Increment between valid values in the dense vector y.  Negative strides are
 supported.

 @result
 On success return SPARSE_SUCCESS and y has been updated with result of the 
 operation.  Will return SPARSE_ILLEGAL_PARAMETER if transa is invalid and y will 
 be unchanged.

 @discussion
 Multiplies the dense vector x by the sparse matrix A and adds the result to
 the dense vector y (y = alpha * op(A) * x + y, where op(A) is either A
 or the transpose of A).
 
 If the desired operation is y = A * x, then an efficient option is to create
 the y buffer of zeros as y = calloc(sizeof(..)*ySize) and then perform
 the operation with the zero filled y.
 */
sparse_status sparse_matrix_vector_product_dense_float( enum CBLAS_TRANSPOSE transa,
                                                  float alpha,
                                                  sparse_matrix_float A,
                                                  const float * __restrict x,
                                                  sparse_stride incx,
                                                  float * __restrict y,
                                                  sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_matrix_vector_product_dense_double( enum CBLAS_TRANSPOSE transa,
                                                   double alpha,
                                                   sparse_matrix_double A,
                                                   const double * __restrict x,
                                                   sparse_stride incx,
                                                   double * __restrict y,
                                                   sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Solve the system of equations x = alpha * T^{-1} * x for x where x is a dense
 vector and T is a triangular sparse matrix.
 
 @param transt
 Specifies whether to perform the operation with T or the transpose of T.
 Must be one of CblasNoTrans or CblasTrans.
 
 @param alpha
 Scalar multiplier of T.
 
 @param T
 The sparse triangular matrix.  Must be upper or lower triangular matrix.
 Will return SPARSE_ILLEGAL_PARAMETER if not a triangular matrix.
 
 @param x
 Pointer to the dense vector x. The dimension must match the dimension of the
 triangular matrix T. Behavior undefined if this is not met.  Negative
 strides are supported.  Note, unlike dense BLAS routines, the pointer points to
 the last element when stride is negative.  On exit holds the solution to the
 system of equations.
 
 @param incx
 Increment between valid values in the dense vector x. Negative strides are
 supported.
 
 @result
 On success, SPARSE_SUCCESS is returned and x has been updated with result of the 
 operation.  Will return SPARSE_ILLEGAL_PARAMETER if transt is invalid or if the
 matrix T is not triangular and x will be unchanged.
 
 @discussion
 Solve the system of equations x = alpha * T^{-1} * x for x where x is a dense
 vector and T is a triangular sparse matrix.  The matrix T must be an upper or
 lower triangular matrix.
 
 */
sparse_status sparse_vector_triangular_solve_dense_float( enum CBLAS_TRANSPOSE transt,
                                              float alpha, sparse_matrix_float T,
                                              float * __restrict x,
                                              sparse_stride incx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_vector_triangular_solve_dense_double( enum CBLAS_TRANSPOSE transt,
                                               double alpha,
                                               sparse_matrix_double T,
                                               double * __restrict x,
                                               sparse_stride incx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Compute the outer product of the dense vector x and the sparse vector y and
 return a new sparse matrix in the uninitialized pointer sparse matrix pointer 
 C.  C = alpha * x * y'
 
 @param M
 The number of rows of x and the resulting matrix.
 
 @param N
 The number of columns of the resulting matrix.  The number of nonzero values 
 must be less than or equal to N.
 
 @param nz
 The number of nonzero values in the sparse vector y.  Must be less than or
 equal to N.
 
 @param alpha
 Scalar multiplier of x.
 
 @param x
 Pointer to the dense vector x.  Must be M number of elements.  Negative strides 
 are supported.  Note, unlike dense BLAS routines, the pointer points to the 
 last element when stride is negative.
 
 @param incx
 Increment between valid values in the dense vector x.  Negative strides are
 supported.
 
 @param y
 Pointer to the dense storage for the values of the sparse vector y.  The
 corresponding entry in indy holds the index of the value.  Contains nz values.
 
 @param indy
 Pointer to the dense storage for the index values of the sparse vector y.  The
 corresponding entry in y holds the values of the vector.  Contains nz values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param C
 Pointer to an uninitialized sparse matrix object.  On success a newly allocated
 sparse matrix object is returned in this pointer.  On error, this set to NULL.
 Caller is responsible for calling sparse_matrix_destroy on this matrix object.
 
 @result
 On success SPARSE_SUCCESS is returned an C is valid matrix object.  The caller is 
 responsible for cleaning up the sparse matrix object with sparse_matrix_destroy.
 Will return SPARSE_ILLEGAL_PARAMETER if nz > N, and C will be unchanged.
 
 @discussion
 Compute the outer product of the dense vector x and the sparse vector y and
 return a new sparse matrix in the uninitialized pointer sparse matrix pointer 
 C.  C = alpha * x * y'.  Caller responsible for calling sparse_matrix_destroy on 
 the returned matrix.
 
 The matrix object returned on success is a point wise based sparse matrix.
 
 */
sparse_status sparse_outer_product_dense_float( sparse_dimension M, sparse_dimension N,
                                    sparse_dimension nz, float alpha,
                                    const float * __restrict x, sparse_stride incx,
                                    const float * __restrict y,
                                    const sparse_index * __restrict indy,
                                    sparse_matrix_float * __restrict C)
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_outer_product_dense_double( sparse_dimension M, sparse_dimension N,
                                     sparse_dimension nz, double alpha,
                                     const double * __restrict x,
                                     sparse_stride incx, const double * __restrict y,
                                     const sparse_index * __restrict indy,
                                     sparse_matrix_double * __restrict C)
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Permute the rows of the sparse matrix A based on the provided permutation
 array.
 
 @param A
 The sparse matrix.
 
 @param perm
 The permutation array.  Holds number of rows in A values indicating the
 permutation of the matrix.  The indices in perm are expected to be 0 based 
 (first element of pointer is ptr[0]).  The indices in perm are expected to
 be within bounds of the matrix.  Undefined behavior if not met.
 
 @result
 On successful return, A has been permuted and SPARSE_SUCCESS is returned.
 
 @discussion
 Permute the rows of the sparse matrix A based on the provided permutation
 array.  For each row in A, swap rows as:
 
 tmp[:] = A[i,:];
 A[i,:] = A[perm[i],:];
 A[perm[i],:] = tmp[:];
 
 */
sparse_status sparse_permute_rows_float( sparse_matrix_float A,
                             const sparse_index * __restrict perm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_permute_rows_double( sparse_matrix_double A,
                              const sparse_index * __restrict perm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Permute the columns of the sparse matrix A based on the provided permutation
 array.
 
 @param A
 The sparse matrix.
 
 @param perm
 The permutation array.  Holds number of columns in A values indicating the
 permutation of the matrix.  The indices in perm are expected to be 0 based
 (first element of pointer is ptr[0]).  The indices in perm are expected to
 be within bounds of the matrix.  Undefined behavior if not met.
 
 @result
 On successful return, A has been permuted and SPARSE_SUCCESS is returned.
 
 @discussion
 Permute the columns of the sparse matrix A based on the provided permutation
 array.  For each column in A, swap columns as:
 
 tmp[:] = A[:,j];
 A[:,j] = A[:,perm[j]];
 A[:,perm[j]] = tmp[:];
 
 */
sparse_status sparse_permute_cols_float( sparse_matrix_float A,
                             const sparse_index * __restrict perm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_permute_cols_double( sparse_matrix_double A,
                              const sparse_index * __restrict perm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Compute the specified elementwise norm of the sparse matrix A.  This is the 
 norm of the matrix treated as a vector, not the operator norm.
 
 @param A
 The sparse matrix.
 
 @param norm
 Specify the norm to be computed.  Must be one of SPARSE_NORM_ONE, SPARSE_NORM_TWO,
 SPARSE_NORM_INF, or SPARSE_NORM_R1.  See discussion for further details.
 
 @result
 Upon success, resulting norm is returned.
 
 @discussion
 Compute the specified norm of the sparse matrix A.  This is the norm of the
 matrix treated as a vector, not the operator norm.  Specify one of:
 1) SPARSE_NORM_ONE : sum over i,j ( | A[i,j] | )
 2) SPARSE_NORM_TWO : sqrt( sum over i,j (A[i,j])^2 )
 3) SPARSE_NORM_INF : max over i,j ( | A[i,j] | )
 4) SPARSE_NORM_R1  : sum over j ( sqrt ( sum over i ( A[i,j]^2 ) ) )
 
 If norm is not one of the enumerated norm types, the default value is
 SPARSE_NORM_INF.
 
 */
float sparse_elementwise_norm_float( sparse_matrix_float A, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_elementwise_norm_double( sparse_matrix_double A, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Compute the specified operator norm of the sparse matrix A.  For elementwise
 norm use elementwise_norm routines.
 
 @param A
 The sparse matrix.
 
 @param norm
 Specify the norm to be computed.  Must be one of SPARSE_NORM_ONE, SPARSE_NORM_TWO,
 or SPARSE_NORM_INF.  See discussion for further details.
 
 @result
 Upon success, resulting norm is returned.
 
 @discussion
 Compute the specified norm of the sparse matrix A.  This is the norm of the
 matrix treated as an linear operator, not the elementwise norm.  Specify one of:
 1) SPARSE_NORM_ONE : max over j ( sum over i ( | A[i,j] | ) )
 2) SPARSE_NORM_TWO : Maximum singular value. This is significantly more
                      expensive to compute than the other norms.
 3) SPARSE_NORM_INF : max over i ( sum over j ( | A[i,j] | ) )
 4) SPARSE_NORM_R1  : Not supported, undefined.
 
 If norm is not one of the enumerated norm types, the default value is
 SPARSE_NORM_INF.
 
 */
float sparse_operator_norm_float( sparse_matrix_float A, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_operator_norm_double( sparse_matrix_double A, sparse_norm norm )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));
    
/*!
 @abstract
 Compute the sum along the specified diagonal of the sparse matrix A.
 
 @param A
 The sparse matrix.
 
 @param offset
 Specifies the diagonal to sum.  A zero value will sum the main diagonal
 (A[i,i]), a value greater than zero will sum diagonals above the main diagonal
 (A[i,i+offset]), and a values less than zero will sum diagonals below the main
 diagonal (A[i-offset,i]).  If offset is out of the bounds of the matrix A, 0 
 is returned.
 
 @result
 On success, the resulting trace is returned.
 
 @discussion
 Compute the sum along the specified diagonal of the sparse matrix A.  The
 diagonal is specified by the parameter offset where zero is the main diagonal,
 values greater than one refer to diagonals above the main diagonal
 (A[i,i+offset]), and values less than one refer to diagonals below the main
 diagonal (A[i-offset, i]).

 If offset is out of the bounds of the matrix A, 0 is returned.
 
 */
float sparse_matrix_trace_float( sparse_matrix_float A, sparse_index offset )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

double sparse_matrix_trace_double( sparse_matrix_double A, sparse_index offset )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));





#pragma mark - Level 3 Routines -

               /* Level 3 Computational Routines */
/*!
 @functiongroup Level 3
 @abstract Level 3 routines consisting of matrix-matrix operations
 */
/*!
 @abstract
 Multiplies the dense matrix B by the sparse matrix A and adds the result to
 the dense matrix C.

 C = alpha * op(A) * B + C; where op(A) is either A or the transpose of A

 @param order
 Specified the storage order for the dense matrices B and C. Must be one of
 CblasRowMajor or CblasColMajor.

 @param transa
 Specifies whether to perform the operation with A or the transpose of A.
 Must be one of CblasNoTrans or CblasTrans.

 @param n
 The number of columns of the matrices B and C.

 @param alpha
 Scalar multiplier of A.

 @param A
 The sparse matrix.

 @param B
 Pointer to the dense matrix B. The number of rows must be equal to the number
 of columns of A and the number of columns is n.  Behavior undefined if this
 is not met. The argument ldb describes how many elements to move between one
 row (row major) or column (column major).

 @param ldb
 Increment in elements between rows (row major) or columns (column major) of B.
 Must be greater than or equal to n when row major, or number of columns of
 A when column major.

 @param C
 Pointer to the dense matrix C. The number of rows must be equal to the number
 of rows of A and the number of columns is n.  Behavior undefined if this
 is not met. The argument ldc describes how many elements to move between one
 row (row major) or column (column major).  C is updated with the result of the
 operation.

 @param ldc
 Increment in elements between rows (row major) or columns (column major) of C.
 Must be greater than or equal to n when row major, or number of rows of
 A when column major.

 @result
 On success, SPARSE_SUCCESS is returned and C has been updated with result of the
 operation.  Will return SPARSE_ILLEGAL_PARAMETER if order or transa is not valid
 or the leading dimension parameters do not meet their dimension requirements.
 On error, C is unchanged.

 @discussion
 Multiplies the dense matrix B by the sparse matrix A and adds the result to
 the dense matrix C (C = alpha * op(A) * B + C, where op(A) is either A
 or the transpose of A). If A is of size M x N, then B is of size N x n and C is
 of size M x n.

 If the desired operation is C = A * B, then an efficient option is to create
 the C buffer of zeros as C = calloc(sizeof(..)*rows*cols) and then perform
 the operation with the zero filled C.
 */
sparse_status sparse_matrix_product_dense_float( enum CBLAS_ORDER order,
                                           enum CBLAS_TRANSPOSE transa,
                                           sparse_dimension n, float alpha,
                                           sparse_matrix_float A,
                                           const float * __restrict B,
                                           sparse_dimension ldb,
                                           float * __restrict C,
                                           sparse_dimension ldc )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_matrix_product_dense_double( enum CBLAS_ORDER order,
                                            enum CBLAS_TRANSPOSE transa,
                                            sparse_dimension n, double alpha,
                                            sparse_matrix_double A,
                                            const double * __restrict B,
                                            sparse_dimension ldb,
                                            double * __restrict C,
                                            sparse_dimension ldc )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Multiplies the sparse matrix B by the sparse matrix A and adds the result to
 the dense matrix C.

 C = alpha * op(A) * B + C; where op(A) is either A or the transpose of A

 @param order
 Specified the storage order for the dense matrix C. Must be one of
 CblasRowMajor or CblasColMajor.

 @param transa
 Specifies whether to perform the operation with A or the transpose of A.
 Must be one of CblasNoTrans or CblasTrans.

 @param alpha
 Scalar multiplier of A.

 @param A
 The sparse matrix A.

 @param B
 The sparse matrix B.

 @param C
 Pointer to the dense matrix C. The number of rows must be equal to the number
 of rows of A and the number of columns must be equal to the number
 of columns of B.  Behavior undefined if this is not met.
 The argument ldc describes how many elements to move between one row (row major)
 or column (column major).  C is updated with the result of the operation.

 @param ldc
 Increment in elements between rows (row major) or columns (column major) of C.
 Must be greater than or equal to the number of columns of B when row major,
 or number of rows of A when column major.

 @result
 On success, SPARSE_SUCCESS is returned and C has been updated with result of the
 operation.  Will return SPARSE_ILLEGAL_PARAMETER if order or transa is not valid
 or the leading dimension parameters do not meet their dimension requirements.
 On error, C is unchanged.

 @discussion
 Multiplies the sparse matrix B by the sparse matrix A and adds the result to
 the dense matrix C (C = alpha * op(A) * B + C, where op(A) is either A
 or the transpose of A). If A is of size M x K, then B is of size K x N and C is
 of size M x N.

 If the desired operation is C = A * B, then an efficient option is to create
 the C buffer of zeros as C = calloc(sizeof(..)*rows*cols) and then perform
 the operation with the zero filled C.
 */
sparse_status sparse_matrix_product_sparse_float(enum CBLAS_ORDER order,
                                                 enum CBLAS_TRANSPOSE transa,
                                                 float alpha,
                                                 sparse_matrix_float A,
                                                 sparse_matrix_float B,
                                                 float * __restrict C,
                                                 sparse_dimension ldc )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_matrix_product_sparse_double(enum CBLAS_ORDER order,
                                                  enum CBLAS_TRANSPOSE transa,
                                                  double alpha,
                                                  sparse_matrix_double A,
                                                  sparse_matrix_double B,
                                                  double * __restrict C,
                                                  sparse_dimension ldc )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Solve the system of equations B = alpha * T^{-1} * B for B where B is a dense
 matrix and T is a triangular sparse matrix.
 
 @param order
 Specified the storage order for the dense matrix B. Must be one of
 CblasRowMajor or CblasColMajor.
 
 @param transt
 Specifies whether to perform the operation with T or the transpose of T.
 Must be one of CblasNoTrans or CblasTrans.
 
 @param nrhs
 The number of columns of the matrix B.
 
 @param alpha
 Scalar multiplier of T.
 
 @param T
 The sparse triangular matrix.  Must be upper or lower triangular matrix.
 Will return SPARSE_ILLEGAL_PARAMETER if not a triangular matrix.
 
 @param B
 Pointer to the dense matrix B. The number of rows must be equal to the number
 of columns of T and the number of columns is nrhs.  Behavior undefined if this
 is not met. The argument ldb describes how many elements to move between one
 row (row major) or column (column major). On exit holds the solution to the
 system of equations.
 
 @param ldb
 Increment in elements between rows (row major) or columns (column major) of B.
 Must be greater than or equal to nrhs when row major, or number of columns of
 A when column major.
 
 @result
 On success, SPARSE_SUCCESS is returned and B has been updated with result of the
 operation.  Will return SPARSE_ILLEGAL_PARAMETER if either of order or trant are
 invalid or the ldb does not meet its dimension requirements.  On error
 B is unchanged.
 
 @discussion
 Solve the system of equations B = alpha * T^{-1} * B for B where B is a dense
 vector and T is a triangular sparse matrix.  If T is of size N x N, then B must
 be of size N x nrhs.  The matrix T must be an upper or lower triangular matrix.
 
 */
sparse_status sparse_matrix_triangular_solve_dense_float( enum CBLAS_ORDER order,
                                   enum CBLAS_TRANSPOSE transt,
                                   sparse_dimension nrhs, float alpha,
                                   sparse_matrix_float T,
                                   float * __restrict B, sparse_dimension ldb )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_matrix_triangular_solve_dense_double( enum CBLAS_ORDER order,
                                    enum CBLAS_TRANSPOSE transt,
                                    sparse_dimension nrhs, double alpha,
                                    sparse_matrix_double T,
                                    double * __restrict B, sparse_dimension ldb )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));




#pragma mark - Point Wise Sparse Matrix Routines -

               /* Point Wise Matrix Routines */
/*!
 @functiongroup Point Wise Matrix Routines
 @abstract Routines to create and insert values in a point wise sparse matrix.
 */

/*!
 @abstract
 Create a sparse matrix object that is stored in point wise format and is ready
 to receive values from the various insert routines.
 
 @param M
 The number of rows of the matrix.  Must be greater than 0.
 
 @param N
 The number of columns of the matrix.  Must be greater than 0.
 
 @result
 On success, returns a matrix object that is ready for receiving entries.  If an
 error occurs, NULL is returned.
 
 @discussion
 Create a sparse matrix object that is stored in point wise format and is ready
 to receive values from the various insert routines.  Point wise format means
 individual values are stored for a given i,j location as opposed to blocks of
 values.  For block support use the block_create routines.  See the various
 insert routines for details on inserting entries into this matrix object.
 
 The dimensions M and N must be greater than 0.  On success a valid matrix
 object is returned, otherwise NULL is returned.
 
 */
sparse_matrix_float sparse_matrix_create_float( sparse_dimension M, sparse_dimension N )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_matrix_double sparse_matrix_create_double( sparse_dimension M, sparse_dimension N )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Use to build a sparse matrix by inserting one scalar entry at a time.  Update
 A[i,j] = val.  A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.

 @param A
 The sparse matrix.  A must have been created with one of sparse_matrix_create_float
 or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if not met.
 
 @param val
 The scalar value to insert into the sparse matrix.
 
 @param i
 The row location to insert the value.  Indices are 0 based (first element of 
 pointer is ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param j
 The column location to insert the value.  Indices are 0 based (first element of
 pointer is ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @result
 On successful insertion, A has been updated with the value and SPARSE_SUCCESS is
 returned.  If A creation requirements are not met, SPARSE_ILLEGAL_PARAMETER is
 returned and A is unchanged.
 
 @discussion
 Use to build a sparse matrix by inserting one scalar entry at a time.  Update
 A[i,j] = val.
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 Note that matrix properties cannot be modified after value insertion begins.
 This includes properties such as specifying a triangular matrix.
 
 Insertion can be expensive, generally speaking it is best to do a batch update.
 Inserted values may be temporarily held internally within the object and only
 inserted into the sparse format when a later computation triggers a need to
 insert.
 
 */
sparse_status sparse_insert_entry_float( sparse_matrix_float A, float val,
                                   sparse_index i, sparse_index j )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_insert_entry_double( sparse_matrix_double A, double val,
                                    sparse_index i, sparse_index j )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Use to build a sparse matrix by providing a list of point entries.  For each
 entry provided, update A[indx[i],jndx[i]] = val[i].  A must have been created
 with one of sparse_matrix_create_float or sparse_matrix_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of sparse_matrix_create_float
 or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if not met.
 
 @param N
 The number of values to insert into A.  Each of indx, jndx and val are of size
 N.
 
 @param val
 Pointer to list of scalar values to insert into the sparse matrix.  The value
 is inserted into the location specified by the corresponding indices in indx
 and jndx.  Must hold N values.
 
 @param indx
 An array of row indices that correspond to the values in val. Must hold N
 values.
 
 Indices are assumed to be unique.  Additionally, indices are assumed to be in the
 bounds of the matrix.  Undefined behavior if any of these assumptions are not 
 met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param jndx
 An array of column indices that correspond to the values in val. Must hold N
 values.
 
 Indices are assumed to be unique.  Additionally, indices are assumed to be in the
 bounds of the matrix.  Undefined behavior if any of these assumptions are not
 met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @result
 On successful insertion, A has been updated with the values and SPARSE_SUCCESS is
 returned.  If A creation requirements are not met, SPARSE_ILLEGAL_PARAMETER is
 returned and A is unchanged.
 
 @discussion
 Use to build a sparse matrix by providing a list of point entries.  For each
 entry provided, update A[indx[i],jndx[i]] = val[i].
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 Note that matrix properties cannot be modified after value insertion begins.
 This includes properties such as specifying a triangular matrix.
 
 Insertion can be expensive, generally speaking it is best to do a batch update.
 Inserted values may be temporarily held internally within the object and only
 inserted into the sparse format when a later computation triggers a need to
 insert.
 
 */
sparse_status sparse_insert_entries_float( sparse_matrix_float A, sparse_dimension N,
                                     const float * __restrict val,
                                     const sparse_index * __restrict indx,
                                     const sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_insert_entries_double( sparse_matrix_double A, sparse_dimension N,
                                      const double * __restrict val,
                                      const sparse_index * __restrict indx,
                                      const sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Use to build a sparse matrix by providing a list of point entries for a single
 column.  For each entry provided, update A[indx[i],j] = val[i].  A must have
 been created with one of sparse_matrix_create_float or sparse_matrix_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of sparse_matrix_create_float
 or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if not met.
 
 @param j
 The column for value insertion.  Indices are 0 based (first element of pointer
 is ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param nz
 The number of values to insert into A.  Each of indx and val are of size
 nz.
 
 @param val
 Pointer to list of scalar values to insert into the sparse matrix.  The value
 is inserted into the location specified by the corresponding indices of indx
 and j.  Must hold nz values.
 
 @param indx
 An array of column indices that correspond to the values in val. Must hold nz
 values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Finally, indices are assumed to be in the
 bounds of the matrix.  Undefined behavior if any of these assumptions are not
 met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @result
 On successful insertion, A has been updated with the value and SPARSE_SUCCESS is
 returned.  If A creation requirements are not met, SPARSE_ILLEGAL_PARAMETER is
 returned and A is unchanged.
 
 @discussion
 Use to build a sparse matrix by providing a list of point entries for a single
 column.  For each entry provided, update A[indx[i],j] = val[i].  This will not
 replace the existing contents of the column, it appends new values and
 overwrites overlapping values.
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 Note that matrix properties cannot be modified after value insertion begins.
 This includes properties such as specifying a triangular matrix.
 
 Insertion can be expensive, generally speaking it is best to do a batch update.
 Inserted values may be temporarily held internally within the object and only
 inserted into the sparse format when a later computation triggers a need to
 insert.
 
 */
sparse_status sparse_insert_col_float( sparse_matrix_float A, sparse_index j,
                                 sparse_dimension nz, const float * __restrict val,
                                 const sparse_index * __restrict indx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_insert_col_double( sparse_matrix_double A, sparse_index j,
                                  sparse_dimension nz, const double * __restrict val,
                                  const sparse_index * __restrict indx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Use to build a sparse matrix by providing a list of point entries for a single
 row.  For each entry provided, update A[i,jndx[i]] = val[i].  A must have been
 created with one of sparse_matrix_create_float or sparse_matrix_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of sparse_matrix_create_float
 or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if not met.
 
 @param i
 The row for value insertion.  Indices are 0 based (first element of pointer is
 ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param nz
 The number of values to insert into A.  Each of jndx and val are of size
 nz.
 
 @param val
 Pointer to list of scalar values to insert into the sparse matrix.  The value
 is inserted into the location specified by the corresponding indices of i and
 jndx.  Must hold nz values.
 
 @param jndx
 An array of column indices that correspond to the values in val. Must hold nz
 values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Finally, indices are assumed to be in the
 bounds of the matrix.  Undefined behavior if any of these assumptions are not
 met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @result
 On successful insertion, A has been updated with the value and SPARSE_SUCCESS is
 returned.  If A creation requirements are not met, SPARSE_ILLEGAL_PARAMETER is
 returned and A is unchanged.
 
 @discussion
 Use to build a sparse matrix by providing a list of point entries for a single
 row.  For each entry provided, update A[i,jndx[i]] = val[i].  This will not
 replace the existing contents of the row, it appends new values and
 overwrites overlapping values.
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 Note that matrix properties cannot be modified after value insertion begins.
 This includes properties such as specifying a triangular matrix.
 
 Insertion can be expensive, generally speaking it is best to do a batch update.
 Inserted values may be temporarily held internally within the object and only
 inserted into the sparse format when a later computation triggers a need to
 insert.
 
 */
sparse_status sparse_insert_row_float( sparse_matrix_float A, sparse_index i,
                                 sparse_dimension nz, const float * __restrict val,
                                 const sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_insert_row_double( sparse_matrix_double A, sparse_index i,
                                  sparse_dimension nz, const double * __restrict val,
                                  const sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Extract the first nz values of the row begining at A[row,column_start] for the
 sparse matrix A.  A must have been created with one of sparse_matrix_create_float 
 or sparse_matrix_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of 
 sparse_matrix_create_float or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is 
 returned if not met.
 
 @param row
 The row for value extraction.  Indices are 0 based (first element of pointer is
 ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param column_start
 The index of the column to start extraction.  Indices are 0 based (first 
 element of pointer is ptr[0]).  Indices expected to be in the bounds of matrix 
 dimensions, undefined behavior if not met.
 
 @param column_end
 On return, holds the column index of the next nonzero value.If there is no
 next nonzero value (because all of them have been copied into the sparse
 vector), it holds the number of columns in the matrix.
 Indices are 0 based (first element of pointer is ptr[0]).
 Indices expected to be in the bounds of matrix dimensions, undefined behavior 
 if not met.
 
 @param nz
 The number of values to extract from A.  Each of jndx and val are of size
 nz.
 
 @param val
 Pointer to array to hold the values extracted from the sparse matrix.  The
 value is extracted from the location specified by the corresponding indices of 
 row and jndx.  Must be of size nz elements.  If less than nz nonzero values are
 found, then the last nz - actual_nonzero_count elements of val are untouched.
 
 @param jndx
 An array to hold the extracted column indices that correspond to the values in 
 val. Note that these indices are relative to the matrix row and not the
 starting column index specified by column_start.  Returned indices are 0 based 
 (first element of pointer is ptr[0]).  Must be of size nz elements.
 
 @result
 On success val and jndx have been updated with the nonzero values of the row'th
 row, column_end holds the column index of the next nonzero value, and
 the number of nonzero values written are returned.  If A creation requirements 
 are not met, SPARSE_ILLEGAL_PARAMETER is returned and val and jndx are unchanged.
 
 @discussion
 Extract the first nz values of the row begining at A[row,column_start] for the
 sparse matrix A.  The number of nonzero values extracted is limited by nz, and
 the number of nonzero's written to jndx and val are returned.  Additionally, 
 the column index of the next nonzero value is returned in column_end.
 For example if nz is returned, not all nonzero values have been extracted,
 and a second extract can start from column_end.
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 */
sparse_status sparse_extract_sparse_row_float( sparse_matrix_float A, sparse_index row,
                                  sparse_index column_start, sparse_index *column_end,
                                  sparse_dimension nz, float * __restrict val,
                                  sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_extract_sparse_row_double( sparse_matrix_double A, sparse_index row,
                                  sparse_index column_start, sparse_index *column_end,
                                  sparse_dimension nz, double * __restrict val,
                                  sparse_index * __restrict jndx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Extract the first nz values of the column begining at A[row_start,column] for 
 the sparse matrix A.  A must have been created with one of 
 sparse_matrix_create_float or sparse_matrix_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of 
 sparse_matrix_create_float or sparse_matrix_create_double.  SPARSE_ILLEGAL_PARAMETER is 
 returned if not met.
 
 @param column
 The column for value extraction.  Indices are 0 based (first element of pointer 
 is ptr[0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param row_start
 The index of the row to start extraction.  Indices are 0 based (first
 element of pointer is ptr[0]).  Indices expected to be in the bounds of matrix 
 dimensions, undefined behavior if not met.
 
 @param row_end
 On return, holds the row index of the next nonzero value.If there is no
 next nonzero value (because all of them have been copied into the sparse
 vector), it holds the number of rows in the matrix.
 Indices are 0 based (first element of pointer is ptr[0]).
 Indices expected to be in the bounds of matrix dimensions, undefined behavior 
 if not met.
 
 @param nz
 The number of values to extract from A.  Each of indx and val are of size
 nz.
 
 @param val
 Pointer to array to hold the values extracted from the sparse matrix.  The
 value is extracted from the location specified by the corresponding indices of 
 column and indx.  Must be of size nz elements.  If less than nz nonzero values 
 are found, then the last nz - actual_nonzero_count elements of val are 
 untouched.
 
 @param indx
 An array to hold the extracted row indices that correspond to the values in
 val. Note that these indices are relative to the matrix column and not the
 starting row index specified by row_start.  Returned indices are 0 based
 (first element of pointer is ptr[0]).  Must be of size nz elements.
 
 @result
 On success val and indx have been updated with the nonzero values of the 
 column'th column, row_end holds the row index of the next nonzero value,
 and the number of nonzero values written are returned.  If A creation
 requirements are not met, SPARSE_ILLEGAL_PARAMETER is returned and val and indx 
 are unchanged.
 
 @discussion
 Extract the first nz values of the column begining at A[column,row_start] for 
 the sparse matrix A.  The number of nonzero values extracted is limited by nz, 
 and the number of nonzero's written to indx and val are returned.  
 Additionally, the row index of the next nonzero value is returned in
 row_end.  For example if nz is returned, not all nonzero values have been
 extracted, and a second extract can start from row_end.
 
 A must have been created with one of sparse_matrix_create_float or
 sparse_matrix_create_double.
 
 */
sparse_status sparse_extract_sparse_column_float( sparse_matrix_float A,
                                            sparse_index column,
                                            sparse_index row_start,
                                            sparse_index *row_end,
                                            sparse_dimension nz,
                                            float * __restrict val,
                                            sparse_index * __restrict indx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_extract_sparse_column_double( sparse_matrix_double A,
                                             sparse_index column,
                                             sparse_index row_start,
                                             sparse_index *row_end,
                                             sparse_dimension nz,
                                             double * __restrict val,
                                             sparse_index * __restrict indx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));



#pragma mark - Block Wise Matrix Routines -

				/* Block Wise Matrix Routines */
/*!
 @functiongroup Block Wise Matrix Routines
 @abstract Routines to create and insert values in a block wise sparse matrix.
 */

 /*!
 @abstract
 Create a sparse matrix object that is stored in block-entry format and is ready
 to receive values from the various block insert routines.  Blocks are of fixed
 dimension k x l.
 
 @param Mb
 The number of rows in terms of blocks of the matrix.  There are a total of
 Mb * k rows in the matrix.  Must be greater than 0.
 
 @param Nb
 The number of columns in terms of blocks of the matrix.  There are a total of
 Nb * l columns in the matrix.  Must be greater than 0.
 
 @param k
 The row dimension of a block in the sparse matrix.  Must be greater than 0.
 
 @param l
 The column dimension of a block in the sparse matrix.  Must be greater than 0.
 
 @result
 On success, returns a matrix object that is ready for receiving entries.  If an
 error occurs, NULL is returned.
 
 @discussion
 Create a sparse matrix object that is stored in block-entry format and is ready
 to receive values from the various block insert routines.  Blocks are are of
 fixed dimensions k x l.  Block-entry format means blocks of dense regions will 
 be stored at block indices i,j.  For point wise format use non block version of
 create. See the various insert routines for details on inserting values into
 this matrix object.
 
 The dimensions Mb, Nb, k, and l must be greater than 0.  On success a valid 
 matrix object is returned, otherwise NULL is returned.

 */
sparse_matrix_float sparse_matrix_block_create_float( sparse_dimension Mb,
                                                sparse_dimension Nb,
                                                sparse_dimension k,
                                                sparse_dimension l )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_matrix_double sparse_matrix_block_create_double( sparse_dimension Mb,
                                                  sparse_dimension Nb,
                                                  sparse_dimension k,
                                                  sparse_dimension l )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Create a sparse matrix object that is stored in block-entry format and is ready
 to receive values from the various block insert routines.  Blocks are of 
 variable dimensions where the i,j'th block has dimensions K[i] x L[j].
 
 @param Mb
 The number of rows in terms of blocks of the matrix.  Must be greater than 0.
 
 @param Nb
 The number of columns in terms of blocks of the matrix.  Must be greater 
 than 0.
 
 @param K
 Array containing row dimensions of the blocks.  The i'th row in terms of blocks
 will have a dimension K[i].  K is expected to hold Mb elements.  All values of
 K are expected to be greater than 0.
 
 @param L
 Array containing column dimensions of the blocks.  The j'th column in terms of 
 blocks will have a dimension L[j].  L is expected to hold Nb elements.  All 
 values of L are expected to be greater than 0.
 
 @result
 On success, returns a matrix object that is ready for receiving entries.  If an
 error occurs, NULL is returned.
 
 @discussion
 Create a sparse matrix object that is stored in block-entry format and is ready
 to receive values from the various block insert routines.  Blocks are are of
 variable dimension where the i,j'th block index has a dimension K[i] x L[j].  
 Block-entry format means blocks of dense regions will be stored at block 
 indices i,j.  For point wise format use non block version of create. See the 
 various insert routines for details on inserting values into this matrix
 object.
 
 The dimensions Mb, Nb and all values in K and L must be greater than 0.  On 
 success a valid matrix object is returned, otherwise NULL is returned.
 
 */
sparse_matrix_float sparse_matrix_variable_block_create_float( sparse_dimension Mb,
                                                         sparse_dimension Nb,
                                                         const sparse_dimension *K,
                                                         const sparse_dimension *L )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_matrix_double sparse_matrix_variable_block_create_double( sparse_dimension Mb,
                                                           sparse_dimension Nb,
                                                           const sparse_dimension *K,
                                                           const sparse_dimension *L )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));
/*!
 @abstract
 Use to build a sparse matrix by providing a dense block for entry at block
 location A[bi,bj].  Block size is determined at object creation time.  A must
 have been created with one of sparse_matrix_block_create_float,
 sparse_matrix_block_create_double, sparse_matrix_variable_block_create_float, or
 sparse_matrix_variable_block_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of 
 sparse_matrix_block_create_float, sparse_matrix_block_create_double, 
 sparse_matrix_variable_block_create_float, or 
 sparse_matrix_variable_block_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if 
 not met.  A holds block dimensions (fixed or variable) set with matrix object 
 creation routine.
 
 @param val
 Pointer to block to be inserted at block index location A[bi,bj].  The block is
 of dimension k x l where k and l are set for bi,bj at object creation time.  
 The strides between elements for rows and columns are provided in row_stride 
 and col_stride.
 
 @param row_stride
 The row stride in number of elements to move from one row to the next for the
 block val.
 
 @param col_stride
 The column stride in number of elements to move from one column to the next for
 the block val.
 
 @param bi
 The block row index where val is to be inserted.  Indexing is zero based, the
 first block is located at 0,0.  Index is assumed to be within the bounds of the
 matrix object, undefined behavior if not met.
 
 @param bj
 The block column index where val is to be inserted.  Indexing is zero based, the
 first block is located at 0,0.  Index is assumed to be within the bounds of the
 matrix object, undefined behavior if not met.
 
 @result
 On successful insertion, A has been updated with the value and SPARSE_SUCCESS is
 returned.  If A creation requirements are not met, SPARSE_ILLEGAL_PARAMETER is
 returned and A is unchanged.
 
 @discussion
 Use to build a sparse matrix by providing a dense block for entry at block
 location A[bi,bj].  Block size is determined at object creation time.  Given a
 block dimension of k x l and for location bi,bj, update as:
 A[bi,bj][i,j] = val[i*row_stride + j*col_stride] for each i in k and each j in
 l.  
 
 A must have been created with one of sparse_matrix_block_create_float,
 sparse_matrix_block_create_double, sparse_matrix_variable_block_create_float, or
 sparse_matrix_variable_block_create_double.
 
 Note that matrix properties cannot be modified after value insertion begins.
 This includes properties such as specifying a triangular matrix.
 
 Insertion can be expensive, generally speaking it is best to do a batch update.
 Inserted values may be temporarily held internally within the object and only
 inserted into the sparse format when a later computation triggers a need to
 insert.
 
 */
sparse_status sparse_insert_block_float( sparse_matrix_float A,
                                   const float * __restrict val,
                                   sparse_dimension row_stride,
                                   sparse_dimension col_stride,
                                   sparse_index bi, sparse_index bj )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_insert_block_double( sparse_matrix_double A,
                                    const double * __restrict val,
                                    sparse_dimension row_stride,
                                    sparse_dimension col_stride,
                                    sparse_index bi, sparse_index bj )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Extract the bi,bj'th block from the sparse matrix A.  A must have been created
 with one of sparse_matrix_block_create_float, sparse_matrix_block_create_double, 
 sparse_matrix_variable_block_create_float, or 
 sparse_matrix_variable_block_create_double.
 
 @param A
 The sparse matrix.  A must have been created with one of
 sparse_matrix_block_create_float, sparse_matrix_block_create_double,
 sparse_matrix_variable_block_create_float, or
 sparse_matrix_variable_block_create_double.  SPARSE_ILLEGAL_PARAMETER is returned if
 not met.  A holds block dimensions (fixed or variable) set with matrix object
 creation routine.
 
 @param bi
 The block row index for value extraction.  Indices are 0 based (first block of 
 matrix is A[0,0]).  Indices expected to be in the bounds of matrix dimensions,
 undefined behavior if not met.
 
 @param bj
 The block column index for value extraction.  Indices are 0 based (first block 
 of matrix is A[0,0]).  Indices expected to be in the bounds of matrix 
 dimensions, undefined behavior if not met.
 
 @param row_stride
 The row stride in number of elements to move from one row to the next for the
 block val.
 
 @param col_stride
 The column stride in number of elements to move from one column to the next for
 the block val.
 
 @param val
 Pointer to dense block to place the extracted values.  Expected to be of size
 K x L where K x L is the block size for the matrix object at block index bi,bj.
 This dimensions is set at matrix object creation time.
 
 @result
 On success SPARSE_SUCCESS is return and val has been updated with the block from
 block index bi,bj. If A creation requirements are not met, 
 SPARSE_ILLEGAL_PARAMETER is returned and val is unchanged.
 
 @discussion
 Extract the bi,bj'th block from the sparse matrix A.  
 
 A must have been created with one of sparse_matrix_block_create_float, 
 sparse_matrix_block_create_double, sparse_matrix_variable_block_create_float, or
 sparse_matrix_variable_block_create_double.
 
 */
sparse_status sparse_extract_block_float( sparse_matrix_float A, sparse_index bi,
                                    sparse_index bj, sparse_dimension row_stride,
                                    sparse_dimension col_stride,
                                    float * __restrict val )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

sparse_status sparse_extract_block_double( sparse_matrix_double A, sparse_index bi,
                                     sparse_index bj, sparse_dimension row_stride,
                                     sparse_dimension col_stride,
                                     double * __restrict val )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the dimension of the block for the i'th row of a sparse block matrix. 
 Returns 0 if the matrix was not created with a block create routine.
 */
long sparse_get_block_dimension_for_row( void *A, sparse_index i )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the dimension of the block for the j'th column of a sparse block matrix.
 Returns 0 if the matrix was not created with a block create routine.
 */
long sparse_get_block_dimension_for_col( void *A, sparse_index j )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));



#pragma mark - General Sparse Matrix Management Routines -

			/* General Sparse Matrix Management Routines */
/*!
 @functiongroup General Sparse Matrix Management Routines
 @abstract Routines to manage and work with sparse matrix properties.
 */

/*!
 @abstract
 Force any recently added values to the matrix to be put into the internal
 sparse storage format.
 
 @param A
 The sparse matrix, which has had values recently inserted into the object.
 
 @result
 On success, A has all values inserted into the internal sparse representation.
 
 @discussion
 Force any recently added values to the matrix to be put into the internal
 sparse storage format.  Values inserted into a matrix object will may not go 
 directly into the sparse representation until needed, for example when a 
 computation occurs.  In some cases is may be beneficial to the caller to know 
 when the cost of the update will occur.  This routine allows the caller to 
 trigger adding values to the internal sparse format.
 
 Adding values to the sparse format can be costly, and batch updates to the 
 matrices are recommended.  Similarly, use of this routine may be expensive, so
 it is best to insert all values of a batch and call this routine once.
 
 */
sparse_status sparse_commit( void *A )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Returns the value of the given property name.
 
 @param A
 The sparse matrix object.
 
 @param pname
 The property name to get the value of.  See matrix properties enumeration for
 options.
 
 @result
 Returns the value of the property for a valid object and property, or 0 
 otherwise.
 
 @discussion
 Returns the value of the given property name. See matrix properties enumeration
 for further property details.

 */
long sparse_get_matrix_property( void *A, sparse_matrix_property pname )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Set the given property for a matrix object that has not had any values 
 inserted.
 
 @param A
 The sparse matrix object.  Note that after elements have been inserted
 properties cannot be changed.
 
 @param pname
 The property name to set true.  See matrix properties enumeration for options.
 
 @result
 Returns SPARSE_SUCCESS when property is successfully set, otherwise return
 SPARSE_CANNOT_SET_PROPERTY.
 
 @discussion
 Set the given property for the matrix object.  The matrix object must not have
 had values inserted, else SPARSE_CANNOT_SET_PROPERTY is returned and the 
 property is not set.
 
 Certain groups of properties are mutually exclusive and setting multiple values
 within a group is undefined.
 
 */
sparse_status sparse_set_matrix_property( void *A, sparse_matrix_property pname )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Return the number of rows of the matrix.
 */
sparse_dimension sparse_get_matrix_number_of_rows( void *A )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the number of columns of the matrix.
 */
sparse_dimension sparse_get_matrix_number_of_columns( void *A )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the number of nonzero values in the matrix.
 */
long sparse_get_matrix_nonzero_count( void *A )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the number of nonzero values for the i'th row.  If index is out of
 bounds of the matrix, 0 is returned.
 */
long sparse_get_matrix_nonzero_count_for_row( void *A, sparse_index i )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Return the number of nonzero values for the j'th column.  If index is out of
 bounds of the matrix, 0 is returned.
 */
long sparse_get_matrix_nonzero_count_for_column( void *A, sparse_index j )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));



/*!
 @abstract
 Release any memory associated with the matrix object.
 
 @param A
 The sparse matrix object.
 
 @result
 All memory associated with the matrix object is released and returns 
 SPARSE_SUCCESS.
 
 @discussion
 Release any memory associated with the matrix object.  Upon return the object 
 is no longer valid and any use of the object is undefined.
 
 */
sparse_status sparse_matrix_destroy( void *A )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));




#pragma mark - Sparse Utilities -
	/* Sparse Utility Routines */
/*!
 @functiongroup Sparse Utilities
 @abstract Various utility routines for creating and working with sparse 
 structures.
 */

/*!
 @abstract
 Return the number of nonzero values in the dense vector x.
 
 @param N
 The number of elements in the dense vector x.
 
 @param x
 Pointer to the vector x.
 
 @param incx
 Increment between valid values in the dense vector x.  Negative strides are
 supported.
 
 @result
 Return the count of the nonzero values in the vector x.
 
 */
long sparse_get_vector_nonzero_count_float( sparse_dimension N,
                                         const float * __restrict x,
                                         sparse_stride incx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

long sparse_get_vector_nonzero_count_double( sparse_dimension N,
                                          const double * __restrict x,
                                          sparse_stride incx )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

/*!
 @abstract
 Pack the first nz nonzero values and indices from the dense vector x and
 place them in y and indy.
 
 @param N
 The number of elements in the dense vector x.
 
 @param nz
 The number of nonzero values to collect.  If less than nz nonzero elements are
 found in the N elements of x, then the last nz - actual_nonzero_count of y and
 indy are unused.
 
 @param x
 Pointer to the dense vector x.
 
 @param incx
 Increment between valid values in the dense vector x.  Negative strides are
 supported.
 
 @param y
 The destination dense storage for nonzero values of x.  Expected to be of size
 nz elements.  On return, any nonzero values are placed in this array, if the 
 actual number of nonzero values is less than nz, then the last 
 nz - actual_nonzero_count elements are unused.
 
 @param indy
 The destination dense storage for nonzero indices of x.  Expected to be of size 
 nz elements.  On return, any nonzero indices are placed in this array, if the 
 actual number of nonzero values is less than nz, then the last 
 nz - actual_nonzero_count elements are unused.  Returned indices are 0 based 
 (the first element of a pointer is ptr[0]).
 
 @result
 On success, y and indy are updated with up to the first nz nonzero indices.  
 The number of nonzero values written is returned.
 
 @discussion
 Pack the first nz nonzero values and indices from the dense vector x and
 place them in y and indy.  If less than nz nonzero elements are found in the N 
 elements of x, then the last nz - actual_nonzero_count elements of y and indy 
 are unused.  The number of indices written can range from 0 to nz values and 
 the number written is returned.
 
 */
long sparse_pack_vector_float( sparse_dimension N, sparse_dimension nz,
                                  const float * __restrict x,
                                  sparse_stride incx, float * __restrict y,
                                  sparse_index * __restrict indy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

long sparse_pack_vector_double( sparse_dimension N, sparse_dimension nz,
                                   const double * __restrict x,
                                   sparse_stride incx, double * __restrict y,
                                   sparse_index * __restrict indy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));


/*!
 @abstract
 Extract elements from the sparse vector x into the corresponding location in
 the dense vector y.  Optionally zero the unused values of y.
 
 @param N
 The number of elements in the dense vector y.
 
 @param nz
 The number of nonzero entries in the sparse vector x.
 
 @param zero
 When true, zero the elements of y which do not have nonzero values written to
 them.  When false ignore the elements of y which do not have nonzero values
 written to them.
 
 @param x
 Pointer to the dense storage for the values of the sparse vector x.  The
 corresponding entry in indx holds the index of the value.  Contains nz values.
 
 @param indx
 Pointer to the dense storage for the index values of the sparse vector x.  The
 corresponding entry in x holds the values of the vector.  Contains nz values.
 
 Indices are always assumed to be stored in ascending order. Additionally,
 indices are assumed to be unique.  Undefined behavior if either of these
 assumptions are not met.
 
 All indices are 0 based (the first element of a pointer is ptr[0]).
 
 @param y
 Pointer to the dense vector y.  Expected to be of size N*abs(incy) elements.  
 Negative strides are supported.  Note, unlike dense BLAS routines, the pointer 
 points to the last element when stride is negative. On exit, the entries 
 described by the indices in indx will be filled with the corresponding values 
 in x and all other values will be unchanged if parameter zero is false, or set 
 to zero if parameter zero is true.
 
 @param incy
 Increment between valid values in the dense vector y.  Negative strides are
 supported.
 
 @result
 On exit y has been updated with the nonzero values. If nz is less than or
 equal to zero y is unchanged.
 
 @discussion
 Extract elements from the sparse vector x into the corresponding location in
 the dense vector y.  Optionally zero the unused values of y.
 
 if (zero) for (i in 0 .. N-1) y[i*incy] = 0;
 
 for (i in 0 .. nz-1) if (indx[i] < N) y[indx[i]*incy] = x[i];
 */
void sparse_unpack_vector_float( sparse_dimension N, sparse_dimension nz, bool zero,
                               const float * __restrict x,
                               const sparse_index * __restrict indx,
                               float * __restrict y, sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

void sparse_unpack_vector_double( sparse_dimension N, sparse_dimension nz, bool zero,
                                const double * __restrict x,
                                const sparse_index * __restrict indx,
                                double * __restrict y, sparse_stride incy )
API_AVAILABLE(macos(10.11), ios(9.0), watchos(3.0), tvos(9.0));

#ifdef __cplusplus
}
#endif

#endif
  /* __SPARSE_BLAS_H */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/Sparse.h
/*  Copyright (c) 2014 Apple Inc.  All rights reserved.                       */

#ifndef __SPARSE_HEADER__
#define __SPARSE_HEADER__

#include <vecLib/Sparse/Types.h>
#include <vecLib/Sparse/BLAS.h>

#endif
  /* __SPARSE_HEADER__ */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/Sparse/SolveImplementation.h
#ifndef SPARSE_SOLVE_HEADER
#error "Do not include this header directly."
#endif

/*  MARK: Implementation of the option-less public interfaces; simply defer
 *  to the versions with options, using the `_SparseDefaultXXXOptions` objects
 *  defined here.                                                             */
static const SparseSymbolicFactorOptions _SparseDefaultSymbolicFactorOptions = {
  .control = SparseDefaultControl,
  .orderMethod = SparseOrderDefault,
  .order = NULL,
  .ignoreRowsAndColumns = NULL,
  .malloc = malloc,
  .free = free
};
static const SparseNumericFactorOptions _SparseDefaultNumericFactorOptions_Double = {
  .control = SparseDefaultControl,
  .scalingMethod = SparseScalingDefault,
  .scaling = NULL,
  .pivotTolerance = 0.01,                   // Recommended value for difficult matrices in double
  .zeroTolerance = 1e-4 * __DBL_EPSILON__,  // "A few" orders of magnitude below epsilon.
};
static const SparseNumericFactorOptions _SparseDefaultNumericFactorOptions_Float = {
  .control = SparseDefaultControl,
  .scalingMethod = SparseScalingDefault,
  .scaling = NULL,
  .pivotTolerance = 0.1,                    // Recommended value for difficult matrices in float
  .zeroTolerance = 1e-4 * __FLT_EPSILON__,  // "A few" orders of magnitude below epsilon.
};

static inline SPARSE_PUBLIC_INTERFACE
SparseOpaqueSymbolicFactorization SparseFactor(SparseFactorization_t type,
                                               SparseMatrixStructure Matrix) {
  return SparseFactor(type, Matrix, _SparseDefaultSymbolicFactorOptions);
}

/*  MARK: Macros and inlines used to implement the public interfaces.         */
#include <stdlib.h>
#include <stdio.h>

#ifndef __has_builtin
# define __has_builtin(_) 0
#endif

#if __has_builtin(__builtin_mul_overflow)
# define sparse_mul_overflow(a, b, res) __builtin_mul_overflow((a), (b), (res))
#else
/*  Workaround if mul_overflow is unavailable.                                */
# define sparse_mul_overflow(a, b, res) (*(res) = (long)(a)*(b), false)
#endif

#if __has_include(<os/log.h>)
# include <os/log.h>
#else
# define os_log_error(_, format, ...) fprintf(stderr, format, ##__VA_ARGS__)
#endif

#ifndef SPARSE_PARAMETER_CHECK
#define SPARSE_PARAMETER_CHECK(condition, result, format, ...)  \
  do {                                                          \
    if (!(condition)) {                                         \
      if (options.reportError) {                                \
        char message[256] = { 0 };                              \
        snprintf(message, 256, format, ##__VA_ARGS__);          \
        options.reportError(message);                           \
      } else {                                                  \
        os_log_error(OS_LOG_DEFAULT, format, ##__VA_ARGS__);    \
        _SparseTrap();                                          \
      }                                                         \
      return result;                                            \
    }                                                           \
  } while (0)
#endif /* SPARSE_PARAMETER_CHECK*/

// Do O(1) validity tests for matrix structure
#ifndef SPARSE_CHECK_VALID_MATRIX_STRUCTURE
#define SPARSE_CHECK_VALID_MATRIX_STRUCTURE(S, result) \
  SPARSE_PARAMETER_CHECK((S).rowCount > 0, result, "%s.rowCount must be > 0, but is %d.\n", #S, (S).rowCount); \
  SPARSE_PARAMETER_CHECK((S).columnCount > 0, result, "%s.columnCount must be > 0, but is %d.\n", #S, (S).rowCount); \
  SPARSE_PARAMETER_CHECK((S).blockSize > 0, result, "%s.blockSize must be > 0, but is %d.]n", #S, (S).blockSize); \
  SPARSE_PARAMETER_CHECK((S).attributes.kind != SparseSymmetric || \
                         (S).rowCount == (S).columnCount, result, \
                         "%s.attributes.kind=SparseSymmetric, but %s.rowCount (%d) != %s.columnCount (%d).\n", \
                         #S, #S, (S).rowCount, #S, (S).columnCount);
#endif /* SPARSE_CHECK_VALID_MATRIX_STRUCTURE */

// Check matrix matches symbolic factor
#ifndef SPARSE_CHECK_MATCH_SYMB_FACTOR
#define SPARSE_CHECK_MATCH_SYMB_FACTOR(A, sf, result) \
  SPARSE_PARAMETER_CHECK((A).structure.rowCount             == (sf).rowCount && \
                         (A).structure.columnCount          == (sf).columnCount && \
                         (A).structure.blockSize            == (sf).blockSize && \
                         (A).structure.attributes.transpose == (sf).attributes.transpose, \
                         result, "%s does not match that used for symbolic factorization stored in %s.\n", \
                         #A, #sf);
#endif /* SPARSE_CHECK_MATCH_SYMB_FACTOR */

// Check symbolic factorization is valid for use
#ifndef SPARSE_CHECK_VALID_SYMBOLIC_FACTOR
#define SPARSE_CHECK_VALID_SYMBOLIC_FACTOR(sf, result, format, ...) \
SPARSE_PARAMETER_CHECK((sf).status == SparseStatusOK && \
                       (sf).factorization, \
                       result, format, ##__VA_ARGS__);
#endif /* SPARSE_CHECK_VALID_SYMBOLIC_FACTOR */

// Check numeric factorization is valid for use
#ifndef SPARSE_CHECK_VALID_NUMERIC_FACTOR
#define SPARSE_CHECK_VALID_NUMERIC_FACTOR(nf, result) \
  SPARSE_PARAMETER_CHECK((nf).symbolicFactorization.status == SparseStatusOK && \
                         (nf).symbolicFactorization.factorization && \
                         (nf).status == SparseStatusOK && \
                         (nf).numericFactorization, \
                         result, \
                         "%s does not hold a completed matrix factorization.\n", #nf);
#endif /* SPARSE_CHECK_VALID_NUMERIC_FACTOR */

// Check if two vectors are consistent with operation Y = (Op) X.
// (Op) is given as an expected xSize and and expected ySize in this case.
#ifndef SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE
#define SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(expectedXSize, expectedYSize, X, Y, result, opName) \
  do { \
    SPARSE_PARAMETER_CHECK((X).columnStride >= (X).rowCount, result, \
      "%s.columnStride (%d) must be at least %s.rowCount (%d).\n", \
      #X, (X).columnStride, #X, (X).rowCount); \
    SPARSE_PARAMETER_CHECK((Y).columnStride >= (Y).rowCount, result, \
      "%s.columnStride (%d) must be at least %s.rowCount (%d).\n", \
      #Y, (Y).columnStride, #Y, (Y).rowCount); \
    int xCount = ((X).attributes.transpose) ? (X).rowCount : (X).columnCount; \
    int yCount = ((Y).attributes.transpose) ? (Y).rowCount : (Y).columnCount; \
    int xSize = ((X).attributes.transpose) ? (X).columnCount : (X).rowCount; \
    int ySize = ((Y).attributes.transpose) ? (Y).columnCount : (Y).rowCount; \
    SPARSE_PARAMETER_CHECK(xCount == yCount, result, \
      "%s (%dx%d) and %s (%dx%d) do not represent the same number of right-hand sides.\n", \
      #X, xSize, xCount, #Y, ySize, yCount); \
    SPARSE_PARAMETER_CHECK(xCount > 0, result, \
      "%s (%dx%d) must represent at least one right-hand side.\n", \
      #X, xSize, xCount); \
    SPARSE_PARAMETER_CHECK(xSize == expectedXSize, result, \
      "%s (size %dx%d) does not match dimensions of %s (%d x %d).\n", \
      #X, xSize, xCount, opName, expectedYSize, expectedXSize); \
    SPARSE_PARAMETER_CHECK(ySize == expectedYSize, result, \
      "%s (size %dx%d) does not match dimensions of %s (%d x %d).\n", \
      #Y, ySize, yCount, opName, expectedYSize, expectedXSize); \
  } while(0)
#endif /* SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE */

// Check if vector is consistent with numeric factor for in-place solve
#ifndef SPARSE_CHECK_CONSISTENT_DS_VEC_IN_PLACE
#define SPARSE_CHECK_CONSISTENT_DS_VEC_IN_PLACE(nf, vec, result) \
  do { \
    int blockSize = (nf).symbolicFactorization.blockSize; \
    int m = (nf).symbolicFactorization.rowCount * blockSize; \
    int n = (nf).symbolicFactorization.columnCount * blockSize; \
    int maxmn = (m>n) ? m : n; \
    SPARSE_PARAMETER_CHECK((vec).count == maxmn, result, \
      "%s.count (%d) is not equal to largest dimension of matrix factorization %s.\n", \
      #vec, (vec).count, #nf); \
  } while(0)
#endif /* SPARSE_CHECK_CONSISTENT_DS_VEC_IN_PLACE */

// Check if matrix is consistent with numeric factor for in-place solve
#ifndef SPARSE_CHECK_CONSISTENT_DS_MAT_IN_PLACE
#define SPARSE_CHECK_CONSISTENT_DS_MAT_IN_PLACE(nf, mat, result) \
  do { \
    SPARSE_PARAMETER_CHECK((mat).columnStride >= (mat).rowCount, result, \
                           "%s.columnStride (%d) must be at least %s.rowCount (%d).\n", \
                           #mat, (mat).columnStride, #mat, (mat).rowCount); \
    int matSize  = ((mat).attributes.transpose) ? (mat).columnCount : (mat).rowCount; \
    int matCount = ((mat).attributes.transpose) ? (mat).rowCount : (mat).columnCount; \
    SPARSE_PARAMETER_CHECK(matCount > 0, result, \
                           "%s (%dx%d) must represent at least one right-hand side.\n", \
                           #mat, matSize, matCount); \
    int blockSize = (nf).symbolicFactorization.blockSize; \
    int m = (nf).symbolicFactorization.rowCount * blockSize; \
    int n = (nf).symbolicFactorization.columnCount * blockSize; \
    int maxmn = (m>n) ? m : n; \
    SPARSE_PARAMETER_CHECK(matSize == maxmn, result, \
      "%s (%dx%d) is not consistent with largest dimension of matrix factorization %s (%d).\n", \
      #mat, matSize, matCount, #nf, maxmn); \
  } while(0)
#endif /* SPARSE_CHECK_CONSISTENT_DS_MAT_IN_PLACE */

// Check if vectors are consistent with numeric factor for out-of-place solve
#ifndef SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE
#define SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE(nf, b, x, result) \
  do { \
    bool trans = Factored.symbolicFactorization.attributes.transpose ^ Factored.attributes.transpose; \
    int blockSize = (nf).symbolicFactorization.blockSize; \
    int rc = (nf).symbolicFactorization.rowCount * blockSize; \
    int cc = (nf).symbolicFactorization.columnCount * blockSize; \
    int m = trans ? cc : rc; \
    int n = trans ? rc : cc; \
    int expectXSize = n; \
    int expectBSize = ((nf).symbolicFactorization.type == SparseFactorizationQR) ? m : n; \
    SPARSE_PARAMETER_CHECK((x).count == expectXSize, result, \
      "%s (size %dx1) does not match dimensions of matrix factorization %s (%d x %d).\n", \
      #x, (x).count, #nf, expectBSize, expectXSize); \
    SPARSE_PARAMETER_CHECK((b).count == expectBSize, result, \
      "%s (size %dx1) does not match dimensions of matrix factorization %s (%d x %d).\n", \
      #b, (b).count, #nf, expectBSize, expectXSize); \
  } while(0)
#endif /* SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE */

// Check if matrices are consistent with numeric factor and each other for out-of-place solve
#ifndef SPARSE_CHECK_CONSISTENT_DS_MAT_OUT_PLACE
#define SPARSE_CHECK_CONSISTENT_DS_MAT_OUT_PLACE(nf, B, X, result) \
  do { \
    bool trans = Factored.symbolicFactorization.attributes.transpose ^ Factored.attributes.transpose; \
    int blockSize = (nf).symbolicFactorization.blockSize; \
    int rc = (nf).symbolicFactorization.rowCount * blockSize; \
    int cc = (nf).symbolicFactorization.columnCount * blockSize; \
    int m = trans ? cc : rc; \
    int n = trans ? rc : cc; \
    int expectedXSize = n; \
    int expectedBSize = ((nf).symbolicFactorization.type == SparseFactorizationQR) ? m : n; \
    SPARSE_CHECK_CONSISTENT_MAT_OUT_PLACE(expectedBSize, expectedXSize, B, X, result, "matrix factorization "#nf); \
  } while(0)
#endif /* SPARSE_CHECK_CONSISTENT_DS_VEC_OUT_PLACE */

/* Iterative Methods */
SPARSE_ENUM(_SparseIterativeMethod, int,
  /*! @abstract Conjugate Gradient */
  _SparseMethodCG = 0,
  /*! @abstract GMRES or variant */
  _SparseMethodGMRES = 1,
  /*! @abstract LSMR (Least Squares MINRES) */
  _SparseMethodLSMR = 2,
  );

/*  MARK: External functions used to implement the public interfaces.         */
#if defined __cplusplus
extern "C" {
#endif

#if __has_feature(nullability)
#pragma clang assume_nonnull begin
#endif

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseOpaqueSymbolicFactorization _SparseSymbolicFactorSymmetric
(SparseFactorization_t factorType,
 const SparseMatrixStructure *Matrix,
 const SparseSymbolicFactorOptions *options);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseOpaqueSymbolicFactorization _SparseSymbolicFactorQR
(SparseFactorization_t factorType,
 const SparseMatrixStructure *Matrix,
 const SparseSymbolicFactorOptions *options);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SparseRetainSymbolic(SparseOpaqueSymbolicFactorization *_Nonnull symbolicFactor);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SparseDestroyOpaqueSymbolic(SparseOpaqueSymbolicFactorization *toFree);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern SparseSymbolicFactorOptions _SparseGetOptionsFromSymbolicFactor(SparseOpaqueSymbolicFactorization *factor);

API_AVAILABLE( macos(10.13), ios(11), watchos(4), tvos(11) )
extern void _SparseTrap(void);

#if __has_feature(nullability)
#pragma clang assume_nonnull end
#endif

#if defined __cplusplus
} /* extern "C" */
#endif

/*  MARK: Implementation of the expert-mode public interfaces.                */
static inline SPARSE_PUBLIC_INTERFACE
SparseOpaqueSymbolicFactorization SparseFactor(SparseFactorization_t type, SparseMatrixStructure Matrix, SparseSymbolicFactorOptions options) {
  SPARSE_PARAMETER_CHECK(Matrix.columnCount > 0,
                         (SparseOpaqueSymbolicFactorization){ .status = SparseParameterError },
                         ".structure.columnCount must be greater than 0.\n");
  switch(type) {
    case SparseFactorizationCholeskyAtA:
    case SparseFactorizationQR:
      return _SparseSymbolicFactorQR(type, &Matrix, &options);

    default:
      SPARSE_PARAMETER_CHECK(Matrix.attributes.kind == SparseSymmetric,
                             (SparseOpaqueSymbolicFactorization){ .status = SparseParameterError },
                             "Requested symmetric factorization of unsymmetric matrix.\n");
      SPARSE_PARAMETER_CHECK(Matrix.rowCount == Matrix.columnCount,
                             (SparseOpaqueSymbolicFactorization){ .status = SparseParameterError },
                             "Matrix purports to be symmetric, but rowCount != columnCount.\n");
      return _SparseSymbolicFactorSymmetric(type, &Matrix, &options);
  }
}

static inline SPARSE_PUBLIC_INTERFACE
SparseOpaqueSymbolicFactorization SparseRetain(SparseOpaqueSymbolicFactorization SymbolicFactor) {
  SparseSymbolicFactorOptions options = _SparseDefaultSymbolicFactorOptions;
  SPARSE_PARAMETER_CHECK(SymbolicFactor.status == SparseStatusOK && SymbolicFactor.factorization,
                         (SparseOpaqueSymbolicFactorization){ .status = SparseParameterError },
                         "Can only retain valid symbolic factorizations.\n");
  _SparseRetainSymbolic(&SymbolicFactor);
  return SymbolicFactor;
}

static inline SPARSE_PUBLIC_INTERFACE
void SparseCleanup(SparseOpaqueSymbolicFactorization toFree) {
  _SparseDestroyOpaqueSymbolic(&toFree);
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseConjugateGradient(void) {
  SparseIterativeMethod result = {
    .method = _SparseMethodCG,
    .options.cg = (SparseCGOptions) {}
  };
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseConjugateGradient(SparseCGOptions options) {
  SparseIterativeMethod result = {
    .method = _SparseMethodCG,
    .options.cg = options
  };
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseGMRES(void) {
  SparseIterativeMethod result = {
    .method = _SparseMethodGMRES,
    .options.gmres = (SparseGMRESOptions) {}
  };
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseGMRES(SparseGMRESOptions options) {
  SparseIterativeMethod result = {
    .method = _SparseMethodGMRES,
    .options.gmres = options
  };
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseLSMR(void) {
  SparseIterativeMethod result = {
    .method = _SparseMethodLSMR,
    .options.lsmr = (SparseLSMROptions) {}
  };
  return result;
}

static inline SPARSE_PUBLIC_INTERFACE
SparseIterativeMethod SparseLSMR(SparseLSMROptions options) {
  SparseIterativeMethod result = {
    .method = _SparseMethodLSMR,
    .options.lsmr = options
  };
  return result;
}

// Include type-specifc implementation for double
#define _SPARSE_IMPLEMENTATION_TYPE double
#define _SPARSE_OLDSTYLE(NAME) NAME ## _double
#define _SPARSE_VARIANT(NAME) NAME ## _Double
#include "SolveImplementationTyped.h"
#undef _SPARSE_IMPLEMENTATION_TYPE
#undef _SPARSE_VARIANT
#undef _SPARSE_OLDSTYLE

// Include type-specifc implementation for float
#define _SPARSE_IMPLEMENTATION_TYPE float
#define _SPARSE_OLDSTYLE(NAME) NAME ## _float
#define _SPARSE_VARIANT(NAME) NAME ## _Float
#include "SolveImplementationTyped.h"
#undef _SPARSE_IMPLEMENTATION_TYPE
#undef _SPARSE_VARIANT
#undef _SPARSE_OLDSTYLE

#undef sparse_mul_overflow
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/vForce.h
/*
vForce.h (from vecLib-728.0)
Copyright (c) 1999-2019 by Apple Inc. All rights reserved.

@APPLE_LICENSE_HEADER_START@

This file contains Original Code and/or Modifications of Original Code
as defined in and that are subject to the Apple Public Source License
Version 2.0 (the 'License'). You may not use this file except in
compliance with the License. Please obtain a copy of the License at
http://www.opensource.apple.com/apsl/ and read it before using this
file.

The Original Code and all software distributed under the License are
distributed on an 'AS IS' basis, WITHOUT WARRANTY OF ANY KIND, EITHER
EXPRESS OR IMPLIED, AND APPLE HEREBY DISCLAIMS ALL SUCH WARRANTIES,
INCLUDING WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT OR NON-INFRINGEMENT.
Please see the License for the specific language governing rights and
limitations under the License.

@APPLE_LICENSE_HEADER_END@
*/

/*! @header
 *  vForce provides fast mathematical operations on large arrays.
 *
 *  There are several differences between vForce and the similar functions
 *  available in libm. They are:
 *  1) vForce can operate on arrays of any size (libm only works on scalars,
 *     and simd.h on small fixed size vectors).
 *  2) vForce may treat some or all denormal numbers as zero.
 *  3) vForce does not guarantee to set floating point flags correctly.
 *
 *  However, unlike some fast math alternatives, vForce respects the closure of
 *  the number system, therefore infinities and NaNs are correctly processed.
 *
 *  Developers should assume that the exact value returned and treatment of
 *  denormal values will vary across different microarchitectures and versions
 *  of the operating system.
 *
 *  For very small vectors, users may wish to consider using simd.h for
 *  increased performance.
 */

#ifndef __VFORCE_H
#define __VFORCE_H

#ifdef __cplusplus
	#include <ciso646>	// Get library version.
	#if	defined _LIBCPP_VERSION
		// When using libc++, include <complex>.
		#include <complex>
	#else
		// When not using libc++, try using definition as given in C++ 98.
		namespace std
		{
			template<class T> class complex;
			template<> class complex<float>;
			template<> class complex<double>;
		}
	#endif
	typedef std::complex<float> __float_complex_t;
	typedef std::complex<double> __double_complex_t;
#else
	typedef _Complex float __float_complex_t;
	typedef _Complex double __double_complex_t;
#endif

#include <math.h>

#ifdef __cplusplus
extern "C" {
#endif

#include <os/availability.h>


#if !defined __has_feature
    #define __has_feature(f)    0
#endif
#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull begin")
#endif



/*! @abstract Calculates the reciprocal for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to 1/x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvrecf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the reciprocal for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to 1/x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvrec (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the quotient of the two source vectors.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to y[i]/x[i].
 *
 *  @param y (input)  Input vector of size *n, numerators in division.
 *
 *  @param x (input)  Input vector of size *n, denominators in division.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvdivf (float * /* z */, const float * /* y */, const float * /* x */, const int *  /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the quotient of the two source vectors.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to y[i]/x[i].
 *
 *  @param y (input)  Input vector of size *n, numerators in division.
 *
 *  @param x (input)  Input vector of size *n, denominators in division.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvdiv (double * /* z */, const double * /* y */, const double * /* x */, const int *  /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the square root for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sqrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsqrtf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the square root for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sqrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsqrt (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the cube root for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cbrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcbrtf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.10), ios(8.0));
/*! @abstract Calculates the cube root for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cbrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcbrt (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.10), ios(8.0));
    
/*! @abstract Calculates the reciprocal square root for each element of a
 *            vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to 1/sqrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvrsqrtf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the reciprocal square root for each element of a
 *            vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to 1/sqrt(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvrsqrt (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the exponential function e**x for each element of a
 *            vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to exp(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexpf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the exponential function e**x for each element of a
 *            vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to exp(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexp (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the base 2 exponential function 2**x for each element
 *            of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to exp2(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexp2f (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Calculates the base 2 exponential function 2**x for each element
 *            of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to exp2(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexp2 (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates (e**x) - 1 for each element of a vector, with high
 *            accuracy around x=0.
 *
 *  @discussion
 *  If x is nearly zero, then the common expression exp(x) - 1.0 will suffer
 *  from catastrophic cancellation and the result will have little or no
 *  precision.  This function provides an alternative means to do this
 *  calculation without the risk of significant loss of precision.
 *
 *  @seealso log1pf
 *
 *  @param y (output) Output vector of size *n. y[i] is set to expm1(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexpm1f (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Calculates (e**x) - 1 for each element of a vector, with high
 *            accuracy around x=0.
 *
 *  @discussion
 *  If x is nearly zero, then the common expression exp(x) - 1.0 will suffer
 *  from catastrophic cancellation and the result will have little or no
 *  precision.  This function provides an alternative means to do this
 *  calculation without the risk of significant loss of precision.
 *
 *  @seealso log1p
 *
 *  @param y (output) Output vector of size *n. y[i] is set to expm1(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvexpm1 (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
	
/*! @abstract Calculates the natural logarithm for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlogf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the natural logarithm for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the logarithm base 10 for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log10(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog10f (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the logarithm base 10 for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log10(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog10 (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates log(1+x) for each element of a vector, with high
 *            accuracy around x=0.
 *
 *  @discussion
 *  If x is nearly zero, the expression log(1+x) will be highly inaccurate
 *  due to floating point rounding errors in (1+x).
 *  This function provides an alternative means to calculate this value with
 *  higher accuracy.
 *
 *  @seealso expm1f
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log1p(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog1pf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Calculates log(1+x) for each element of a vector, with high
 *            accuracy around x=0.
 *
 *  @discussion
 *  If x is nearly zero, the expression log(1+x) will be highly inaccurate
 *  due to floating point rounding errors in (1+x).
 *  This function provides an alternative means to calculate this value with
 *  higher accuracy.
 *
 *  @seealso expm1
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log1p(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog1p (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates the base 2 logarithm for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log2(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog2f (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Calculates the base 2 logarithm for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log2(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlog2 (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
	
/*! @abstract Returns, as a floating-point value, the unbiased floating-point
 *            exponent for each element of a vector.
 *
 *  @discussion
 *  For a non-zero finite floating-point number f, logb is defined to be the
 *  integer that satisfies abs(f) = significand * 2**logb(f), with significand
 *  in [1,2).
 *
 *  If x is +/-0, then y is set to -inf.
 *  If x is +/-inf, then y is set to +inf.
 *  If x is NaN, then y is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log10(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlogbf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Returns, as a floating-point value, the unbiased floating-point
 *            exponent for each element of a vector.
 *
 *  @discussion
 *  For a non-zero finite floating-point number f, logb is defined to be the
 *  integer that satisfies abs(f) = significand * 2**logb(f), with significand
 *  in [1,2).
 *
 *  If x is +/-0, then y is set to -inf.
 *  If x is +/-inf, then y is set to +inf.
 *  If x is NaN, then y is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to log10(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvlogb (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
	
/*! @abstract Returns the absolute value for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to fabs(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvfabsf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Returns the absolute value for each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to fabs(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvfabs (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates, elementwise, x**y for two vectors x and y.
 *
 *  @discussion
 *  The following special values of x and y produce the given value of z:
 *         y            x         z
 *  ==============   =======   =======
 *  odd integer,<0    +/-0     +/-inf
 *  odd integer,>0    +/-0     +/-0
 *  otherwise,  <0    +/-0       +inf
 *  otherwise,  >0    +/-0       +0
 *      +/-inf          -1        1
 *       NaN            +1        1
 *      +/-0           NaN        1
 *        -inf        |x|<1      +inf
 *        -inf        |x|>1      +0
 *        +inf        |x|<1      +0
 *        +inf        |x|>1      +inf
 *  odd integer,<0     -inf      -0
 *  odd integer,>0     -inf      -inf
 *  otherwise,  <0     -inf      +0
 *  otherwise,  >0     -inf      +inf
 *        <0           +inf      +0
 *        >0           +inf      +inf
 *    non-integer       <0       NaN
 *
 *  @param z (output) Output vector of size *n. z[i] is set to pow(x[i], y[i]).
 *
 *  @param y (input)  Input vector of size *n, exponent in calculation.
 *
 *  @param x (input)  Input vector of size *n, base in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.
 */
void vvpowf (float * /* z */, const float * /* y */, const float * /* x */, const int *  /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates, elementwise, x**y for two vectors x and y.
 *
 *  @discussion
 *  The following special values of x and y produce the given value of z:
 *         y            x         z
 *  ==============   =======   =======
 *  odd integer,<0    +/-0     +/-inf
 *  odd integer,>0    +/-0     +/-0
 *  otherwise,  <0    +/-0       +inf
 *  otherwise,  >0    +/-0       +0
 *      +/-inf          -1        1
 *       NaN            +1        1
 *      +/-0           NaN        1
 *        -inf        |x|<1      +inf
 *        -inf        |x|>1      +0
 *        +inf        |x|<1      +0
 *        +inf        |x|>1      +inf
 *  odd integer,<0     -inf      -0
 *  odd integer,>0     -inf      -inf
 *  otherwise,  <0     -inf      +0
 *  otherwise,  >0     -inf      +inf
 *        <0           +inf      +0
 *        >0           +inf      +inf
 *    non-integer       <0       NaN
 *
 *  @param z (output) Output vector of size *n. z[i] is set to pow(x[i], y[i]).
 *
 *  @param y (input)  Input vector of size *n, exponent in calculation.
 *
 *  @param x (input)  Input vector of size *n, base in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.
 */
void vvpow (double * /* z */, const double * /* y */, const double * /* x */, const int *  /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates, elementwise, x**y for a vector x and a scalar y.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to pow(x[i], y).
 *
 *  @param y (input)  Input scalar, exponent in calculation.
 *
 *  @param x (input)  Input vector of size *n, base in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvpowsf (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.9), ios(6.0));
/*! @abstract Calculates, elementwise, x**y for a vector x and a scalar y.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to pow(x[i], y).
 *
 *  @param y (input)  Input scalar, exponent in calculation.
 *
 *  @param x (input)  Input vector of size *n, base in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvpows (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.9), ios(6.0));

/*! @abstract Returns the sine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sin(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsinf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the sine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sin(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsin (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the cosine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcosf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the cosine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcos (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the tangent for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tan(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtanf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the tangent for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tan(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtan (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the principal value of arc sine for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [-pi/2, +pi/2].
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If |x[i]| > 1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to asin(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvasinf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the principal value of arc sine for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [-pi/2, +pi/2].
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If |x[i]| > 1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to asin(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvasin (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the principal value of arc cosine for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [0, pi].
 *  If x[i] is 1, y[i] is set to +0.
 *  If |x[i]| > 1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to acos(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvacosf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the principal value of arc cosine for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [0, pi].
 *  If x[i] is 1, y[i] is set to +0.
 *  If |x[i]| > 1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to acos(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvacos (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the principal value of arc tangent for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [-pi/2, pi/2].
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-pi/2.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to atan(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvatanf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the principal value of arc tangent for each element of a
 *            vector.
 *
 *  @discussion
 *  The calculated values are in the range [-pi/2, pi/2].
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-pi/2.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to atan(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvatan (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates, elementwise, the principal value of the arc tangent
 *            of y/x, for two vectors x and y.
 *
 *  @discusssion
 *  The signs of both arguments are used to determine the quadrant of the
 *  calculated value.
 *
 *  The following special values of x and y produce the given value of z:
 *     y         x         z
 *  =======   =======   =======
 *   +/-0       -0       +/-pi
 *   +/-0       +0       +/-0
 *   +/-0       <0       +/-pi
 *   +/-0       >0       +/-0
 *    >0       +/-0      +pi/2
 *    <0       +/-0      -pi/2
 *   +/-y      -inf      +/-pi       y>0, finite
 *   +/-y      +inf      +/-0        y>0, finite
 *  +/-inf      x       +/-pi/2      x finite
 *  +/-inf     -inf     +/-3pi/4
 *  +/-inf     +inf     +/-pi/4
 *
 *  @param z (output) Output vector of size *n. z[i] is set to atan2(y,x).
 *
 *  @param y (input)  Input vector of size *n.
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvatan2f (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates, elementwise, the principal value of the arc tangent
 *            of y/x, for two vectors x and y.
 *
 *  @discusssion
 *  The signs of both arguments are used to determine the quadrant of the
 *  calculated value.
 *
 *  The following special values of x and y produce the given value of z:
 *     y         x         z
 *  =======   =======   =======
 *   +/-0       -0       +/-pi
 *   +/-0       +0       +/-0
 *   +/-0       <0       +/-pi
 *   +/-0       >0       +/-0
 *    >0       +/-0      +pi/2
 *    <0       +/-0      -pi/2
 *   +/-y      -inf      +/-pi       y>0, finite
 *   +/-y      +inf      +/-0        y>0, finite
 *  +/-inf      x       +/-pi/2      x finite
 *  +/-inf     -inf     +/-3pi/4
 *  +/-inf     +inf     +/-pi/4
 *
 *  @param z (output) Output vector of size *n. z[i] is set to atan2(y,x).
 *
 *  @param y (input)  Input vector of size *n.
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvatan2 (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the sine and cosine for each element of a vector.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to sin(x[i]).
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsincosf (float * /* z */, float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the sine and cosine for each element of a vector.
 *
 *  @param z (output) Output vector of size *n. z[i] is set to sin(x[i]).
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsincos (double * /* z */, double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Returns the complex number on the unit circle corresponding to
 *            the angle given by each element of a vector.
 *
 *  @discussion
 *  Sets the real part of C to the cosine of x, and the imaginary part to the
 *  sine of y.
 *
 *  The typedef __float_complex_t is defined on a per language basis, and the
 *  following types should be used:
 *  In C,   _Complex float
 *  In C++, std::complex<float>.
 *
 *  @param C (output) Output vector of size *n.
 *                    C[i] is set to cos(x[i]) + I*sin(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcosisinf (__float_complex_t * /* C */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Returns the complex number on the unit circle corresponding to
 *            the angle given by each element of a vector.
 *
 *  @discussion
 *  Sets the real part of C to the cosine of x, and the imaginary part to the
 *  sine of y.
 *
 *  The typedef __double_complex_t is defined on a per language basis, and the
 *  following types should be used:
 *  In C,   _Complex double
 *  In C++, std::complex<double>.
 *
 *  @param C (output) Output vector of size *n.
 *                    C[i] is set to cos(x[i]) + I*sin(x[i]).
 *
 *  @param x (input)  Input vector of size *n, in radians.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcosisin (__double_complex_t * /* C */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the hyperbolic sine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sinh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsinhf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the hyperbolic sine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sinh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsinh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the hyperbolic cosine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] is set to 1.
 *  If x[i] is +/-inf, y[i] is set to +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cosh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcoshf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the hyperbolic cosine for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] is set to 1.
 *  If x[i] is +/-inf, y[i] is set to +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cosh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcosh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the hyperbolic tangent for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-1.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tanh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtanhf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the hyperbolic tangent for each element of a vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-1.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tanh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtanh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the inverse hyperbolic sine for each element of a
 *            vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to asinh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvasinhf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the inverse hyperbolic sine for each element of a
 *            vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-inf, y[i] is set to +/-inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to asinh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvasinh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the principal value of inverse hyperbolic cosine for
 *            each element of a vector.
 *
 *  @discussion
 *  The calculated values are in the range [0, +inf].
 *  If x[i] == 1, y[i] is set to +0.
 *  If x[i] < 1, y[i] is set to NaN.
 *  If x[i] == +inf, y[i] is set to +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to acosh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvacoshf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the principal value of inverse hyperbolic cosine for
 *            each element of a vector.
 *
 *  @discussion
 *  The calculated values are in the range [0, +inf].
 *  If x[i] == 1, y[i] is set to +0.
 *  If x[i] < 1, y[i] is set to NaN.
 *  If x[i] == +inf, y[i] is set to +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to acosh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvacosh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the inverse hyperbolic tangent for each element of a
 *            vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-1, y[i] is set to +/-inf.
 *  If |x[i]|>1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to atanh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvatanhf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the inverse hyperbolic tangent for each element of a
 *            vector.
 *
 *  @discussion
 *  If x[i] is +/-0, y[i] preserves the signed zero.
 *  If x[i] is +/-1, y[i] is set to +/-inf.
 *  If |x[i]|>1, y[i] is set to NaN.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to atanh(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvatanh (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the integer truncation for each element of a vector.
 *
 *  @discussion
 *  The behavior of this function is equivalent to the libm function truncf().
 *  It rounds x[i] to the nearest integer in the direction of zero,
 *  equivalent to the C typecast y[i] = (float) (int) x[i].
 *
 *  @param y (output) Output vector of size *n. y[i] is set to integer
 *                    trunction of x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvintf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the integer truncation for each element of a vector.
 *
 *  @discussion
 *  The behavior of this function is equivalent to the libm function trunc().
 *  It rounds x[i] to the nearest integer in the direction of zero,
 *  equivalent to the C typecast y[i] = (double) (int) x[i].
 *
 *  @param y (output) Output vector of size *n. y[i] is set to integer
 *                    trunction of x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvint (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the rounding to the nearest integer for each element
 *            of a vector.
 *
 *  @discussion
 *  Rounds x[i] to the nearest integer, with ties rounded to even.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to the nearest
 *                    integer to x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvnintf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the rounding to the nearest integer for each element
 *            of a vector.
 *
 *  @discussion
 *  Rounds x[i] to the nearest integer, with ties rounded to even.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to the nearest
 *                    integer to x[i].
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvnint (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the ceiling function for each element of a vector.
 *
 *  @discussion
 *  Rounds to smallest integral value not less than x[i]. That is to say,
 *  rounds towards +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to ceil(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvceilf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the ceiling function for each element of a vector.
 *
 *  @discussion
 *  Rounds to smallest integral value not less than x[i]. That is to say,
 *  rounds towards +inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to ceil(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvceil (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates the floor function for each element of a vector.
 *
 *  @discussion
 *  Rounds to smallest integral value not greater than x[i]. That is to say,
 *  rounds towards -inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to floor(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvfloorf (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));
/*! @abstract Calculates the floor function for each element of a vector.
 *
 *  @discussion
 *  Rounds to smallest integral value not greater than x[i]. That is to say,
 *  rounds towards -inf.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to floor(x[i]).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvfloor (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.4), ios(5.0));

/*! @abstract Calculates, elementwise, the floating-point remainder of y/x,
 *            for two vectors x and y.
 *
 *  @discussion
 *  Specifically, the function calculates z=y-k*x, for some integer k such that,
 *  if x is non-zero, the result has the same sign as y, and magnitude less than
 *  that of x.
 *
 *  If y[i] is +/-0, and x[i] is not 0 or NaN, z[i] is set to +/-0.
 *  If y[i] is +/-inf, or x[i] is +/-0, z[i] is set to NaN.
 *  If x[i] is +/-inf, and y is finite, z[i] is set to y[i].
 *
 *  Note that argument labels are switched with respect to the libm function
 *  fmod().
 *
 *  @seealso vvremainderf
 *
 *  @param z (output) Output vector of size *n. z[i] is set to fmod(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, numerator in calculation.
 *
 *  @param x (input)  Input vector of size *n, denominator in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvfmodf (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Calculates, elementwise, the floating-point remainder of y/x,
 *            for two vectors x and y.
 *
 *  @discussion
 *  Specifically, the function calculates z=y-k*x, for some integer k such that,
 *  if x is non-zero, the result has the same sign as y, and magnitude less than
 *  that of x.
 *
 *  If y[i] is +/-0, and x[i] is not 0 or NaN, z[i] is set to +/-0.
 *  If y[i] is +/-inf, or x[i] is +/-0, z[i] is set to NaN.
 *  If x[i] is +/-inf, and y is finite, z[i] is set to y[i].
 *
 *  Note that argument labels are switched with respect to the libm function
 *  fmod().
 *
 *  @seealso vvremainder
 *
 *  @param z (output) Output vector of size *n. z[i] is set to fmod(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, numerator in calculation.
 *
 *  @param x (input)  Input vector of size *n, denominator in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvfmod (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates, elementwise, the difference between k*x and y, where
 *            k is the nearest integer to y/x.
 *
 *  @discussion
 *  Specifically, the function calculates z=y-k*x, for the integer k nearest the
 *  exact value of y/x, with ties rounded to even.
 *  The result z satisfies abs(z) <= abs(x)/2.
 *
 *  If y-k*x is zero, it is given the same sign as y.
 *
 *  If y[i] is +/-inf, or x[i] is +/-0, z[i] is set to NaN.
 *  If x[i] is +/-inf, and y is finite, z[i] is set to y[i].
 *
 *  @seealso vvfmodf
 *
 *  @param z (output) Output vector of size *n. z[i] is set to y[i]-k[i]*x[i].
 *
 *  @param y (input)  Input vector of size *n, numerator in calculation.
 *
 *  @param x (input)  Input vector of size *n, denominator in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvremainderf (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Calculates, elementwise, the difference between k*x and y, where
 *            k is the nearest integer to y/x.
 *
 *  @discussion
 *  Specifically, the function calculates z=y-k*x, for the integer k nearest the
 *  exact value of y/x, with ties rounded to even.
 *  The result z satisfies abs(z) <= abs(x)/2.
 *
 *  If y-k*x is zero, it is given the same sign as y.
 *
 *  If y[i] is +/-inf, or x[i] is +/-0, z[i] is set to NaN.
 *  If x[i] is +/-inf, and y is finite, z[i] is set to y[i].
 *
 *  @seealso vvfmod
 *
 *  @param z (output) Output vector of size *n. z[i] is set to y[i]-k[i]*x[i].
 *
 *  @param y (input)  Input vector of size *n, numerator in calculation.
 *
 *  @param x (input)  Input vector of size *n, denominator in calculation.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvremainder (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
	
/*! @abstract Copies, elementwise, the sign of x with the value of y, for two
 *            vectors x and y.
 *
 *  @param z (output) Output vector of size *n.
 *                    z[i] is set to copysign(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, used for the magnitude.
 *
 *  @param x (input)  Input vector of size *n, used for the sign.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvcopysignf (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Copies, elementwise, the sign of x with the value of y, for two
 *            vectors x and y.
 *
 *  @param z (output) Output vector of size *n.
 *                    z[i] is set to copysign(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, used for the magnitude.
 *
 *  @param x (input)  Input vector of size *n, used for the sign.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvcopysign (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Determines, elementwise, the next machine representable number
 *            from y in the direction of x.
 *
 *  @param z (output) Output vector of size *n.
 *                    z[i] is set to nextafter(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, used for the magnitude.
 *
 *  @param x (input)  Input vector of size *n, used for the sign.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvnextafterf (float * /* z */, const float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.5), ios(5.0));
/*! @abstract Determines, elementwise, the next machine representable number
 *            from y in the direction of x.
 *
 *  @param z (output) Output vector of size *n.
 *                    z[i] is set to nextafter(y[i], x[i]).
 *
 *  @param y (input)  Input vector of size *n, used for the magnitude.
 *
 *  @param x (input)  Input vector of size *n, used for the sign.
 *
 *  @param n (input)  The number of elements in the vectors x, y and z.       */
void vvnextafter (double * /* z */, const double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates the sine for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sin(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsinpif (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Calculates the sine for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to sin(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvsinpi (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates the cosine for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcospif (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Calculates the cosine for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to cos(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvcospi (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

/*! @abstract Calculates the tangent for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tan(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtanpif (float * /* y */, const float * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));
/*! @abstract Calculates the tangent for pi times each element of a vector.
 *
 *  @param y (output) Output vector of size *n. y[i] is set to tan(x[i]*PI).
 *
 *  @param x (input)  Input vector of size *n.
 *
 *  @param n (input)  The number of elements in the vectors x and y.          */
void vvtanpi (double * /* y */, const double * /* x */, const int * /* n */) API_AVAILABLE(macos(10.7), ios(5.0));

#if __has_feature(assume_nonnull)
    _Pragma("clang assume_nonnull end")
#endif
	
#ifdef __cplusplus
}
#endif
#endif /* __VFORCE_H */
// ==========  Accelerate.framework/Frameworks/vecLib.framework/Headers/cblas.h
/* cblas.h
 *
 * This header defines C bindings for the Basic Linear Algebra Subprograms,
 * providing optimized basic operations on vectors and matrices.  Single-
 * and double-precision, real and complex data formats are supported by this
 * library.
 *
 * A note on complex data layouts:
 *
 * In order to allow straightforward interoperation with other libraries and
 * complex types in C and C++, complex data in BLAS is passed through an opaque
 * pointer (void *).  The layout requirements on this complex data are that
 * the real and imaginary parts are stored consecutively in memory, and have
 * the alignment of the corresponding real type (float or double).  The BLAS
 * complex interfaces are compatible with the following types:
 *
 *     - The C complex types, defined in <complex.h>.
 *     - The C++ std::complex types, defined in <complex>.
 *     - The LAPACK complex types, defined in <Accelerate/vecLib/clapack.h>.
 *     - The vDSP types DSPComplex and DSPDoubleComplex, defined in <Accelerate/vecLib/vDSP.h>.
 *     - An array of size two of the corresponding real type.
 *     - A structure containing two elements, each of the corresponding real type.
 *
 */

#ifndef CBLAS_H

#ifdef __cplusplus
extern "C" {
#endif
  
#ifndef CBLAS_ENUM_DEFINED_H
#define CBLAS_ENUM_DEFINED_H
  enum CBLAS_ORDER {CblasRowMajor=101, CblasColMajor=102 };
  enum CBLAS_TRANSPOSE {CblasNoTrans=111, CblasTrans=112, CblasConjTrans=113,
    AtlasConj=114};
  enum CBLAS_UPLO  {CblasUpper=121, CblasLower=122};
  enum CBLAS_DIAG  {CblasNonUnit=131, CblasUnit=132};
  enum CBLAS_SIDE  {CblasLeft=141, CblasRight=142};
#endif  /* CBLAS_ENUM_DEFINED_H */
  
#ifndef CBLAS_ENUM_ONLY
#define CBLAS_H
#define CBLAS_INDEX int
  
#include <stdint.h>
#if __has_include(<os/availability.h>)
# include <os/availability.h>
#else // __has_include(<os/availability.h>)
# undef API_AVAILABLE
# define API_AVAILABLE(...) /* Nothing */
#endif // __has_include(<os/availability.h>)
  
int cblas_errprn(int __ierr, int __info, char *__form, ...)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_xerbla(int __p, char *__rout, char *__form, ...)
API_AVAILABLE(macos(10.2), ios(4.0));

/*
 * ===========================================================================
 * Prototypes for level 1 BLAS functions (complex are recast as routines)
 * ===========================================================================
 */
float  cblas_sdsdot(const int __N, const float __alpha, const float *__X,
                    const int __incX, const float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
double cblas_dsdot(const int __N, const float *__X, const int __incX,
                   const float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
float  cblas_sdot(const int __N, const float *__X, const int __incX,
                  const float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
double cblas_ddot(const int __N, const double *__X, const int __incX,
                  const double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
/*
 * Functions having prefixes Z and C only
 */
void   cblas_cdotu_sub(const int __N, const void *__X, const int __incX,
                       const void *__Y, const int __incY, void *__dotu)
API_AVAILABLE(macos(10.2), ios(4.0));
void   cblas_cdotc_sub(const int __N, const void *__X, const int __incX,
                       const void *__Y, const int __incY, void *__dotc)
API_AVAILABLE(macos(10.2), ios(4.0));

void   cblas_zdotu_sub(const int __N, const void *__X, const int __incX,
                       const void *__Y, const int __incY, void *__dotu)
API_AVAILABLE(macos(10.2), ios(4.0));
void   cblas_zdotc_sub(const int __N, const void *__X, const int __incX,
                       const void *__Y, const int __incY, void *__dotc)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Functions having prefixes S D SC DZ
 */
float  cblas_snrm2(const int __N, const float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
float  cblas_sasum(const int __N, const float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

double cblas_dnrm2(const int __N, const double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
double cblas_dasum(const int __N, const double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

float  cblas_scnrm2(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
float  cblas_scasum(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

double cblas_dznrm2(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
double cblas_dzasum(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Functions having standard 4 prefixes (S D C Z)
 */
CBLAS_INDEX cblas_isamax(const int __N, const float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
CBLAS_INDEX cblas_idamax(const int __N, const double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
CBLAS_INDEX cblas_icamax(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
CBLAS_INDEX cblas_izamax(const int __N, const void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

/*
 * ===========================================================================
 * Prototypes for level 1 BLAS routines
 * ===========================================================================
 */

/*
 * Routines with standard 4 prefixes (s, d, c, z)
 */
void cblas_sswap(const int __N, float *__X, const int __incX, float *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_scopy(const int __N, const float *__X, const int __incX, float *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_saxpy(const int __N, const float __alpha, const float *__X,
                 const int __incX, float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_saxpby(const int __N, const float __alpha, const float *__X,
                   const int __incX, const float __beta, float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_sset(const int __N, const float __alpha, float *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_dswap(const int __N, double *__X, const int __incX, double *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dcopy(const int __N, const double *__X, const int __incX,
                 double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_daxpy(const int __N, const double __alpha, const double *__X,
                 const int __incX, double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_daxpby(const int __N, const double __alpha, const double *__X,
                   const int __incX, const double __beta, double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_dset(const int __N, const double __alpha, double *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_cswap(const int __N, void *__X, const int __incX, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ccopy(const int __N, const void *__X, const int __incX, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_caxpy(const int __N, const void *__alpha, const void *__X,
                 const int __incX, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_caxpby(const int __N, const void *__alpha, const void *__X,
                   const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_cset(const int __N, const void *__alpha, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_zswap(const int __N, void *__X, const int __incX, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zcopy(const int __N, const void *__X, const int __incX, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zaxpy(const int __N, const void *__alpha, const void *__X,
                 const int __incX, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_zaxpby(const int __N, const void *__alpha, const void *__X,
                   const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void catlas_zset(const int __N, const void *__alpha, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Routines with S and D prefix only
 */
void cblas_srotg(float *__a, float *__b, float *__c, float *__s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_srotmg(float *__d1, float *__d2, float *__b1, const float __b2,
                  float *__P)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_srot(const int __N, float *__X, const int __incX, float *__Y,
                const int __incY, const float __c, const float __s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_srotm(const int __N, float *__X, const int __incX, float *__Y,
                 const int __incY, const float *__P)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_drotg(double *__a, double *__b, double *__c, double *__s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_drotmg(double *__d1, double *__d2, double *__b1, const double __b2,
                  double *__P)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_drot(const int __N, double *__X, const int __incX, double *__Y,
                const int __incY, const double __c, const double __s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_drotm(const int __N, double *__X, const int __incX, double *__Y,
                 const int __incY, const double *__P)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Routines with S D C Z CS and ZD prefixes
 */
void cblas_sscal(const int __N, const float __alpha, float *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dscal(const int __N, const double __alpha, double *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cscal(const int __N, const void *__alpha, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zscal(const int __N, const void *__alpha, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_csscal(const int __N, const float __alpha, void *__X,
                  const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zdscal(const int __N, const double __alpha, void *__X,
                  const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

/*
 * Extra reference routines provided by ATLAS, but not mandated by the standard
 */
void cblas_crotg(void *__a, void *__b, void *__c, void *__s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zrotg(void *__a, void *__b, void *__c, void *__s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_csrot(const int __N, void *__X, const int __incX, void *__Y,
                 const int __incY, const float __c, const float __s)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zdrot(const int __N, void *__X, const int __incX, void *__Y,
                 const int __incY, const double __c, const double __s)
API_AVAILABLE(macos(10.2), ios(4.0));

/*
 * ===========================================================================
 * Prototypes for level 2 BLAS
 * ===========================================================================
 */

/*
 * Routines with standard 4 prefixes (S, D, C, Z)
 */
void cblas_sgemv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const float __alpha, const float *__A, const int __lda,
                 const float *__X, const int __incX, const float __beta, float *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_sgbmv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const int __KL, const int __KU, const float __alpha, const float *__A,
                 const int __lda, const float *__X, const int __incX,
                 const float __beta, float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_strmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const float *__A, const int __lda, float *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_stbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const float *__A, const int __lda,
                 float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_stpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const float *__Ap, float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_strsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const float *__A, const int __lda, float *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_stbsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const float *__A, const int __lda,
                 float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_stpsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const float *__Ap, float *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_dgemv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const double __alpha, const double *__A, const int __lda,
                 const double *__X, const int __incX, const double __beta, double *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dgbmv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const int __KL, const int __KU, const double __alpha,
                 const double *__A, const int __lda, const double *__X,
                 const int __incX, const double __beta, double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtrmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const double *__A, const int __lda, double *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const double *__A, const int __lda,
                 double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const double *__Ap, double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtrsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const double *__A, const int __lda, double *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtbsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const double *__A, const int __lda,
                 double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtpsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const double *__Ap, double *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_cgemv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__X,
                 const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cgbmv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const int __KL, const int __KU, const void *__alpha, const void *__A,
                 const int __lda, const void *__X, const int __incX, const void *__beta,
                 void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctrmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__A, const int __lda, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const void *__A, const int __lda,
                 void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__Ap, void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctrsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__A, const int __lda, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctbsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const void *__A, const int __lda,
                 void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctpsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__Ap, void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_zgemv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__X,
                 const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zgbmv(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA, const int __M, const int __N,
                 const int __KL, const int __KU, const void *__alpha, const void *__A,
                 const int __lda, const void *__X, const int __incX, const void *__beta,
                 void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztrmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__A, const int __lda, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const void *__A, const int __lda,
                 void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__Ap, void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztrsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__A, const int __lda, void *__X,
                 const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztbsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const int __K, const void *__A, const int __lda,
                 void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztpsv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __TransA, const enum CBLAS_DIAG __Diag,
                 const int __N, const void *__Ap, void *__X, const int __incX)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Routines with S and D prefixes only
 */
void cblas_ssymv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const float __alpha, const float *__A, const int __lda,
                 const float *__X, const int __incX, const float __beta, float *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const int __K, const float __alpha, const float *__A,
                 const int __lda, const float *__X, const int __incX,
                 const float __beta, float *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_sspmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const float __alpha, const float *__Ap,
                 const float *__X, const int __incX, const float __beta, float *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_sger(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                const float __alpha, const float *__X, const int __incX,
                const float *__Y, const int __incY, float *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssyr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const float __alpha, const float *__X, const int __incX,
                float *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_sspr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const float __alpha, const float *__X, const int __incX,
                float *__Ap)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssyr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const float __alpha, const float *__X, const int __incX,
                 const float *__Y, const int __incY, float *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_sspr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const float __alpha, const float *__X, const int __incX,
                 const float *__Y, const int __incY, float *__A)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_dsymv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const double __alpha, const double *__A,
                 const int __lda, const double *__X, const int __incX,
                 const double __beta, double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const int __K, const double __alpha, const double *__A,
                 const int __lda, const double *__X, const int __incX,
                 const double __beta, double *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dspmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const double __alpha, const double *__Ap,
                 const double *__X, const int __incX, const double __beta, double *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dger(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                const double __alpha, const double *__X, const int __incX,
                const double *__Y, const int __incY, double *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsyr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const double __alpha, const double *__X,
                const int __incX, double *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dspr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const double __alpha, const double *__X,
                const int __incX, double *__Ap)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsyr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const double __alpha, const double *__X,
                 const int __incX, const double *__Y, const int __incY, double *__A,
                 const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dspr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const double __alpha, const double *__X,
                 const int __incX, const double *__Y, const int __incY, double *__A)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Routines with C and Z prefixes only
 */
void cblas_chemv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__A, const int __lda,
                 const void *__X, const int __incX, const void *__beta, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_chbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const int __K, const void *__alpha, const void *__A,
                 const int __lda, const void *__X, const int __incX, const void *__beta,
                 void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_chpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__Ap, const void *__X,
                 const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cgeru(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                 const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cgerc(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                 const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cher(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const float __alpha, const void *__X, const int __incX,
                void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_chpr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const float __alpha, const void *__X, const int __incX,
                void *__A)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cher2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_chpr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__Ap)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_zhemv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__A, const int __lda,
                 const void *__X, const int __incX, const void *__beta, void *__Y,
                 const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zhbmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const int __K, const void *__alpha, const void *__A,
                 const int __lda, const void *__X, const int __incX, const void *__beta,
                 void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zhpmv(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__Ap, const void *__X,
                 const int __incX, const void *__beta, void *__Y, const int __incY)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zgeru(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                 const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zgerc(const enum CBLAS_ORDER __Order, const int __M, const int __N,
                 const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zher(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const double __alpha, const void *__X, const int __incX,
                void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zhpr(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                const int __N, const double __alpha, const void *__X, const int __incX,
                void *__A)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zher2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__A, const int __lda)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zhpr2(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const int __N, const void *__alpha, const void *__X, const int __incX,
                 const void *__Y, const int __incY, void *__Ap)
API_AVAILABLE(macos(10.2), ios(4.0));

/*
 * ===========================================================================
 * Prototypes for level 3 BLAS
 * ===========================================================================
 */

/*
 * Routines with standard 4 prefixes (S, D, C, Z)
 */
void cblas_sgemm(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_TRANSPOSE __TransB, const int __M, const int __N,
                 const int __K, const float __alpha, const float *__A, const int __lda,
                 const float *__B, const int __ldb, const float __beta, float *__C,
                 const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssymm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const float __alpha, const float *__A, const int __lda,
                 const float *__B, const int __ldb, const float __beta, float *__C,
                 const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssyrk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const float __alpha, const float *__A, const int __lda,
                 const float __beta, float *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ssyr2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const float __alpha, const float *__A, const int __lda,
                  const float *__B, const int __ldb, const float __beta, float *__C,
                  const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_strmm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const float __alpha, const float *__A, const int __lda, float *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_strsm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const float __alpha, const float *__A, const int __lda, float *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_dgemm(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_TRANSPOSE __TransB, const int __M, const int __N,
                 const int __K, const double __alpha, const double *__A,
                 const int __lda, const double *__B, const int __ldb,
                 const double __beta, double *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsymm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const double __alpha, const double *__A, const int __lda,
                 const double *__B, const int __ldb, const double __beta, double *__C,
                 const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsyrk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const double __alpha, const double *__A, const int __lda,
                 const double __beta, double *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dsyr2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const double __alpha, const double *__A, const int __lda,
                  const double *__B, const int __ldb, const double __beta, double *__C,
                  const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtrmm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const double __alpha, const double *__A, const int __lda, double *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_dtrsm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const double __alpha, const double *__A, const int __lda, double *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_cgemm(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_TRANSPOSE __TransB, const int __M, const int __N,
                 const int __K, const void *__alpha, const void *__A, const int __lda,
                 const void *__B, const int __ldb, const void *__beta, void *__C,
                 const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_csymm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__B,
                 const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_csyrk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const void *__alpha, const void *__A, const int __lda,
                 const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_csyr2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const void *__alpha, const void *__A, const int __lda, const void *__B,
                  const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctrmm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, void *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ctrsm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, void *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));

void cblas_zgemm(const enum CBLAS_ORDER __Order,
                 const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_TRANSPOSE __TransB, const int __M, const int __N,
                 const int __K, const void *__alpha, const void *__A, const int __lda,
                 const void *__B, const int __ldb, const void *__beta, void *__C,
                 const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zsymm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__B,
                 const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zsyrk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const void *__alpha, const void *__A, const int __lda,
                 const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zsyr2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const void *__alpha, const void *__A, const int __lda, const void *__B,
                  const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztrmm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, void *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_ztrsm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const enum CBLAS_TRANSPOSE __TransA,
                 const enum CBLAS_DIAG __Diag, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, void *__B,
                 const int __ldb)
API_AVAILABLE(macos(10.2), ios(4.0));


/*
 * Routines with prefixes C and Z only
 */
void cblas_chemm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__B,
                 const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cherk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const float __alpha, const void *__A, const int __lda,
                 const float __beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_cher2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const void *__alpha, const void *__A, const int __lda, const void *__B,
                  const int __ldb, const float __beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zhemm(const enum CBLAS_ORDER __Order, const enum CBLAS_SIDE __Side,
                 const enum CBLAS_UPLO __Uplo, const int __M, const int __N,
                 const void *__alpha, const void *__A, const int __lda, const void *__B,
                 const int __ldb, const void *__beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zherk(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                 const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                 const double __alpha, const void *__A, const int __lda,
                 const double __beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));
void cblas_zher2k(const enum CBLAS_ORDER __Order, const enum CBLAS_UPLO __Uplo,
                  const enum CBLAS_TRANSPOSE __Trans, const int __N, const int __K,
                  const void *__alpha, const void *__A, const int __lda, const void *__B,
                  const int __ldb, const double __beta, void *__C, const int __ldc)
API_AVAILABLE(macos(10.2), ios(4.0));

/* Apple extensions to the BLAS interface. */

/* These routines perform linear operations (scalar multiplication and addition)
 * on matrices, with optional transposition.  Specifically, the operation is:
 *
 *      C = alpha * A + beta * B
 *
 * where A and B are optionally transposed as indicated by the value of transA
 * and transB.  This is a surprisingly useful operation; although its function
 * is fairly trivial, efficient implementation has enough subtlety to justify
 * a library interface.
 *
 * As an added convenience, this function supports in-place operation for
 * square matrices; in-place operation for non-square matrices in the face of
 * transposition is a subtle problem outside the scope of this interface.
 * In-place operation is only supported if the leading dimensions match as well
 * as the pointers.  If C overlaps A or B but does not have equal leading
 * dimension, or does not exactly match the source that it overlaps, the
 * behavior of this function is undefined.
 *
 * If alpha or beta is zero, then A (or B, respectively) is ignored entirely,
 * meaning that the memory is not accessed and the data does not contribute
 * to the result (meaning you can pass B == NULL if beta is zero).
 *
 * Note that m and n are the number of rows and columns of C, respectively.
 * If either A or B is transposed, then they are interpreted as n x m matrices.
 */

extern void appleblas_sgeadd(const enum CBLAS_ORDER __order,
                             const enum CBLAS_TRANSPOSE __transA,
                             const enum CBLAS_TRANSPOSE __transB, const int __m, const int __n,
                             const float __alpha, const float *__A, const int __lda,
                             const float __beta, const float *__B, const int __ldb, float *__C,
                             const int __ldc)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(3.0), tvos(8.0));

extern void appleblas_dgeadd(const enum CBLAS_ORDER __order,
                             const enum CBLAS_TRANSPOSE __transA,
                             const enum CBLAS_TRANSPOSE __transB, const int __m, const int __n,
                             const double __alpha, const double *__A, const int __lda,
                             const double __beta, const double *__B, const int __ldb, double *__C,
                             const int __ldc)
API_AVAILABLE(macos(10.10), ios(8.0), watchos(3.0), tvos(8.0));

/* The BLAS standard defines a function, cblas_xerbla( ), and suggests that
 * programs provide their own implementation in order to override default
 * error handling.  This scheme is incompatible with the shared library /
 * framework environment of OS X and iOS.
 *
 * Instead, if you wish to change the default BLAS error handling (which is to
 * print an english error message and exit( )), you need to install your own
 * error handlger by calling SetBLASParamErrorProc.
 *
 * Your error handler should adhere to the BLASParamErrorProc interface; it
 * need to terminate program execution.  If your error handler returns normally,
 * then the BLAS call will return normally following its execution without
 * performing any further processing.                                         */

typedef void (*BLASParamErrorProc)(const char *funcName, const char *paramName,
                                   const int *paramPos,  const int *paramValue);

void SetBLASParamErrorProc(BLASParamErrorProc __ErrorProc)
API_AVAILABLE(macos(10.2), ios(4.0));
  
#endif /* CBLAS_ENUM_ONLY */
  
#ifdef __cplusplus
}
#endif

#endif /* CBLAS_H */
