// ==========  MetalPerformanceShaders.framework/Headers/MetalPerformanceShaders.h
/*!
 *  @header MetalPerformanceShaders.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 */

#ifndef __METAL_VERSION__
#import <MPSCore/MPSCore.h>
#import <MPSImage/MPSImage.h>
#import <MPSMatrix/MPSMatrix.h>
#import <MPSNeuralNetwork/MPSNeuralNetwork.h>
#endif
#import <MPSRayIntersector/MPSRayIntersector.h>

#ifdef __cplusplus
extern "C" {
#endif

#ifndef __METAL_VERSION__
/*!
 *  MPSSupportsMTLDevice
 *  @abstract   Determine whether a MetalPerformanceShaders.framework  supports a MTLDevice.
 *  @discussion Use this function to determine whether a MTLDevice can be used with interfaces in MetalPerformanceShaders.framework.
 *  @param      device          A valid MTLDevice
 *  @return     YES             The device is supported.
 *              NO              The device is not supported
 */
BOOL    MPSSupportsMTLDevice( __nullable id <MTLDevice> device )  MPS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0));

    
/*! @abstract Hint to MPS how much memory your application expects to need for the command buffer
 *  @discussion This will cause MPS to prefetch a MTLHeap into its internal cache of
 *              the indicated size, which will be sub-allocated to back the temporary images,
 *              matrices, vectors and states in used over the course of the command buffer.
 *              This can be helpful in certain pathological situations when allocation sizes
 *              needed to support temporary objects do not allow for reuse of previous allocations
 *              for new objects.
 *
 *              Example: if the temporary resources need progressively larger MTLHeaps
 *              over the course of the MTLCommandBuffer, such as 1 MB, 2MB, 4 MB and 8MB,
 *              the first allocation might create a 1 MB heap, this might be released,
 *              but since the second allocation needs a 2 MB heap and the 1 MB heap is too
 *              small to be used, a new heap would need to be made, and so forth.  Using
 *              MPSHintTemporaryMemoryHighWaterMark(), a single 8 MB heap might be made manifest,
 *              and all four allocations can use it if they don't overlap temporally. Otherwise,
 *              a total of 1+2+4+8=15 MB might be allocated.
 *
 *              The application should be careful not to pass the sum of all allocations over
 *              the course of the command buffer. As we expect that not all temporary resources
 *              need to coexist at the same time, and so can alias one another, that would waste
 *              memory. The application should instead track the high water mark of the most
 *              memory in use at any single point over the course of the command buffer.
 *
 *              This can be simply done by traversing your graph creating all the temporary images,
 *              states, matrices and vectors that you will need in advance. Since the allocation of
 *              the underlying MTLHeaps that they use is deferred until you actually attempt to write
 *              to these resources or get the underlying MTLTexture or MTLBuffer, you can create all
 *              the objects, then call MPSHintTemporaryMemoryUsage, then call the various -encode
 *              methods and the heap should be sized correctly before memory is distributed to the
 *              temporary objects. In this exercise, assume that memory is not distributed to the
 *              temporary object until it is used to hold data, and is reclaimed for reuse when readCount
 *              reaches zero. The expected size temporary memory used by each object can be queried
 *              using its -resourceSize method.
 *
 *              Notes: The MPSNNGraph does this automatically for its workload. It is not necessary to
 *              prefetch for that. If a MTLHeap large enough to satisfy the size is already cached,
 *              no new one will be created. If the prefetched heap turns out to be too small, additional
 *              small heaps will be created as needed dynamically. If the prefetched heap is too big,
 *              any additional memory is wasted.
 *
 *              When the graph is known in advance, this method is preferred over
 *              +[MPSTemporaryImage prefetchStorageWithCommandBuffer:imageDescriptorList:]
 *              as the latter can not estimate the time period over which each resource is used, and is
 *              likely to conservatively prefetch too small a heap.
 *
 *  @param      cmdBuf      The scope of the MTLHeap
 *  @param      bytes       The size, in bytes, of the prefetched heap. The actual size ussed may be rounded
 *                          up according to device alignment requirements. This should be the maximum
 *`                         amount of temporary memory used at any point in the command buffer.
 */
void    MPSHintTemporaryMemoryHighWaterMark( __nonnull id <MTLCommandBuffer> cmdBuf,
                                             NSUInteger   bytes );
    
/*! @abstract   Set the timeout after which unused cached MTLHeaps are released
 *  @discussion MPS maintains a private set of MTLHeaps attached to each MTLCommandBuffer
 *              for use by temporary images, matrices, vectors and states, and also for its own
 *              private usage for temporary storage in some (typically multipass) filters. When the
 *              command buffer completes, these are returned to a MTLDevice level cache for reuse.
 *              If it is not reused within the heap cache duration, then the MTLHeaps are released
 *              and the memory is returned to the operating system for general reuse. The intent
 *              of this second level cache is to avoid surrendering the GPU performance advantage
 *              on repetitive workloads to  allocation, zero-fill and deallocation and reallocation
 *              of large MTLHeaps, which otherwise can easily occur.
 *
 *              Default: 5s.
 *
 *  @param      cmdBuf  The scope over which to set the heap cache duration. If the MTLCommandBuffer
 *              has already been committed, behavior is undefined.
 *  @param      seconds The number of seconds to cache used MTLHeaps before retiring them.
 *              NaN will be interpeted as 0.
 */
void    MPSSetHeapCacheDuration( __nonnull id <MTLCommandBuffer> cmdBuf,
                                 double seconds );

//
//  These headers contain doxygen formatted documentation. They are human readable as is,
//  but can be processed as such to make something a bit nicer looking.  Our version of
//  the doxygen PDF output may be found in MetalPerformanceShaders.framework/Documentation:
//
//      open `xcrun --show-sdk-path -sdk iphoneos9.0`/System/Library/Frameworks/MetalPerformanceShaders.framework/Documentation/MetalPerformanceShaders.pdf
//
//  Associated man pages are installed in the SDKROOT.
//
//      man -M `xcrun --show-sdk-path -sdk iphoneos9.0`/usr/share/man  MPSKernel
//

/*! @mainpage Metal Performance Shaders - High Performance Kernels on Metal
 *  @section  section_introduction  Introduction
 *
 *  MetalPerformanceShaders.framework is a framework of highly optimized compute and graphics shaders that are
 *  designed to integrate easily and efficiently into your Metal application.  These data-parallel 
 *  primitives are specially tuned to take advantage of the unique hardware characteristics of each
 *  iOS GPU to ensure optimal performance. Applications adopting MetalPerformanceShaders can be sure of achieving
 *  optimal performance without needing to update their own hand-written shaders for each new iOS GPU 
 *  generation. MetalPerformanceShaders can be used along with the application's existing Metal resources (such as 
 *  the MTLCommandBuffer, MTLBuffer and MTLTexture objects) and shaders.
 *
 *  In iOS 9, MetalPerformanceShaders.framework provides a series of commonly-used image processing primitives for 
 *  image effects on Metal textures.
 *
 *  In iOS 10, MetalPerformanceShaders.framework adds support for the following kernels:
 *  -  collection of kernels to implement and run neural networks using previously obtained training data, on the GPU
 *  -  new image processing filters to perform color-conversion and for building a gaussian pyramid
 *
 *  In iOS11, MetalPerformanceShaders.framework adds support for the following kernels:
 *  -  Image Processing Filters:  FindKeypoints, Statistics (Min-Max, Mean-Variance, Mean), Arithmetic Operations, Bilinear scale
 *                                Histogram filter takes a minPixelThresholdValue when computing histogram
 *  -  Linear Algebra Primitives: Triangular, LU and Cholesky Solvers, LU and Cholesky Decomposition
 *                                Support for multiple input types for Matrix-Matrix Multiplication
 *                                Matrix-Vector Multiply (gemv)
 *  -  Convolution Neural Networks:  New Neuron Functions: HardSigmoid, SoftELU, ELU, PReLU, ReLUN
 *                                   Convolution Transpose, Depth-wise Convolution, Dilated Convolution, Sub-pixel Convolution
 *                                   Dilated Pooling, Upsampling
 *  -  Recurrent Neural Networks
 *  -  A neural network graph API that makes it easy to create and execute neural networks on the GPU
 *
 *  The MetalPerformanceShaders.framework is now also available as API in macOS 10.13.  All primitives/filters supported
 *  by the framework in iOS 11 are also avalable on macOS 10.13.
 *
 *  @subsection subsection_usingMPS  Using MPS
 *  To use MPS:
 *   @code
 *      link:     -framework MetalPerformanceShaders
 *      include:  #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 *
 *      Advisory: MetalPerformanceShaders features are broken up into many subheaders which are
 *                included by MetalPerformanceShaders.h.   The exact placement of interfaces in
 *                headers is subject to change, as functionality in component sub-frameworks can
 *                move into MPSCore.framework when the functionality needs to be shared by
 *                multiple components when features are added. To avoid source level breakage,
 *                #include the top level MetalPerformanceShaders.h header instead of lower
 *                level headers.  iOS 11 already broke source compatibility for lower level headers
 *                and future releases will probably do so again. The only supported method of
 *                including MPS symbols is the top level framework header.
 *    @endcode
 *    On macOS, MetalPerformanceShaders.framework is 64-bit only.  If you are still supporting
 *    the 32-bit i386 architecture, you can link just your 64-bit slice to MPS using a Xcode
 *    user defined build setting.  For example, you can add a setting called LINK_MPS:
 *    @code
 *        LINK_MPS
 *            Debug    -framework MetalPerformanceShaders
 *                Intel architecture            <leave this part empty>
 *            Release  -framework MetalPerformanceShaders
 *                Intel architecture            <leave this part empty>
 *    @endcode
 *
 *    The 64-bit intel architectures will inherit from the generic definition on the Debug and
 *    Release lines. Next, add $(MPS_LINK) to the Other Linker Flags line in your Xcode build
 *    settings.
 *
 *    In code segments built for both i386 and x86-64 you will need to keep the i386 segment
 *    from attempting to use MPS. In C, C++ and Objective C, a simple #ifdef will work fine.
 *    @code
 *        BOOL IsMPSSupported( id <MTLDevice> device )
 *        {
 *        #ifdef __i386__
 *            return NO;
 *        #else
 *            return MPSSupportsMTLDevice(device);
 *        #endif
 *        }
 *    @endcode
 *
 *
 *  @section section_data    Data containers
 *  @subsection subsection_metal_containers  MTLTextures and MTLBuffers
 *
 *  Most data operated on by Metal Performance Shaders must be in a portable data container appropriate
 *  for use on the GPU, such as a MTLTexture, MTLBuffer or MPSImage/MPSMatrix/MPSVector.  The first two
 *  should be self-explanatory based on your previous experience with Metal.framework. MPS will use these
 *  directly when it can.  The other three are wrapper classes designed to make MTLTextures and MTLBuffers
 *  easier to use, especially when the data may be packed in the texture or buffer in an unusual order, or
 *  typical notions like texel do not map to the abstraction (e.g. feature channel) well. MPSImages and
 *  MPSMatrices also come in "temporary" variants. Temporary images and matrices aggressively share
 *  memory with one another, saving a great deal of processor time allocating and tearing down textures.
 *  (This uses MTLHeaps underneath, if you are familiar with that feature.) MPS manages the aliasing to
 *  keep you safe. In exchange you must manage the resource readCount.
 *
 *  Most MPSImage and MPSCNN filters operate only on floating-point or normalized texture formats.
 *  If your data is in a UInteger or Integer MTLPixelFormat (e.g. MTLPixelFormatR8Uint as opposed
 *  to MTLPixelFormatR8Unorm) then you may need to make a texture view of the texture to change
 *  the type using [(id <MTLTexture>) newTextureViewWithPixelFormat:(MTLPixelFormat)pixelFormat],
 *  to reinterpret the data to a normalized format of corresponding signedness and precision. In certain
 *  cases such as thresholding corresponding adjustments (e.g. /255) may have to also be made to
 *  parameters passed to the MPSKernel.
 *
 *  @subsection subsection_mpsimage  MPSImages
 *  Convolutional neural networking (CNN) filters may need more than the four data channels that a
 *  MTLTexture can provide. In these cases, the MPSImage is used instead as an abstraction
 *  layer on top of a MTLTexture. When more than 4 channels are needed, additional textures in the texture2d 
 *  array are added to hold additional channels in sets of four. The MPSImage tracks this information as the 
 *  number of "feature channels" in an image.
 *
 *  @subsection subsection_mpstemporaryimage  MPSTemporaryImages
 *  The MPSTemporaryImage (subclass of MPSImage) extends the MPSImage to provide advanced caching of
 *  reusable memory to increase performance and reduce memory footprint. They are intended as fast
 *  GPU-only storage for intermediate image data needed only transiently within a single MTLCommandBuffer.
 *  They accelerate the common case of image data which is created only to be consumed and destroyed
 *  immediately by the next operation(s) in a MTLCommandBuffer.  MPSTemporaryImages provide convenient and 
 *  simple way to save memory by automatically aliasing other MPSTemporaryImages in the MTLCommandBuffer.
 *  Because they alias (share texel storage with) other textures in the same MTLCommandBuffer, the valid 
 *  lifetime of the data in a MPSTeporaryImage is extremely short, limited to a portion of a MTLCommandBuffer. 
 *  You can not read or write data to a MPSTemporaryImage using the CPU, or use the data in other MTLCommandBuffers.
 *  Use regular MPSImages for more persistent storage.
 *
 *  Why do we need MPSTemporaryImages?  Consider what it would be like to write an app without a heap.
 *  All allocations would have to be either on the stack or staticly allocated at app launch. You
 *  would find that allocations that persist for the lifetime of the app are very wasteful when an object
 *  is only needed for a few microseconds. Likewise, once the memory is statically partitioned in this way,
 *  it is hard to dynamically reuse memory for other purposes as different tasks are attempted and the needs
 *  of the app change. Finally, having to plan everything out in advance is just plain inconvenient! Isn't it
 *  nicer to just call malloc() or new as needed? Yes, it is. Even if it means we have to also call free(),
 *  find leaks and otherwise manage the lifetime of the allocation through some mechanism like reference counting,
 *  or add __strong and __weak so that ARC can help us, we do it.
 *
 *  It should be therefore of little surprise that after the heap data structure by JWJ Williams in 1964, the
 *  heap has been a mainstay of computer science since. The heap allocator was part of the C language a decade
 *  later. Yet, 50 years on, why is it not used in GPU programming? Developers routinely still allocate resources
 *  up front that stay live for the lifetime of the program (command buffer). Why would you do that?
 *  MPSTemporaryImages are MPSImages that use a memory allocated by a command buffer associated heap to store
 *  texels. They only use the memory they need for the part of the command buffer that they need it in, and the
 *  memory is made available for other MPSTemporaryImages that live in another part of the same command buffer.
 *  This allows for a very high level of memory reuse. In the context of a MPSNNGraph, for example, the
 *  InceptionV3 neural network requires 121 MPSImages to hold intermediate results. However, since it uses
 *  MPSTemporaryImages instead, these are reduced to just four physical allocations of the same size as one of
 *  the original images. Do you believe most of your work should be done using MPSTemporaryImages? You should.
 *  You only need the persistent MPSImage for storage needed outside the context of the command buffer, for
 *  example those images that might be read from or written to by the CPU. Use MPSTemporaryImages for
 *  transient storage needs. In aggregate, they are far less expensive than regular MPSImages. Create them,
 *  use them, throw them away, all within a few lines of code. Make more just in time as needed.
 *
 *  @section section_discussion     The MPSKernel
 *
 *  The MPSKernel is the base class for all MPS kernels. It defines baseline behavior for all MPS 
 *  kernels, declaring the device to run the kernel on, some debugging options and a user-friendly
 *  label, should one be required. From this are derived the MPSUnaryImageKernel and MPSBinaryImageKernel
 *  sub-classes which define shared behavior for most image processing kernels (filters) such as
 *  edging modes, clipping and tiling support for image operations that consume one or two source textures.
 *  Neither these or the MPSKernel are typically used directly. They just provide API abstraction
 *  and in some cases may allow some level of polymorphic manipulation of MPS image kernel objects.
 *
 *  Subclasses of the MPSUnaryImageKernel and MPSBinaryImageKernel provide specialized -init and -encode 
 *  methods to encode various image processing primitives into your MTLCommandBuffer, and may also
 *  provide additional configurable properties on their own. Many such image filters are available:
 *  There are convolutions (generic, box, Sobel, and Gaussian) to do edge detection, sharpening and 
 *  blurring, morphological operators -- Min, Max, Dilate and Erode -- and histogram operations. 
 *  In addition, there are median, resampling filters and others. All of these run on the GPU directly 
 *  on MTLTextures and MTLBuffers.
 *
 *  As the MPSKernel/MPSUnaryImageKernel/MPSBinaryImageKernel classes serve to unify a diversity of image
 *  operations into a simple consistent interface and calling sequence to apply image filters, 
 *  subclasses implement details that diverge from the norm. For example, some filters may take a small 
 *  set of parameters (e.g. a convolution kernel) to govern how they function. However, the overall 
 *  sequence for using MPSKernel subclasses remains the same:
 *
 *  -#  Allocate the usual Metal objects: MTLDevice, MTLCommandQueue, and MTLCommandBuffer 
 *      to drive a Metal compute pipeline. If your application already uses Metal, chances are you 
 *      have most of these things already. MPS will fit right in to this workflow. It can 
 *      encode onto MTLCommandBuffers inline with your own workload.
 *
 *  -#  Create an appropriate MPSKernel object. For example, if you want to do a Gaussian blur, make
 *      a MPSImageGaussianBlur object.  MPSKernel objects are generally light weight but can be reused
 *      to save some setup time. They can not be used by multiple threads concurrently, so if you
 *      are using Metal from many threads concurrently, make extra MPSKernel objects. MPSKernel objects
 *      conform to <NSCopying>.
 *
 *  -#  Call [MPSKernelSubclass  encodeToCommandBuffer:...]. Parameters for other -encode... calls
 *      vary by filter type, but operate similarly. They create a MTLCommandEncoder, write commands to 
 *      run the filter into the MTLCommandBuffer and then end the MTLCommandEncoder.  This means
 *      you must call -endEncoding on your current MTLCommandEncoder before calling a MPSKernel encode
 *      method. You can at this point release the MPSKernel or keep it around to use again to save
 *      some setup cost.
 *
 *  -#  If you wish to encode futher commands of your own on the MTLCommandBuffer, you must
 *      create a new MTLCommandEncoder to do so.  
 *
 *  -#  (Standard Metal) When you are done with the MTLCommandBuffer, submit it to the device using
 *      typical Metal commands, such as [MTLCommandBuffer commit]. The MPS filter will begin
 *      running on the GPU. You can either use [MTLCommandBuffer waitUntilCompleted] or
 *      [MTLCommandBuffer addCompletedHandler:] to be notified when the work is done.
 *
 *  Each MPSKernel is allocated against a particular MTLDevice. A single filter may not be used with
 *  multiple MTLDevices. (You will need to make multiple MPSKernels for that.) This is necessary because
 *  the [MPSKernel initWithDevice:...] methods sometimes allocate MTLBuffers and MTLTextures to hold
 *  data passed in as parameters to the -init method and a MTLDevice is required to make those. MPSKernels
 *  provide a copy method that allow them to be copied for a new device.
 *
 *  MPSKernel objects are not entirely thread safe. While they may be used in a multithreaded context,
 *  you should not attempt to have multiple MPSKernel objects writing to the same MTLCommandBuffer at
 *  the same time. They share restrictions with the MTLCommandEncoder in this regard. In limited
 *  circumstances, the same MPSKernel can be used to write to multiple MTLCommandBuffers concurrently.
 *  However, that only works if the MPSKernel is treated as an immutable object. That is, if MPSKernel
 *  subclass properties of a shared filter are changed, then the change can be reflected on the other
 *  thread while the other thread is encoding its work, leading to undefined behavior. It is generally
 *  safest to just make copies of MPSKernel objects, one for each thread.
 *
 *  For more information, please see MPSTypes.h.
 *
 *  @subsection subsection_properties  MPS{Unary/Binary}ImageKernel properties
 *  The MPS{Unary/Binary}ImageKernel base classes define several properties common to all MPSKernels:
 *
 *  @subsubsection  subsubsection_clipRect  MPSKernel clipRect
 *  The clipRect property, common to MPSKernel sublcasses that write to a destination texture, describes
 *  the sub-rectangle of the destination texture overwritten by the filter. If the clipRect is larger than
 *  the destination texture, the intersection between the clipRect and destination texture bounds will be used.
 *  The clipRect may be used to avoid doing work to obscured regions of the destination image, or to
 *  manage tiling and to limit operations to parts of an image if for example, the user drew a rectangle
 *  on the screen and asked you to just apply the filter there.
 *
 *      extern MTLRegion MPSRectNoClip; //Pass this rectangle to fill the entire destination texture.
 *
 *  @subsubsection  subsubsection_MPSoffset  MPSOffset
 *  The offset (or primaryOffset or secondaryOffset) property, common to MPSKernel subclasses that 
 *  use a source texture from which pixel data is read, describes the positioning of the source image
 *  relative to the result texture. A offset of {0,0,0} indicates that the top left pixel of the source
 *  texture is the center pixel used to  create the top left corner of the destination texture clipRect.
 *  An offset of {1,2,0} positions the top left corner of the clipRect at {x=1, y=2, z=0} of the source
 *  image. The offset is the position of the top left corner of the clipRect in the source coordinate
 *  frame. It can be used for tiling and for translating an image up/down or left/right by pixel increments. 
 *  If there is no clipRect then the offset is the top left corner of the region read by the filter.
 *  If there are multiple source textures, then the primaryOffset describes the top left corner of the 
 *  region read in the primary source texture. The secondaryOffset describes the top left corner of the
 *  region read in the secondary source texture, and so forth.
 *
 *  @subsubsection  subsubsection_edgemode  MPSKernelEdgeMode
 *  The edgeMode (or primaryEdgeMode or secondaryEdgeMode)describes the behavior of texture reads that
 *  stray off the edge of the source image. This can happen if the offset is negative, meaning read
 *  off the top or left edge of the image.  It can also happen if the clipRect.size + offset
 *  is larger than the source image, meaning read off the bottom and right of the image. It is
 *  also possible for filters that have a filter window that stretches to examine neighboring pixels,
 *  such as convolution, morphology and resampling filters.  If there are multiple source textures, 
 *  then the primaryEdgeMode describes the MPSKernelEdgeMode to use with primary source texture. 
 *  The secondaryEdgeMode describes the MPSKernelEdgeMode to use with the secondary source texture, 
 *  and so forth.
 *
 *      typedef NS_ENUM(NSUInteger, MPSImageEdgeMode)
 *
 *      MPSImageEdgeModeZero       Out of bound pixels are (0,0,0,1) for image formats without
 *                                      alpha channel and (0,0,0,0) for image with pixel format with an
 *                                      alpha channel
 *
 *      MPSImageEdgeModeClamp      Out of bound pixels are clamped to nearest edge pixel
 *
 *  @subsubsection  subsubsection_options  MPSKernelOptions
 *  Each MPSKernel takes a MPSKernelOptions bit mask to indicate various options to use when running the filter:
 *  @code
 *      typedef NS_OPTIONS(NSUInteger, MPSKernelOptions)
 *
 *      MPSKernelOptionsNone
 *             Use default options
 *
 *      MPSKernelOptionsSkipAPIValidation
 *          Do not spend time looking at parameters passed to MPS for errors.
 *
 *      MPSKernelOptionsAllowReducedPrecision
 *          When possible, MPSKernels use a higher precision data representation internally than
 *          the destination storage format to avoid excessive accumulation of computational
 *          rounding error in the result. MPSKernelOptionsAllowReducedPrecision advises the
 *          MPSKernel that the destination storage format already has too much precision for
 *          what is ultimately required downstream, and the MPSKernel may use reduced precision
 *          internally when it feels that a less precise result would yield better performance.
 *          When enabled, the precision of the result may vary by hardware and operating system.
 *  @endcode
 *  @section subsection_availableFilters     Available MPSKernels
 *
 *  @subsection subsection_convolution  Image Convolution
 *  @subsubsection subsubsection_kernel The Image Convolution Kernel
 *  The convolution filter is at its simplest the weighted average of a pixel with its nearest neighbors.
 *  The weights are provided by a convolution kernel.  The number and position of the nearest neighbors 
 *  that are considered are given by the size of the convolution kernel. For example, a convolution kernel
 *  might be the following 5x5 array of weights:
 *  @code
 *              1       2       3       2       1
 *              2       4       6       4       2
 *              3       6      [9]      6       3
 *              2       4       6       4       2
 *              1       2       3       2       1
 *  @endcode
 *  In order to calculate this 5x5 convolution result, one would multiply all of the pixels in the
 *  image within (5/2=) 2 pixels of the desired pixel by its corresponding weight, add them up and divide by
 *  a divisor to renormalize the results. Then, repeat for all other pixels in the area you wish to convolve.
 *
 *  For those MPS filters where the convolution kernel is passed in, you provide the kernel as a
 *  normalized float array. That is, the kernel weights have the divisor already divided into them and
 *  as a consequence should usually sum to 1.0. In our tent example above, the sum over the area of the kernel 
 *  is 81, so one would normalize it as follows:
 *  @code
 *              1/81    2/81    3/81    2/81    1/81
 *              2/81    4/81    6/81    4/81    2/81
 *              3/81    6/81   [9/81]   6/81    3/81
 *              2/81    4/81    6/81    4/81    2/81
 *              1/81    2/81    3/81    2/81    1/81
 *  @endcode
 *  It is not strictly necessary that the filter weights add up to 1.0f.  Edge detection filters frequently add up
 *  to zero. You may decide to have the area under the filter be a bit bigger or smaller than 1.0 to increase
 *  or reduce the contrast in the result.
 *
 *  The MxN kernel is passed in as a 1-dimensional data array in row major order.
 *
 *  Some convolution filters also have a notion of a bias. This is a number to be added to the
 *  result before it is written out to result texture. A bias might be used to uniformly brighten an image,
 *  set a video range baseline (e.g. 0 might actually be encoded as 16/255) or to make negative signal
 *  representable on a unorm image. 
 *
 *          A unorm image is an image comprised of unsigned normalized samples. A typical 8-bit image (e.g. 
 *          MTLPixelFormatRGBA8Unorm) is a unorm image. It has unsigned samples that represent values between
 *          [0,1]. In the case of MTLPixelFormatRGBA8Unorm, the encoding of 0 is 0, and the encoding of 1.0f
 *          is UINT8_MAX (255).
 *
 *  @subsubsection subsubsection_box  The Box, Tent and Gaussian Filters
 *  There are many different convolution kernel shapes which can produce different results. A kernel consisting
 *  of all 1's is called a Box filter. It is very quick to calculate and may run nearly as fast as a texture
 *  copy, even for very large blur radii. The blur effect that you get, however, can be square in appearance
 *  and may not be entirely appealing under close scrutiny. A second pass of the box will lead to a Tent kernel. 
 *  (The 5x5 tent above can be reduced into two 3x3 Box filters.) Its appearance is more pleasing. Tent 
 *  operations can be found in sample code for window shadows. Both Box and Tent filters are provided by 
 *  MPS. Multiple passes of a box and/or tent  filters will tend to converge towards a gaussian line shape 
 *  and produce a smoother blur effect. MPS also provides a Gaussian blur, though it uses a different method.
 *
 *  @subsubsection subsubsection_laplacian   Laplacian and Unsharp Mask Filters
 *  One can in practice also subtract a blur from the image to produce a sharpening effect (unsharp mask). This
 *  is done by preparing a convolution kernel which is a scaled image less a blur to reduce the low frequency
 *  component of an image. This can reduce blur, but may also emphasize noise in the image.  As an example, we
 *  can do identity minus a box blur:
 *  @code
 *                  | 1   1   1 |
 *      k0 * [1] -  | 1   1   1 | * k2
 *                  | 1   1   1 |
 *  @endcode
 *  If we pick k0 = 9 and k2 = 1, so that the two kernels have equal weight, we arrive at:
 *  @code
 *      |-1  -1  -1 |
 *      |-1   8  -1 |
 *      |-1  -1  -1 |
 *  @endcode
 *  This is a Laplacian filter for calculating image gradients (including diagonals in this case).
 *
 *      Caution: because this convolution kernel has negative regions, it can produce negative 
 *      results as well as positive ones from ordinary image data. If you intend to store the 
 *      result in a unorm texture, you'll need to scale it and add a positive value to it to avoid 
 *      having the negative signal clamped off. (e.g. p' = 0.5*p+0.5).
 *
 *  An unsharp mask filter is the sum between the original image and a scaled result of the Laplacian 
 *  filter. The scaling factor (and filter size and shape) adjusts the nature of the low frequency 
 *  signal and the degree to which it is removed. This work can usually be combined into the convolution 
 *  kernel, to do the whole thing in one pass.
 *
 *  @subsubsection subsubsection_sobel   Sobel Edge detection
 *  Instead of integrating over an area, Convolution filters can also differentiate over an
 *  area by subtracting adjacent pixels.  One such  filter is the Sobel edge detection filter. 
 *  It produces bright signal where there are large differences between one pixel and the next 
 *  and black elsewhere:
 *  @code
 *      | -1  0   1 |       | 1   2   1 |
 *   Gx=| -2  0   2 |   Gy= | 0   0   0 |
 *      | -1  0   1 |       |-1  -2  -1 |
 *
 *
 *      result = sqrt( Convolve(src, Gx) * Convolve(src * Gx) +
 *                     Convolve(src, Gy) * Convolve(src * Gy))
 *  @endcode
 *
 *  @subsubsection subsubsection_otherfilters   Other Filters
 *  Other effects can be achieved as well, such as emboss:
 *  @code
 *     -1   0   0
 *      0   0   0
 *      0   0   1
 *  @endcode
 *  @subsubsection subsubsection_separability   Separable Convolution
 *  Some convolution kernels are separable. That is, the filter weights can be factored into the product
 *  of two smaller sets of weights. As an example, the tent kernel shown above can be factored into a 
 *  horizontal and vertical 1-dimensional kernel each containing [1 2 3 2 1]. In this way, what might otherwise
 *  have been a 5x5 convolution with 25 multiplies and 24 adds is instead a 5x1 and 1x5 convolution with 
 *  a total of 10 multiplies and 8 adds and possibly some extra load/store traffic for the two-pass algorithm. 
 *  The savings get bigger for bigger filter areas. MPS convolution filters will automatically separate 
 *  kernels to run faster, when possible. Some filters with fixed kernels such as Box and Guassian are inherently
 *  separable. We attempt to factor the general convolution kernel into 2 1D kernels in the -initWithDevice:...
 *  method.  If you want to factor it yourself, make two MPSImageConvolution objects with 1D kernels.
 *
 *  @subsubsection subsubsection_convolveAvailability   Convolutions in MPS
 *  Convolution filters provided by MPS include:
 *
 *      MPSImageConvolution       <MPSImage/MPSImageConvolution.h>        General convolution
 *      MPSImageGassianBlur       <MPSImage/MPSImageConvolution.h>        Gaussian blur
 *      MPSImageBox               <MPSImage/MPSImageConvolution.h>        Box blur
 *      MPSImageTent              <MPSImage/MPSImageConvolution.h>        Tent blur
 *
 * @subsection subsection_morphology  Morphology
 *  Morphological operators are similar to convolutions in that they find a result by looking at the nearest
 *  neighbors of each pixel in the image. Instead of calculating a weighted average, morphological operators
 *  scan the kernel area looking for the maximum or minimum pixel value. The MPSImageAreaMax and 
 *  MPSImageAreaMin filters return the raw maximum and minimum color channel value in the kernel area for
 *  each pixel, respectively. The MPSImageDilate and MPSImageErode filters do the same thing, except that the probe 
 *  shape need not be a rectangle, and instead can be nearly any shape you desire, such as a antialiased 
 *  oval, star or heart shape.
 *
 *  When applied, the max and dilate filters have the effect of adding their shape on to the periphery of
 *  bright objects in the image. A single bright pixel, such as might be found in a photograph of a starry
 *  night sky will become the shape of a probe -- a rectangle for max, and perhaps a 5-pointed star if
 *  that is the shape you chose for the dilate filter kernel. Larger objects will adopt more rectangular
 *  or star quality into their shape. (An oval or circular probe would round the corners of a rectangular 
 *  object, for example.)  The min and erode filters do similar things to the darker regions of the image.
 *
 *  When a dilate filter is followed by an erode filter (or max followed by min) with similar filters, the 
 *  effect is known as a close operator. Expanding bright areas only to erode them away again leaves most of
 *  the image in roughly the same shape as it started, but small dark areas that are completely removed by the 
 *  dilate operator are not replaced by the erode. Dark noise may be removed. Small enclosed dark area may
 *  be completely filled in by bright signal.  Similarly erode followed by dilate is an open operator. It 
 *  will tend to remove bright fine detail and fill in small bright areas surrounded by dark lines.
 *
 *  To make a MPS morphology filter with a text glyph draw black text on a white background. MPS
 *  morphology filters must have a center pixel with value 0.
 *
 *
 *  Morphology filters provided by MPS include:
 *
 *      MPSImageAreaMax  <MPSImage/MPSImageMorphology.h>       Area Max
 *      MPSImageAreaMin  <MPSImage/MPSImageMorphology.h>       Area Min
 *      MPSImageDilate   <MPSImage/MPSImageMorphology.h>       Dilate
 *      MPSImageErode    <MPSImage/MPSImageMorphology.h>       Erode
 *
 *  @subsection subsection_histogram     Histogram
 *  A image may be examined by taking the histogram of its pixels. This gives the distribution of the various
 *  intensities per color channel. The MPSImageHistogram filter can be used to calculate a histogram for a MTLTexture.
 *
 *  In some cases, as a result of image processing operations the very dark and light regions of the intensity
 *  spectrum can become unpopulated. Perhaps a photograph is underexposed or overexposed. The MPSImageHistogramEqualization
 *  filter will redistribute the intensities to a more uniform distribution, correcting such problems. 
 *  The MPSImageHistogramSpecification class allows you to cause an image to conform to a different histogram. 
 *
 *
 *  Histogram filters provided by MPS include:
 *
 *      MPSImageHistogram              <MPSImage/MPSImageHistogram.h>     Calculate the histogram of an image
 *      MPSImageHistogramEqualization  <MPSImage/MPSImageHistogram.h>     Redistribute intensity in an image to equalize
 *                                                                          the histogram
 *      MPSImageHistogramSpecification <MPSImage/MPSImageHistogram.h>     A generalized version of histogram equalization
 *                                                                          operation. Convert the image so that its histogram
 *                                                                          matches the desired histogram provided to the
 *                                                                          histogram specification filter.
 *
 * @subsection subsection_median  Image Median
 *  Median filters find the median value in a region surrounding each pixel in the source image.  It is frequently
 *  used to remove noise from the image, but may also be used to remove fine detail like a open filter. It is widely
 *  used in image processing because in many cases it can remove noise while at the same time preserving edges.
 *
 *  Median filters provided by MPS include:
 *
 *      MPSImageMedian                <MPSImage/MPSImageMedian.h>         Calculate the median of an image using a
 *                                                                     square filter window.
 *
 *  @subsection subsection_resampling  Image Resampling
 *  Resampling operations are used to convert one regular array of pixels to another regular array of pixels,
 *  typically along a different set of axes and/or using a different sampling period. Changing the sampling period
 *  will enlarge or reduce images and/or distort the aspect ratio. Change of axis results in rotations or arbitrary
 *  affine transforms. 
 *
 *  For most imaging work on the GPU, resampling can be quickly and simply done as part of another pass using a 
 *  Euler matrices or quaternions to transform the coordinate space followed by linear filtering to interpolate the
 *  value found there. However, this can lead to somewhat muddy images and may result in loss of signal when 
 *  downsampling by more than a factor of two unless a low pass filter is applied first. It is also prone to 
 *  the development of Moire patterns in regions of the image with regularly repeating signal, such as a picture
 *  of a masonry grid on the side of a building. 
 *
 *  The MPS resampling routines use a higher quality (but more expensive) Lanczos resampling algorithm. 
 *  Especially with photographic images, it will usually produce a much nicer result. It does not require a low pass
 *  filter be applied to the image before down sampling. However, some ringing can occur near high frequency regions 
 *  of the image, making the algorithm less suitable for vector art.
 *
 *  MetalPerformanceShaders.framework provides a MPSImageScale functions to allow for simple resizing of images into the clipRect
 *  of the result image. They can operate with preservation of aspect ratio or not.
 *
 *      MPSImageLanczosScale        <MPSImage/MPSResample.h>   Resize or adjust aspect ratio of an image using a Lanczos filter.
 *      MPSImageBilinearScale       <MPSImage/MPSResample.h>   Resize or adjust aspect ratio of an image using bilinear interpolation.
 *
 *  Each method has its own advantages. The bilinear method is faster. However, downsampling by more than a factor
 *  of two will lead to data loss, unless a low pass filter is applied before the downsampling operation.  The
 *  lanczos filter method does not have this problem and usually looks better. However, it can lead to ringing
 *  at sharp edges, making it better for photographs than vector art.
 *
 *  @subsection subsection_threshold     Image Threshold
 *  Thresholding operations are commonly used to separate elements of image structure from the rest of an image. 
 *  Generally, these operate by making some sort of simple comparison test, for example color_intensity > 0.5, and
 *  then writing out 0 or 1 (actual values configurable) depending on the truth or falsehood of the result. It is 
 *  frequently used in computer vision, or to accentuate edge detection filters. 
 *
 *  A variety of thresholding operators are supported:
 *
 *      MPSImageThresholdBinary           <MPSImage/MPSImageThreshold.h>  srcPixel > thresholdVal ? maxVal : 0
 *      MPSImageThresholdBinaryInverse    <MPSImage/MPSImageThreshold.h>  srcPixel > thresholdVal ? 0 : maxVal
 *      MPSImageThresholdTruncate         <MPSImage/MPSImageThreshold.h>  srcPixel > thresholdVal ? thresholdVal : srcPixel
 *      MPSImageThresholdToZero           <MPSImage/MPSImageThreshold.h>  srcPixel > thresholdVal ? srcPixel : 0
 *      MPSImageThresholdToZeroInverse    <MPSImage/MPSImageThreshold.h>  srcPixel > thresholdVal ? 0 : srcPixel
 *      MPSImageKeypoint                  <MPSImage/MPSImageKeypoint.h>  return a list of pixels that are greathr than a threshold value
 *
 *  @subsection subsection_images_statistics  Image Statistics
 *  Several statistical operators are available which return statistics for the entire image, or
 *  a subregion. These operators are:
 *
 *      MPSImageStatisticsMinAndMax       <MPSImage/MPSImageStatistics.h> return maximum and minimum values in the image for each channel
 *      MPSImageStatisticsMean            <MPSImage/MPSImageStatistics.h> return the mean channel value over the region of interest
 *      MPSImageStatisticsMeanAndVariance <MPSImage/MPSImageStatistics.h> return the mean channel value and variance of each channel over the region of interest
 *
 *  These filters return the results in a small (1x1 or 2x1) texture. The region over which the
 *  statistical operator is applied is regulated by the clipRectSource property.
 *
 *  @subsection subsection_math     Math Filters
 *  Arithmetic filters take two source images, a primary source image and a secondary source image, as input and
 *  output a single destination image. The filters apply an element-wise arithmetic operator to each pixel in a primary source
 *  image and a corresponding pixel in a secondary source image over a specified region. The supported arithmetic operators
 *  are addition, subtraction, multiplication, and division.
 *
 *  These filters take additional parameters: primaryScale, secondaryScale, and bias and apply them to the primary source
 *  pixel (x) and secondary source pixel (y) in the following way:
 *
 *      MPSImageAdd         <MPSImage/MPSImageMath.h>  Element-wise addition operator:      result = ((primaryScale * x) + (secondaryScale * y)) + bias
 *      MPSImageSubtract    <MPSImage/MPSImageMath.h>  Element-wise subtraction operator    result = ((primaryScale * x) - (secondaryScale * y)) + bias
 *      MPSImageMultiply    <MPSImage/MPSImageMath.h>  Element-wise multiplication operator result = ((primaryScale * x) * (secondaryScale * y)) + bias
 *      MPSImageDivide      <MPSImage/MPSImageMath.h>  Element-wise division operator       result = ((primaryScale * x) / (secondaryScale * y)) + bias
 *
 *  These filters also take the following additional parameters: secondarySourceStrideInPixelsX and secondarySourceStrideInPixelsY.
 *  The default value of these parameters is 1. Setting both of these parameters to 0 results in the secondarySource image being
 *  handled as a single pixel.
 *
 *  @subsection subsection_CNN     Convolutional Neural Networks
 *  Convolutional Neural Networks (CNN) is a machine learning technique that attempts to model the visual cortex as a sequence 
 *  of convolution, rectification, pooling and normalization steps. Several CNN filters commonly derived from the MPSCNNKernel
 *  base class are provided to help you implement these steps as efficiently as possible.
 *

 *      MPSCNNNeuron                    <MPSNeuralNetwork/MPSCNNNeuron.h>            Neuron activation function filter
 *      Activation filter types:
 *          MPSCNNNeuronTypeLinear                                                   A linear neuron activation function
 *          MPSCNNNeuronTypeReLU                                                     A neuron activation function with rectified linear units
 *          MPSCNNNeuronTypeSigmoid                                                  A sigmoid neuron activation function 1/(1+e**-x)
 *          MPSCNNNeuronTypeHardSigmoid                                              A hard sigmoid neuron activation function clamp((a*x)+b, 0, 1)
 *          MPSCNNNeuronTypeTanH                                                     A neuron activation function using hyperbolic tangent
 *          MPSCNNNeuronTypeAbsolute                                                 An absolute neuron activation function |x|
 *          MPSCNNNeuronTypeSoftPlus                                                 A parametric SoftPlus neuron activation function a*log(1+e**(b*x))
 *          MPSCNNNeuronTypeSoftSign                                                 A SoftSign neuron activation function x/(1+|x|)
 *          MPSCNNNeuronTypeELU                                                      A parametric ELU neuron activation function x<0 ? (a*(e**x-1)) : x
 *          MPSCNNNeuronTypeReLUN                                                    A rectified linear N neuron activation function min((x>=0?x:a*x), b)
 *          MPSCNNNeuronTypePReLU                                                    ReLU, except a different a value is provided for each feature channel
 *          MPSCNNNeuronPower                                                        A Power neuron activation function (a*x+b)^c
 *          MPSCNNNeuronExponential                                                  A Exponential neuron activation function c^(a*x+b)
 *          MPSCNNNeuronLogarithm                                                    A Logarithm neuron activation function log_c(a*x+b)
 *      MPSCNNConvolution               <MPSNeuralNetwork/MPSCNNConvolution.h>       A 4D convolution tensor
 *      MPSCNNConvolutionTranspose      <MPSNeuralNetwork/MPSCNNConvolution.h>       A 4D convolution transpose tensor
 *      MPSCNNFullyConnected            <MPSNeuralNetwork/MPSCNNConvolution.h>       A fully connected CNN layer
 *      MPSCNNPoolingMax                <MPSNeuralNetwork/MPSCNNPooling.h>           The maximum value in the pooling area
 *      MPSCNNPoolingAverage            <MPSNeuralNetwork/MPSCNNPooling.h>           The average value in the pooling area
 *      MPSCNNPoolingL2Norm             <MPSNeuralNetwork/MPSCNNPooling.h>           The L2-Norm value in the pooling area
 *      MPSCNNDilatedPoolingMax         <MPSNeuralNetwork/MPSCNNPooling.h>           The maximum value in the dilated pooling area
 *      MPSCNNSpatialNormalization      <MPSNeuralNetwork/MPSCNNNormalization.h>
 *      MPSCNNCrossChannelNormalization <MPSNeuralNetwork/MPSCNNNormalization.h>
 *      MPSCNNSoftmax                   <MPSNeuralNetwork/MPSCNNSoftMax.h>           exp(pixel(x,y,k))/sum(exp(pixel(x,y,0)) ... exp(pixel(x,y,N-1))
 *      MPSCNNLogSoftmax                <MPSNeuralNetwork/MPSCNNSoftMax.h>           pixel(x,y,k) - ln(sum(exp(pixel(x,y,0)) ... exp(pixel(x,y,N-1)))
 *      MPSCNNUpsamplingNearest         <MPSNeuralNetwork/MPSCNNUpsampling.h>        A nearest upsampling layer.
 *      MPSCNNUpsamplingBilinear        <MPSNeuralNetwork/MPSCNNUpsampling.h>        A bilinear upsampling layer.
 *      MPSCNNDropout                   <MPSNeuralNetwork/MPSCNNDropout.h>           A dropout layer.
 *
 *  MPSCNNKernels operate on MPSImages.  MPSImages are at their core MTLTextures. However, whereas
 *  MTLTextures commonly represent image or texel data, a MPSImage is a more abstract representation
 *  of image features. The channels within a MPSImage do not necessarily correspond to colors in a
 *  color space. (Though, they can.) As a result, there can be many more than four of them. 32 or 64 channels
 *  per pixel is not uncommon.  This is achieved on the MTLTexture hardware abstraction by inserting
 *  extra RGBA pixels to handle the additional feature channels (if any) beyond 4. These extra pixels are
 *  stored as multiple slices of a 2D image array.  Thus, each CNN pixel in a 32-channel image is represented
 *  as 8 array slices, with 4-channels stored per-pixel in each slice.  The width and height of the MTLTexture
 *  is the same as the width and height of the MPSImage.  The number of slices in the MTLTexture is given by
 *  the number of feature channels rounded up to a multiple of 4.
 *
 *  MPSImages can be created from existing MTLTextures. They may also be created anew from a MPSImageDescriptor
 *  and backed with either standard texture memory, or as MPSTemporaryImages using memory drawn from MPS's
 *  internal cached texture backing store.  MPSTemporaryImages can provide great memory usage and CPU time savings,
 *  but come with significant restrictions that should be understood before using them. For example, their contents
 *  are only valid during the GPU-side execution of a single MTLCommandBuffer and can not be read from or written to
 *  by the CPU. They are provided as an efficient way to hold CNN computations that are used immediately within the
 *  scope of the same MTLCommandBuffer and then discarded. We also support concatenation by allowing the user to 
 *  define from which destination feature channel to start writing the output of the current layer. In this way
 *  the application can make a large MPSImage or MPSTemporaryImage and fill in parts of it with multiple layers
 *  (as long as the destination feature channel offset is a multiple of 4).
 *
 *  The standard MPSCNNConvolution operator also does dilated convolution, sub-pixel convolution and
 *  depth-wise convolution. There are also bit-wise convolution operators that can use only a single bit
 *  for precision of the weights. The precision of the image can be reduced to 1 bit in this case as well.
 *  The bit {0,1} represents {-1,1}.
 *
 *  Some CNN Tips:
 *  - Think carefully about the edge mode requested for pooling layers. The default is clamp to zero, but there
 *    are times when clamp to edge value may be better.
 *  - To avoid falling off the edge of an image for filters that have a filter area (convolution, pooling) set the
 *    MPSCNNKernel.offset = (MPSOffset){ .x = kernelWidth/2, .y = kernelHeight/2, .z = 0}; and reduce the size 
 *    of the output image by {kernelWidth-1, kernelHeight-1,0}. The filter area stretcheds up and to the left
 *    of the MPSCNNKernel.offset by {kernelWidth/2, kernelHeight/2}. While consistent with other MPS imaging operations,
 *    this behavior is different from some other CNN implementations.
 *  - If setting the offset and making MPSImages to hold intermediates are taking up a lot of your time,
 *    consider using the MPSNNGraph instead. It will automate these tasks.
 *  - Please remember:
 *      MPSCNNConvolution takes weights in the order weight[outputChannels][kernelHeight][kernelWidth][inputChannels / groups]
 *      MPSCNNFullyConnected takes weights in the order weight[outputChannels][sourceWidth][sourceHeight][inputChannels]
 *  - Initialize MPSCNNKernels once and reuse them
 *  - You can use MPSCNNNeurons and other Filters in MPS to perform pre-processing of images, such as scaling and resizing.
 *  - Specify a neuron filter with MPSCNNConvolution descriptor to combine the convolution and neuron operations.
 *  - Use MPSTemporaryImages for intermediate images that live for a short period of time (less than one MTLCommandBuffer).
 *      MPSTemporaryImages can reduce the amount of memory used by the convolutional neural network by several fold, and
 *      similarly reduce the amount of CPU time spent allocating storage and latency between MTLCommandBuffer.commit
 *      and when the work actually starts on the GPU.  MPSTemporaryImage are for short lived storage within the time 
 *      period of the execution of a single MTLCommandBuffer. You can not read or write to a MPSTemporaryImage using the CPU.
 *      Generally, they should be created as needed and thrown away promptly.  Persistent objects should not retain them.
 *      Please be sure to understand the use of the MPSTemporaryImage.readCount.
 *  - Because MPS encodes its work in place in your MTLCommandBuffer, you always have the option to insert your own
 *      code in between MPSCNNKernels as a Metal shader for tasks not covered by MPS. You need not use MPS for everything.
 *
 *
 *  @subsection subsection_RNN     Recurrent Neural Networks
 *
 *  @subsection subsection_matrix_primitives     Matrix Primitives
 *  MPS provides kernels for performing common linear algebra operations.  These kernels operate on MPSMatrix objects.  MPSMatrix
 *  objects are constructed from MTLBuffer objects and represent matrices which serve as inputs and outputs of MPSMatrix
 *  kernels.
 *
 *  MPSMatrix objects allow the data in a MTLBuffer to be interpreted as a two-dimensional array or, a set of two-dimensional arrays.
 *  This is done by associating length and layout parameters with the user-supplied MTLBuffer.  These parameters are encapsulated as
 *  an MPSMatrixDescriptor object and are used, along with the user-supplied MTLBuffer, to construct an MPSMatrix.
 *
 *  Two initialization methods are provided for MPSMatrixDescriptor objects:
 *
 *      +[MPSMatrixDescriptor matrixDescriptorWithRows:columns:rowBytes:dataType]  and
 *      +[MPSMatrixDescriptor matrixDescriptorWithRows:columns:matrices:rowBytes:matrixBytes:dataType]
 *
 *  'matrices' and 'matrixBytes' are used when the data will represent a set of multiple matrices with identical layouts.  Initializing
 *  an MPSMatrixDescriptor object without these parameters constructs a descriptor to be used when the data will represent a single
 *  matrix.  The size of a matrix is specified using the 'rows' and 'columns' arguments and indicate the number of rows and columns
 *  in the matrix respectively.  'rowBytes' and 'matrixBytes' represent the stride, in bytes, between consecutive rows and matrices
 *  respectively.  'dataType' is a parameter of type MPSDataType and specifies the type of the provided data.
 *
 *  Notes:
 *      'rowBytes' must be a multiple of the size, in bytes, of a single data element.  'matrixBytes' must be a multiple of the value
 *      of 'rowBytes'.
 *      The value of 'rowBytes' can also have an impact on the performance of kernels which use the resulting MPSMatrix object; the
 *      convenience method, +[MPSMatrixDescriptor rowBytesForColumns:dataType], can be used to query a performant value which may then
 *      be used to initialize the data.
 *
 *  MPSMatrix objects are initialized using MPSMatrixDescriptor objects, specifying the layout, and MTLBuffer objects, containing the
 *  data.  Some MPS kernels operate on one-dimensional arrays of data, these are known as MPSVector objects and are initialized in a
 *  manner analogous to MPSMatrix objects.
 *
 *  The following kernels allow performing linear algebra operations using MPSMatrix/MPSVector objects:
 *
 *      MPSMatrixMultiplication         <MPSMatrix/MPSMatrixMultiplication.h>       Generalized matrix-matrix multiplication.
 *      MPSMatrixVectorMultiplication   <MPSMatrix/MPSMatrixMultiplication.h>       Generalized matrix-vector multiplication.
 *      MPSMatrixSolveTriangular        <MPSMatrix/MPSMatrixSolve.h>                Solve a system of equations using a triangular coefficient matrix.
 *      MPSMatrixSolveLU                <MPSMatrix/MPSMatrixSolve.h>                Solve a system of equations using an LU factorization.
 *      MPSMatrixSolveCholesky          <MPSMatrix/MPSMatrixSolve.h>                Solve a system of equations using a Cholesky factorization.
 *      MPSMatrixDecompositionLU        <MPSMatrix/MPSMatrixDecomposition.h>        Perform an LU decomposition of a matrix.
 *      MPSMatrixDecompositionCholesky  <MPSMatrix/MPSMatrixDecomposition.h>        Perform a Cholesky decomposition of a matrix.
 *
 *  MPSMatrix kernels allow operations on sub-regions of the data referenced by an MPSMatrix object.  This is done through offset and
 *  batching properties of the kernel.  For example, MPSMatrixUnaryKernel kernels have the properties 'sourceMatrixOrigin' and
 *  'resultMatrixOrigin' to indicate where in a given matrix to begin reading and writing data respectively.  The properties 'batchStart' and
 *  'batchSize' also allow the kernel to reference only a subset of the provided matrices.
 *
 *  @section  section_validation    MPS API validation
 *  MPS uses the same API validation layer that Metal uses to alert you to API mistakes while
 *  you are developing your code. While this option is turned on (Xcode: Edit Scheme: options: Metal API Validation),
 *  common programming errors will either trigger an assert or send a warning to the the debug log. Except
 *  in the case of serious errors, little or no spew should arrive in the console under standard usage. 
 *  You can also try the MPSKernel.options parameter MPSKernelOptionsSkipAPIValidation to skip most of this checking.
 *  The flag may also lead to small reductions in CPU cost.
 *
 *  Note: where APIs are tagged nonnull, MPS expects that the value is not NULL. The validation layer
 *        may do some checking and assert. If you turn that off, then undefined behavior is the result of 
 *        passing nil, and your application will likely be terminated.
 *
 *
 *  @section  section_usage         How to Add MetalPerformanceShaders.framework to your project
 *
 *  Xcode:
 *
 *      1.  Click project file at left, then appropriate target, then select Build Phases.
 *      2.  Open the "Link Binary With Libraries" disclosure triangle
 *      3.  Click the [+] button in the "Link Binary With Libraries" view to add a new library
 *      4.  Select MetalPerformanceShaders.framework from the list.
 *      5.  Click the Add button.
 *
 *  Command Line:
 *
 *      clang  -framework MetalPerformanceShaders    file.c -o file.o
 *
 *  @section  section_support   How to Determine if MPS Works on Your Device
 *  To test whether MPS works on your device, you may call MPSSupportsMTLDevice(id<MTLDevice>).
 *  It will return YES if the device is supported.
 *
 *  @section  section_inplace   In Place Operation
 *  Some MPS filters can operate in place. In-place operation means that the
 *  same texture is used to hold both the input image and the result image. Operating
 *  in place is a great way to save memory, time and energy. You can use a MPS
 *  filter in place using [MPSKernel encodeToCommandBuffer:inPlaceTexture:copyAllocator:].
 *
 *  Unfortunately, it is not always possible for MPS filters to run in place.
 *  Whether a particular MPSKernel can operate in place can vary according to the
 *  hardware it is running on, the operating system version and the parameters and 
 *  properties passed to it. You may not assume that because a MPSKernel works in place
 *  today on a particular device that it will continue to do so in the future. 
 *
 *  To simplify error handling with failed in-place operation, [MPSKernel 
 *  encodeToCommandBuffer:inPlaceTexture:fallbackCopyAllocator:] takes an optional MPSCopyAllocator
 *  parameter. It is used to create a new texture when in-place operation is not possible 
 *  so as to allow the operation to proceed out of place in a reliable fashion instead. 
 *  (When this happens the input texture is released and replaced with a new texture.) 
 *  To make use of this feature, you will need to write a MPSCopyAllocator block.
 *
 *  @subsection subsection_micopyallocator  MPSCopyAllocator
 *  Some MPSKernel objects may not be able to operate in place. When that occurs, and in-place
 *  operation is requested, MPS will call back to this block to get a new texture
 *  to overwrite instead. To avoid spending long periods of time allocating pages to back the
 *  MTLTexture, the block should attempt to reuse textures. The texture returned from the
 *  MPSCopyAllocator will be returned instead of the sourceTexture from the MPSKernel method
 *  on return. Here is a minimal MPSCopyAllocator implementation:
 *  @code
 *  // A MPSCopyAllocator to handle cases where in-place operation fails.
 *  MPSCopyAllocator myAllocator = ^id <MTLTexture>( MPSKernel * __nonnull filter,
 *                                                  __nonnull id <MTLCommandBuffer> cmdBuf,
 *                                                  __nonnull id <MTLTexture> sourceTexture)
 *  {
 *      MTLPixelFormat format = sourceTexture.pixelFormat;  // FIXME: is this format writable?
 *      MTLTextureDescriptor *d = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat: format
 *                                   width: sourceTexture.width
 *                                  height: sourceTexture.height
 *                               mipmapped: NO];
 *      d.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;
 *
 *      //FIXME: Allocating a new texture each time is slow. They take up to 1 ms each.
 *      //       There are not too many milliseconds in a video frame! You can recycle
 *      //       old textures (or MTLBuffers and make textures from them) and reuse
 *      //       the memory here.
 *      id <MTLTexture> result = [cmdBuf.device newTextureWithDescriptor: d];
 *
 *      // FIXME: If there is any metadata associated with sourceTexture such as colorspace
 *      //        information, MTLResource.label, MTLResource.cpuCacheMode mode,
 *      //        MTLResource.MTLPurgeableState, etc., it may need to be similarly associated
 *      //        with the new texture, to avoid losing your metadata.
 *
 *      // FIXME: If filter.clipRect doesn't cover the entire image, you may need to copy
 *      //        pixels from sourceTexture to result or regions of result will be
 *      //        uninitialized. You can make a MTLCommandEncoder to encode work on the
 *      //        MTLCommandBuffer here to do that work, if necessary. It will be scheduled
 *      //        to run immediately before the MPSKernel work. Do not call
 *      //        [MTLCommandBuffer enqueue/commit/waitUntilCompleted/waitUntilScheduled]
 *      //        in the MPSCopyAllocator block. Make sure to call -endEncoding on the
 *      //        MTLCommandEncoder so that the MTLCommandBuffer has no active encoder
 *      //        before returning.
 *
 *      // CAUTION: The next command placed on the MTLCommandBuffer after the MPSCopyAllocator
 *      //          returns is almost assuredly going to be encoded with a MTLComputeCommandEncoder.
 *      //          Creating any other type of encoder in the MPSCopyAllocator will probably cost
 *      //          an additional 0.5 ms of both CPU _AND_ GPU time (or more!) due to a double
 *      //          mode switch penalty.
 *
 *      return result;
 *      // d is autoreleased
 *  };
 *  @endcode
 *
 *       filter          A valid pointer to the MPSKernel that is calling the MPSCopyAllocator. From
 *                       it you can get the clipRect of the intended operation.
 *       cmdBuf          A valid MTLCommandBuffer. It can be used to obtain the device against
 *                       which to allocate the new texture. You may also enqueue operations on
 *                       the commandBuffer to initialize the texture. You may not submit, enqueue
 *                       or wait for completion of the command buffer.
 *       sourceTexture   The texture that is providing the source image for the filter. You may
 *                       wish to copy its size and MTLPixelFormat for the new texture, but it is
 *                       not requred.
 *
 *       return          A new valid MTLTexture to use as the destination for the MPSKernel.
 *                       The format of the returned texture does not need to match sourceTexture.
 *
 *  @section  section_mpsnngraph   The MPSNNGraph
 *  New for macOS 10.13, iOS/tvOS 11 is a higher level graph API, intended to simplify the creation of
 *  neural networks. The graph is a network of MPSNNFilterNodes, MPSNNImageNodes and  MPSNNStateNodes. 
 *  MPSNNImageNodes represent MPSImages or MPSTemporaryImages. MPSNNFilterNodes represent MPSCNNKernel
 *  objects -- each of the lower level MPSCNNKernel subclasses has a sister object that is a
 *  subclass of the MPSNNFilterNode. Finally, MPSNNStateNodes stand in for MPSState objects. 
 *
 *  MPSState objects are also new for macOS 10.13, iOS/tvOS 11. They stand in for bits of opaque state that
 *  need to be handed  between filter nodes.  For example, a MPSCNNConvolutionTranspose filter may need to
 *  know the original size of the filter passed into the corresponding MPSCNNConvolution node farther up the
 *  tree. There is a corresponding MPSCNNConvolutionGradientState object that tracks this information. You will
 *  encounter state objects only infrequently. Most graphs are made up of images and filters.
 *
 *  To represent a graph, one usually first creates a MPSNNImageNode. This represents the input image or
 *  tensor into the graph. Next one creates a the first filter node to process that input image. For example,
 *  we may make a MPSCNNNeuronLinearNode to normalize the data before the rest of the graph sees it. (y = 2x-1)
 *  Then, we can add our first convolution in the graph.
 *  @code
 *      //  Graph:   [Image] -> [Linear neuron filter] -> (result image) -> [convolution filter] -> (result image)...
 *      MPSNNImageNode *startNode = [MPSImageNode nodeWithHandle: nil];
 *      MPSCNNNeuronLinearNode *norm = [MPSCNNNeuronLinearNode nodeWithSource: startNode a: 2.0f b: -1.0f ];
 *      MPSCNNConvolutionNode *c = [MPSCNNConvolutionNode nodeWithSource: norm.resultImage
 *                                                               weights: [[MyWeights alloc] initWithPath: "weights1.dat"]];
 *      ...
 *  @endcode
 *  There are some features to notice about each object. First of all, each image node can have a handle 
 *  associated with it. The handle is your object that you write. It should conform to the <MPSHandle>
 *  protocol, which specifies that the object should have a label and conform to NSSecureCoding. (The MTLTexture
 *  does have a label property but doesn't conform to NSSecureCoding.) NSSecureCoding is used when you
 *  save the graph to disk using a NSCoder. It isn't used otherwise. You can use a MTLResource here if 
 *  you don't plan to save the graph to disk.  What is the handle for?  When the MPSNNGraph object is 
 *  constructed -- the MPSNNGraph takes the network of filter, state and image nodes and rolls it into
 *  an object that can actually encode work to a MTLCommandBuffer -- the graph object will traverse the
 *  graph from back to front and determine which image nodes are not produced by filters in the graph.
 *  These, it will inteprety to be graph input images.  There may be input states too. When it does so, 
 *  it will represent these image and state nodes as the handles you attach to them. Therefore, the handles
 *  probably should be objects of your own making that refer back to your own data structures that identify
 *  various images that you know about. 
 *
 *  Continuing on to the neuron filter, which we are using to just take the usual image [0,1] range and stretch
 *  to [-1,1] before the rest of the graph sees it, we see we can pass in the linear filter parameters when constructing
 *  it here. All filter nodes also produce a result image. This is used as the argument when constructing the
 *  convolution filter node next, to show that the product of the neuron filter is the input to the convolution
 *  filter.
 *
 *  The convolution object constructor also takes a weights object. The weights object is an object that you write
 *  that conforms to the MPSCNNConvolutionDataSource protocol. MPS does not provide an object that conforms
 *  to this protocol, though you can see some examples in sample code that use this interface.  The convolution
 *  data source object is designed to provide for deferred loading of convolution weights. Convolution weights
 *  can be large. In aggregate, the storage for all the weights in the MPSNNGraph, plus the storage for your
 *  copy of them might start to approach the storage limits of the machine for larger graphs. In order to lessen
 *  this impact, the convolution weights are unpacked for each convolution in turn and then purged from memory
 *  so that only the single MPSNNGraph copy remains.  This happens when the MPSCNNConvolutionDataSource load
 *  and purge methods are called. You should not load the weights until -load is called. (You probably should
 *  however verify that the file, if any, is there and is well formed in the object -init method.) When -purge 
 *  is called, you should release any bulky storage that the object owns and and make the object as light weight
 *  as is reasonable. The MPSCNNConvolutionDataSource.descriptor may include a neuron filter operation.
 *
 *  Other object types should be straightforward. 
 *
 *  @section  subsection_mpsnngraph_usage   MPSNNGraph usage
 *  Once the network of MPSNNFilterNodes, MPSNNImageNodes and MPSNNStateNodes is created, the next
 *  step is to identify the MPSNNImageNode that contains the result of your graph -- typically, this
 *  is the last one you made -- and make a MPSNNGraph with it:
 *  @code
 *      MPSNNGraph *graph = [[MPSNNGraph alloc] initWithDevice: mtlDevice
 *                                                 resultImage: resultImage];
 *  @endcode
 *  If graph creation fails, nil will be returned here. When it is constructed, the graph iterates over
 *  the network of nodes, starting at the result image and working backwards. Any MPSNNImageNodes and states 
 *  that are used that are not created by a MPSNNFilterNode are interpreted to be graph inputs. The
 *  identity of these are given by the MPSNNGraph.sourceImageHandles and MPSNNGraph.sourceStateHandles. 
 *  Each handle is your object that refers back to a particular image or state node. The order of the handles
 *  matches the order of the images or states that should be passed to the [MPSNNGraph encodeToCommandBuffer:...]
 *  call. Similarly, you can get the identity of any intermediate images that you requested to see (See
 *  MPSNNImageNode.exportFromGraph property) and the identity of any result MPSStates that are produced
 *  by the graph that are not used.   The graph has a destinationImageAllocator that overrides the 
 *  MPSNNImageNode.destinationImageAllocator. (see subsection MPSNNGraph intermediate image allocation)
 *  Typically, this serves to make a default temporary image into a normal image, as a convenience.
 *
 *  When you are ready to encode a graph to a command buffer, the operation follows as per much of the 
 *  rest of MPS. 
 *  @code
 *      id <MTLDevice> mtlDevice = MTLCreateSystemDefaultDevice();
 *      id <MTLCommandQueue> mtlCommandQueue.commandBuffer = mtlDevice.newCommandQueue;
 *      id <MTLCommandBuffer> cmdBuf = mtlCommandQueue.commandBuffer;
 *      MPSImage *inputImage = [[MPSImage alloc] initWithDevice: mtlDevice imageDescriptor: myDescriptor];
 *      // put some data into the input image here. See MTLTexture.replaceBytes...
 *      MPSImage * result = [myGraph encodeToCommandBuffer: cmdBuf sourceImages: @[inputImage] ];
 *      [cmdBuf addCompletedHandler: ^(id <MTLCommandBuffer> buf){
 *            // Notify your app that the work is done and the values in result
 *            // are ready for inspection.
 *       }];
 *      [cmdBuf commit];
 *
 *      // While we are working on that, encode something else
 *      id <MTLCommandBuffer> cmdBuf2 = mtlCommandQueue.commandBuffer;
 *      MPSImage * result2 = [myGraph encodeToCommandBuffer: cmdBuf2 sourceImages: @[inputImage2] ];
 *      [cmdBuf2 addCompletedHandler: ^(id <MTLCommandBuffer> buf){
 *            // Notify your app that the work is done and the values in result2
 *            // are ready for inspection.
 *       }];
 *      [cmdBuf2 commit];
 *      ...
 *  @endcode
 *  The extra synchronization from [id <MTLCommandBuffer> waitForCompletion] should be avoided. It can
 *  be exceptionally costly because the wait for new work to appear allows the GPU clock to spin down.
 *  Factor of two or more performance increases are common with -addCompletedHandler:.
 *
 *  A graph can also be encoded using the higher level -[MPSNNGraph executeAsyncWithSourceImages:completionHandler:]
 *  which requires minimal experience with Metal. Assuming you have already gotten a list of MPSImages as input
 *  to your graph (typically one), you may use that instead:
 *
 *  @code
 *      MPSImage * result = [k[0] executeAsyncWithSourceImages: @[image]
 *                                           completionHandler: ^(MPSImage * __nullable i, NSError * __nullable error ){
 *          if( error)
 *              MyLogError("Error: -computeAsyncWithSourceImages:completionHandler: failed: %s\n\t",
 *                   [error.localizedDescription cStringUsingEncoding: NSASCIIStringEncoding],
 *                   [error.localizedFailureReason cStringUsingEncoding: NSASCIIStringEncoding]);
 *
 *          MyProcessResult(i);
 *      }];
 *  @endcode
 *  The image returned directly from the left hand side of -executeAsyncWithSourceImages:completionHandler:
 *  and passed into the completion hander are the same. The contents of the image will be valid once the
 *  completion handler is called.
 *
 *  @section  subsection_mpsnngraph_sizing   MPSNNGraph intermediate image sizing and centering
 *  The MPSNNGraph will automatically size and center the intermediate images that appear in the graph.
 *  However, different neural network frameworks do so differently. In addition, some filters may 
 *  at times operate on only valid pixels in the source image, whereas others may "look beyond the
 *  edges" so as to keep the result image size the same as the input. Occasionally some filters will
 *  want to produce results for which any input is valid. Perhaps some want to behave in between. Torch
 *  has some particularly inventive edging policies for pooling that have valid invalid regions and 
 *  invalid invalid regions beyond the edges of the image.
 *
 *  Whatever the behavior, you will use the MPSNNFilter.paddingPolicy property to configure behavior.
 *  In its simplest form, a paddingPolicy is a object (possibly written by you, though MPS provides some)
 *  that conforms to the MPSNNPadding protocol. It should at minimum provide a padding method, which codes
 *  for common methods to size the result image, how to center it on the input image and where to place
 *  the remainder in cases where the image size isn't exactly divisible by the stride. This is a bitfield.
 *  You can use:
 *  @code
 *      [MPSNNDefaultPadding paddingWithMethod: MPSNNPaddingMethodAlign... | MPSNNPaddingMethodAddRemainderTo...
 *                                              MPSNNPaddingMethodSize... ];
 *  @endcode
 *  To quickly configure one of these. The filters also have a default padding policy, which may be
 *  appropriate most of the time.  
 *
 *  Occasionally, something fancy needs to be done. In that case, the padding policy should set the 
 *  MPSNNPaddingMethodCustom bit and implement the optional destinationImageDescriptorForSourceImages:
 *  sourceStates:forKernel:suggestedDescriptor: method. The MPSNNGraph will use the MPSNNPadding.paddingMethod
 *  to generate an initial guess for the configuration of the MPSCNNKernel.offset and the size and formatting
 *  of the result image and hand that to you in the form of a MPSImageDescriptor. You can modify the descriptor
 *  or the kernel (also passed to you) in your custom destinationImageDescriptorForSourceImages:sourceStates:
 *  forKernel:suggestedDescriptor: method, or just ignore it and make a new descriptor.
 *
 *  @section  subsection_mpsnngraph_image_allocation   MPSNNGraph intermediate image allocation
 *  Typically the graph will make MPSTemporaryImages for these, based on the MPSImageDescriptor obtained
 *  from the padding policy. Temporary images alias one another and can be used to save a lot of memory,
 *  in the same way that malloc saves memory in your application by allowing you to reserve memory for 
 *  a time, use it, then free it for reuse for something else. Ideally, most of the storage in your graph
 *  should be temporary images.
 *
 *  Because temporary images don't (shouldn't) last long, and can't be read by the CPU, some images
 *  probably can't be temporary. By default, the final image returned from the graph is not temporary.
 *  (See MPSNNGraph.destinationImageAllocator to adjust).  Also, you may request that certain intermediate
 *  images be non-temporary so that you can access their contents from outside the graph using the
 *  MPSNNImageNode.exportFromGraph property. 
 *
 *  Temporary images often take up almost no additional memory. Regular images always do.  Some large graphs will only
 *  be able to run using temporary memory, as regular images would overwhelm the machine. Even if you allocate
 *  all your images up front and reuse them over and over, you will still very likely use much more memory with
 *  regular images, than if you just allocate temporary images as needed. Because temporary images do not
 *  generally allocate large amounts of storage, they are much cheaper and faster to use.
 *
 *  What kind of image is created after each filter node can be adjusted using the MPSNNImageNode.imageAllocator 
 *  property.  Two standard allocators are provided as MPSImage.defaultAllocator and MPSTemporaryImage.defaultAllocator.
 *  You may of course write your own. This might be necessary for example if you wish to maintain your own 
 *  MTLHeap and allocate from it.
 *
 *  @section  subsection_mpsnngraph_debugging   MPSNNGraph debugging tips
 *  In typical usage, some refinement, especially of padding policies, may be required to get the expected answer
 *  from MPS. If the result image is the wrong size, padding is typically the problem. When the answers are incorrect,
 *  the MPSCNNKernel.offset or other property may be incorrectly configured at some stage.  As the graph is generated 
 *  starting from an output image node, you may create other graphs starting at any image node within the graph. 
 *  This will give you a view into the result produced from each intermediate layer with a minimum of fuss.  In 
 *  addition, the usual NSObject -debugDescription method is available to inspect objects to make sure they conform 
 *  to expectation.
 *
 *  Note that certain operations such as neuron filters that follow convolution filters and image concatenation
 *  may be optimized away by the MPSNNGraph when it is constructed. The convolution can do neuron operations as
 *  part of its operation.  Concatenation is best done by writing the result of earlier filter passes in the right
 *  place using MPSCNNKernel.destinationFeatureChannelOffset rather than by adding an extra copy. Other optimizations 
 *  may be added as framework capabilities improve.
 *
 *  @section  section_samplecode   Sample Image Processing Example
 *      @code
 *       #import <MetalPerformanceShaders/MetalPerformanceShaders.h>
 *
 *       // Blur the input texture (in place if possible) on MTLCommandQueue q, and return the new texture.
 *       // This is a trivial example. It is not necessary or necessarily advised to enqueue a MPSKernel on
 *       // its own MTLCommandBuffer  or using its own MTLComputeCommandEncoder. Group work together.
 *       //
 *       // Here we assume that you have already gotten a MTLDevice using MTLCreateSystemDefaultDevice() or
 *       // MTLCopyAllDevices(), used it to create a MTLCommandQueue with MTLDevice.newCommandQueue, and 
 *       // similarly made textures with the device as needed.
 *       void  MyBlurTextureInPlace( id <MTLTexture> __strong *inTexture, float blurRadius, id <MTLCommandQueue> q)
 *       {
 *          // Create "the usual Metal objects".
 *          // MPS does not need a dedicated MTLCommandBuffer or MTLComputeCommandEncoder. 
 *          // This is a trivial example. You should reuse the MTL objects you already have, if you have them.
 *          id <MTLDevice> device = q.device;
 *          id <MTLCommandBuffer> buffer = [q commandBuffer];
 *
 *          // Create a MPS filter.
 *          MPSImageGaussianBlur *blur = [[MPSImageGaussianBlur alloc] initWithDevice: device];
 *          if( nil == blur )
 *              MyHandleError(kOutOfMemory);
 *
 *          // Set all MPSKernel properties to taste.
 *          blur.sigma = blurRadius;
 *          // defaults are okay here for other MPSKernel properties. (clipRect, origin, edgeMode)
 *
 *          // Attempt to do the work in place.  Since we provided a copyAllocator as an out-of-place
 *          // fallback, we don't need to check to see if it succeeded or not.
 *          [ blur encodeToCommandBuffer: commandBuffer
 *                        inPlaceTexture: inTexture         // may replace *inTexture
 *                         copyAllocator: myAllocator ];    // See MPSCopyAllocator definition for a sample myAllocator
 *          [ blur release];
 *
 *          // the usual metal enqueue process
 *          [buffer waitUntilCompleted];    // slow!  Try enqueing more work on this or the next 
 *                                          // command buffer instead of waiting every time.
 *
 *          return result;
 *      }
 *
 *      @endcode
 *
 *  @section section_tuning     MPS Tuning Hints
 *  MPS has been tuned for excellent performance across a diversity of devices
 *  and filter parameters. The tuning process focuses on minimizing both CPU and GPU
 *  latency for back to back calls on the same MTLCommmandBuffer.  It is possible,
 *  however, to inadvertently undo this optimization effort by introducing costly 
 *  operations into the pipeline around the MPS filter, leading to
 *  disappointing overall results.
 *
 *  Here are some elements of good practice to avoid common pitfalls:
 *
 *  -#  Don't wait for results to complete before enqueuing more work.
 *  There can be a significant delay (up to 2.5 ms) just to get
 *  an empty MTLCommandBuffer through the pipeline to where
 *  [MTLCommandBuffer  waitUntilCompleted] returns. Instead,
 *  start encoding the next command buffer(s) while you wait
 *  for the first one to complete. Enqueue them too, so they can
 *  start immediately after the previous one exits the GPU. Don't
 *  wait for the CPU kernel to notice the first command buffer
 *  is done and start taking it apart and eventually make a callback
 *  to userland before beginning work on encoding the next one.
 *  By allowing the CPU and GPU to work concurrently in this way,
 *  throughput can be enhanced by up to a factor of ten.
 *
 *  -#  There is a large cost to allocating buffers and textures.
 *  The cost can swamp the CPU, preventing you from keeping
 *  the GPU busy. Try to preallocate and reuse MTLResource
 *  objects as much as possible. The MPSTemporaryImage may be
 *  used instead for short-lived dynamic allocations.
 *
 *  -#  There is a cost to switching between render and compute
 *  encoders. Each time a new render encoder is used, there
 *  can be a substantial GPU mode switch cost that may
 *  undermine your throughput. To avoid the cost, try to
 *  batch compute work together. Since making a new MTLCommandBuffer
 *  forces you to make a new MTLCommandEncoder too, try to
 *  do more work with fewer MTLCommandBuffers.
 *
 *  -#  On currently available iOS devices we find that for some 
 *  image operations, particularly those involving multiple passes -
 *  for example, if you are chaining multiple MPS image filters
 *  together — performance can be improved by up a factor of two 
 *  by breaking the work into tiles about 512 kB in size. Use
 *  -sourceRegionForDestinationSize: to find the MPSRegion needed
 *  for each tile.
 *
 *  @section using_graph_for_training   Using MPSNNGraph for Training
 *  This version of MPSNNGraph uses an explicit node structure to graph out
 *  the training process. That is, you must build out the training segment
 *  of the graph manually in addition to the inference passes. MPS doesn't do
 *  this automatically. However, MPS will provide some help along the way.
 *
 *  To construct a training graph from a complete graph of inference MPSNNFilterNodes,
 *  append a MPSNNLossNode to calculate the loss function for the inference pass. This
 *  produces the first gradient MPSNNImageNode as its image result node. Next, walk
 *  backwards up the graph adding gradient conjugate operations for each node in the
 *  preceding inference section. For example, for a MPSCNNConvolutionNode, add a
 *  MPSCNNConvolutionGradientNode.  This is most easily done using
 *  [MPSNNFilterNode gradientFilterWithSources:] as you go. The gradient image result
 *  from each MPSNNGradientFilterNode serves as the gradient input to the next one. Most
 *  of the wiring up of image and state nodes will be done automatically for you, along
 *  with the copying of various node properties. You need only mirror the topology
 *  of MPSNNFilterNodes and MPSNNImageNodes with MPSNNGradientFilterNodes and
 *  gradient MPSNNImageNodes on the way back up.
 *
 *  The loss data is passed to the graph at -encode time in a MPSNNLossGradientState
 *  object. As typically there is only one state object passed in to a training graph,
 *  you probably won't need to invest much in MPSHandles to disambiguate inputs here.
 *  The images are passed in as the image argument. Training is generally done in batches
 *  of images, so these will actually be passed in as MPSImageBatch and MPSNNStateBatch
 *  instead. If you want to configure the node corresponding to this MPSNNLossGradientState
 *  object, you can get it from the MPSNNLossNode.
 *
 *  The weight updates are handled through new callbacks into the MPSCNNConvolutionDataSource
 *  and MPSCNNBatchNormalizationDataSource protocols. You will be handed gradients and
 *  old weights and will be expected to calculate the new weights based on these.
 *  You can add a metal command buffer completion callback at this time to save these to
 *  the disk to mark your progress, if you like. Most updates happen on the GPU. 
 *  Typically, your Application will use the same data source for both the MPSNNFilterNode
 *  forward inference pass and the conjugate MPSNNGradientFilterNode pass. The gradient pass
 *  will trigger weight updates. These will be applied to both nodes, provided that they use
 *  the same data source.
 *
 *  If necessary, appropriate -init methods are provided that allow you to insert
 *  differently configured nodes if you want to do something unusual instead. To manually
 *  configure such, each MPSNNGradientFilterNode will consume a MPSNNGradientState node that
 *  records MPSNNFilterNode settings at the time it was run. The MPSNNGradientFilterNode
 *  will also need to see the image node used as input to the inference MPSNNFilterNode.
 *  Finally, there is the input gradient calculated by the previous MPSNNGradientFilterNode
 *  in the chain. These are all passed to the MPSNNGradientFilterNode when it is created.
 *  If there are any configurable properties on the MPSNNFilterNode, these need to be copied
 *  to the MPSNNGradientFilterNode as well. This complexity is handled for you in
 *  [MPSNNFilterNode gradientFilterWithSources:], so it is recommended that when possible,
 *  you use that instead to help make sure everything is wired up correctly.
 *
 *  @section release_notes   MPS Release Notes
 *  @subsection macosX_13_4    macOS X.13.4  iOS/tvOS 11.3
 *  A preview for neural network training support is provided in macOS X.13.4.
 *  It is intended to facilitate MPS adoption by major third party neural
 *  networking frameworks. All interfaces marked macos(10.13.4) should
 *  be considered experimental, subject to change as a result of
 *  feedback from the machine learning community before final release
 *  in a major OS revision. To allow for changes during the comment period,
 *  binary compatibility between experimental releases and the final release
 *  is not guaranteed. To provide feedback, please file bugs against Metal
 *  Performance Shaders / macOS using http://bugreporter.apple.com.  Feedback
 *  in other forums such as social media will likely not be successful in
 *  attracting our attention.
 *
 *  To avoid compile time versioning warnings while using these features on a
 *  macOS X.13 SDK, set MACOSX_DEPLOYMENT_TARGET = 10.13.4 in your
 *  .xcodeproj/project.pbxproj. New features very rarely come in support
 *  updates and the Xcode GUI is not pre-configured to make this easy for you.
 *  As a workaround, set the deployment target using Xcode GUI to some much
 *  older, easily recognized version such as 10.11 to ensure that
 *  MACOSX_DEPLOYMENT_TARGET appears in your project.pbxproj. Then, use a text
 *  editor to adjust the value to 10.13.4 manually.
 *
 *  @subsection macosX_14    macOS X.14  iOS/tvOS 12.0
 *  All macOS 10.13.4 and iOS 11.3 APIs that were previously subject to change
 *  are now fully supported, with the following exception:
 *
 *      MPSKeyedUnarchiver, introduced in macOS 10.13.4, iOS/tvOS 11.3, is a
 *      convenience implementation of the MPSDeviceProvider protocol on top of
 *      NSKeyedUnarchiver. Though the old interfaces are still there, the version
 *      in macOS X.14 and iOS/tvOS 12.0 is not in a practical sense binary compatibile
 *      with the original version of the class. All previous methods of creating or
 *      initializing a MPSKeyedUnarchiver now return nil. NSKeyedUnarchiver deprecated
 *      most of its API in favor of NSSecureCoding. The old MPSKeyedUnarchiver interface
 *      didn't provide enough information to responsibly update to the secure coding
 *      requirements in the new NSKeyedUnarchiver APIs.
 *
 *  New classes:
 *      MPSImageLaplacianPyramid                MPSMatrixCopyToImage
 *      MPSMatrixSoftMaxGradient                MPSMatrixLogSoftMaxGradient
 *      MPSCNNYOLOLoss                          MPSMatrixBatchNormalization
 *      MPSMatrixBatchNormalizationGradient     MPSMatrixFullyConnectedGradient
 *      MPSMatrixNeuronGradient                 MPSNNOptimizerStochasticGradientDescent
 *      MPSNNOptimizerRMSProp                   MPSNNOptimizerAdam
 *      MPSNNReduceFeatureChannelsArgumentMin   MPSNNReduceFeatureChannelsArgumentMax
 *      MPSRNNMatrixTrainingLayer               MPSRaytracer
 *      MPSInstanceAccelerationStructure        MPSTriangleAccelerationStructure
 *
 *  Notable simplification of weight updates during neural network training:
 *      GPU: A number of different methods are now provided to calculate weight updates on the GPU
 *              See MPSNNOptimizers.
 *      CPU: -[MPSNNGraph reloadFromDataSources] allows updating graph weights without
 *              making a new MPSNNGraph.
 *
 *  We spent most of our time tuning neural network training for performance and reduced
 *  memory consumption.
 *
 *  One more thing, we added the MPSRayIntersector subframework for ray tracing.
 */
#endif

#ifdef __cplusplus
}
#endif

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSRayIntersectorTypes.h
/*!
 *  @header MPSRayIntersectorTypes.h
 *  @framework MPSRayIntersector.framework
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *
 *  @discussion MPSRayIntersector type definitions shared between the host and device.
 */

#ifndef MPSRayIntersectorTypes_h
#define MPSRayIntersectorTypes_h

#define MPS_ALWAYS_INLINE __attribute__((always_inline)) inline

#ifdef __METAL_VERSION__
#include <metal_stdlib>

using namespace metal;

typedef packed_float3 MPSPackedFloat3;
#else
#import <Metal/Metal.h>
#import <simd/simd.h>
#import <MPSCore/MPSCoreTypes.h>

typedef struct _MPSPackedFloat3 {
    union {
        struct {
            float x;
            float y;
            float z;
        };
        float elements[3];
    };

#ifdef __cplusplus
    MPS_ALWAYS_INLINE _MPSPackedFloat3()
        : x(0.0f), y(0.0f), z(0.0f)
    {
    }

    MPS_ALWAYS_INLINE _MPSPackedFloat3(float x, float y, float z)
        : x(x), y(y), z(z)
    {
    }

    MPS_ALWAYS_INLINE _MPSPackedFloat3(vector_float3 xyz)
        : x(xyz.x), y(xyz.y), z(xyz.z)
    {
    }

    MPS_ALWAYS_INLINE operator vector_float3() const {
        return (vector_float3){ x, y, z };
    }

    MPS_ALWAYS_INLINE float & operator[](int idx) {
        return elements[idx];
    }

    MPS_ALWAYS_INLINE const float & operator[](int idx) const {
        return elements[idx];
    }
#endif
} MPSPackedFloat3;
#endif

/**
 * @brief An axis aligned bounding box with a min and max point
 */
typedef struct _MPSAxisAlignedBoundingBox {
    /**
     * @brief Minimum point
     */
    vector_float3 min;

    /**
     * @brief Maximum point
     */
    vector_float3 max;

#ifdef __cplusplus
    MPS_ALWAYS_INLINE _MPSAxisAlignedBoundingBox()
        : min(INFINITY),
          max(-INFINITY)
    {
    }

    MPS_ALWAYS_INLINE _MPSAxisAlignedBoundingBox(vector_float3 p)
        : min(p),
          max(p)
    {
    }

    MPS_ALWAYS_INLINE _MPSAxisAlignedBoundingBox(vector_float3 min, vector_float3 max)
        : min(min),
          max(max)
    {
    }
#endif
} MPSAxisAlignedBoundingBox;

/**
 * @brief Represents a 3D ray with an origin and a direction
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Ray origin. The intersection test will be skipped if the origin contains NaNs
     * or infinities.
     */
    vector_float3 origin;

    /**
     * @brief Ray direction. Does not need to be normalized. The intersection test will be
     * skipped if the direction has length zero or contains NaNs or infinities.
     */
    vector_float3 direction;
} MPSRayOriginDirection;

/**
 * @brief Represents a 3D ray with an origin, a direction, and an intersection
 * distance range from the origin
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Ray origin. The intersection test will be skipped if the origin contains NaNs
     * or infinities.
     */
    MPSPackedFloat3 origin;

    /**
     * @brief Minimum intersection distance from the origin along the ray direction. The
     * intersection test will be skipped if the minimum distance is equal to positive
     * infinity or NaN.
     */
    float minDistance;

    /**
     * @brief Ray direction. Does not need to be normalized. The intersection test will be
     * skipped if the direction has length zero or contains NaNs or infinities.
     */
    MPSPackedFloat3 direction;

    /**
     * @brief Maximum intersection distance from the origin along the ray direction. May be
     * infinite. The intersection test will be skipped if the maximum distance is less than
     * zero, NaN, or less than the minimum intersection distance.
     */
    float maxDistance;
} MPSRayOriginMinDistanceDirectionMaxDistance;

/**
 * @brief Represents a 3D ray with an origin, a direction, and a mask to filter out intersections
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Ray origin. The intersection test will be skipped if the origin contains NaNs
     * or infinities.
     */
    MPSPackedFloat3 origin;

    /**
     * @brief Ray mask which is bitwise AND-ed with instance and primitive masks to filter out
     * intersections. The intersection test will be skipped if the mask is zero.
     */
    unsigned int mask;

    /**
     * @brief Ray direction. Does not need to be normalized. The intersection test will be
     * skipped if the direction has length zero or contains NaNs or infinities.
     */
    MPSPackedFloat3 direction;

    /**
     * @brief Maximum intersection distance from the origin along the ray direction. May be
     * infinite. The intersection test will be skipped if the maximum distance is less than
     * zero or NaN.
     */
    float maxDistance;
} MPSRayOriginMaskDirectionMaxDistance;

/**
 * @brief Returned intersection result which contains the distance from the ray origin to the
 * intersection point
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Distance from the ray origin to the intersection point along the ray direction
     * vector such that intersection = ray.origin + ray.direction * distance. Is negative if
     * there is no intersection. If the intersection type is MPSIntersectionTypeAny, is
     * a positive value for a hit or a negative value for a miss.
     */
    float distance;
} MPSIntersectionDistance;

/**
 * @brief Intersection result which contains the distance from the ray origin to the
 * intersection point and the index of the intersected primitive
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Distance from the ray origin to the intersection point along the ray direction
     * vector such that intersection = ray.origin + ray.direction * distance. Is negative if
     * there is no intersection. If the intersection type is MPSIntersectionTypeAny, is
     * a positive value for a hit or a negative value for a miss.
     */
    float distance;

    /**
     * @brief Index of the intersected primitive. Undefined if the ray does not intersect
     * a primitive or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int primitiveIndex;
} MPSIntersectionDistancePrimitiveIndex;

/**
 * @brief Intersection result which contains the distance from the ray origin to the intersection
 * point, the index of the intersected primitive, and the first two barycentric coordinates of
 * the intersection point.
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Distance from the ray origin to the intersection point along the ray direction
     * vector such that intersection = ray.origin + ray.direction * distance. Is negative if
     * there is no intersection. If the intersection type is MPSIntersectionTypeAny, is
     * a positive value for a hit or a negative value for a miss.
     */
    float distance;

    /**
     * @brief Index of the intersected primitive. Undefined if the ray does not intersect
     * a primitive or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int primitiveIndex;

    /**
     * @brief The first two barycentric coordinates U and V of the intersection point. The
     * third coordinate W = 1 - U - V. Undefined if the ray does not intersect a primitive
     * or if the intersection type is MPSIntersectionTypeAny.
     */
    vector_float2 coordinates;
} MPSIntersectionDistancePrimitiveIndexCoordinates;

/**
 * @brief Intersection result which contains the distance from the ray origin to the intersection
 * point, the index of the intersected primitive, and the index of the intersected instance.
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Distance from the ray origin to the intersection point along the ray direction
     * vector such that intersection = ray.origin + ray.direction * distance. Is negative if
     * there is no intersection. If the intersection type is MPSIntersectionTypeAny, is
     * a positive value for a hit or a negative value for a miss.
     */
    float distance;

    /**
     * @brief Index of the intersected primitive. Undefined if the ray does not intersect
     * a primitive or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int primitiveIndex;

    /**
     * @brief Index of the intersected instance. Undefined if the ray does not intersect a
     * primitive, if the acceleration structure is not an instance acceleration structure,
     * or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int instanceIndex;
} MPSIntersectionDistancePrimitiveIndexInstanceIndex;

/**
 * @brief Intersection result which contains the distance from the ray origin to the intersection
 * point, the index of the intersected primitive, the index of the intersected instance, and the
 * first two barycentric coordinates of the intersection point.
 *
 * @discussion This type is available from the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 */
typedef struct {
    /**
     * @brief Distance from the ray origin to the intersection point along the ray direction
     * vector such that intersection = ray.origin + ray.direction * distance. Is negative if
     * there is no intersection. If the intersection type is MPSIntersectionTypeAny, is
     * a positive value for a hit or a negative value for a miss.
     */
    float distance;

    /**
     * @brief Index of the intersected primitive. Undefined if the ray does not intersect
     * a primitive or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int primitiveIndex;

    /**
     * @brief Index of the intersected instance. Undefined if the ray does not intersect a
     * primitive, if the acceleration structure is not an instance acceleration structure,
     * or if the intersection type is MPSIntersectionTypeAny.
     */
    unsigned int instanceIndex;

    /**
     * @brief The first two barycentric coordinates U and V of the intersection point. The
     * third coordinate W = 1 - U - V. Undefined if the ray does not intersect a primitive
     * or if the intersection type is MPSIntersectionTypeAny.
     */
    vector_float2 coordinates;
} MPSIntersectionDistancePrimitiveIndexInstanceIndexCoordinates;

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSAccelerationStructureGroup.h
/*!
 *  @header MPSAccelerationStructureGroup.h
 *  @framework MPSRayIntersector
 *  @description  MPSRayIntersector acceleration structure group interface.
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 */

#ifndef MPSAccelerationStructureGroup_h
#define MPSAccelerationStructureGroup_h

#import <MPSCore/MPSKernel.h>

/**
 * @brief A group of acceleration structures which may be used together in an instance acceleration
 * structure.
 *
 * @discussion All acceleration structures in an instance acceleration structures must be created
 * with the same group, although they do not all need to be used in the same instance acceleration
 * structure. The acceleration structures in a group share internal GPU memory allocations, so
 * the total number and size of acceleration structures that can be created with the same group is
 * limited by the Metal device's buffer size limits. Therefore, do not group acceleration
 * structures unless they are likely to be used in the same instance acceleration structure.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSAccelerationStructureGroup : NSObject

/**
 * @brief The Metal device this acceleration structure group was created with
 */
@property (nonatomic, readonly) id <MTLDevice> _Nonnull device;

- (nonnull instancetype)init NS_UNAVAILABLE;

- (nonnull instancetype)initWithDevice:(id <MTLDevice> _Nonnull)device;

@end

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSAccelerationStructure.h
/*!
 *  @header MPSAccelerationStructure.h
 *  @framework MPSRayIntersector
 *  @description  MPSRayIntersector acceleration structure interface.
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 */

#ifndef MPSAccelerationStructure_h
#define MPSAccelerationStructure_h

#import <MPSCore/MPSCoreTypes.h>
#import <MPSCore/MPSKernel.h>

#import <MPSRayIntersector/MPSRayIntersectorTypes.h>

@class MPSAccelerationStructure;
@class MPSAccelerationStructureGroup;

/**
 * @brief A block of code invoked when an operation on an MPSAccelerationStructure is completed
 */
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
typedef void (^MPSAccelerationStructureCompletionHandler)(MPSAccelerationStructure * _Nullable);

/**
 * @brief Options describing how an acceleration structure will be used
 */
typedef NS_OPTIONS(NSUInteger, MPSAccelerationStructureUsage) {
    /**
     * @brief No usage options specified
     */
    MPSAccelerationStructureUsageNone = 0,

    /**
     * @brief Enable support for refitting the acceleration structure after it has been built.
     * This option may reduce raytracing performance so do not use it unless the acceleration
     * structure will be refit.
     */
    MPSAccelerationStructureUsageRefit = 1,

    /**
     * @brief Option indicating that the acceleration structure will be rebuilt frequently. In this
     * case, the acceleration structure may choose a higher performance but lower quality
     * acceleration structure construction algorithm. This option may reduce raytracing performance
     * performance so do not use it unless reduced acceleration structure build time is
     * worth reduced raytracing performance. This option may be useful if, for example, the user
     * is interactively editing a live view of the scene.
     */
    MPSAccelerationStructureUsageFrequentRebuild = 2,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Possible values of the acceleration structure status property
 */
typedef NS_ENUM(NSUInteger, MPSAccelerationStructureStatus) {
    /**
     * @brief The acceleration structure has not been built yet
     */
    MPSAccelerationStructureStatusUnbuilt = 0,

    /**
     * @brief The acceleration structure has finished building
     */
    MPSAccelerationStructureStatusBuilt = 1
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief A data structure built over geometry used to acceleration ray tracing
 *
 * @discussion Do not use this base class directly. Use one of the derived classes instead.
 * The general pattern for creating an acceleration structure is as follows. First, create the
 * acceleration structure:
 *
 *     @code
 *     MPSTriangleAccelerationStructure *accelerationStructure = nil;
 *     accelerationStructure = [[MPSTriangleAccelerationStructure alloc] initWithDevice:device];
 *     @endcode
 *
 * Then, assign values to the acceleration structure's properties:
 *
 *     @code
 *     accelerationStructure.vertexBuffer = vertexBuffer;
 *     accelerationStructure.triangleCount = triangleCount;
 *     @endcode
 *
 * Finally, the acceleration structure must be built:
 *
 *     @code
 *     [accelerationStructure rebuild];
 *     @endcode
 *
 * The acceleration structure can then be used to encode ray intersection tests with an
 * MPSRayIntersector:
 *
 *     @code
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:0
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:0
 *                                         rayCount:rayCount
 *                            accelerationStructure:accelerationStructure];
 *     @endcode
 *
 * Asynchronous Acceleration Structure Builds: Rebuilding an acceleration structure is an expensive
 * operation. Note that there is also a method to rebuild the acceleration structure asynchronously
 * to avoid blocking the main thread.
 *
 *     @code
 *     [accelerationStructure rebuildWithCompletionHandler:^(MPSAccelerationStructure *accel) {
 *         // Kick off ray intersection work
 *     }];
 *     @endcode
 *
 * Streaming Geometry Updates: It is generally safe to change buffer properties such as the vertex
 * buffer after intersection tests have been encoded into a command buffer, but the contents of
 * those buffers cannot be safely changed by the CPU until the command buffer has finished
 * executing on the GPU. It is also not safe to rebuild the acceleration structure until the
 * command buffer has completed.
 *
 * If the CPU needs to stream geometry updates to the GPU, ensure the vertex and other buffers are
 * double or triple buffered.
 *
 *     @code
 *     #define MAX_ASYNC_OPERATIONS 3
 *
 *     // Initialization:
 *
 *     // Create a semaphore with the maximum number of asynchronous operations in flight
 *     dispatch_semaphore_t asyncOperationSemaphore = dispatch_semaphore_create(MAX_ASYNC_OPERATIONS);
 *
 *     // Create an acceleration structure for each vertex buffer range
 *     NSMutableArray *accelerationStructures = [NSMutableArray array];
 *
 *     NSUInteger vertexBufferLength = sizeof(float3) * vertexCount * MAX_ASYNC_OPERATIONS;
 *     id <MTLBuffer> vertexBuffer = [device newBufferWithLength:vertexBufferLength
 *                                                       options:MTLResourceStorageModeManaged];
 *
 *     for (NSUInteger i = 0; i < MAX_ASYNC_OPERATIONS; i++) {
 *         MPSTriangleAccelerationStructure *accel = nil;
 *         accel = [[MPSTriangleAccelerationStructure alloc] initWithDevice:device];
 *
 *         // Configure acceleration structure
 *         accel.vertexBuffer = vertexBuffer;
 *         accel.vertexBufferOffset = i * sizeof(float3) * vertexCount;
 *
 *         [accelerationStructures addObject:accel];
 *     }
 *
 *     NSUInteger asyncOperationIndex = 0;
 *
 *     // Encode intersection testing:
 *
 *     // Wait until there is a free acceleration structure
 *     dispatch_semaphore_wait(asyncOperationSemaphore, DISPATCH_TIME_FOREVER);
 *
 *     MPSTriangleAccelerationStructure *accel = accelerationStructures[asyncOperationIndex];
 *     asyncOperationIndex = (asyncOperationIndex + 1) % MAX_ASYNC_OPERATIONS;
 *
 *     float3 *vertices = (float3 *)((uint8_t *)vertexBuffer.contents + accel.vertexBufferOffset);
 *     // Update vertices
 *     [vertexBuffer didModifyRange:NSMakeRange(accel.vertexBufferOffset, sizeof(float3) * vertexCount)];
 *
 *     // Rebuild the acceleration structure
 *     [accel rebuild];
 *
 *     // Encode actual intersection work
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:rayBufferOffset
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:intersectionBufferOffset
 *                                         rayCount:rayCount
 *                            accelerationStructure:accel];
 *
 *     // Register a completion handler to run when the GPU finishes executing
 *     [commandBuffer addCompletedHandler:^(id <MTLCommandBuffer> commandBuffer) {
 *         Intersection *intersections = (Intersection *)((uint8_t *)intersectionBuffer.contents +
 *             intersectionBufferOffset);
 *
 *         // Process intersections
 *
 *         // Signal that the acceleration structure is now available for reuse
 *         dispatch_semaphore_signal(asyncOperationSemaphore);
 *     }];
 *
 *     // Commit the command buffer to allow the GPU to start executing
 *     [commandBuffer commit];
 *     @endcode
 *
 * Refitting acceleration structures: If geometry has only moved slightly and not added or removed
 * from the scene, it can be much faster to refit the existing topology of an acceleration
 * structure to the new geometry than to rebuild the acceleration structure from scratch. Refitting
 * can also be pipelined with other GPU work such as intersection testing. If the geometry is
 * transformed entirely on the GPU, it is not necessary to use double or triple buffering. For
 * example:
 *
 *     @code
 *     id <MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
 *
 *     id <MTLComputeCommandEncoder> encoder = [commandBuffer computeCommandEncoder];
 *
 *     [encoder setBuffer:untransformedVertexBuffer offset:0 atIndex:0];
 *
 *     [encoder setBuffer:accelerationStructure.vertexBuffer
 *                 offset:accelerationStructure.vertexBufferOffset
 *                atIndex:1];
 *
 *     [encoder setBuffer:transformationMatrices offset:0 atIndex:2];
 *
 *     [encoder setComputePipelineState:transformVerticesPipeline];
 *
 *     [encoder dispatchThreads:MTLSizeMake(accelerationStructure.triangleCount * 3, 1, 1)
 *        threadsPerThreadgroup:MTLSizeMake(64, 1, 1)];
 *
 *     [encoder endEncoding];
 *
 *     [accelerationStructure encodeRefitToCommandBuffer:commandBuffer];
 *
 *     [commandBuffer commit];
 *     @endcode
 * 
 * Serializing Acceleration Structures: Instead of rebuilding acceleration structures from scratch
 * they can be built offline, serialized, and reloaded at runtime using the NSSecureCoding
 * protocol:
 *
 *     @code
 *     // Build time:
 *     NSError *error = nil;
 *     NSData *data = [NSKeyedArchiver archivedDataWithRootObject:accel
 *                                          requiringSecureCoding:YES
 *                                                          error:&error];
 *        
 *     if (!data)
 *         NSLog(@"Error archiving MPSAccelerationStructure: %@",
 *             error.localizedDescription);
 *
 *     // Runtime:
 *     MPSTriangleAccelerationStructure *accel;
 *     accel = [NSKeyedUnarchiver unarchivedObjectOfClass:[MPSTriangleAccelerationStructure class]
 *                                               fromData:data
 *                                                  error:&error];
 *
 *     if (!accel)
 *         NSLog(@"Error unarchiving MPSAccelerationStructure: %@",
 *             error.localizedDescription);
 *     @endcode
 *
 * Copying Acceleration Structures: Acceleration structures can be copied using the NSCopying
 * protocol, even to a different Metal device. This can be used for multi-GPU raytracing. Buffer
 * properties are not copied to the new acceleration structure. These buffers must instead be
 * copied to the new Metal device and assigned to the new acceleration structure. For example:
 *
 *     @code
 *     MPSTriangleAccelerationStructure *copy = [accelerationStructure copyWithZone:nil
 *                                                                           device:newDevice];
 *
 *     copy.vertexBuffer = [self copyBuffer:accelerationStructure.vertexBuffer
 *                               withDevice:newDevice];
 *     @endcode
 *
 * Performance Guidelines:
 *
 *     - Provide accurate acceleration structure hints: if an acceleration structure does not
 *       require support for refitting, a higher quality construction algorithm can be used.
 *       However, if an acceleration structure must be rebuilt frequently, a lower quality
 *       but higher performance construction algorithm can be used.
 *
 *     - Consider refitting existing acceleration structures rather than rebuilding them from
 *       scratch. This is typically much faster and can result in a reasonably high quality
 *       tree if the geometry has not been modified dramatically. Refitting can also be pipelined
 *       with other GPU work. If objects have been added to or removed from the scene, it is
 *       typically necessary to rebuild the acceleration structure rather than refit it.
 *
 *     - Rebuild acceleration structures asynchronously when possible to avoid blocking the main
 *       thread. Consider presenting a UI indicating that work is happening in the background while
 *       allowing the user to consider interacting with your application.
 *
 *     - If you need to mix intersection testing with acceleration structure builds (e.g. if the
 *       user is interactively editing the scene while rendering or if objects are moving
 *       significantly) consider allocating two independent acceleration structures that refer to
 *       two copies of the scene data. Then, asynchronously rebuild one acceleration structure
 *       while the other one is used for rendering. Once the rebuild has completed, swap the
 *       acceleration structures. The intermediate frames could be filled by refitting the
 *       rendering acceleration structure until the rebuilt acceleration structure is ready.
 *
 *     - When running in Xcode, disable "Enable Backtrace Recording" in your scheme settings.
 *       Enabling this setting can significantly increase acceleration structure build time.
 *
 * Thread Safety: MPSAccelerationStructures are generally not thread safe. Changing properties
 * and rebuilding acceleration structures from multiple threads result in undefined behavior.
 * However, it is safe to encode intersection tests with a single acceleration structure
 * from multiple threads as long as each thread uses its own MPSRayIntersector.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSAccelerationStructure : MPSKernel <NSSecureCoding, NSCopying>

/**
 * @brief The group this acceleration structure was created with
 */
@property (nonatomic, readonly) MPSAccelerationStructureGroup * _Nonnull group;

/**
 * @brief The bounding box fully enclosing the geometry this acceleration structure was built over.
 *
 * @discussion The value of this property is not available until the acceleration structure
 * has finished rebuilding or refitting
 */
@property (nonatomic, readonly) MPSAxisAlignedBoundingBox boundingBox;

/**
 * @brief Status indicating whether the acceleration structure has finished building
 */
@property (nonatomic, readonly) MPSAccelerationStructureStatus status;

/**
 * @brief Acceleration structure usage options. Changes to this property require rebuilding the
 * acceleration structure. Defaults to MPSAccelerationStructureUsageNone.
 */
@property (nonatomic) MPSAccelerationStructureUsage usage;

- (nonnull instancetype)init NS_UNAVAILABLE;

/**
 * @brief Initialize the acceleration structure with a Metal device
 */
- (nonnull instancetype)initWithDevice:(nonnull id <MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/**
 * @brief Initialize the acceleration structure with an NSCoder and a Metal device. Buffer
 * properties such as the vertex buffer, instance buffer, etc. are set to nil. Encode and decode
 * these buffers along with the acceleration structure instead.
 */
- (nullable instancetype)initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/**
 * @brief Initialize the acceleration structure with an acceleration structure group, if the
 * acceleration structure will be used in an instance hierarchy.
 *
 * @discussion The Metal device is determined from the acceleration structure group. All
 * acceleration structures in the instance hierarchy must share the same group.
 */
- (nonnull instancetype)initWithGroup:(MPSAccelerationStructureGroup * _Nonnull)group
      NS_DESIGNATED_INITIALIZER;

/**
 * @brief Initialize the acceleration structure with an NSCoder and an acceleration structure
 * group, if the acceleration structure will be used in an instance hierarchy. All acceleration
 * structures in the instance hierarchy must share the same group. Buffer properties such as the
 * vertex buffer, instance buffer, etc. are set to nil. Encode and decode these buffers along with
 * the acceleration structure instead.
 */
- (nullable instancetype)initWithCoder:(NSCoder * __nonnull)aDecoder
                                 group:(nonnull MPSAccelerationStructureGroup *)group
                                 NS_DESIGNATED_INITIALIZER;

/**
 * @brief Rebuild the acceleration structure
 *
 * @discussion This method must be called before any intersection tests can be scheduled with this
 * acceleration structure. Before calling this method, fill out the properties of the acceleration
 * structure such as vertex buffer, instance buffer, etc. The acceleration structure should be
 * rebuilt when its contents (e.g. vertices in a triangle acceleration structure) have been
 * modified significantly and must be rebuilt when properties such as triangle count,
 * vertex stride, etc. have changed. When the contents of the acceleration structure have only been
 * modified slightly, it may be cheaper to refit the acceleration structure instead.
 *
 * This method blocks until the acceleration structure has been rebuilt. Until the rebuild has
 * completed, the acceleration structure cannot be copied, encoded with NSSecureCoding, rebuilt, or
 * refit. Before this method can be called, any pending GPU writes to the vertex buffer, index
 * buffer, etc. must be completed (and, for managed buffers, synchronized). Any prior intersection
 * tests must also be completed before the acceleration structure can be rebuilt.
 */
- (void)rebuild;

/**
 * @brief Rebuild the acceleration structure asynchronously
 *
 * @discussion This method must be called before any intersection tests can be scheduled with this
 * acceleration structure. Before calling this method, fill out the properties of the acceleration
 * structure such as vertex buffer, instance buffer, etc. The acceleration structure should be
 * rebuilt when its contents (e.g. vertices in a triangle acceleration structure) have been
 * modified significantly and must be rebuilt when properties such as triangle count,
 * vertex stride, etc. have changed. When the contents of the acceleration structure have only been
 * modified slightly, it may be cheaper to refit the acceleration structure instead.
 *
 * Until the rebuild has completed, the acceleration structure cannot be copied, encoded with
 * NSSecureCoding, rebuilt, or refit. Before this method can be called, any pending GPU writes to
 * the vertex buffer, index buffer, etc. must be completed (and, for managed buffers,
 * synchronized). Any prior intersection tests must also be completed before the acceleration
 * structure can be rebuilt.
 */
- (void)rebuildWithCompletionHandler:(nonnull MPSAccelerationStructureCompletionHandler)completionHandler;

/**
 * @brief Refit the existing acceleration structure to new data
 *
 * @discussion This method is used to refit the acceleration structure to new vertex data,
 * index data, instance data, etc. while preserving the existing acceleration structure topology.
 * This is typically much faster than a full rebuild of the acceleration structure. Refitting can
 * also be pipelined with other GPU work such as ray intersection.
 *
 * Until the command buffer has completed, the acceleration structure cannot be copied,
 * encoded with NSSecureCoding, or rebuilt. Changes to properties such as the triangle count or
 * instance might not be reflected. These changes require that the acceleration structure be
 * rebuilt instead. The acceleration structure must be rebuilt at least once before this method can
 * be called.
 */
- (void)encodeRefitToCommandBuffer:(nonnull id <MTLCommandBuffer>)commandBuffer
    MPS_SWIFT_NAME(encodeRefit(commandBuffer:));

/**
 * @brief Create a a copy of this acceleration structure
 *
 * @discussion The acceleration structure may be copied to a different Metal device. Buffer
 * properties of the acceleration structure such as the vertex buffer, instance, buffer, etc. are
 * set to nil. Copy these buffers to new Metal device and assign them to the new acceleration
 * structure instead. Do not copy the acceleration structure until any prior refit or rebuild
 * operations have completed.
 *
 * @param zone   This parameter is ignored. Memory zones are no longer used by Objective-C.
 * @param device New Metal device
 */
- (nonnull instancetype)copyWithZone:(nullable NSZone *)zone
                              device:(nullable id <MTLDevice>)device;

/**
 * @brief Create a a copy of this acceleration structure
 *
 * @discussion The acceleration structure may be copied with a different acceleration structure
 * group. Buffer properties of the acceleration structure such as the vertex buffer, instance
 * buffer, etc. are set to nil. Copy these buffers with the new Metal device and assign them to
 * the new acceleration structure instead. Do not copy the acceleration structure until any prior
 * refit or rebuild operations have completed.
 *
 * @param zone  This parameter is ignored. Memory zones are no longer used by Objective-C.
 * @param group New acceleration structure group
 */
- (nonnull instancetype)copyWithZone:(nullable NSZone *)zone
                               group:(nonnull MPSAccelerationStructureGroup *)group;

/**
 * @brief Encode the acceleration structure with an NSCoder
 *
 * @discussion Buffer properties such as the vertex buffer, index buffer, etc. are not be encoded.
 * Encode and decode these buffers along with the acceleration structure instead. Do not encode
 * the acceleration structure until any prior refit or rebuild operations have completed.
 *
 * @param coder An archiver object
 */
- (void)encodeWithCoder:(NSCoder * __nonnull)coder;

@end

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSInstanceAccelerationStructure.h
/*!
 *  @header MPSInstanceAccelerationStructure.h
 *  @framework MPSRayIntersector
 *  @description  MPSRayIntersector instance acceleration structure interface.
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 */

#ifndef MPSInstanceAccelerationStructure_h
#define MPSInstanceAccelerationStructure_h

#import <MPSRayIntersector/MPSAccelerationStructure.h>

@class MPSAccelerationStructureGroup;
@class MPSTriangleAccelerationStructure;

/**
 * @brief Instance transformation type options
 */
typedef NS_ENUM(NSUInteger, MPSTransformType) {
    /**
     * @brief Instance transformations are represented by a 4x4 column major matrix of 32 bit
     * floats
     */
    MPSTransformTypeFloat4x4 = 0,

    /**
     * @brief All instances have the identity transformation (no transformation). This can be used
     * to compose multiple triangle acceleration structures in an instance acceleration structure
     * without the cost of transforming instances. For example, geometry can be divided into
     * static and dynamic triangle acceleration structures which can be rebuilt and refit
     * independently.
     */
    MPSTransformTypeIdentity = 1
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief An acceleration structure built over instances of other acceleration structures
 *
 * @discussion Instancing can be used to reduce memory usage in scenes that contain many copies
 * of the same object(s) or to combine multiple acceleration structures such as a static and
 * dynamic acceleration structure into a two-level instance hierarchy.
 *
 * The typical pattern for creating an instance acceleration structure is as follows. First,
 * create individual bottom-level acceleration structures. Then assign these acceleration
 * structures to the accelerationStructures property of an instance acceleration structure.
 *
 * All of the acceleration structures in the instance hierarchy must share the same
 * MPSAccelerationStructureGroup. Furthermore, all of the bottom-level acceleration structures
 * must share the same vertex buffer, index buffer, etc. although they may have different offsets
 * within those buffers.
 *
 *     @code
 *     MPSAccelerationStructureGroup *group = nil;
 *     group = [[MPSAccelerationStructureGroup alloc] initWithDevice:device];
 *
 *     MPSInstanceAccelerationStructure *instanceAccel = nil;
 *     instanceAccel = [[MPSInstanceAccelerationStructure alloc] initWithGroup:group];
 *
 *     NSMutableArray *accelerationStructures = [NSMutableArray array];
 *     instanceAccel.accelerationStructures = accelerationStructures;
 *
 *     instanceAccel.instanceCount = instanceCount;
 *
 *     for (ObjectType *objectType in objectTypes) {
 *         MPSTriangleAccelerationStructure *triAccel = nil;
 *         triAccel = [[MPSTriangleAccelerationStructure alloc] initWithGroup:group];
 *
 *         triAccel.vertexBuffer = objectType.vertexBuffer;
 *         triAccel.vertexBufferOffset = objectType.vertexBufferOffset;
 *         triAccel.triangleCount = objectType.triangleCount;
 *
 *         [triAccel rebuild];
 *
 *         [accelerationStructures addObject:triAccel];
 *     }
 *     @endcode
 *
 * Next, create a buffer containing the acceleration structure index for each instance, and
 * another acceleration structure containing the transformation matrix for each instance:
 *
 *     @code
 *     NSUInteger instanceBufferLength = sizeof(uint32_t) * instanceCount;
 *    
 *     id <MTLBuffer> instanceBuffer =
 *         [device newBufferWithLength:instanceBufferLength
 *                             options:MTLResourceStorageModeManaged];
 *    
 *     memcpy(instanceBuffer.contents, instances,
 *         instanceBufferLength);
 *     [instanceBuffer
 *         didModifyRange:NSMakeRange(0, instanceBufferLength)];
 *    
 *     instanceAccel.instanceBuffer = instanceBuffer;
 *
 *     // Similar for transformation matrix buffer
 *     @endcode
 *
 * Finally, rebuild the instance acceleration structure:
 *
 *     @code
 *     [instanceAccel rebuild];
 *     @endcode
 *
 * Refitting and Rebuilding Bottom-Level Acceleration Structures: when a bottom level acceleration
 * structure is rebuild or refit, its' bounding box may change. Therefore, the instance
 * acceleration structure also needs to be rebuilt or refit.
 *
 * Copying and Serializing Instance Acceleration Structures: When an instance acceleration
 * structure is copied or serialized, the bottom level acceleration structures are not copied or
 * serialized. These must be copied or serialized along with the instance acceleration structure
 * and assigned to the new instance acceleration structure. This also applies to buffer properties
 * such as the instance buffer, transformation buffer, etc.
 *
 * Performance Guidelines:
 *
 *     - Use instancing to reduce memory usage: if there are many copies of the same object(s) in
 *       a scene, using instances of the same object can reduce memory usage and acceleration
 *       structure build time. Rebuilding or refitting the top level acceleration structure can
 *       also be much faster than rebuilding a large single level acceleration structure.
 *
 *     - Consider flattening your instance hierarchy into a single acceleration structure if the
 *       increased memory usage and acceleration structure build time are not a concern.
 *       Intersecting a two level acceleration structure can have a significant performance cost so
 *       only use it when necessary. Which technique to use depends on the scene and use case. For
 *       example, in a rendering application, it may be best to use an instance hierarchy for
 *       interactive scene editing and preview and flattening the instance hierarchy for the final
 *       render. For smaller scenes, it may also be sufficient to refit a flattened acceleration
 *       structure rather than rebuilding an instance hierarchy.
 *
 *     - If there is only a single object in the scene, intersect its acceleration structure
 *       directly instead of using an instance hierarchy.
 *
 *     - Consider dividing objects into static and dynamic acceleration structures. If dynamic
 *       objects require the acceleration structure to be rebuilt frequently, create a high quality
 *       static acceleration structure and a lower quality but faster to build dynamic acceleration
 *       structure. These two acceleration structures can then be combined with a two level
 *       acceleration structure. Use MPSTransformTypeIdentity to reduce the overhead of this
 *       technique. Whether this technique is more efficient than rebuilding the entire
 *       acceleration structure depends on the scene.
 *
 * See MPSAccelerationStructure for more information
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSInstanceAccelerationStructure : MPSAccelerationStructure

/**
 * @brief Acceleration structures available for use in this instance acceleration structure. Each
 * instance must provide an index into this array in the instance buffer as well as a
 * transformation matrix in the transform buffer. All acceleration structures must share a single
 * vertex buffer, optional index buffer, and optional mask buffer, though they may have different
 * offsets within each buffer, and all acceleration structures must share the same acceleration
 * structure group. If a triangle acceleration structure is rebuilt or refit, the instance
 * acceleration structure must subsequently be rebuilt or refit.
 */
@property (nonatomic, retain) NSArray <MPSTriangleAccelerationStructure *> * _Nullable accelerationStructures;

/**
 * @brief Buffer containing the 32 bit unsigned integer index into the acceleration structure array
 * for each instance
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable instanceBuffer;

/**
 * @brief Offset, in bytes, into the instance buffer. Defaults to 0 bytes. Must be aligned to 4
 * bytes.
 */
@property (nonatomic) NSUInteger instanceBufferOffset;

/**
 * @brief Buffer containing one column major matrix_float4x4 transformation matrix per instance
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable transformBuffer;

/**
 * @brief Offset, in bytes, into the transform buffer. Defaults to 0 bytes. Must be aligned to the
 * stride of the transform type.
 */
@property (nonatomic) NSUInteger transformBufferOffset;

/**
 * @brief Instance transform type. Defaults to MPSTransformTypeFloat4x4. Changes to this property
 * require rebuilding the acceleration structure.
 */
@property (nonatomic) MPSTransformType transformType;

/**
 * @brief Mask buffer containing one uint32_t mask per instance. May be nil.
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable maskBuffer;

/**
 * @brief Offset, in bytes, into the mask buffer. Defaults to 0 bytes. Must be aligned to 4 bytes.
 */
@property (nonatomic) NSUInteger maskBufferOffset;

/**
 * @brief Number of instances. Changes to this property require rebuilding the acceleration
 * structure.
 */
@property (nonatomic) NSUInteger instanceCount;

@end

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSTriangleAccelerationStructure.h
/*!
 *  @header MPSTriangleAccelerationStructure.h
 *  @framework MPSRayIntersector
 *  @description  MPSRayIntersector triangle acceleration structure interface.
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 */

#ifndef MPSTriangleAccelerationStructure_h
#define MPSTriangleAccelerationStructure_h

#import <MPSRayIntersector/MPSAccelerationStructure.h>

/**
 * @brief An acceleration structure built over triangles
 *
 * @discussion See MPSAccelerationStructure for more information
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSTriangleAccelerationStructure : MPSAccelerationStructure

/**
 * @brief Vertex buffer containing vertex data encoded as three 32 bit floats per vertex. Note
 * that by default each vertex is aligned to the alignment of the vector_float3 type: 16 bytes.
 * This can be changed using the vertexStride property. A vertex buffer must be provided before
 * the acceleration structure is built. Degenerate (zero or negative area) triangles are ignored
 * during acceleration structure construction. This can be used to pad triangle indices if needed.
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable vertexBuffer;

/**
 * @brief Offset, in bytes, into the vertex buffer. Defaults to 0 bytes. Must be aligned to 4
 * bytes.
 */
@property (nonatomic) NSUInteger vertexBufferOffset;

/**
 * @brief Offset, in bytes, between consecutive vertices in the vertex buffer. Defaults to 0 bytes,
 * indicating that the vertices are packed according to the natural alignment of the vector_float3
 * type: 16 bytes.
 *
 * @discussion This can be used to skip past any additional per-vertex data which may be stored
 * alongside the position such as the vertex normal and texture coordinates. Must be a multiple of
 * 4 bytes, and must be at least 12 bytes. Changes to this property require rebuilding the
 * acceleration structure.
 */
@property (nonatomic) NSUInteger vertexStride;

/**
 * @brief Index buffer containing index data encoded as uint32_t per index. Each index references
 * a vertex in the vertex buffer. May be nil.
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable indexBuffer;

/**
 * @brief Index type. Defaults to MPSDataTypeUInt32. Only MPSDataTypeUInt16 and MPSDataTypeUInt32
 * are supported.
 */
@property (nonatomic) MPSDataType indexType;

/**
 * @brief Offset, in bytes, into the index buffer. Defaults to 0 bytes. Must be aligned to a
 * multiple of the index type. Changes to this property require rebuilding the acceleration
 * structure.
 */
@property (nonatomic) NSUInteger indexBufferOffset;

/**
 * @brief Mask buffer containing one uint32_t mask per triangle. May be nil. Otherwise, the mask
 * type must be specified on the MPSRayIntersector with which it is used.
 */
@property (nonatomic, retain) id <MTLBuffer> _Nullable maskBuffer;

/**
 * @brief Offset, in bytes, into the mask buffer. Defaults to 0 bytes. Must be aligned to 4 bytes.
 */
@property (nonatomic) NSUInteger maskBufferOffset;

/**
 * @brief Number of triangles. Changes to this property require rebuilding the acceleration
 * structure.
 */
@property (nonatomic) NSUInteger triangleCount;

@end

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSRayIntersector.framework/Headers/MPSRayIntersector.h
/*!
 *  @header MPSRayIntersector.h
 *  @framework MPSRayIntersector
 *  @description  This header is the public entrypoint into the MPSRayIntersector subframework
 *                of MetalPerformanceShaders.
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 */

#ifndef MPSRayIntersector_h
#define MPSRayIntersector_h

#ifdef __METAL_VERSION__
#import <MPSRayIntersector/MPSRayIntersectorTypes.h>
#endif

#ifndef __METAL_VERSION__
#import <MPSRayIntersector/MPSAccelerationStructureGroup.h>
#import <MPSRayIntersector/MPSTriangleAccelerationStructure.h>
#import <MPSRayIntersector/MPSInstanceAccelerationStructure.h>

/**
 * @brief Options for the MPSRayIntersector intersection type property
 */
typedef NS_ENUM(NSUInteger, MPSIntersectionType) {
    /**
     * @brief Find the closest intersection to the ray's origin along the ray direction. This is
     * potentially slower than MPSIntersectionTypeAny but is well suited to primary visibility
     * rays.
     */
    MPSIntersectionTypeNearest = 0,

    /**
     * @brief Find any intersection along the ray direction. This is potentially faster than
     * MPSIntersectionTypeNearest and is well suited to shadow and occlusion rays.
     */
    MPSIntersectionTypeAny = 1,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Options for the MPSRayIntersector triangle intersection test type property
 */
typedef NS_ENUM(NSUInteger, MPSTriangleIntersectionTestType) {
    /**
     * @brief Use the default ray/triangle intersection test
     */
    MPSTriangleIntersectionTestTypeDefault = 0,

    /**
     * @brief Use a watertight ray/triangle intersection test which avoids gaps along shared
     * triangle edges. Shared vertices may still have gaps. This intersection test may be slower
     * than MPSTriangleIntersectionTestTypeDefault.
     */
    MPSTriangleIntersectionTestTypeWatertight = 1,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Options for the MPSRayIntersector bounding box intersection test type property
 */
typedef NS_ENUM(NSUInteger, MPSBoundingBoxIntersectionTestType) {
    /**
     * @brief Use the default ray/bounding box intersection test
     */
    MPSBoundingBoxIntersectionTestTypeDefault = 0,

    /**
     * @brief The default ray/bounding box intersection test can generate false negatives for
     * axis aligned rays (i.e. rays which have one or more components of their direction set to
     * zero). These rays often do not come up in practice due to perspective projections and
     * randomized ray distributions. However, synthetic ray distributions or orthographic
     * projections can generate these rays. This intersection test properly reports intersections
     * between axis aligned rays and bounding boxes, but may be slower than
     * MPSBoundingBoxIntersectionTestTypeDefault. It may be faster to slightly perturb the ray
     * direction and use the default intersection test type.
     */
    MPSBoundingBoxIntersectionTestTypeAxisAligned = 1,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Options for the MPSRayIntersector ray mask options property
 */
typedef NS_OPTIONS(NSUInteger, MPSRayMaskOptions) {
    /**
     * @brief Disable primitive and instance masks
     */
    MPSRayMaskOptionNone = 0,

    /**
     * @brief Enable primitive masks
     */
    MPSRayMaskOptionPrimitive = 1,

    /**
     * @brief Enable instance masks
     */
    MPSRayMaskOptionInstance = 2,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Options for the MPSRayIntersector ray data type property
 */
typedef NS_ENUM(NSUInteger, MPSRayDataType) {
    /**
     * @brief Use the MPSRayOriginDirection struct type
     */
    MPSRayDataTypeOriginDirection = 0,

    /**
     * @brief Use the MPSRayOriginMinDistanceDirectionMaxDistance struct type
     */
    MPSRayDataTypeOriginMinDistanceDirectionMaxDistance = 1,

    /**
     * @brief Use the MPSRayOriginMaxDistanceDirectionMask struct type
     */
    MPSRayDataTypeOriginMaskDirectionMaxDistance = 2,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @brief Intersection data type options
 */
typedef NS_ENUM(NSUInteger, MPSIntersectionDataType) {
    /**
     * @brief Use the MPSIntersectionDistance struct type
     */
    MPSIntersectionDataTypeDistance = 0,

    /**
     * @brief Use the MPSIntersectionDistancePrimitiveIndex struct type
     */
    MPSIntersectionDataTypeDistancePrimitiveIndex = 1,

    /**
     * @brief Use the MPSIntersectionDistancePrimitiveIndexCoordinates struct type
     */
    MPSIntersectionDataTypeDistancePrimitiveIndexCoordinates = 2,

    /**
     * @brief Use the DistancePrimitiveIndexInstanceIndex struct type
     */
    MPSIntersectionDataTypeDistancePrimitiveIndexInstanceIndex = 3,

    /**
     * @brief Use the DistancePrimitiveIndexInstanceIndexCoordinates struct type
     */
    MPSIntersectionDataTypeDistancePrimitiveIndexInstanceIndexCoordinates = 4,
} MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/**
 * @class MPSRayIntersector
 * @brief Performs intersection tests between rays and the geometry in an MPSAccelerationStructure
 *
 * @discussion An MPSRayIntersector is used to schedule intersection tests between rays and geometry
 * into an MTLCommandBuffer. First, create a raytracer with a Metal device. Then, configure the
 * properties of the raytracer:
 *
 *     @code
 *     id <MTLDevice> device = MTLCreateSystemDefaultDevice();
 *     id <MTLCommandQueue> commandQueue = [device newCommandQueue];
 *  
 *     MPSRayIntersector *raytracer = [[MPSRayIntersector alloc] initWithDevice:device];
 *
 *     // Configure raytracer properties
 *     @endcode
 *
 * Before scheduling intersection tests, an MPSAccelerationStructure must be created. The
 * acceleration structure is built over geometry and is used to accelerate intersection testing.
 * For example, to create a triangle acceleration structure, allocate an
 * MPSTriangleAccelerationStructure object. Then, configure the properties of the acceleration
 * structure. For example, triangle acceleration structures require a vertex buffer and a triangle
 * count:
 *
 *     @code
 *     MPSTriangleAccelerationStructure *accelerationStructure = 
 *         [[MPSTriangleAccelerationStructure alloc] initWithDevice:device];
 *  
 *     accelerationStructure.vertexBuffer = vertexBuffer;
 *     accelerationStructure.triangleCount = triangleCount;
 *     @endcode
 *
 * Acceleration structures must be built at least once before they are used for intersection
 * testing, and must be rebuilt when the geometry changes. Rebuilding an acceleration structure
 * is a time consuming operation, so an asynchronous version of this method is also available.
 *
 *     @code
 *     [accelerationStructure rebuild];
 *     @endcode
 *
 * The raytracer is then used to schedule intersection tests into an MTLCommandBuffer. Rays
 * are provided in batches through a Metal buffer, and intersection results are returned through
 * another Metal buffer in the same order, one intersection per ray.
 *
 * There are several choices of ray data type controlled by the rayDataType property. The default
 * ray data type is MPSRayOriginDirection, which includes just the ray origin direction. The other
 * data types add support for minimum and maximum intersection distances and ray masks. These
 * data types are available in the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header. Additional application specific
 * per-ray data can also be appended to the end of the ray data type using the rayStride property.
 * This data will be ignored by the intersector.
 *
 * If the rays were generated on the CPU:
 *
 *     @code
 *     typedef MPSRayOriginDirection Ray;
 *
 *     // Create a buffer to hold the rays
 *     id <MTLBuffer> rayBuffer = [device newBufferWithLength:sizeof(Ray) * rayCount options:0];
 *
 *     // Copy the rays into the ray buffer
 *     memcpy(rayBuffer.contents, rays, sizeof(Ray) * rayCount);
 *  
 *     // Create a buffer to hold the intersections
 *     id <MTLBuffer> intersectionBuffer = [device newBufferWithLength:sizeof(Intersection) * rayCount
 *                                                             options:0];
 *     @endcode
 *
 * It can be useful to prevent certain rays from participating in intersection testing. For
 * example: rays which have bounced out of the scene in previous intersection tests. It may be
 * more efficient to do this by compacting the ray buffer so that threads with invalid rays are
 * not left idle during intersection testing. However, it can be more convenient to disable the
 * ray in place. This can be done by setting most fields to invalid values. For example, setting
 * the maximum distance to a negative value, setting the mask to zero, setting the direction to
 * the zero vector, etc.
 *
 * Finally, the intersection testing is encoded into an MTLCommandBuffer. There are two
 * intersection types. The "nearest" intersection type returns the closest intersection along each
 * ray. The "any" intersection type returns immediately when the first intersection is found. The
 * "any" intersection type is useful for determining whether a point is visible from another point
 * for, e.g., shadow rays or ambient occlusion rays and is typically much faster than the "nearest"
 * intersection type.
 * 
 *     @code
 *     id <MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
 *  
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:0
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:0
 *                                         rayCount:rayCount
 *                            accelerationStructure:accelerationStructure];
 *
 *     [commandBuffer commit];
 *     @endcode
 *
 * The intersection results are not available until the command buffer has finished executing
 * on the GPU. It is not safe for the CPU to write or read the contents of the ray buffer,
 * intersection buffer, vertex buffer, etc. until the command buffer has finished executing.
 * Use the waitUntilCompleted or addCompletedHandler methods of the MTLCommandBuffer to block
 * the CPU until the GPU has finished executing. Then retrieve the intersection results
 * from the intersection buffer:
 *
 *     @code
 *     typedef MPSIntersectionDistancePrimitiveIndexCoordinates Intersection;
 *     @endcode
 *
 *     @code
 *     [commandBuffer waitUntilCompleted];
 *
 *     Intersection *intersections = (Intersection *)intersectionBuffer.contents;
 *     @endcode
 *
 * There are also several choices of intersection data type controlled by the intersectionDataType
 * property. The default intersection data type is MPSIntersectionDistancePrimitiveIndexCoordinates,
 * which includes the intersection distance, primitive index, and barycentric coordinates. The
 * other data types remove the primitive index or barycentric coordinates, which can be used to
 * reduce the memory and memory bandwidth usage of the intersection buffer. These data types are
 * available in the Metal Shading Language by including the
 * MetalPerformanceShaders/MetalPerformanceShaders.h header.
 *
 * The intersection distance field is positive when an intersection has been found and negative
 * when there is no intersection. When using the "nearest" intersection type, the intersection
 * point is the ray origin plus the ray direction multiplied by the intersection distance. The
 * other fields are not valid if there is no intersection. Only the intersection distance field is
 * valid for the "any" intersection type, and the distance is either a negative or positive value
 * to indicate an intersection or miss. It does not necessarily contain the actual intersection
 * distance when using the "any" intersection type.
 *
 * Asynchronous Raytracing: Copying rays and intersections to and from the CPU is expensive.
 * Furthermore, generating rays and consuming intersections on the CPU causes the CPU and GPU to
 * block each other. If the CPU must generate rays and consume intersections, it is better
 * to add an asynchronous completion handler to the MTLCommandBuffer. The CPU can then proceed
 * to do other useful work and will be notified when the GPU has finished executing. Use double
 * or triple buffered ray and intersection buffers to avoid race conditions such as the CPU
 * overwriting data the GPU may be reading. Then the CPU can safely write to one range of the
 * buffer while the GPU reads from another range of the buffer. Once the GPU is done 
 * executing, the CPU and GPU can advance to the next range of the buffer. This method can be
 * implemented using a completion handler and a semaphore:
 *
 *     @code
 *     #define MAX_ASYNC_OPERATIONS 3
 *
 *     // Initialization:
 *
 *     // Create a semaphore with the maximum number of asynchronous operations in flight
 *     dispatch_semaphore_t asyncOperationSemaphore = dispatch_semaphore_create(MAX_ASYNC_OPERATIONS);
 *
 *     // Create a ray and intersection buffer large enough for the maximum number of operations
 *     id <MTLBuffer> rayBuffer =
 *         [device newBufferWithLength:sizeof(Ray) * rayCount * MAX_ASYNC_OPERATIONS
 *                             options:0];
 *
 *     id <MTLBuffer> intersectionBuffer =
 *         [device newBufferWithLength:sizeof(Intersection) * rayCount * MAX_ASYNC_OPERATIONS
 *                             options:0];
 *
 *     NSUInteger asyncOperationIndex = 0;
 *
 *     // Encode intersection testing:
 *
 *     // Wait until there is a free buffer range
 *     dispatch_semaphore_wait(asyncOperationSemaphore, DISPATCH_TIME_FOREVER);
 *
 *     // Copy rays into ray buffer
 *     NSUInteger rayBufferOffset = sizeof(Ray) * rayCount * asyncOperationIndex;
 *     NSUInteger intersectionBufferOffset = sizeof(Intersection) * rayCount * asyncOperationIndex;
 *
 *     memcpy((uint8_t *)rayBuffer.contents + rayBufferOffset, rays, sizeof(Ray) * rayCount);
 *
 *     // Advance the async operation index
 *     asyncOperationIndex = (asyncOperationIndex + 1) % MAX_ASYNC_OPERATIONS;
 *
 *     // Create a command buffer
 *     id <MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
 *
 *     // Encode actual intersection work
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:rayBufferOffset
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:intersectionBufferOffset
 *                                         rayCount:rayCount
 *                            accelerationStructure:accelerationStructure];
 *
 *     // Register a completion handler to run when the GPU finishes executing
 *     [commandBuffer addCompletedHandler:^(id <MTLCommandBuffer> commandBuffer) {
 *         Intersection *intersections = (Intersection *)((uint8_t *)intersectionBuffer.contents +
 *             intersectionBufferOffset);
 *
 *         // Process intersections
 *
 *         // Signal that the ray and intersection buffer ranges are now available for reuse
 *         dispatch_semaphore_signal(asyncOperationSemaphore);
 *     }];
 *
 *     // Commit the command buffer to allow the GPU to start executing
 *     [commandBuffer commit];
 *     @endcode
 *
 * GPU Driven Raytracing: Pipelining CPU and GPU work with asynchronous raytracing is better than
 * allowing the CPU and GPU block each other, but it is even better to produce rays and consume
 * intersections entirely on the GPU. This avoids the need to copy rays and intersections to and
 * from the GPU and avoids any kind of CPU/GPU synchronization. To do this, encode compute kernels
 * before and after intersection testing. By processing rays in parallel, the compute kernels may
 * also be able to generate and consume rays faster than the CPU. The ray generation kernel
 * typically produces rays according to some camera model, and the intersection consumption kernel
 * typically updates the output buffer or texture according to some shading model.
 *
 * Since the rays and intersections will never leave the GPU, store them in private Metal buffers
 * that are allocated in GPU memory rather than system memory. Because the ray generation,
 * intersection testing, and intersection consumption kernels are pipelined on the GPU, there
 * is no need to double or triple buffer the ray or intersection buffers, which saves memory.
 *
 *     @code
 *     id <MTLBuffer> rayBuffer =
 *         [device newBufferWithLength:sizeof(Ray) * rayCount
 *                             options:MTLResourceStorageModePrivate];
 *     id <MTLBuffer> intersectionBuffer =
 *         [device newBufferWithLength:sizeof(Intersection) * rayCount
 *                             options:MTLResourceStorageModePrivate];
 *
 *     id <MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
 *
 *     // Generate rays
 *     id <MTLComputeCommandEncoder> encoder = [commandBuffer computeCommandEncoder];
 *
 *     [encoder setBuffer:rayBuffer offset:0 atIndex:0];
 *     [encoder setBytes:&uniformData length:sizeof(uniformData) atIndex:1];
 *
 *     [encoder setComputePipelineState:cameraPipeline];
 *
 *     [encoder dispatchThreads:MTLSizeMake(rayCount, 1, 1)
 *        threadsPerThreadgroup:MTLSizeMake(64, 1, 1)];
 *
 *     [encoder endEncoding];
 *
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:0
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:0
 *                                         rayCount:rayCount
 *                            accelerationStructure:accelerationStructure];
 *
 *     // Perform shading at intersections and update framebuffer texture
 *     encoder = [commandBuffer computeCommandEncoder];
 *
 *     [encoder setBuffer:rayBuffer offset:0 atIndex:0];
 *     [encoder setBuffer:intersectionBuffer offset:0 atIndex:1];
 *     [encoder setBytes:&uniformData length:sizeof(uniformData) atIndex:2];
 *
 *     [encoder setTexture:framebufferTexture atIndex:0];
 *
 *     [encoder setComputePipelineState:shadingPipeline];
 *
 *     [encoder dispatchThreads:MTLSizeMake(rayCount, 1, 1)
 *        threadsPerThreadgroup:MTLSizeMake(64, 1, 1)];
 *
 *     [encoder endEncoding];
 *
 *     [commandBuffer commit];
 *     @endcode
 *
 * Note that the intersection consumption kernel can in turn produce new rays that can be passed
 * back to the MPSRayIntersector. This technique can be used to implement iterative techniques such as
 * progressive path tracing without leaving the GPU. For example, the shading kernel in the example
 * above could produce both a secondary ray that will be passed back to the raytracer in the
 * next iteration as well as a shadow ray that will be used to sample the direct lighting. A
 * final kernel can consume the shadow ray intersections to accumulate lighting contributions
 * into the framebuffer.
 *
 * There is an alternative version of the intersection test encoding method that does not accept
 * a literal ray count. The ray count is instead fetched indirectly by the GPU. For example,
 * this can be combined with a parallel reduction on the GPU to compact the ray buffer after each
 * iteration as rays bounce out of the scene or are absorbed. Alternatively, setting the maximum
 * distance of a ray to a negative number indicates that the ray has become inactive and causes the
 * raytracer to ignore the ray.
 *
 *     @code
 *     [raytracer encodeIntersectionToCommandBuffer:commandBuffer
 *                                 intersectionType:MPSIntersectionTypeNearest
 *                                        rayBuffer:rayBuffer
 *                                  rayBufferOffset:0
 *                               intersectionBuffer:intersectionBuffer
 *                         intersectionBufferOffset:0
 *                                   rayCountBuffer:rayCountBuffer
 *                             rayCountBufferOffset:0
 *                            accelerationStructure:accelerationStructure];
 *     @endcode
 *     
 * Multi-GPU Raytracing: to implement multi-GPU raytracing, create the MPSRayIntersector and
 * MPSAccelerationStructure objects first with one Metal device and copy them to the other Metal
 * device(s). The raytracing process can then proceed independently on each GPU. For example,
 * divide the output image into tiles or slices that are rendered independently. Then composite
 * finished tiles or slices back together on one GPU and present the output image to the screen.
 * The workload should be distributed across GPUs according to their performance to avoid a more
 * powerful GPU idly waiting for a less powerful GPU to finish.
 *
 * Acceleration Structure Serialization: MPSAccelerationStructure objects can be serialized
 * and deserialized using the NSSecureCoding protocol. This can be used to build acceleration
 * structures offline and reload them at runtime rather than building them from scratch.
 *
 * Performance Guidelines:
 *
 *     - For vertex buffers, ray buffers, intersection buffers, etc., use private or managed
 *       buffers rather than shared buffers when possible on discrete memory GPU architectures as
 *       they are much faster than fetching data over the PCIe bus. If the CPU only writes once
 *       to a ray buffer once and reads once from the intersection buffer, then a shared buffer may
 *       be acceptable and avoids extra copies to and from the GPU. However, it is generally
 *       preferable to generate and consume rays and intersections on the GPU instead, in which
 *       case a private buffer should be used. Vertex data is typically static and reused many
 *       times so it should be stored in private or managed buffers.
 *
 *     - If the CPU must generate and consume rays and intersections, use double or triple
 *       buffering as described above. This avoids the CPU and GPU mutually blocking each other.
 *
 *     - In general, disable any unused features such as ray masks, backface culling,
 *       etc. Enabling extra features increases the number of instructions and register usage of
 *       the ray intersection kernel(s), reducing intersection performance. For example, it may be
 *       more efficient to compute barycentric coordinates in your intersection consumption
 *       kernel rather getting them from the raytracer. Use of an index buffer may also reduce
 *       performance, so consider disabling the index buffer if there is enough memory available.
 *
 *     - Try to submit rays in large batches. This amortizes the costs involved in dispatching
 *       work to the GPU and also allows the GPU to perform more effective latency hiding.
 *       Use the recommendedMinimumRayBatchSizeForRayCount method to get an estimate of the
 *       minimum recommended ray batch size. For this reason, small images or sample counts
 *       may not perform as well as large images or sample counts. Note, however, that submitting
 *       rays in very large batches can reduce the responsiveness of the system because the GPU
 *       will be busy for long periods. Experiment to find a balance between raytracing throughput
 *       and system responsiveness.
 *
 *     - When possible, organize rays within a batch for spatial locality. Rays that originate
 *       at nearby points or are oriented in similar directions tend to access the same
 *       locations in memory and can therefore make more effective use of the GPU's caches.
 *       For example, the camera rays associated with nearby pixels in the output image will likely
 *       originate at the same point and travel in very similar directions. Therefore, divide the
 *       output image into small tiles (e.g., 8x8). Rather than laying out all of the rays in the
 *       ray buffer in scanline order, first lay out the ray in scanline order within each tile,
 *       then lay out the tiles in scanline order or according to some space filling curve.
 *
 *     - If CPU encode time is an issue, disable Metal API validation and enable
 *       MPSKernelOptionsSkipAPIValidation.
 *
 *     - Choose the minimal ray and intersection data types for your use case. Loading and storing
 *       extra values such as ray masks or triangle indices can reduce raytracing performance, so
 *       use a simpler data type if they are not needed. For example, camera rays typically have no
 *       need for a maximum distance field, while shadow rays do.
 *
 *     - Use MPSTriangleIntersectionTestTypeAny when possible: this is typically much faster than
 *       MPSTriangleIntersectionTestTypeNearest and can be used when you only need to check for
 *       binary visibility between two points such as shadow and ambient occlusion rays. Combine
 *       this with MPSRayDataTypeDistance to minimize memory bandwidth usage.
 *
 *     - Try to keep the geometry, textures, ray buffers, etc. within the Metal device's
 *       recommended working set size. Paging data into GPU memory can significantly reduce
 *       raytracing performance.
 *
 *     - Changes to MPSRayIntersector properties can trigger internal pipeline compilations when
 *       intersection tests are next encoded. If you need to avoid hitches due to pipeline
 *       compilation, encode a small ray intersection with each raytracer configuration you will
 *       use at encode-time. This creates and caches the corresponding pipelines.
 *
 *     - Disable rays which should not participate in intersection testing. This can be done either
 *       by compacting the ray buffer such that it only contains valid rays, or by setting fields
 *       of the ray struct to invalid values. For example, setting the maximum distance to a
 *       negative value, setting the mask to zero, setting the direction to the zero vector, etc.
 *       In particular, rays should NOT be disabled using schemes such as moving their origin
 *       outside the scene. These rays will still partially traverse the acceleration structure,
 *       potentially evicting data from the cache which could have been used by valid rays. Note
 *       that it is preferable to provide only valid rays so that threads are not left idle if
 *       their rays are found to be invalid, but it can be convenient to disable rays in place in
 *       the ray buffer.
 *
 * See MPSAccelerationStructure and MPSInstanceAccelerationStructure for more performance
 * guidelines.
 *
 * Thread Safety: MPSRayIntersectors are generally not thread safe: changing properties and encoding
 * intersection tests from multiple threads result in undefined behavior. Instead, multiple
 * threads should copy or create their own MPSRayIntersectors.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSRayIntersector : MPSKernel <NSSecureCoding, NSCopying>

/**
 * @brief Whether to ignore intersections between rays and back-facing or front-facing triangles.
 * Defaults to MTLCullModeNone.
 *
 * @discussion A triangle is back-facing if its normal points in the same direction as a ray and
 * front-facing if its normal points in the opposite direction as a ray. If the cull mode is set to
 * MTLCullModeBack, then back-facing triangles which be ignored. If the cull mode is set to
 * MTLCullModeFront, then front-facing triangles will be ignored. Otherwise, if the cull mode is
 * set to MTLCullModeNone, no triangles will be ignored. The front and back faces can be swapped
 * using the frontFacingWinding property.
 *
 * Backface culling is necessary for some scenes but can reduce raytracing performance.
 */
@property (nonatomic) MTLCullMode cullMode;

/**
 * @brief Winding order used to determine which direction a triangle's normal points when back face
 * or front face culling is enabled. Defaults to MTLWindingClockwise.
 *
 * @discussion If the front face winding is set to MTLWindingClockwise, the triangle normal is
 * considered to point towards the direction where the vertices are in clockwise order when
 * viewed from that direction. Otherwise, if the front facing winding is set to
 * MTLWindingCounterClockwise, the triangle normal is considered to point in the opposite
 * direction.
 */
@property (nonatomic) MTLWinding frontFacingWinding;

/**
 * @brief Ray/triangle intersection test type. Defaults to MPSTriangleIntersectionTestTypeDefault.
 */
@property (nonatomic) MPSTriangleIntersectionTestType triangleIntersectionTestType;

/**
 * @brief Ray/bounding box intersection test type. Defaults to
 * MPSBoundingBoxIntersectionTestTypeDefault.
 */
@property (nonatomic) MPSBoundingBoxIntersectionTestType boundingBoxIntersectionTestType;

/**
 * @brief Whether to enable primitive and instance masks. Defaults to MPSRayMaskOptionNone.
 *
 * @discussion If MPSRayMaskOptionPrimitive or MPSRayMaskOptionInstance is enabled, each ray and
 * primitive and/or instance is associated with a 32 bit unsigned integer mask. Before checking
 * for intersection between a ray and a primitive or instance, the corresponding masks are
 * logically AND-ed together. If the result is zero, the intersection is skipped.
 *
 * This can be used to make certain primitives or instances invisible to certain rays. For example,
 * objects can be grouped into layers and their visibility can be toggled by modifying the ray
 * masks rather than removing the objects from the scene and rebuilding the acceleration structure.
 * Alternatively, certain objects can be prevented from casting shadows by making them invisible to
 * shadow rays.
 *
 * Enabling this option may reduce raytracing performance.
 */
@property (nonatomic) MPSRayMaskOptions rayMaskOptions;

/**
 * @brief Offset, in bytes, between consecutive rays in the ray buffer. Defaults to 0, indicating
 * that the rays are packed according to their natural aligned size.
 *
 * @discussion This can be used to skip past any additional per-ray data that may be stored
 * alongside the MPSRay struct such as the current radiance along the ray or the source pixel
 * coordinates. Must be aligned to the alignment of the ray data type.
 */
@property (nonatomic) NSUInteger rayStride;

/**
 * @brief Offset, in bytes, between consecutive intersections in the intersection buffer. Defaults
 * to 0, indicating that the intersections are packed according to their natural aligned size.
 *
 * @discussion This can be used to skip past any additional per-intersection that which may be
 * stored alongside the MPSRayIntersection struct such as the surface normal at the point
 * of intersection. Must be aligned to the alignment of the intersection data type.
 */
@property (nonatomic) NSUInteger intersectionStride;

/**
 * @brief Ray data type. Defaults to MPSRayDataTypeOriginDirection.
 */
@property (nonatomic) MPSRayDataType rayDataType;

/**
 * @brief Intersection data type. Defaults to
 * MPSIntersectionDataTypeDistancePrimitiveIndexCoordinates.
 */
@property (nonatomic) MPSIntersectionDataType intersectionDataType;

- (nonnull instancetype)init NS_UNAVAILABLE;

/**
 * @brief Initialize the raytracer with a Metal device
 */
- (nonnull instancetype)initWithDevice:(nonnull id <MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/**
 * @brief Initialize the raytracer with an NSCoder and a Metal device
 */
- (nullable instancetype)initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/**
 * @brief Copy the raytracer with a Metal device
 *
 * @param zone   The NSZone in which to allocate the object
 * @param device The Metal device for the new MPSRayIntersector
 *
 * @return A pointer to a copy of this MPSRayIntersector
 */
- (nonnull instancetype)copyWithZone:(nullable NSZone *)zone
                              device:(nullable id <MTLDevice>)device;

/**
 * @brief Get the recommended minimum number of rays to submit for intersection in one batch
 *
 * @discussion In order to keep the system responsive, and to limit the amount of memory allocated
 * to ray and intersection buffers, it may be desirable to divide the rays to be intersected
 * against an acceleration structure into smaller batches. However, submitting too few rays in a
 * batch reduces GPU utilization and performance. This method provides a recommended minimum
 * number of rays to submit in any given batch. For example, for a 1920x1080 image, this method may
 * recommend that the image be divided into 512x512 tiles. The actual recommendation varies per
 * device and total ray count.
 *
 * @param rayCount The total number of rays to be submitted
 *
 * @return The recommended minimum ray batch size
 */
- (NSUInteger)recommendedMinimumRayBatchSizeForRayCount:(NSUInteger)rayCount
    MPS_SWIFT_NAME(recommendedMinimumRayBatchSize(rayCount:));

- (void)encodeWithCoder:(NSCoder * __nonnull)coder;

/**
 * @brief Schedule intersection tests between rays and an acceleration structure
 *
 * @param commandBuffer            Command buffer to schedule intersection testing in
 * @param intersectionType         Which type of intersection to test for
 * @param rayBuffer                Buffer containing rays to intersect against the acceleration
 *                                 structure. The ray data type is defined by the rayDataType
 *                                 and rayStride properties.
 * @param rayBufferOffset          Offset, in bytes, into the ray buffer. Must be a multiple of
 *                                 the ray stride.
 * @param intersectionBuffer       Buffer to store intersection in. Intersections are stored in
 *                                 the same order as the ray buffer, one intersection per ray.
 *                                 The intersection data type is defined by the
 *                                 intersectionDataType and intersectionStride properties.
 * @param intersectionBufferOffset Offset, in bytes, into the intersection buffer. Must be a
 *                                 multiple of the intersection stride.
 * @param rayCount                 Number of rays
 * @param accelerationStructure    Acceleration structure to test against
 */
- (void)encodeIntersectionToCommandBuffer:(nonnull id <MTLCommandBuffer>)commandBuffer
                         intersectionType:(MPSIntersectionType)intersectionType
                                rayBuffer:(nonnull id <MTLBuffer>)rayBuffer
                          rayBufferOffset:(NSUInteger)rayBufferOffset
                       intersectionBuffer:(nonnull id <MTLBuffer>)intersectionBuffer
                 intersectionBufferOffset:(NSUInteger)intersectionBufferOffset
                                 rayCount:(NSUInteger)rayCount
                    accelerationStructure:(nonnull MPSAccelerationStructure *)accelerationStructure
    MPS_SWIFT_NAME(encodeIntersection(commandBuffer:intersectionType:rayBuffer:rayBufferOffset:intersectionBuffer:intersectionBufferOffset:rayCount:accelerationStructure:));

/**
 * @brief Schedule intersection tests between rays and an acceleration structure with a ray count
 * provided in a buffer
 *
 * @param commandBuffer            Command buffer to schedule intersection testing in
 * @param intersectionType         Which type of intersection to test for
 * @param rayBuffer                Buffer containing rays to intersect against the acceleration
 *                                 structure. The ray data type is defined by the rayDataType
 *                                 and rayStride properties.
 * @param rayBufferOffset          Offset, in bytes, into the ray buffer. Must be a multiple of
 *                                 the ray stride.
 * @param intersectionBuffer       Buffer to store intersection in. Intersections are stored in
 *                                 the same order as the ray buffer, one intersection per ray.
 *                                 The intersection data type is defined by the
 *                                 intersectionDataType and intersectionStride properties.
 * @param intersectionBufferOffset Offset, in bytes, into the intersection buffer. Must be a
 *                                 multiple of the intersection stride.
 * @param rayCountBuffer           Buffer containing number of rays as a 32 bit unsigned integer
 * @param rayCountBufferOffset     Offset, in bytes, into the ray count buffer. Must be a multiple
 *                                 of 4 bytes.
 * @param accelerationStructure    Acceleration structure to test against
 */
- (void)encodeIntersectionToCommandBuffer:(nonnull id <MTLCommandBuffer>)commandBuffer
                         intersectionType:(MPSIntersectionType)intersectionType
                                rayBuffer:(nonnull id <MTLBuffer>)rayBuffer
                          rayBufferOffset:(NSUInteger)rayBufferOffset
                       intersectionBuffer:(nonnull id <MTLBuffer>)intersectionBuffer
                 intersectionBufferOffset:(NSUInteger)intersectionBufferOffset
                           rayCountBuffer:(nonnull id <MTLBuffer>)rayCountBuffer
                     rayCountBufferOffset:(NSUInteger)rayCountBufferOffset
                    accelerationStructure:(nonnull MPSAccelerationStructure *)accelerationStructure
                    MPS_SWIFT_NAME(encodeIntersection(commandBuffer:intersectionType:rayBuffer:rayBufferOffset:intersectionBuffer:intersectionBufferOffset:rayCountBuffer:rayCountBufferOffset:accelerationStructure:));

@end
#endif

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNKernel.h
//
//  MPSCNNKernel.h
//  MPS
//
//  Created by Ian Ollmann on 8/21/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNKernel_h
#define MPSCNNKernel_h

#include <MPSCore/MPSKernel.h>
#include <MPSCore/MPSImage.h>
#include <MPSNeuralNetwork/MPSNeuralNetworkTypes.h>
#include <MPSNeuralNetwork/MPSNNGradientState.h>


/*!
 *  @class      MPSCNNKernel
 *  @dependency This depends on Metal.framework
 *  @abstract   Describes a convolution neural network kernel.
 *  @discussion A MPSCNNKernel consumes one MPSImage and produces one MPSImage.
 *
 *              The region overwritten in the destination MPSImage is described
 *              by the clipRect.  The top left corner of the region consumed (ignoring
 *              adjustments for filter size -- e.g. convolution filter size) is given
 *              by the offset. The size of the region consumed is a function of the
 *              clipRect size and any subsampling caused by pixel strides at work,
 *              e.g. MPSCNNPooling.strideInPixelsX/Y.  Where the offset + clipRect
 *              would cause a {x,y} pixel address not in the image to be read, the
 *              edgeMode is used to determine what value to read there.
 *
 *              The Z/depth component of the offset, clipRect.origin and clipRect.size
 *              indexes which images to use. If the MPSImage contains only a single image
 *              then these should be offset.z = 0, clipRect.origin.z = 0
 *              and clipRect.size.depth = 1. If the MPSImage contains multiple images,
 *              clipRect.size.depth refers to number of images to process. Both source
 *              and destination MPSImages must have at least this many images. offset.z
 *              refers to starting source image index. Thus offset.z + clipRect.size.depth must
 *              be <= source.numberOfImages. Similarly, clipRect.origin.z refers to starting
 *              image index in destination. So clipRect.origin.z + clipRect.size.depth must be
 *              <= destination.numberOfImage.
 *
 *              destinationFeatureChannelOffset property can be used to control where the MPSKernel will
 *              start writing in feature channel dimension. For example, if the destination image has
 *              64 channels, and MPSKernel outputs 32 channels, by default channels 0-31 of destination
 *              will be populated by MPSKernel. But if we want this MPSKernel to populate channel 32-63
 *              of the destination, we can set destinationFeatureChannelOffset = 32.
 *              A good example of this is concat (concatenation) operation in Tensor Flow. Suppose
 *              we have a src = w x h x Ni which goes through CNNConvolution_0 which produces
 *              output O0 = w x h x N0 and CNNConvolution_1 which produces output O1 = w x h x N1 followed
 *              by concatenation which produces O = w x h x (N0 + N1). We can achieve this by creating
 *              an MPSImage with dimensions O = w x h x (N0 + N1) and using this as destination of
 *              both convolutions as follows
 *                  CNNConvolution0: destinationFeatureChannelOffset = 0, this will output N0 channels starting at
 *                                   channel 0 of destination thus populating [0,N0-1] channels.
 *                  CNNConvolution1: destinationFeatureChannelOffset = N0, this will output N1 channels starting at
 *                                   channel N0 of destination thus populating [N0,N0+N1-1] channels.
 *
 *
 *              A MPSCNNKernel can be saved to disk / network using NSCoders such as NSKeyedArchiver. 
 *              When decoding, the system default MTLDevice will be chosen unless the NSCoder adopts 
 *              the <MPSDeviceProvider> protocol.  To accomplish this you will likely need to subclass your
 *              unarchiver to add this method.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNKernel : MPSKernel

/*! @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     A pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                NS_DESIGNATED_INITIALIZER;


/*! @property   offset
 *  @abstract   The position of the destination clip rectangle origin relative to the source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and source image align.
 *              offset.z is the index of starting source image in batch processing mode.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                offset;

/*! @property   clipRect
 *  @abstract   An optional clip rectangle to use when writing data. Only the pixels in the rectangle will be overwritten.
 *  @discussion A MTLRegion that indicates which part of the destination to overwrite. If the clipRect does not lie
 *              completely within the destination image, the intersection between clip rectangle and destination bounds is
 *              used.   Default: MPSRectNoClip (MPSKernel::MPSRectNoClip) indicating the entire image.
 *              clipRect.origin.z is the index of starting destination image in batch processing mode. clipRect.size.depth
 *              is the number of images to process in batch processing mode.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_clipRect
 */
@property (readwrite, nonatomic) MTLRegion               clipRect;


/*! @property   destinationFeatureChannelOffset
 *  @abstract   The number of channels in the destination MPSImage to skip before writing output.
 *  @discussion This is the starting offset into the destination image in the feature channel dimension
 *              at which destination data is written.
 *              This allows an application to pass a subset of all the channels in MPSImage as output of MPSKernel.
 *              E.g. Suppose MPSImage has 24 channels and a MPSKernel outputs 8 channels. If
 *              we want channels 8 to 15 of this MPSImage to be used as output, we can set destinationFeatureChannelOffset = 8.
 *              Note that this offset applies independently to each image when the MPSImage
 *              is a container for multiple images and the MPSCNNKernel is processing multiple images (clipRect.size.depth > 1).
 *              The default value is 0 and any value specifed shall be a multiple of 4. If MPSKernel outputs N channels,
 *              the destination image MUST have at least destinationFeatureChannelOffset + N channels. Using a destination
 *              image with insufficient number of feature channels will result in an error.
 *              E.g. if the MPSCNNConvolution outputs 32 channels, and the destination has 64 channels, then it is an error to set
 *              destinationFeatureChannelOffset > 32.
 */
@property (readwrite, nonatomic) NSUInteger              destinationFeatureChannelOffset;

/*! @property   sourceFeatureChannelOffset
 *  @abstract   The number of channels in the source MPSImage to skip before reading the input.
 *  @discussion This is the starting offset into the source image in the feature channel dimension
 *              at which source data is read. Unit: feature channels
 *              This allows an application to read a subset of all the channels in MPSImage as input of MPSKernel.
 *              E.g. Suppose MPSImage has 24 channels and a MPSKernel needs to read 8 channels. If
 *              we want channels 8 to 15 of this MPSImage to be used as input, we can set sourceFeatureChannelOffset = 8.
 *              Note that this offset applies independently to each image when the MPSImage
 *              is a container for multiple images and the MPSCNNKernel is processing multiple images (clipRect.size.depth > 1).
 *              The default value is 0 and any value specifed shall be a multiple of 4. If MPSKernel inputs N channels,
 *              the source image MUST have at least sourceFeatureChannelOffset + N channels. Using a source
 *              image with insufficient number of feature channels will result in an error.
 *              E.g. if the MPSCNNConvolution inputs 32 channels, and the source has 64 channels, then it is an error to set
 *              sourceFeatureChannelOffset > 32.
 */
@property (readwrite, nonatomic) NSUInteger              sourceFeatureChannelOffset;

/*! @property   sourceFeatureChannelMaxCount
 *  @abstract   The maximum number of channels in the source MPSImage to use
 *  @discussion Most filters can insert a slice operation into the filter for free.
 *              Use this to limit the size of the feature channel slice taken from
 *              the input image. If the value is too large, it is truncated to be
 *              the remaining size in the image after the sourceFeatureChannelOffset
 *              is taken into account.  Default: ULONG_MAX
 */
@property (readwrite, nonatomic) NSUInteger              sourceFeatureChannelMaxCount;

/*! @property   edgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of an image
 *  @discussion Most MPSKernel objects can read off the edge of the source image. This can happen
 *              because of a negative offset property, because the offset + clipRect.size is larger
 *              than the source image or because the filter looks at neighboring pixels, such as a
 *              Convolution filter.   Default:  MPSImageEdgeModeZero.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_edgemode
 *              Note: For @ref MPSCNNPoolingAverage specifying edge mode @ref MPSImageEdgeModeClamp
 *                      is interpreted as a "shrink-to-edge" operation, which shrinks the effective
 *                      filtering window to remain within the source image borders.
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        edgeMode;


/*! @property   kernelWidth
 *  @abstract   The width of the MPSCNNKernel filter window
 *  @discussion This is the horizontal diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNKernel does not have a filter window, then
 *              1 will be returned.
 *
 *              Warning: This property was lowered to this class in ios/tvos 11
 *                       The property may not be available on iOS/tvOS 10 for
 *                       all subclasses of MPSCNNKernel
 */
@property (readonly, nonatomic) NSUInteger              kernelWidth;

/*! @property   kernelHeight
 *  @abstract   The height of the MPSCNNKernel filter window
 *  @discussion This is the vertical diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNKernel does not have a filter window, then 
 *              1 will be returned.
 *
 *              Warning: This property was lowered to this class in ios/tvos 11
 *                       The property may not be available on iOS/tvOS 10 for
 *                       all subclasses of MPSCNNKernel
 */
@property (readonly, nonatomic) NSUInteger              kernelHeight;

/*! @property   strideInPixelsX
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the horizontal dimension
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 *
 *              Warning: This property was lowered to this class in ios/tvos 11
 *                       The property may not be available on iOS/tvOS 10 for
 *                       all subclasses of MPSCNNKernel
 */
@property(readonly, nonatomic) NSUInteger               strideInPixelsX;

/*! @property   strideInPixelsY
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the vertical dimension
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 *
 *              Warning: This property was lowered to this class in ios/tvos 11
 *                       The property may not be available on iOS/tvOS 10 for
 *                       all subclasses of MPSCNNKernel
 */
@property(readonly, nonatomic) NSUInteger               strideInPixelsY;

/*! @property   dilationRateX
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the X dimension.
 */
@property(readonly, nonatomic) NSUInteger               dilationRateX;

/*! @property   dilationRate
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the Y dimension.
 */
@property(readonly, nonatomic) NSUInteger               dilationRateY;


/*! @property   isBackwards
 *  @abstract   YES if the filter operates backwards.
 *  @discussion This influences how strideInPixelsX/Y should be interpreted. 
 *              Most filters either have stride 1 or are reducing, meaning that
 *              the result image is smaller than the original by roughly a factor
 *              of the stride.  A few "backward" filters (e.g convolution transpose) are intended
 *              to "undo" the effects of an earlier forward filter, and so 
 *              enlarge the image. The stride is in the destination coordinate frame
 *              rather than the source coordinate frame.
 */
@property(readonly, nonatomic) BOOL                     isBackwards
                    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*! @abstract   Returns true if the -encode call modifies the state object it accepts.  */
@property (readonly, nonatomic) BOOL                    isStateModified
                    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   padding
 *  @abstract   The padding method used by the filter
 *  @discussion This influences how the destination image is sized and how
 *              the offset into the source image is set.  It is used by the
 *              -encode methods that return a MPSImage from the left hand side.
 */
@property (readwrite, nonatomic, nonnull, retain) id <MPSNNPadding>    padding
                    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*! @abstract   Method to allocate the result image for -encodeToCommandBuffer:sourceImage:
 *  @discussion Default: MPSTemporaryImage.defaultAllocator
 */
@property (readwrite, nonatomic, retain, nonnull) id <MPSImageAllocator> destinationImageAllocator
                    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));



/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure. */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*! @abstract   Encode a MPSCNNKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @discussion This is the older style of encode which reads the offset, doesn't change it,
 *              and ignores the padding method.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImage         A valid MPSImage object containing the source image.
 *  @param      destinationImage    A valid MPSImage to be overwritten by result image. destinationImage may not alias sourceImage.
*/
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
             destinationImage: (MPSImage * __nonnull) destinationImage
                MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:destinationImage:));

/*! @abstract   Encode a MPSCNNKernel with a destination state into a command Buffer.
 *  @discussion This is typically used during training. The state is commonly a MPSNNGradientState.
 *              Please see -resultStateForSourceImages:SourceStates: and batch+temporary variants.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImage         A valid MPSImage object containing the source image.
 *  @param      destinationState    A state to be overwritten by additional state information.
 *  @param      destinationImage    A valid MPSImage to be overwritten by result image. destinationImage may not alias sourceImage. */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
             destinationState: (MPSState * __nonnull) destinationState
             destinationImage: (MPSImage * __nonnull) destinationImage
    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
    MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:destinationState:destinationImage:));

/*! @abstract   Encode a MPSCNNKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @discussion This is the older style of encode which reads the offset, doesn't change it,
 *              and ignores the padding method.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImages        A valid MPSImage object containing the source images.
 *  @param      destinationImages   A valid MPSImage to be overwritten by result images.
 *                                  destinationImages may not alias sourceImages, even at different
 *                                  indices. */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages
                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceImages:destinationImages:));

/*! @abstract   Encode a MPSCNNKernel with a destination state into a command Buffer.
 *  @discussion This is typically used during training. The state is commonly a MPSNNGradientState.
 *              Please see -resultStateForSourceImages:SourceStates:destinationImage and batch+temporary variants.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImages        A valid MPSImage object containing the source images.
 *  @param      destinationStates   A list of states to be overwritten by results
 *  @param      destinationImages   A valid MPSImage to be overwritten by result images.
 *                                  destinationImages may not alias sourceImages, even at different
 *                                  indices. */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
                 destinationStates: (MPSStateBatch * __nullable) destinationStates
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages
                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))    // added in .5 support update
                MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceImages:destinationStates:destinationImages:));


/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture to hold the result and return it.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the 
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImage         A MPSImage to use as the source images for the filter.
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The offset property will be adjusted to reflect the offset used during the encode.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                  sourceImage: (MPSImage *  __nonnull) sourceImage
                    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
                    MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture and state to hold the results and return them.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationState:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImage         A MPSImage to use as the source images for the filter.
 *  @param          outState            A new state object is returned here.
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The offset property will be adjusted to reflect the offset used during the encode.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                  sourceImage: (MPSImage *  __nonnull) sourceImage
                             destinationState: (__autoreleasing MPSState * __nullable * __nonnull) outState
                  destinationStateIsTemporary: (BOOL) isTemporary
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
        MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:destinationState:destinationStateIsTemporary:));


/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture to hold the result and return it.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImages         A MPSImages to use as the source images for the filter.
 *  @result         An array of MPSImages or MPSTemporaryImages allocated per the destinationImageAllocator
 *                  containing the output of the graph. The offset property will be adjusted to reflect the
 *                  offset used during the encode. The returned images will be automatically released when
 *                  the command buffer completes. If you want to keep them around for longer, retain the images.
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                           sourceImages: (MPSImageBatch *  __nonnull) sourceImages
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceImages:));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a MPSImageBatch and MPSStateBatch to hold the results and return them.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *                  Usage:
 *                  @code
 *                  MPSStateBatch * outStates = nil;    // autoreleased
 *                  MPSImageBatch * result = [k encodeBatchToCommandBuffer: cmdBuf
 *                                                            sourceImages: sourceImages
 *                                                       destinationStates: &outStates ];
 *                  @endcode
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImages         A MPSImages to use as the source images for the filter.
 *  @param          outStates            A pointer to storage to hold a MPSStateBatch* where output states are returned
 *  @result         An array of MPSImages or MPSTemporaryImages allocated per the destinationImageAllocator
 *                  containing the output of the graph. The offset property will be adjusted to reflect the
 *                  offset used during the encode. The returned images will be automatically released when
 *                  the command buffer completes. If you want to keep them around for longer, retain the images.
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                           sourceImages: (MPSImageBatch *  __nonnull) sourceImages
                                      destinationStates: (__autoreleasing MPSStateBatch * __nullable * __nonnull) outStates
                            destinationStateIsTemporary: (BOOL) isTemporary
                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))  // added in .5 support update
                MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceImages:destinationStates:destinationStateIsTemporary:));

/*! @abstract   Allocate a MPSState (subclass) to hold the results from a -encodeBatchToCommandBuffer... operation
 *  @discussion A graph may need to allocate storage up front before executing.  This may be
 *              necessary to avoid using too much memory and to manage large batches.  The function
 *              should allocate any MPSState objects that will be produced by an -encode call
 *              with the indicated sourceImages and sourceStates inputs. Though the states
 *              can be further adjusted in the ensuing -encode call, the states should
 *              be initialized with all important data and all MTLResource storage allocated.
 *              The data stored in the MTLResource need not be initialized, unless the ensuing
 *              -encode call expects it to be.
 *
 *              The MTLDevice used by the result is derived from the source image.
 *              The padding policy will be applied to the filter before this is called
 *              to give it the chance to configure any properties like MPSCNNKernel.offset.
 *
 *              CAUTION:
 *              The kernel must have all properties set to values that will ultimately be
 *              passed to the -encode call that writes to the state, before
 *              -resultStateForSourceImages:sourceStates:destinationImage: is called or behavior is undefined.
 *              Please note that -destinationImageDescriptorForSourceImages:sourceStates:
 *              will alter some of these properties automatically based on the padding policy.
 *              If you intend to call that to make the destination image, then you should
 *              call that before -resultStateForSourceImages:sourceStates:destinationImage:. This will ensure the
 *              properties used in the encode call and in the destination image creation
 *              match those used to configure the state.
 *
 *              The following order is recommended:
 *
 *                  // Configure MPSCNNKernel properties first
 *                  kernel.edgeMode = MPSImageEdgeModeZero;
 *                  kernel.destinationFeatureChannelOffset = 128; // concatenation without the copy
 *                  ...
 *
 *                  // ALERT: will change MPSCNNKernel properties
 *                  MPSImageDescriptor * d = [kernel destinationImageDescriptorForSourceImage: source
 *                                                                               sourceStates: states];
 *                  MPSTemporaryImage * dest = [MPSTemporaryImage temporaryImageWithCommandBuffer: cmdBuf
 *                                                                                imageDescriptor: d];
 *
 *                  // Now that all properties are configured properly, we can make the result state
 *                  // and call encode.
 *                  MPSState * __nullable destState = [kernel resultStateForSourceImage: source
 *                                                                         sourceStates: states
 *                                                                     destinationImage: dest];
 *
 *                  // This form of -encode will be declared by the MPSCNNKernel subclass
 *                  [kernel encodeToCommandBuffer: cmdBuf
 *                                    sourceImage: source
 *                               destinationState: destState
 *                               destinationImage: dest ];
 *
 *              Default: returns nil
 *
 *  @param      sourceImage         The MPSImage consumed by the associated -encode call.
 *  @param      sourceStates        The list of MPSStates consumed by the associated -encode call,
 *                                  for a batch size of 1.
 *  @param      destinationImage    The destination image for the encode call
 *  @return     The list of states produced by the -encode call for batch size of 1.
 *              When the batch size is not 1, this function will be called repeatedly unless
 *              -isResultStateReusedAcrossBatch returns YES. If  -isResultStateReusedAcrossBatch
 *              returns YES, then it will be called once per batch and the MPSStateBatch array will
 *              contain MPSStateBatch.length references to the same object.
 */
-(MPSState * __nullable) resultStateForSourceImage: (MPSImage *__nonnull) sourceImage
                                      sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                  destinationImage: (MPSImage *__nonnull) destinationImage
                        MPS_SWIFT_NAME( resultState(sourceImage:sourceStates:destinationImage:))
                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));
-(MPSStateBatch * __nullable) resultStateBatchForSourceImage: (MPSImageBatch * __nonnull) sourceImage
                                                sourceStates: (NSArray<MPSStateBatch *> * __nullable) sourceStates
                                            destinationImage: (MPSImageBatch *__nonnull) destinationImage
                        MPS_SWIFT_NAME( resultStateBatch(sourceImage:sourceStates:destinationImage:))
                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));


/*! @abstract   Allocate a temporary MPSState (subclass) to hold the results from a -encodeBatchToCommandBuffer... operation
 *  @discussion A graph may need to allocate storage up front before executing.  This may be
 *              necessary to avoid using too much memory and to manage large batches.  The function
 *              should allocate any MPSState objects that will be produced by an -encode call
 *              with the indicated sourceImages and sourceStates inputs. Though the states
 *              can be further adjusted in the ensuing -encode call, the states should
 *              be initialized with all important data and all MTLResource storage allocated.
 *              The data stored in the MTLResource need not be initialized, unless the ensuing
 *              -encode call expects it to be.
 *
 *              The MTLDevice used by the result is derived from the command buffer.
 *              The padding policy will be applied to the filter before this is called
 *              to give it the chance to configure any properties like MPSCNNKernel.offset.
 *
 *              CAUTION:
 *              The kernel must have all properties set to values that will ultimately be
 *              passed to the -encode call that writes to the state, before
 *              -resultStateForSourceImages:sourceStates:destinationImage: is called or behavior is undefined.
 *              Please note that -destinationImageDescriptorForSourceImages:sourceStates:destinationImage:
 *              will alter some of these properties automatically based on the padding policy.
 *              If you intend to call that to make the destination image, then you should
 *              call that before -resultStateForSourceImages:sourceStates:destinationImage:.  This will ensure the
 *              properties used in the encode call and in the destination image creation
 *              match those used to configure the state.
 *
 *              The following order is recommended:
 *
 *                  // Configure MPSCNNKernel properties first
 *                  kernel.edgeMode = MPSImageEdgeModeZero;
 *                  kernel.destinationFeatureChannelOffset = 128; // concatenation without the copy
 *                  ...
 *
 *                  // ALERT: will change MPSCNNKernel properties
 *                  MPSImageDescriptor * d = [kernel destinationImageDescriptorForSourceImage: source
 *                                                                               sourceStates: states];
 *                  MPSTemporaryImage * dest = [MPSTemporaryImage temporaryImageWithCommandBuffer: cmdBuf
 *                                                                                imageDescriptor: d];
 *
 *                  // Now that all properties are configured properly, we can make the result state
 *                  // and call encode.
 *                  MPSState * __nullable destState = [kernel temporaryResultStateForCommandBuffer: cmdBuf
 *                                                                                     sourceImage: source
 *                                                                                    sourceStates: states];
 *
 *                  // This form of -encode will be declared by the MPSCNNKernel subclass
 *                  [kernel encodeToCommandBuffer: cmdBuf
 *                                    sourceImage: source
 *                               destinationState: destState
 *                               destinationImage: dest ];
 *
 *              Default: returns nil
 *
 *  @param      commandBuffer       The command buffer to allocate the temporary storage against
 *                                  The state will only be valid on this command buffer.
 *  @param      sourceImage         The MPSImage consumed by the associated -encode call.
 *  @param      sourceStates        The list of MPSStates consumed by the associated -encode call,
 *                                  for a batch size of 1.
 *  @param      destinationImage    The destination image for the encode call
 *  @return     The list of states produced by the -encode call for batch size of 1.
 *              When the batch size is not 1, this function will be called repeatedly unless
 *              -isResultStateReusedAcrossBatch returns YES. If  -isResultStateReusedAcrossBatch
 *              returns YES, then it will be called once per batch and the MPSStateBatch array will
 *              contain MPSStateBatch.length references to the same object.
 */
-(MPSState * __nullable) temporaryResultStateForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                  sourceImage: (MPSImage *__nonnull) sourceImage
                                                 sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                             destinationImage: (MPSImage *__nonnull) destinationImage
                         MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                         MPS_SWIFT_NAME( temporaryResultState(commandBuffer:sourceImage:sourceStates:destinationImage:));
-(MPSStateBatch * __nullable) temporaryResultStateBatchForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                            sourceImage: (MPSImageBatch *__nonnull) sourceImage
                                                           sourceStates: (NSArray <MPSStateBatch *> *__nullable) sourceStates
                                                       destinationImage: (MPSImageBatch *__nonnull) destinationImage
                            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                            MPS_SWIFT_NAME( temporaryResultStateBatch(commandBuffer:sourceImage:sourceStates:destinationImage:));

/*! @abstract   Returns YES if the same state is used for every operation in a batch
 *  @discussion If NO, then each image in a MPSImageBatch will need a corresponding
 *              (and different) state to go with it. Set to YES to avoid allocating
 *              redundant state in the case when the same state is used all the time.
 *              Default: NO    */
-(BOOL)     isResultStateReusedAcrossBatch MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Returns YES if the filter must be run over the entire batch before its
 *              results may be used
 *  @discussion Nearly all filters do not need to see the entire batch all at once and can
 *              operate correctly with partial batches. This allows the graph to
 *              strip-mine the problem, processing the graph top to bottom on a subset
 *              of the batch at a time, dramatically reducing memory usage. As the full
 *              nominal working set for a graph is often so large that it may not fit
 *              in memory, sub-batching may be required forward progress.
 *
 *              Batch normalization statistics on the other hand must complete the batch
 *              before the statistics may be used to normalize the images in the batch
 *              in the ensuing normalization filter. Consequently, batch normalization statistics
 *              requests the graph insert a batch barrier following it by returning
 *              YES from -appendBatchBarrier. This tells the graph to complete the batch
 *              before any dependent filters can start. Note that the filter itself may
 *              still be subject to sub-batching in its operation. All filters must be able to
 *              function without seeing the entire batch in a single -encode call. Carry
 *              over state that is accumulated across sub-batches is commonly carried in
 *              a shared MPSState containing a MTLBuffer. See -isResultStateReusedAcrossBatch.
 *
 *              Caution: on most supported devices, the working set may be so large
 *              that the graph may be forced to throw away and recalculate most
 *              intermediate images in cases where strip-mining can not occur because
 *              -appendBatchBarrier returns YES. A single batch barrier can commonly
 *              cause a memory size increase and/or performance reduction by many fold
 *              over the entire graph.  Filters of this variety should be avoided.
 *
 *              Default: NO
 */
-(BOOL)     appendBatchBarrier MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Get a suggested destination image descriptor for a source image
 *  @discussion Your application is certainly free to pass in any destinationImage
 *              it likes to encodeToCommandBuffer:sourceImage:destinationImage,
 *              within reason. This is the basic design for iOS 10. This method
 *              is therefore not required.
 *
 *              However, calculating the MPSImage size and MPSCNNKernel properties
 *              for each filter can be tedious and complicated work, so this method
 *              is made available to automate the process. The application may
 *              modify the properties of the descriptor before a MPSImage is made from
 *              it, so long as the choice is sensible for the kernel in question.
 *              Please see individual kernel descriptions for restrictions.
 *
 *              The expected timeline for use is as follows:
 *
 *                1) This method is called:
 *                  a) The default MPS padding calculation is applied. It
 *                     uses the MPSNNPaddingMethod of the .padding property to
 *                     provide a consistent addressing scheme over the graph.
 *                     It creates the MPSImageDescriptor and adjusts the .offset
 *                     property of the MPSNNKernel. When using a MPSNNGraph, the
 *                     padding is set using the MPSNNFilterNode as a proxy.
 *
 *                  b) This method may be overridden by MPSCNNKernel subclass
 *                     to achieve any customization appropriate to the object type.
 *
 *                  c) Source states are then applied in order. These may modify the
 *                     descriptor and may update other object properties. See:
 *                      -destinationImageDescriptorForSourceImages:sourceStates:
 *                       forKernel:suggestedDescriptor:  This is the typical way
 *                      in which MPS may attempt to influence the operation of
 *                      its kernels.
 *
 *                  d) If the .padding property has a custom padding policy method
 *                      of the same name, it is called. Similarly, it may also adjust
 *                      the descriptor and any MPSCNNKernel properties. This is the
 *                      typical way in which your application may attempt to influence
 *                      the operation of the MPS kernels.
 *
 *                2) A result is returned from this method and the caller
 *                     may further adjust the descriptor and kernel properties
 *                     directly.
 *
 *                3) The caller uses the descriptor to make a new MPSImage to
 *                   use as the destination image for the -encode call in step 5.
 *
 *                4) The caller calls -resultStateForSourceImage:sourceStates:destinationImage:
 *                    to make any result states needed for the kernel. If there isn't
 *                    one, it will return nil. A variant is available to return a
 *                    temporary state instead.
 *
 *                5) a -encode method is called to encode the kernel.
 *
 *              The entire process 1-5 is more simply achieved by just calling an -encode...
 *              method that returns a MPSImage out the left hand sid of the method. Simpler
 *              still, use the MPSNNGraph to coordinate the entire process from end to end.
 *              Opportunities to influence the process are of course reduced, as (2) is no longer
 *              possible with either method. Your application may opt to use the five step method
 *              if it requires greater customization as described, or if it would like to estimate
 *              storage in advance based on the sum of MPSImageDescriptors before processing
 *              a graph. Storage estimation is done by using the MPSImageDescriptor to create
 *              a MPSImage (without passing it a texture), and then call -resourceSize. As long
 *              as the MPSImage is not used in an encode call and the .texture property is not
 *              invoked, the underlying MTLTexture is not created.
 *
 *              No destination state or destination image is provided as an argument to this
 *              function because it is expected they will be made / configured after this
 *              is called. This method is expected to auto-configure important object properties
 *              that may be needed in the ensuing destination image and state creation steps.
 *
 *  @param      sourceImages    A array of source images that will be passed into the -encode call
 *                              Since MPSCNNKernel is a unary kernel, it is an array of length 1.
 *  @param      sourceStates    An optional array of source states that will be passed into the -encode call
 *  @return     an image descriptor allocated on the autorelease pool
 */
-(MPSImageDescriptor*__nonnull) destinationImageDescriptorForSourceImages: (NSArray<MPSImage *> * __nonnull) sourceImages
                                                             sourceStates: (NSArray<MPSState *> * __nullable) sourceStates
                                MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
                                MPS_SWIFT_NAME( destinationImageDescriptor(sourceImages:sourceStates:));

@end


/*!
 *  @class      MPSCNNBinaryKernel
 *  @dependency This depends on Metal.framework
 *  @abstract   Describes a convolution neural network kernel.
 *  @discussion A MPSCNNKernel consumes two MPSImages, primary and secondary, and produces one MPSImage.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNBinaryKernel : MPSKernel

/*! @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     A pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                NS_DESIGNATED_INITIALIZER;

/*! @property   primaryOffset
 *  @abstract   The position of the destination clip rectangle origin relative to the primary source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and primary source image align.
 *              offset.z is the index of starting source image in batch processing mode.
 *
 *              See Also: @ref subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                primaryOffset;

/*! @property   secondaryOffset
 *  @abstract   The position of the destination clip rectangle origin relative to the secondary source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and secondary source image align.
 *              offset.z is the index of starting source image in batch processing mode.
 *
 *              See Also: @ref subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                secondaryOffset;

/*! @property   clipRect
 *  @abstract   An optional clip rectangle to use when writing data. Only the pixels in the rectangle will be overwritten.
 *  @discussion A MTLRegion that indicates which part of the destination to overwrite. If the clipRect does not lie
 *              completely within the destination image, the intersection between clip rectangle and destination bounds is
 *              used.   Default: MPSRectNoClip (MPSKernel::MPSRectNoClip) indicating the entire image.
 *              clipRect.origin.z is the index of starting destination image in batch processing mode. clipRect.size.depth
 *              is the number of images to process in batch processing mode.
 *
 *              See Also: @ref subsubsection_clipRect
 */
@property (readwrite, nonatomic) MTLRegion               clipRect;

/*! @property   destinationFeatureChannelOffset
 *  @abstract   The number of channels in the destination MPSImage to skip before writing output.
 *  @discussion This is the starting offset into the destination image in the feature channel dimension
 *              at which destination data is written.
 *              This allows an application to pass a subset of all the channels in MPSImage as output of MPSKernel.
 *              E.g. Suppose MPSImage has 24 channels and a MPSKernel outputs 8 channels. If
 *              we want channels 8 to 15 of this MPSImage to be used as output, we can set destinationFeatureChannelOffset = 8.
 *              Note that this offset applies independently to each image when the MPSImage
 *              is a container for multiple images and the MPSCNNKernel is processing multiple images (clipRect.size.depth > 1).
 *              The default value is 0 and any value specifed shall be a multiple of 4. If MPSKernel outputs N channels,
 *              destination image MUST have at least destinationFeatureChannelOffset + N channels. Using a destination
 *              image with insufficient number of feature channels result in an error.
 *              E.g. if the MPSCNNConvolution outputs 32 channels, and destination has 64 channels, then it is an error to set
 *              destinationFeatureChannelOffset > 32.
 */
@property (readwrite, nonatomic) NSUInteger              destinationFeatureChannelOffset;

/*! @property   primarySourceFeatureChannelOffset
 *  @abstract   The number of channels in the primary source MPSImage to skip before reading the input.
 *  @discussion This is the starting offset into the primary source image in the feature channel dimension
 *              at which source data is read. Unit: feature channels
 *              This allows an application to read a subset of all the channels in MPSImage as input of MPSKernel.
 *              E.g. Suppose MPSImage has 24 channels and a MPSKernel needs to read 8 channels. If
 *              we want channels 8 to 15 of this MPSImage to be used as input, we can set primarySourceFeatureChannelOffset = 8.
 *              Note that this offset applies independently to each image when the MPSImage
 *              is a container for multiple images and the MPSCNNKernel is processing multiple images (clipRect.size.depth > 1).
 *              The default value is 0 and any value specifed shall be a multiple of 4. If MPSKernel inputs N channels,
 *              the source image MUST have at least primarySourceFeatureChannelOffset + N channels. Using a source
 *              image with insufficient number of feature channels will result in an error.
 *              E.g. if the MPSCNNConvolution inputs 32 channels, and the source has 64 channels, then it is an error to set
 *              primarySourceFeatureChannelOffset > 32.
 */
@property (readwrite, nonatomic) NSUInteger              primarySourceFeatureChannelOffset MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   secondarySourceFeatureChannelOffset
 *  @abstract   The number of channels in the secondary source MPSImage to skip before reading the input.
 *  @discussion This is the starting offset into the secondary source image in the feature channel dimension
 *              at which source data is read. Unit: feature channels
 *              This allows an application to read a subset of all the channels in MPSImage as input of MPSKernel.
 *              E.g. Suppose MPSImage has 24 channels and a MPSKernel needs to read 8 channels. If
 *              we want channels 8 to 15 of this MPSImage to be used as input, we can set secondarySourceFeatureChannelOffset = 8.
 *              Note that this offset applies independently to each image when the MPSImage
 *              is a container for multiple images and the MPSCNNKernel is processing multiple images (clipRect.size.depth > 1).
 *              The default value is 0 and any value specifed shall be a multiple of 4. If MPSKernel inputs N channels,
 *              the source image MUST have at least primarySourceFeatureChannelOffset + N channels. Using a source
 *              image with insufficient number of feature channels will result in an error.
 *              E.g. if the MPSCNNConvolution inputs 32 channels, and the source has 64 channels, then it is an error to set
 *              primarySourceFeatureChannelOffset > 32.
 */
@property (readwrite, nonatomic) NSUInteger              secondarySourceFeatureChannelOffset MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   primarySourceFeatureChannelMaxCount
 *  @abstract   The maximum number of channels in the primary source MPSImage to use
 *  @discussion Most filters can insert a slice operation into the filter for free.
 *              Use this to limit the size of the feature channel slice taken from
 *              the input image. If the value is too large, it is truncated to be
 *              the remaining size in the image after the sourceFeatureChannelOffset
 *              is taken into account.  Default: ULONG_MAX
 */
@property (readwrite, nonatomic) NSUInteger              primarySourceFeatureChannelMaxCount;

/*! @property   secondarySourceFeatureChannelMaxCount
 *  @abstract   The maximum number of channels in the secondary source MPSImage to use
 *  @discussion Most filters can insert a slice operation into the filter for free.
 *              Use this to limit the size of the feature channel slice taken from
 *              the input image. If the value is too large, it is truncated to be
 *              the remaining size in the image after the sourceFeatureChannelOffset
 *              is taken into account.  Default: ULONG_MAX
 */
@property (readwrite, nonatomic) NSUInteger              secondarySourceFeatureChannelMaxCount;

/*! @property   primaryEdgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of the primary source image
 *  @discussion Most MPSKernel objects can read off the edge of the source image. This can happen
 *              because of a negative offset property, because the offset + clipRect.size is larger
 *              than the source image or because the filter looks at neighboring pixels, such as a
 *              Convolution filter.   Default:  MPSImageEdgeModeZero.
 *
 *              See Also: @ref subsubsection_edgemode
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        primaryEdgeMode;

/*! @property   secondaryEdgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of the primary source image
 *  @discussion Most MPSKernel objects can read off the edge of the source image. This can happen
 *              because of a negative offset property, because the offset + clipRect.size is larger
 *              than the source image or because the filter looks at neighboring pixels, such as a
 *              Convolution filter.   Default:  MPSImageEdgeModeZero.
 *
 *              See Also: @ref subsubsection_edgemode
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        secondaryEdgeMode;

/*! @property   primaryKernelWidth
 *  @abstract   The width of the MPSCNNBinaryKernel filter window
 *  @discussion This is the horizontal diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNKernel does not have a filter window, then
 *              1 will be returned.
 */
@property (readonly, nonatomic) NSUInteger              primaryKernelWidth MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   primaryKernelHeight
 *  @abstract   The height of the MPSCNNBinaryKernel filter window
 *  @discussion This is the vertical diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNKernel does not have a filter window, then
 *              1 will be returned.
 */
@property (readonly, nonatomic) NSUInteger              primaryKernelHeight MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   kernelWidth
 *  @abstract   The width of the MPSCNNBinaryKernel filter window for the second image source
 *  @discussion This is the horizontal diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNBinaryKernel does not have a filter window, then
 *              1 will be returned.
 */
@property (readonly, nonatomic) NSUInteger              secondaryKernelWidth MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   kernelHeight
 *  @abstract   The height of the MPSCNNBinaryKernel filter window for the second image source
 *  @discussion This is the vertical diameter of the region read by the filter for each
 *              result pixel. If the MPSCNNBinaryKernel does not have a filter window, then
 *              1 will be returned.
 */
@property (readonly, nonatomic) NSUInteger              secondaryKernelHeight MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @property   primaryStrideInPixelsX
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the horizontal dimension
 *              for the primary source image
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 */
@property(readwrite, nonatomic) NSUInteger              primaryStrideInPixelsX MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   primaryStrideInPixelsY
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the vertical dimension
 *              for the primary source image
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 */
@property(readwrite, nonatomic) NSUInteger              primaryStrideInPixelsY MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   secondaryStrideInPixelsX
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the horizontal dimension
 *              for the secondary source image
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 */
@property(readwrite, nonatomic) NSUInteger              secondaryStrideInPixelsX MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   secondaryStrideInPixelsY
 *  @abstract   The downsampling (or upsampling if a backwards filter) factor in the vertical dimension
 *              for the secondary source image
 *  @discussion If the filter does not do up or downsampling, 1 is returned.
 */
@property(readwrite, nonatomic) NSUInteger              secondaryStrideInPixelsY MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   dilationRateX
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the X dimension.
 */
@property(readonly, nonatomic) NSUInteger               primaryDilationRateX MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   dilationRate
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the Y dimension.
 */
@property(readonly, nonatomic) NSUInteger               primaryDilationRateY MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   dilationRateX
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the X dimension.
 *  @discussion As applied to the secondary source image.
 */
@property(readonly, nonatomic) NSUInteger               secondaryDilationRateX MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   dilationRate
 *  @abstract   Stride in source coordinates from one kernel tap to the next in the Y dimension.
 *  @discussion As applied to the secondary source image.
 */
@property(readonly, nonatomic) NSUInteger               secondaryDilationRateY MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @property   isBackwards
 *  @abstract   YES if the filter operates backwards.
 *  @discussion This influences how strideInPixelsX/Y should be interpreted.
 */
@property(readonly, nonatomic) BOOL                     isBackwards;

/*! @abstract   Returns true if the -encode call modifies the state object it accepts.  */
@property (readonly, nonatomic) BOOL                    isStateModified
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @property   padding
 *  @abstract   The padding method used by the filter
 *  @discussion This influences how strideInPixelsX/Y should be interpreted.
 *              Default:  MPSNNPaddingMethodAlignCentered | MPSNNPaddingMethodAddRemainderToTopLeft | MPSNNPaddingMethodSizeSame
 *              Some object types (e.g. MPSCNNFullyConnected) may override this default with something appropriate to its operation.
 */
@property (readwrite, nonatomic, nonnull, retain) id <MPSNNPadding>    padding;

/*! @abstract   Method to allocate the result image for -encodeToCommandBuffer:sourceImage:
 *  @discussion Default: MPSTemporaryImage.defaultAllocator
 */
@property (readwrite, nonatomic, retain, nonnull) id <MPSImageAllocator>  destinationImageAllocator;



/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSCNNKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @discussion This is the older style of encode which reads the offset, doesn't change it,
 *              and ignores the padding method.
 *  @param      commandBuffer        A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryImage         A valid MPSImage object containing the primary source image.
 *  @param      secondaryImage       A valid MPSImage object containing the secondary source image.
 *  @param      destinationImage     A valid MPSImage to be overwritten by result image. destinationImage may not alias primarySourceImage or secondarySourceImage.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 primaryImage: (MPSImage * __nonnull) primaryImage
               secondaryImage: (MPSImage * __nonnull) secondaryImage
             destinationImage: (MPSImage * __nonnull) destinationImage
                MPS_SWIFT_NAME( encode(commandBuffer:primaryImage:secondaryImage:destinationImage:));

/*!
 *  @abstract   Encode a MPSCNNKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @discussion This is the older style of encode which reads the offset, doesn't change it,
 *              and ignores the padding method. Multiple images are processed concurrently.
 *              All images must have MPSImage.numberOfImages = 1.
 *  @param      commandBuffer         A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryImages         An array of MPSImage objects containing the primary source images.
 *  @param      secondaryImages       An array MPSImage objects containing the secondary source images.
 *  @param      destinationImages     An array of MPSImage objects to contain the result images.
 *                                    destinationImages may not alias primarySourceImages or secondarySourceImages
 *                                    in any manner.
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                     primaryImages: (MPSImageBatch * __nonnull) primaryImages
                   secondaryImages: (MPSImageBatch * __nonnull) secondaryImages
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages
                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                MPS_SWIFT_NAME( encodeBatch(commandBuffer:primaryImages:secondaryImages:destinationImages:));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture to hold the result and return it.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property.  See discussion in MPSNeuralNetworkTypes.h.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          primaryImage        A MPSImages to use as the primary source images for the filter.
 *  @param          secondaryImage      A MPSImages to use as the secondary source images for the filter.
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                 primaryImage: (MPSImage * __nonnull) primaryImage
                               secondaryImage: (MPSImage * __nonnull) secondaryImage
                        MPS_SWIFT_NAME( encode(commandBuffer:primaryImage:secondaryImage:));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create textures to hold the results and return them.
 *  @discussion     In the first iteration on this method, encodeBatchToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property.  See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          primaryImage        A MPSImages to use as the primary source images for the filter.
 *  @param          secondaryImage      A MPSImages to use as the secondary source images for the filter.
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                          primaryImages: (MPSImageBatch * __nonnull) primaryImage
                                        secondaryImages: (MPSImageBatch * __nonnull) secondaryImage
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( encodeBatch(commandBuffer:primaryImages:secondaryImages:));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture and state to hold the results and return them.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationState:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          primaryImage        A MPSImage to use as the source images for the filter.
 *  @param          secondaryImage      A MPSImage to use as the source images for the filter.
 *  @param          outState            The address of location to write the pointer to the result state of the operation
 *  @param          isTemporary         YES if the outState should be a temporary object
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The offset property will be adjusted to reflect the offset used during the encode.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                 primaryImage: (MPSImage *  __nonnull) primaryImage
                               secondaryImage: (MPSImage *  __nonnull) secondaryImage
                             destinationState: (__autoreleasing MPSState * __nullable * __nonnull) outState
                  destinationStateIsTemporary: (BOOL) isTemporary
                    MPS_SWIFT_NAME( encode(commandBuffer:primaryImage:secondaryImage:destinationState:destinationStateIsTemporary:))
                    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture and state to hold the results and return them.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationState:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *                  All images in a batch must have MPSImage.numberOfImages = 1.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          primaryImages       A MPSImage to use as the source images for the filter.
 *  @param          secondaryImages     A MPSImage to use as the source images for the filter.
 *  @param          outState            A new state object is returned here.
 *  @param          isTemporary         YES if the outState should be a temporary object
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The offset property will be adjusted to reflect the offset used during the encode.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                          primaryImages: (MPSImageBatch *  __nonnull) primaryImages
                                        secondaryImages: (MPSImageBatch *  __nonnull) secondaryImages
                                      destinationStates: (__autoreleasing MPSStateBatch * __nullable * __nonnull) outState
                            destinationStateIsTemporary: (BOOL) isTemporary
            MPS_SWIFT_NAME( encodeBatch(commandBuffer:primaryImages:secondaryImages:destinationStates:destinationStateIsTemporary:))
            MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));



/*! @abstract   Allocate a MPSState (subclass) to hold the results from a -encodeBatchToCommandBuffer... operation
 *  @discussion A graph may need to allocate storage up front before executing.  This may be
 *              necessary to avoid using too much memory and to manage large batches.  The function
 *              should allocate a MPSState object (if any) that will be produced by an -encode call
 *              with the indicated sourceImages and sourceStates inputs. Though the states
 *              can be further adjusted in the ensuing -encode call, the states should
 *              be initialized with all important data and all MTLResource storage allocated.
 *              The data stored in the MTLResource need not be initialized, unless the ensuing
 *              -encode call expects it to be.
 *
 *              The MTLDevice used by the result is derived from the source image.
 *              The padding policy will be applied to the filter before this is called
 *              to give it the chance to configure any properties like MPSCNNKernel.offset.
 *
 *              CAUTION: the result state should be made after the kernel properties are
 *                       configured for the -encode call that will write to the state, and
 *                       after -destinationImageDescriptorForSourceImages:sourceStates:
 *                       is called (if it is called). Otherwise, behavior is undefined.
 *                       Please see the description of
 *                       -[MPSCNNKernel resultStateForSourceImage:sourceStates:destinationImage:] for more.
 *
 *              Default: returns nil
 *
 *  @param      primaryImage        The MPSImage consumed by the associated -encode call.
 *  @param      secondaryImage      The MPSImage consumed by the associated -encode call.
 *  @param      sourceStates        The list of MPSStates consumed by the associated -encode call,
 *                                  for a batch size of 1.
 *  @return     The list of states produced by the -encode call for batch size of 1.
 *              When the batch size is not 1, this function will be called repeatedly unless
 *              -isResultStateReusedAcrossBatch returns YES. If  -isResultStateReusedAcrossBatch
 *              returns YES, then it will be called once per batch and the MPSStateBatch array will
 *              contain MPSStateBatch.length references to the same object.
 */
-(MPSState * __nullable) resultStateForPrimaryImage: (MPSImage * __nonnull) primaryImage
                                     secondaryImage: (MPSImage * __nonnull) secondaryImage
                                       sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                   destinationImage: (MPSImage * __nonnull) destinationImage
                            MPS_SWIFT_NAME( resultState(primaryImage:secondaryImage:sourceStates:destinationImage:))
                            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

-(MPSStateBatch * __nullable) resultStateBatchForPrimaryImage: (MPSImageBatch * __nonnull) primaryImage
                                               secondaryImage: (MPSImageBatch * __nonnull) secondaryImage
                                                 sourceStates: (NSArray <MPSStateBatch *> *__nullable) sourceStates
                                             destinationImage: (MPSImageBatch * __nonnull) destinationImage
                            MPS_SWIFT_NAME( resultStateBatch(primaryImage:secondaryImage:sourceStates:destinationImage:))
                            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @abstract   Allocate a temporary MPSState (subclass) to hold the results from a -encodeBatchToCommandBuffer... operation
 *  @discussion A graph may need to allocate storage up front before executing.  This may be
 *              necessary to avoid using too much memory and to manage large batches.  The function
 *              should allocate any MPSState objects that will be produced by an -encode call
 *              with the indicated sourceImages and sourceStates inputs. Though the states
 *              can be further adjusted in the ensuing -encode call, the states should
 *              be initialized with all important data and all MTLResource storage allocated.
 *              The data stored in the MTLResource need not be initialized, unless the ensuing
 *              -encode call expects it to be.
 *
 *              The MTLDevice used by the result is derived from the command buffer.
 *              The padding policy will be applied to the filter before this is called
 *              to give it the chance to configure any properties like MPSCNNKernel.offset.
 *
 *              CAUTION: the result state should be made after the kernel properties are
 *                       configured for the -encode call that will write to the state, and
 *                       after -destinationImageDescriptorForSourceImages:sourceStates:
 *                       is called (if it is called). Otherwise, behavior is undefined.
 *                       Please see the description of
 *                       -[MPSCNNKernel resultStateForSourceImage:sourceStates:destinationImage] for more.
 *
 *              Default: returns nil
 *
 *  @param      commandBuffer       The command buffer to allocate the temporary storage against
 *                                  The state will only be valid on this command buffer.
 *  @param      primaryImage        The MPSImage consumed by the associated -encode call.
 *  @param      secondaryImage      The MPSImage consumed by the associated -encode call.
 *  @param      sourceStates        The list of MPSStates consumed by the associated -encode call,
 *                                  for a batch size of 1.
 *  @return     The list of states produced by the -encode call for batch size of 1.
 *              When the batch size is not 1, this function will be called repeatedly unless
 *              -isResultStateReusedAcrossBatch returns YES. If  -isResultStateReusedAcrossBatch
 *              returns YES, then it will be called once per batch and the MPSStateBatch array will
 *              contain MPSStateBatch.length references to the same object.
 */
-(MPSState * __nullable) temporaryResultStateForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                 primaryImage: (MPSImage *__nonnull) primaryImage
                                               secondaryImage: (MPSImage *__nonnull) secondaryImage
                                                 sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                             destinationImage: (MPSImage *__nonnull) destinationImage
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( temporaryResultState(commandBuffer:primaryImage:secondaryImage:sourceStates:destinationImage:));

-(MPSStateBatch * __nullable) temporaryResultStateBatchForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                           primaryImage: (MPSImageBatch *__nonnull) primaryImage
                                                         secondaryImage: (MPSImageBatch *__nonnull) secondaryImage
                                                           sourceStates: (NSArray <MPSStateBatch *> *__nullable) sourceStates
                                                       destinationImage: (MPSImageBatch *__nonnull) destinationImage
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( temporaryResultStateBatch(commandBuffer:primaryImage:secondaryImage:sourceStates:destinationImage:));

/*! @abstract   Returns YES if the same state is used for every operation in a batch
 *  @discussion If NO, then each image in a MPSImageBatch will need a corresponding
 *              (and different) state to go with it. Set to YES to avoid allocating
 *              redundant state in the case when the same state is used all the time.
 *              Default: NO    */
-(BOOL)     isResultStateReusedAcrossBatch MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Returns YES if the filter must be run over the entire batch before its
 *              results may be considered complete
 *  @discussion The MPSNNGraph may split batches into sub-batches to save memory. However,
 *              some filters, like batch statistics calculations, need to operate over
 *              the entire batch to calculate a valid result, in this case, the mean and
 *              variance per channel over the set of images.
 *
 *              In such cases, the accumulated result is commonly stored in a MPSState
 *              containing a MTLBuffer. (MTLTextures may not be able to be read from
 *              and written to in the same filter on some devices.) -isResultStateReusedAcrossBatch
 *              is set to YES, so that the state is allocated once and passed in for each
 *              sub-batch and the filter accumulates its results into it, one sub-batch
 *              at a time. Note that sub-batches may frequently be as small as 1.
 *
 *              Default: NO
 */
-(BOOL)     appendBatchBarrier MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Get a suggested destination image descriptor for a source image
 *  @discussion Your application is certainly free to pass in any destinationImage
 *              it likes to encodeToCommandBuffer:sourceImage:destinationImage,
 *              within reason. This is the basic design for iOS 10. This method
 *              is therefore not required.
 *
 *              However, calculating the MPSImage size and MPSCNNBinaryKernel properties
 *              for each filter can be tedious and complicated work, so this method
 *              is made available to automate the process. The application may
 *              modify the properties of the descriptor before a MPSImage is made from
 *              it, so long as the choice is sensible for the kernel in question.
 *              Please see individual kernel descriptions for restrictions.
 *
 *              The expected timeline for use is as follows:
 *
 *                1) This method is called:
 *                  a) The default MPS padding calculation is applied. It
 *                     uses the MPSNNPaddingMethod of the .padding property to
 *                     provide a consistent addressing scheme over the graph.
 *                     It creates the MPSImageDescriptor and adjusts the .offset
 *                     property of the MPSNNKernel. When using a MPSNNGraph, the
 *                     padding is set using the MPSNNFilterNode as a proxy.
 *
 *                  b) This method may be overridden by MPSCNNBinaryKernel subclass
 *                     to achieve any customization appropriate to the object type.
 *
 *                  c) Source states are then applied in order. These may modify the
 *                     descriptor and may update other object properties. See:
 *                      -destinationImageDescriptorForSourceImages:sourceStates:
 *                       forKernel:suggestedDescriptor:  This is the typical way
 *                      in which MPS may attempt to influence the operation of
 *                      its kernels.
 *
 *                  d) If the .padding property has a custom padding policy method
 *                      of the same name, it is called. Similarly, it may also adjust
 *                      the descriptor and any MPSCNNBinaryKernel properties. This is the
 *                      typical way in which your application may attempt to influence
 *                      the operation of the MPS kernels.
 *
 *                2) A result is returned from this method and the caller
 *                     may further adjust the descriptor and kernel properties
 *                     directly.
 *
 *                3) The caller uses the descriptor to make a new MPSImage to
 *                   use as the destination image for the -encode call in step 5.
 *
 *                4) The caller calls -resultStateForSourceImage:sourceStates:destinationImage:
 *                    to make any result states needed for the kernel. If there isn't
 *                    one, it will return nil. A variant is available to return a
 *                    temporary state instead.
 *
 *                5) a -encode method is called to encode the kernel.
 *
 *              The entire process 1-5 is more simply achieved by just calling an -encode...
 *              method that returns a MPSImage out the left hand sid of the method. Simpler
 *              still, use the MPSNNGraph to coordinate the entire process from end to end.
 *              Opportunities to influence the process are of course reduced, as (2) is no longer
 *              possible with either method. Your application may opt to use the five step method
 *              if it requires greater customization as described, or if it would like to estimate
 *              storage in advance based on the sum of MPSImageDescriptors before processing
 *              a graph. Storage estimation is done by using the MPSImageDescriptor to create
 *              a MPSImage (without passing it a texture), and then call -resourceSize. As long
 *              as the MPSImage is not used in an encode call and the .texture property is not
 *              invoked, the underlying MTLTexture is not created.
 *
 *              No destination state or destination image is provided as an argument to this
 *              function because it is expected they will be made / configured after this
 *              is called. This method is expected to auto-configure important object properties
 *              that may be needed in the ensuing destination image and state creation steps.
 *
 *  @param      sourceImages    A array of source images that will be passed into the -encode call
 *                              Since MPSCNNBinaryKernel is a binary kernel, it is an array of length 2.
 *  @param      sourceStates    An optional array of source states that will be passed into the -encode call
 *  @return     an image descriptor allocated on the autorelease pool
 */
-(MPSImageDescriptor*__nonnull) destinationImageDescriptorForSourceImages: (NSArray<MPSImage *> * __nonnull) sourceImages
                                                             sourceStates: (NSArray<MPSState *> * __nullable) sourceStates;

@end


/*! @class  MPSCNNGradientKernel
 *  @discussion Gradient kernels are the backwards pass of a MPSCNNKernel
 *              used during training to calculate gradient back propagation.
 *              These take as arguments the gradient result from the next filter
 *              and the source image for the forward version of the filter.
 *              There is also a MPSNNGradientState passed from MPSCNNKernel
 *              to MPSCNNGradientKernel that contains information about the
 *              MPSCNNKernel parameters at the time it encoded and possibly
 *              also additional MTLResources to enable it to do its job.
 *
 *@code
 *          Training graph (partial):
 *
 *              ---> input image ---------> MPSCNNKernel ------>  resultImage ------>-->-->-->.
 *                             \                  |                                           |
 *                              '------.    MPSNNGradientState                         loss estimation
 *                                      \         |                                           |
 *                                       V        V                                           V
 *              <--- result gradient <- MPSCNNGradientKernel <---  input gradient <--<--<--<---'
 *
 *              In general operation, starting with the input image, the sequence of events is:
 *              1a)  Invoke padding policy to find result size for MPSCNNKernel.  This
 *                   also configures some MPSCNNKernel parameters such as offset.
 *              1b)  Use the MPSImageDescriptor from 1a to make resultImage.
 *              1c)  Call MPSCNNKernel -encode...
 *              2) stages 1a-c are repeated for other forward passes in the inference portion of the graph
 *              3) We estimate the loss resulting from the whole inference computation so far (see MPSCNNLoss.h>
 *              4) stages 5a-c are repeated for corresponding backward gradient passes in the graph
 *              5a) Invoke padding policy on the MPSCNNGradientKernel shown above. This sets the
 *                  MPSCNNGradientKernel parameters to correspond with those in the forward pass
 *              5b) The result gradient for the MPSCNNGradientKernel is created from the MPSImageDescriptor from 5a
 *              5c) Call MPSCNNGradientKernel -encode with the input image, input gradient, result gradient and MPSNNGradientState
 *              6) pass the result gradient on to leftward gradient passes.
 *@endcode
 *
 *              For MPSCNNKernels that are trained, there may be other accompanying training kernels that
 *              need to be called in addition to the gradient kernel to update convolution weights or batch
 *              normalization parameters, for example. Steps 1a-c and 5a-c can be combined in a single -encode
 *              call. These return the result image or gradient out the left hand side.
 *
 *              For purposes of inheritance the gradient image is the MPSCNNBinaryKernel primary image
 *              and the source image is the MPSCNNBinaryKernel secondary image. Various secondary properties
 *              such as kernel size are copies of the forward inference pass parameters of similar name
 *              are set automatically when -[MPSCNNGradientKernel destinationImageDescriptorForSourceImages:sourceStates:]
 *              is called.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNGradientKernel : MPSCNNBinaryKernel

/*! @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     A pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


/*! @property   kernelOffsetX
 *  @abstract   Offset in the kernel reference frame to position the kernel in the X dimension
 *  @discussion In some cases, the input gradient must be upsampled with zero insertion
 *              to account for things like strides in the forward MPSCNNKernel pass.
 *              As such, the offset, which describes a X,Y offset in the source coordinate
 *              space is insufficient to fully describe the offset applied to a kernel.
 *              The kernel offset is the offset after upsampling. Both the source offset
 *              and kernel offset are additive:  effective offset = source offset * stride + kernel offset.
 *              The offset is applied to the (upsampled) source gradient  */
@property (readwrite, nonatomic) NSInteger                kernelOffsetX;

/*! @property   kernelOffsetY
 *  @abstract   Offset in the kernel reference frame to position the kernel in the Y dimension
 *  @discussion In some cases, the input gradient must be upsampled with zero insertion
 *              to account for things like strides in the forward MPSCNNKernel pass.
 *              As such, the offset, which describes a X,Y offset in the source coordinate
 *              space is insufficient to fully describe the offset applied to a kernel.
 *              The kernel offset is the offset after upsampling. Both the source offset
 *              and kernel offset are additive:  effective offset = source offset * stride + kernel offset.
*               The offset is applied to the (upsampled) source gradient */
@property (readwrite, nonatomic) NSInteger                kernelOffsetY;

/*! @abstract   Encode a gradient filter and return a gradient
 *  @discussion During training, gradient filters are used to calculate the gradient
 *              associated with the loss for each feature channel in the forward pass
 *              source image. For those nodes that are trainable, these are then used
 *              to refine the value used in the trainable parameter. They consume
 *              a source gradient image which contains the gradients corresponding
 *              with the forward pass destination image, and calculate the gradients
 *              corresponding to the forward pass source image.
 *
 *              A gradient filter consumes a MPSNNGradientState object which captured
 *              various forward pass properties such as offset and edgeMode at the time
 *              the forward pass was encoded. These are transferred to the MPSCNNBinaryKernel
 *              secondary image properties automatically when this method creates its
 *              destination image.
 *
 *  @param      commandBuffer   The MTLCommandBuffer on which to encode
 *  @param      sourceGradient  The gradient image from the "next" filter in the graph (in the inference direction)
 *  @param      sourceImage     The image used as source image by the forward inference pass
 *  @param      gradientState   The MPSNNGradientState or MPSNNBinaryGradientState subclass produced by the forward
 *                              inference pass
 *  @result   The result gradient from the gradient filter */
-(MPSImage*__nonnull) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                             sourceGradient: (MPSImage * __nonnull ) sourceGradient        // a.k.a. primaryImage
                                sourceImage: (MPSImage * __nonnull ) sourceImage           // source Image from forward pass, a.k.a. secondaryImage
                              gradientState: (MPSState * __nonnull ) gradientState         // MPSNNGradientState/MPSNNBinaryGradientState from forward pass;
            MPS_SWIFT_NAME( encode(commandBuffer:sourceGradient:sourceImage:gradientState:));

/*! @abstract   Encode a gradient filter and return a gradient
 *  @discussion During training, gradient filters are used to calculate the gradient
 *              associated with the loss for each feature channel in the forward pass
 *              source image. For those nodes that are trainable, these are then used
 *              to refine the value used in the trainable parameter. They consume
 *              a source gradient image which contains the gradients corresponding
 *              with the forward pass destination image, and calculate the gradients
 *              corresponding to the forward pass source image.
 *
 *              A gradient filter consumes a MPSNNGradientState object which captured
 *              various forward pass properties such as offset and edgeMode at the time
 *              the forward pass was encoded. These are transferred to the MPSCNNBinaryKernel
 *              secondary image properties automatically when you use -[MPSCNNGradientKernel
 *              destinationImageDescriptorForSourceImages:sourceStates:]. If you do not call
 *              this method, then you are responsible for configuring all of the primary and
 *              secondary image properties in MPSCNNBinaryKernel. Please see class description
 *              for expected ordering of operations.
 *
 *  @param      commandBuffer   The MTLCommandBuffer on which to encode
 *  @param      sourceGradient  The gradient image from the "next" filter in the graph
 *  @param      sourceImage     The image used as source image from the forward pass
 *  @param      gradientState   The MPSNNGradientState and MPSNNBinaryGradientState subclass produced by the
 *                              forward pass
 *  @param      destinationGradient  The MPSImage into which to write the filter result*/
-(void) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
               sourceGradient: (MPSImage * __nonnull ) sourceGradient        // a.k.a. primaryImage
                  sourceImage: (MPSImage * __nonnull ) sourceImage           // source Image from forward pass, a.k.a. secondaryImage
                gradientState: (MPSState * __nonnull ) gradientState         // MPSNNGradientState/MPSNNBinaryGradientState from forward pass;
          destinationGradient: (MPSImage * __nonnull ) destinationGradient
    MPS_SWIFT_NAME( encode(commandBuffer:sourceGradient:sourceImage:gradientState:destinationGradient:));

/*! @abstract   Encode a gradient filter and return a gradient
 *  @discussion During training, gradient filters are used to calculate the gradient
 *              associated with the loss for each feature channel in the forward pass
 *              source image. For those nodes that are trainable, these are then used
 *              to refine the value used in the trainable parameter. They consume
 *              a source gradient image which contains the gradients corresponding
 *              with the forward pass destination image, and calculate the gradients
 *              corresponding to the forward pass source image.
 *
 *              A gradient filter consumes a MPSNNGradientState object which captured
 *              various forward pass properties such as offset and edgeMode at the time
 *              the forward pass was encoded. These are transferred to the MPSCNNBinaryKernel
 *              secondary image properties automatically when this method creates its
 *              destination image.
 *  @param      commandBuffer    The MTLCommandBuffer on which to encode
 *  @param      sourceGradients  The gradient images from the "next" filter in the graph
 *  @param      sourceImages     The images used as source image from the forward pass
 *  @param      gradientStates   The MPSNNGradientState or MPSNNBinaryGradientState subclass produced by the
 *                               forward pass
 */
-(MPSImageBatch*__nonnull) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                                      sourceGradients: (MPSImageBatch * __nonnull ) sourceGradients        // a.k.a. primaryImage
                                         sourceImages: (MPSImageBatch * __nonnull ) sourceImages           // source Image from forward pass, a.k.a. secondaryImage
                                       gradientStates: (MPSStateBatch * __nonnull ) gradientStates
        MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceGradients:sourceImages:gradientStates:));

/*! @abstract   Encode a gradient filter and return a gradient
 *  @discussion During training, gradient filters are used to calculate the gradient
 *              associated with the loss for each feature channel in the forward pass
 *              source image. For those nodes that are trainable, these are then used
 *              to refine the value used in the trainable parameter. They consume
 *              a source gradient image which contains the gradients corresponding
 *              with the forward pass destination image, and calculate the gradients
 *              corresponding to the forward pass source image.
 *
 *              A gradient filter consumes a MPSNNGradientState object which captured
 *              various forward pass properties such as offset and edgeMode at the time
 *              the forward pass was encoded. These are transferred to the MPSCNNBinaryKernel
 *              secondary image properties automatically when you use -[MPSCNNGradientKernel
 *              destinationImageDescriptorForSourceImages:sourceStates:]. If you do not call
 *              this method, then you are responsible for configuring all of the primary and
 *              secondary image properties in MPSCNNBinaryKernel. Please see class description
 *              for expected ordering of operations.
 *  @param      commandBuffer    The MTLCommandBuffer on which to encode
 *  @param      sourceGradients  The gradient images from the "next" filter in the graph
 *  @param      sourceImages     The image used as source images from the forward pass
 *  @param      gradientStates   An array of the MPSNNGradientState or MPSNNBinaryGradientState subclass
 *                               produced by the forward pass
 *  @param      destinationGradients  The MPSImages into which to write the filter result */
-(void) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                   sourceGradients: (MPSImageBatch * __nonnull ) sourceGradients        // a.k.a. primaryImage
                      sourceImages: (MPSImageBatch * __nonnull ) sourceImages           // source Image from forward pass, a.k.a. secondaryImage
                    gradientStates: (MPSStateBatch * __nonnull ) gradientStates
              destinationGradients: (MPSImageBatch * __nonnull ) destinationGradients
        MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceGradients:sourceImages:gradientStates:destinationGradients:));

@end

#pragma mark -

#endif /* MPSCNNKernel_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSMatrixSum.h
/*!
 *  @header MPSMatrixSum.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders filter base classes
 */

#ifndef MPSMatrixSum_h
#define MPSMatrixSum_h

#include <MPSNeuralNetwork/MPSCNNNeuronType.h>
#include <MPSMatrix/MPSMatrix.h>

/*!
 *  @class      MPSMatrixSum
 *  @dependency This depends on Metal.framework
 *  @discussion MPSMatrixSum performs a pointwise summation of N MPSMatrix
 *              objects and applies an optional bias term and neuron activation
 *              function.
 *
 *              MPSMatrix A = empty matrix;
 *              for (i = 0; i < N; ++i)
 *                  A += alpha[i]*B[i];
 *
 *              if (bias)
 *                  A += broadcast(bias);
 *
 *              if (neuron)
 *                  A = applyNeuron(A);
 *
 *              Where B is the array of MPSMatrix objects, A is the destination
 *              MPSMatrix, alpha is an array of scalar values, bias is a vector
 *              which is broadcast and accumulated across each row of the intermediate
 *              result, and applyNeuron is a neuron activation function.
 *
 *              Each matrix in the array may have an independent origin.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixSum : MPSKernel
/*
 * Use initWithDevice:count:rows:columns:transpose instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract   Initialize a MPSMatrixSum kernel.
 *  @param      device                      The device on which to initialize the kernel.
 *  @param      count                       The number of matrices to be summed.
 *  @param      rows                        The number of rows to use in the input matrices.
 *  @param      columns                     The number of columns to use in the input matrices.
 *  @param      transpose                   If YES the result of the summation is to be transposed
 *                                          prior to applying the bias and activation.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                 count: (NSUInteger) count
                                  rows: (NSUInteger) rows
                               columns: (NSUInteger) columns
                             transpose: (BOOL) transpose
NS_DESIGNATED_INITIALIZER;

/*! @abstract   The number of rows to sum. */
@property (nonatomic, readonly) NSUInteger rows;

/*! @abstract   The number of columns to sum. */
@property (nonatomic, readonly) NSUInteger columns;

/*! @abstract   The number of matrices to sum. */
@property (nonatomic, readonly) NSUInteger count;

/*! @abstract   The transposition used to initialize the kernel. */
@property (nonatomic, readonly) BOOL transpose;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. An MPSMatrixSum kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 @abstract      Neuron parameter A.
 */
@property (nonatomic, readonly) float neuronParameterA;

/*!
 @abstract      Neuron parameter B.
 */
@property (nonatomic, readonly) float neuronParameterB;

/*!
 @abstract      Neuron parameter C.
 */
@property (nonatomic, readonly) float neuronParameterC;

/*! @abstract   Encode the operations to the command buffer
 *  @param          buffer              The command buffer in which to encode the operation.
 *  @param          sourceMatrices      A list of matrices from which the matrix data is read.
 *  @param          resultMatrix        The result matrix.
 *  @param          scaleVector         A MPSVector of type MPSDataTypeFloat32 containing the list of
 *                                      scale factors, specified in single precision.
 *  @param          offsetVector        A MPSVector of type MPSDataTypeUInt32 containing the list of
 *                                      offsets, stored as a packed array of MPSMatrixOffset values.
 *  @param          biasVector          A MPSVector containing the bias terms to add to the result
 *                                      prior to applying the neuron function, if any.  May be nil.
 *  @param          startIndex          The starting index into the scale and offset vectors.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) buffer
               sourceMatrices: (NSArray<MPSMatrix *>*__nonnull) sourceMatrices
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
                  scaleVector: (MPSVector * __nullable) scaleVector
                 offsetVector: (MPSVector * __nullable) offsetVector
                   biasVector: (MPSVector * __nullable) biasVector
                   startIndex: (NSUInteger) startIndex;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixSum kernel.
 *  @param      device      The MTLDevice on which to make the MPSMatrixSum object.
 *  @return     A new MPSMatrixSum object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;
@end // MPSMatrixSum
#endif /* MPSMatrixSum_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNGraph.h
//
//  MPSNNGraph.h
//  MPS
//
//  Created by Ian Ollmann on 10/17/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSNNGraph_h
#define MPSNNGraph_h

#include <MPSNeuralNetwork/MPSNNGraphNodes.h>
#include <MPSCore/MPSKernel.h>

/*! @abstract   A notification when computeAsyncWithSourceImages:completionHandler: has finished
 *  @param      result  If no error, the image produced by the graph operation.
 *  @param      error   If an error occurs, more information might be found here.
 */
typedef void (^MPSNNGraphCompletionHandler)( MPSImage * __nullable result,
                                             NSError * __nullable error);

/*! @class      MPSNNGraph
 *  @abstract   Optimized representation of a graph of MPSNNImageNodes and MPSNNFilterNodes
 *  @discussion Once you have prepared a graph of MPSNNImageNodes and MPSNNFilterNodes
 *              (and if needed MPSNNStateNodes), you may initialize a MPSNNGraph using 
 *              the MPSNNImageNode that you wish to appear as the result. The MPSNNGraph 
 *              object will introspect the graph representation and determine which nodes
 *              are needed for inputs, and which nodes are produced as output state (if any).
 *              Nodes which are not needed to calculate the result image node are ignored.
 *              Some nodes may be internally concatenated with other nodes for better 
 *              performance.
 *
 *              Note: the MPSNNImageNode that you choose as the result node may be interior
 *                    to a graph. This feature is provided as a means to examine intermediate
 *                    computations in the full graph for debugging purposes.
 *
 *              During MPSNNGraph construction, the graph attached to the result node will 
 *              be parsed and reduced to an optimized representation. This representation may
 *              be saved using the NSSecureCoding protocol for later recall.
 *                  
 *              When decoding a MPSNNGraph using a NSCoder, it will be created against 
 *              the system default MTLDevice. If you would like to set the MTLDevice,
 *              your NSCoder should conform to the <MPSDeviceProvider> protocol.
 *
 *              You may find it helpful to set MPSKernelOptionsVerbose on the graph when
 *              debugging. To turn this on during MPSKernel initialization (including
 *              MPSNNGraph initialization) set the MPS_LOG_INFO environment variable.
 *              There is a lot of information about what optimizations are done to your
 *              graph, including some information on why certain optimizations were not
 *              made.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNGraph : MPSKernel <NSCopying, NSSecureCoding>

/*! @abstract   Initialize a MPSNNGraph object on a device starting with resultImage working backward
 *  @discussion The MPSNNGraph constructor will start with the indicated result image, and look
 *              to see what MPSNNFilterNode produced it, then look to its dependencies and so
 *              forth to reveal the subsection of the graph necessary to compute the image.
 *  @param      device      The MTLDevice on which to run the graph
 *  @param      resultImage The MPSNNImageNode corresponding to the last image in the graph.
 *                          This is the image that will be returned.  Note: the imageAllocator
 *                          for this node is ignored and the MPSNNGraph.destinationImageAllocator
 *                          is used for this node instead.
 *  @param      resultIsNeeded Commonly, when training a graph, the last MPSImage out of the
 *                             graph is not used. The final gradient filter is run solely to update
 *                             some weights. If resultIsNeeded is set to NO, nil will
 *                             be returned from the left hand side of the -encode call instead,
 *                             and computation to produce the last image may be pruned away.
 *  @result     A new MPSNNGraph.
 */

-(nullable instancetype)  initWithDevice: (nonnull id <MTLDevice>) device
                             resultImage: (MPSNNImageNode * __nonnull) resultImage
                     resultImageIsNeeded: (BOOL) resultIsNeeded NS_DESIGNATED_INITIALIZER;


+(nullable instancetype) graphWithDevice: (nonnull id <MTLDevice>) device
                             resultImage: (MPSNNImageNode * __nonnull) resultImage
                     resultImageIsNeeded: (BOOL) resultIsNeeded;


-(nullable instancetype)  initWithDevice: (nonnull id <MTLDevice>) device
                             resultImage: (MPSNNImageNode * __nonnull) resultImage
MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use -initWithDevice:resultImage:resultIsNeeded: instead. Without this information, too much or too little work may occur. Results may be undefined.",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0,11.3));

+(nullable instancetype) graphWithDevice: (nonnull id <MTLDevice>) device
                             resultImage: (MPSNNImageNode * __nonnull) resultImage
MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use +graphWithDevice:resultImage:resultIsNeeded: instead. Without this information, too much or too little work may occur. Results may be undefined.",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0,11.3));

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract Use initWithDevice:resultImage: instead */
-(nonnull instancetype) initWithDevice:(__nonnull id<MTLDevice>)device NS_UNAVAILABLE;

/*! @abstract   Get a list of identifiers for source images needed to calculate the result image */
@property (readonly, nonnull, copy, nonatomic) NSArray<id <MPSHandle>> * sourceImageHandles;

/*! @abstract   Get a list of identifiers for source state objects needed to calculate the result image 
 *  @discussion Not guaranteed to be in the same order as resultStateHandles  */
@property (readonly, nullable, copy, nonatomic) NSArray<id <MPSHandle>> * sourceStateHandles;

/*! @abstract   Get a list of identifiers for intermediate images objects produced by the graph */
@property (readonly, nullable, copy, nonatomic) NSArray<id <MPSHandle>> * intermediateImageHandles;

/*! @abstract   Get a list of identifiers for result state objects produced by the graph
 *  @discussion Not guaranteed to be in the same order as sourceStateHandles  */
@property (readonly, nullable, copy, nonatomic) NSArray<id <MPSHandle>> * resultStateHandles;

/*! @abstract   Get a handle for the graph result image */
@property (readonly, nullable, nonatomic) id <MPSHandle> resultHandle;

/*! @abstract   Should MPSState objects produced by -encodeToCommandBuffer... be temporary objects.
 *  @discussion See MPSState description. Default: NO
 */
@property (readwrite, nonatomic) BOOL  outputStateIsTemporary;

/*! @abstract   Method to allocate the result image from -encodeToCommandBuffer...
 *  @discussion This property overrides the allocator for the final result image in
 *              the graph. Default: MPSImage.defaultAllocator
 */
@property (readwrite, nonatomic, retain, nonnull) id <MPSImageAllocator> destinationImageAllocator;


/*! @abstract   The default storage format used for graph intermediate images
 *  @discussion This doesn't affect how data is stored in buffers in states.
 *              Nor does it affect the storage format for weights
 *              such as convolution weights stored by individual filters.
 *              Default: MPSImageFeatureChannelFormatFloat16 */
@property (readwrite, nonatomic) MPSImageFeatureChannelFormat   format;

/*! @abstract   Set at -init time.
 *  @discussion If NO, nil will be returned from -encode calls and some computation
 *              may be omitted. */
@property (readonly, nonatomic) BOOL resultImageIsNeeded;

/*! @abstract   Reinitialize all graph nodes from data sources
 *  @discussion A number of the nodes that make up a graph have a data source
 *              associated with them, for example a MPSCNNConvolutionDataSource
 *              or a MPSCNNBatchNormalizationDataSource. Generally, the data
 *              is read from these once at graph initialization time and then
 *              not looked at again, except during the weight / parameter update
 *              phase of the corresponding gradient nodes and then only if CPU
 *              updates are requested.  Otherwise, update occurs on the GPU,
 *              and the data in the data source is thereafter ignored.
 *
 *              It can happen, though, that your application has determined the
 *              graph should load a new set of weights from the data source.
 *              When this method is called, the graph will find all nodes that
 *              support reloading and direct them to reinitialize themselves
 *              based on their data source.
 *
 *              This process occurs immediately. Your application will
 *              need to make sure any GPU work being done by the graph is complete
 *              to ensure data coherency. Most nodes do not have a data source
 *              and will not be modified. Nodes that are not used by the graph
 *              will not be updated.
 */
-(void) reloadFromDataSources;

/*! @abstract       Encode the graph to a MTLCommandBuffer
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImages        A list of MPSImages to use as the source images for the graph.
 *                                      These should be in the same order as the list returned from MPSNNGraph.sourceImageHandles.
 *                                      The images may be image arrays. Typically, this is only one or two images
 *                                      such as a .JPG decoded into a MPSImage*.  If the sourceImages are MPSTemporaryImages,
 *                                      the graph will decrement the readCount by 1, even if the graph actually
 *                                      reads an image multiple times.
 *  @param          sourceStates        A list of MPSState objects to use as state for a graph.
 *                                      These should be in the same order as the list returned from MPSNNGraph.sourceStateHandles.
 *                                      May be nil, if there is no source state. If the sourceStates are temporary,
 *                                      the graph will decrement the readCount by 1, even if the graph actually
 *                                      reads the state multiple times.
 *  @param      intermediateImages      An optional NSMutableArray to receive any MPSImage objects exported as part of its operation.
 *                                      These are only the images that were tagged with MPSNNImageNode.exportFromGraph = YES. The 
 *                                      identity of the states is given by -resultStateHandles.  If temporary, each intermediateImage 
 *                                      will have a readCount of 1.  If the result was tagged exportFromGraph = YES, it will be here
 *                                      too, with a readCount of 2.
 *  @param      destinationStates       An optional NSMutableArray to receive any MPSState objects created as part of its operation.
 *                                      The identity of the states is given by -resultStateHandles.
 *  @result     A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *              It will be automatically released when commandBuffer completes.
 */
-(MPSImage * __nullable)  encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                   sourceImages: (NSArray<MPSImage*> * __nonnull) sourceImages
                                   sourceStates: (NSArray<MPSState*> * __nullable) sourceStates
                             intermediateImages: (NSMutableArray<MPSImage*> * __nullable) intermediateImages
                              destinationStates: (NSMutableArray<MPSState*> * __nullable) destinationStates;

/*! @abstract       Encode the graph to a MTLCommandBuffer
 *  @discussion     This interface is like the other except that it operates on a batch of images all
 *                  at once.  In addition, you may specify whether the result is needed.
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImages        A list of MPSImages to use as the source images for the graph.
 *                                      These should be in the same order as the list returned from MPSNNGraph.sourceImageHandles.
 *                                      The images may be image arrays. Typically, this is only one or two images
 *                                      such as a .JPG decoded into a MPSImage*.  If the sourceImages are MPSTemporaryImages,
 *                                      the graph will decrement the readCount by 1, even if the graph actually
 *                                      reads an image multiple times.
 *  @param          sourceStates        A list of MPSState objects to use as state for a graph.
 *                                      These should be in the same order as the list returned from MPSNNGraph.sourceStateHandles.
 *                                      May be nil, if there is no source state. If the sourceStates are temporary,
 *                                      the graph will decrement the readCount by 1, even if the graph actually
 *                                      reads the state multiple times.
 *  @param      intermediateImages      An optional NSMutableArray to receive any MPSImage objects exported as part of its operation.
 *                                      These are only the images that were tagged with MPSNNImageNode.exportFromGraph = YES. The
 *                                      identity of the states is given by -resultStateHandles.  If temporary, each intermediateImage
 *                                      will have a readCount of 1.  If the result was tagged exportFromGraph = YES, it will be here
 *                                      too, with a readCount of 2.
 *  @param      destinationStates       An optional NSMutableArray to receive any MPSState objects created as part of its operation.
 *                                      The identity of the states is given by -resultStateHandles.
 *  @result     A MPSImageBatch or MPSTemporaryImageBatch allocated per the destinationImageAllocator containing the output of the graph.
 *              It will be automatically released when commandBuffer completes. If resultIsNeeded == NO, then this
 *              will return nil.
 */
-(MPSImageBatch * __nullable) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                                            sourceImages: (NSArray<MPSImageBatch*> * __nonnull) sourceImages
                                            sourceStates: (NSArray<MPSStateBatch*> * __nullable) sourceStates
                                      intermediateImages: (NSMutableArray <MPSImageBatch*> * __nullable) intermediateImages
                                       destinationStates: (NSMutableArray <MPSStateBatch *> * __nullable) destinationStates
                                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Encode the graph to a MTLCommandBuffer
 *
 *  @discussion     IMPORTANT:  Please use [MTLCommandBuffer addCompletedHandler:] to determine when this work is
 *                  done. Use CPU time that would have been spent waiting for the GPU to encode the next command
 *                  buffer and commit it too.  That way, the work for the next command buffer is ready to go the
 *                  moment the GPU is done. This will keep the GPU busy and running at top speed.
 *
 *                  Those who ignore this advice and use [MTLCommandBuffer waitUntilCompleted] instead will likely
 *                  cause their code to slow down by a factor of two or more. The CPU clock spins down while it
 *                  waits for the GPU. When the GPU completes, the CPU runs slowly for a while until it spins up.
 *                  The GPU has to wait for the CPU to  encode more work (at low clock), giving it plenty of time to
 *                  spin its own clock down. In typical CNN graph usage, neither may ever reach maximum clock
 *                  frequency, causing slow down far beyond what otherwise would be expected from simple failure
 *                  to schedule CPU and GPU work concurrently. Regrattably, it is probable that every performance
 *                  benchmark you see on the net will be based on [MTLCommandBuffer waitUntilCompleted].
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImages        A list of MPSImages to use as the source images for the graph.
 *                                      These should be in the same order as the list returned from MPSNNGraph.sourceImageHandles.
 *  @result     A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *              It will be automatically released when commandBuffer completes.  It can be nil if resultImageIsNeeded == NO
 */
-(MPSImage * __nullable) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                 sourceImages: (NSArray<MPSImage*> * __nonnull) sourceImages;

/*! @abstract Convenience method to encode a batch of images*/
-(MPSImageBatch * __nullable) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                            sourceImages: (NSArray<MPSImageBatch*> * __nonnull) sourceImages
                                            sourceStates: (NSArray<MPSStateBatch*> * __nullable) sourceStates;


/*! @abstract Convenience method to execute a graph without having to manage many Metal details
 *  @discussion   This function will synchronously encode the graph on a private command buffer,
 *                commit it to a MPS internal command queue and return. The GPU will start working.
 *                When the GPU is done, the completion handler will be called.  You should use
 *                the intervening time to encode other work for execution on the GPU, so that
 *                the GPU stays busy and doesn't clock down.
 *
 *                The work will be performed on the MTLDevice that hosts the source images.
 *
 *                This is a convenience API.  There are a few situations it does not handle optimally.
 *                These may be better handled using [encodeToCommandBuffer:sourceImages:].
 *                Specifically:
 *                @code
 *                    o     If the graph needs to be run multiple times for different images,
 *                          it would be better to encode the graph multiple times on the same
 *                          command buffer using [encodeToCommandBuffer:sourceImages:]  This
 *                          will allow the multiple graphs to share memory for intermediate
 *                          storage, dramatically reducing memory usage.
 *
 *                    o     If preprocessing or post-processing of the MPSImage is required,
 *                          such as resizing or normalization outside of a convolution, it would
 *                          be better to encode those things on the same command buffer.
 *                          Memory may be saved here too for intermediate storage. (MPSTemporaryImage
 *                          lifetime does not span multiple command buffers.)
 *                @endcode
 *
 *
 *  @param  sourceImages    A list of MPSImages to use as the source images for the graph.
 *                          These should be in the same order as the list returned from
 *                          MPSNNGraph.sourceImageHandles. They should be allocated against
 *                          the same MTLDevice. There must be at least one source image.
 *                          Note: this array is intended to handle the case where multiple
 *                          input images are required to generate a single graph result.
 *                          That is, the graph itself has multiple inputs.  If you need to
 *                          execute the graph multiple times, then call this API multiple
 *                          times, or better yet use [encodeToCommandBuffer:sourceImages:]
 *                          multiple times. (See discussion)
 *
 *  @param  handler         A block to receive any errors generated. This block may run
 *                          on any thread and may be called before this method returns.
 *                          The image, if any, passed to this callback is the same image
 *                          as that returned from the left hand side.
 *
 *  @return    A MPSImage to receive the result. The data in the image will not be valid until
 *             the completionHandler is called.
 */
 -(MPSImage * __nonnull) executeAsyncWithSourceImages: (NSArray<MPSImage*> * __nonnull) sourceImages
                                    completionHandler: (MPSNNGraphCompletionHandler __nonnull) handler;

@end



#endif /* MPSNNGraph_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNGraphNodes.h
/*!
 *  @header MPSNNGraphNodes.h
 *  @framework MetalPerformanceShaders
 *
 *  Created by Ian Ollmann on 10/19/16.
 *  @copyright Copyright © 2016 Apple. All rights reserved.
 *
 *  @discussion This header describes building blocks to prepare a graph
 *              of MPS images, kernels and state objects. You should prepare
 *              your graph by creating a MPSNNImageNode for each of the
 *              graph input textures. These are then used as inputs to
 *              MPSNNFilterNode subclasses. These in turn produce more 
 *              image nodes as results, which can be linked to more 
 *              MPSNNFilterNodes as inputs.  When the graph representation
 *              is complete, make a MPSNNGraph object to interpret and
 *              optimize the graph. The MPSNNGraph may be used to encode 
 *              the entire graph on a MTLCommandBuffer.
 *
 *              Objects presented here are generally light weight. They do not
 *              have a MTLDevice reference, and so can not create MTLResource
 *              objects. In the few cases when data is expected to be large
 *              (e.g. convolution weights), the nodes are designed to defer
 *              allocation of storage, preferring to leave them on disk or network
 *              or other persistent storage to hold the data until it is actually
 *              needed to initialize a MPSKernel object.  Not until the MPSNNGraph 
 *              is constructed does the heavy lifting begin. MPSNNGraphs in 
 *              contrast can be extremely heavy. A large graph may use most of 
 *              the memory available to your system!  Nearly all of this is due 
 *              to convolution weights. Construct your <MPSCNNConvolutionDataSource>
 *              to only load data when it is needed.
 */

#ifndef MPSNNGraphNodes_h
#define MPSNNGraphNodes_h

#include <MPSCore/MPSImage.h>
#include <MPSCore/MPSState.h>
#include <MPSNeuralNetwork/MPSNeuralNetworkTypes.h>
#include <MPSNeuralNetwork/MPSCNNNeuronType.h>

#pragma mark -
#pragma mark Base Classes


/*! @protocol   MPSHandle
 *  @abstract   MPS resource identification
 *  @discussion Most of the time, there is only one image and one or fewer states needed as 
 *              input to a graph, so the order of the images and states passed to
 *              [MPSNNGraph encodeToCommandBuffer:sourceImages:] or 
 *              [MPSNNGraph encodeToCommandBuffer:sourceImages:sourceStates:intermediateImages:destinationStates:] 
 *              is clear. There is only one order. However, sometimes graphs have more than one 
 *              input image or state. What order should they appear in the NSArray passed to 
 *              these methods? 
 *
 *              Each MPSNNImageNode or MPSNNStateNode can be tagged with a MPSHandle. MPSNNGraph 
 *              keeps track of these. You can request that the MPSNNGraph return them to you in 
 *              an array in the same order as needed to encode the MPSNNGraph, using 
 *              MPSNNGraph.sourceImageHandles and MPSNNGraph.sourceStateHandles.
 *
 *              Example:
 *              @code
 *              @interface MyHandle : NSObject <MPSHandle>
 *                  // Add a method for my use to get the image needed based on the handle to it.
 *                  // This isn't part of the MPSHandle protocol, but we need it for MyEncodeGraph
 *                  // below. Since it is my code calling my object, we can add whatever we want like this.
 *                  -(MPSImage*__nonnull) image;    // return the MPSImage corresponding to the handle
 *
 *                  // required by MPSHandle protocol
 *                  -(NSString * __nullable) label;
 *
 *                  // MPSHandle implies NSSecureCoding too
 *                  +(BOOL) supportsSecureCoding;
 *                  - (void)encodeWithCoder:(NSCoder * __nonnull )aCoder;
 *                  - (nullable instancetype)initWithCoder:(NSCoder * __nonnull )aDecoder; // NS_DESIGNATED_INITIALIZER
 *              @end
 *
 *              // Encode a graph to cmdBuf using handles for images
 *              // Here we assume that the MPSNNImageNodes that are graph inputs (not produced 
 *              // by the graph) are tagged with a unique instance of MyHandle that can be used 
 *              // to get the appropriate image for that node.
 *              static void MyEncodeGraph( MPSNNGraph * graph, id <MTLCommandBuffer> cmdBuf )
 *              {
 *                  @autoreleasepool{
 *                      // prepare an array of source images, using the handles
 *                      NSArray<MyHandle*> * handles = graph.sourceImageHandles;
 *                      unsigned long count = handles.count;
 *                      NSMutableArray<MPSImage*> * __nonnull images = [NSMutableArray arrayWithCapacity: count];
 *                      for( unsigned long i = 0; i < count; i++ )
 *                          images[i] = handles[i].image;
 *
 *                      // encode the graph using the array
 *                      [ graph encodeToCommandBuffer: cmdBuf
 *                                       sourceImages: images ];
 *                  }
 *              }
 *              @endcode
 */
@protocol MPSHandle <NSSecureCoding, NSObject>

@required
/*! @abstract   A label to be attached to associated MTLResources for this node
 *  @return     A human readable string for debugging purposes
 */
-(NSString * __nullable) label;

/* Note: You must also implement the NSSecureCoding protocol. These objects will be
 *       serialized along with the MPSNNGraph when it is serialized. */
@end

@protocol MPSNNTrainableNode <NSObject>
@required

/*! @abstract   Configure whether and when neural network training parameters are updated
 *  @discussion Default: MPSNNTrainingStyleUpdateDeviceGPU      */
@property (readwrite, nonatomic)    MPSNNTrainingStyle trainingStyle;

@end

/*! @class      MPSNNImageNode
 *  @abstract   A placeholder node denoting the position of a MPSImage in a graph
 *  @discussion MPS neural network graphs are made up of filter nodes connected by
 *              image (or state) nodes. An image node is produced by one filter but
 *              may be consumed by more than one filter.
 *
 *              Most image nodes will be created by MPS and made available through
 *              MPSNNFilterNode.resultImage. Image nodes that are not created by MPS
 *              (i.e. "the graph inputs") must be created by you.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNImageNode : NSObject

-(nonnull instancetype) initWithHandle: (NSObject <MPSHandle> * __nullable) handle NS_DESIGNATED_INITIALIZER;
+(nonnull instancetype) nodeWithHandle: (NSObject <MPSHandle> * __nullable) handle;

/*! @abstract Create a autoreleased MPSNNImageNode with exportFromGraph = YES.
 *  @discussion Note: image is still temporary. See MPSNNImageNode.imageAllocator parameter. */
+(nonnull instancetype) exportedNodeWithHandle: (NSObject <MPSHandle> * __nullable) handle;

-(nonnull instancetype) init    NS_UNAVAILABLE;

/*! @abstract   MPS resource identifier
 *  @discussion See MPSHandle protocol description.  Default: nil
 */
@property (nonatomic, nullable, readwrite, retain) id <MPSHandle>   handle;

/*! @abstract   The preferred precision for the image
 *  @discussion Default: MPSImageFeatureChannelFormatNone, meaning MPS should pick a format
 *                       Typically, this is 16-bit floating-point.
 */
@property (nonatomic, readwrite) MPSImageFeatureChannelFormat   format;

/*! @abstract   Configurability for image allocation
 *  @discussion Allows you to influence how the image is allocated
 *              Default: MPSTemporaryImage.defaultAllocator  */
@property (readwrite, nonatomic, nonnull, retain) id <MPSImageAllocator>  imageAllocator;

/*  The height, width, number of feature channels, and number of most image nodes is automatically determined from
 *  the size of the input images presented to the graph. Please use [MPSNNGraph resultDescriptorForSourceImages:]
 *  to find the size of the output image. */

/*! @abstract   Tag a image node for view later
 *  @discussion Most image nodes are private to the graph. These alias memory heavily and
 *              consequently generally have invalid state when the graph exits.  When
 *              exportFromGraph = YES, the image is preserved and made available through
 *              the [MPSNNGraph encode... intermediateImages:... list. 
 *
 *              CAUTION: exporting an image from a graph prevents MPS from 
 *                       recycling memory. It will nearly always cause the
 *                       amount of memory used by the graph to increase by the size
 *                       of the image. There will probably be a performance
 *                       regression accordingly.  This feature should generally
 *                       be used only when the node is needed as an input for
 *                       further work and recomputing it is prohibitively costly.
 *
 *              Default: NO
 */
@property (nonatomic, readwrite) BOOL       exportFromGraph;


/*! @abstract   Set to true to cause the resource to be synchronized with the CPU
 *  @discussion It is not needed on iOS/tvOS devices, where it does nothing.  */
@property (nonatomic, readwrite) BOOL       synchronizeResource MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Stop training graph automatic creation at this node.
 *  @discussion An inference graph of MPSNNFilterNodes, MPSNNStateNodes and MPSNNImageNodes can be automatically
 *              converted to a training graph using -[MPSNNFilterNode trainingGraphWithSourceGradient:nodeHandler:].
 *              Sometimes, an inference graph may contain extra nodes at start to do operations like resampling or range
 *              adjustment that should not be part of the training graph. To prevent gradient operations for these extra
 *              nodes from being included in the training graph, set <undesired node>.resultImage.stopGradient = YES.
 *              This will prevent gradient propagation beyond this MPSNNImageNode.
 *              Default: NO */
@property (nonatomic, readwrite) BOOL       stopGradient     MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(112.0));

@end

/*! @class      MPSNNStateNode
 *  @abstract   A placeholder node denoting the position in the graph of a MPSState object
 *  @discussion Some filters need additional information about an image in order to function. For example
 *              a max-pooling gradient filter needs to know which position the max result came from in the
 *              original pooling filter in order to select the right data for gradient computation.  In other cases,
 *              state may be moved into a MPSState object in order to keep the filter itself immutable.
 *              The MPSState object typically encapsulates one or more MTLResource objects.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNStateNode : NSObject
-(nonnull instancetype) init NS_UNAVAILABLE;    // These are typically obtained using  MPSNNFilterNode.resultState
                                                // If you do need a state node, make a MPSNNStateNode subclass, instead.
/*! @abstract   MPS resource identification
 *  @discussion See MPSHandle protocol reference.  Default: nil
 */
@property (nullable, retain, nonatomic) id <MPSHandle> handle;

/*! @abstract   Tag a state node for view later
 *  @discussion Most state nodes are private to the graph. These alias memory heavily and
 *              consequently generally have invalid state when the graph exits.  When
 *              exportFromGraph = YES, the image is preserved and made available through
 *              the [MPSNNGraph encode... resultStates:... list.
 *
 *              CAUTION: exporting an state from a graph prevents MPS from
 *                       recycling memory. It will nearly always cause the
 *                       amount of memory used by the graph to increase by the size
 *                       of the state. There will probably be a performance
 *                       regression accordingly.  This feature should generally
 *                       be used only when the node is needed as an input for
 *                       further work and recomputing it is prohibitively costly.
 *
 *              Default: NO
 */
@property (nonatomic, readwrite) BOOL       exportFromGraph;

/*! @abstract   Set to true to cause the resource to be synchronized with the CPU
 *  @discussion Ignored on non-MacOS.   */
@property (nonatomic, readwrite) BOOL       synchronizeResource MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

@end

/*! @class MPSNNGradientStateNode
 *  @discussion During training, each MPSNNFilterNode has a corresponding
 *              MPSNNGradientFilterNode for the gradient computation for
 *              trainable parameter update. The two communicate through a
 *              MPSNNGradientStateNode or subclass which carries information
 *              about the inference pass settings to the gradient pass.
 *              You can avoid managing these -- there will be many! -- by
 *              using -[MPSNNFilterNode gradientFilterWithSources:] to make
 *              the MPSNNGradientFilterNodes. That method will append
 *              the necessary extra information like MPSNNGradientState
 *              nodes and inference filter source image nodes to the object as
 *              needed.*/
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNGradientStateNode : MPSNNStateNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNConvolutionGradientStateNode : MPSNNGradientStateNode
@end

/*! @class MPSNNBinaryGradientStateNode
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNBinaryGradientStateNode : MPSNNStateNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNArithmeticGradientStateNode : MPSNNBinaryGradientStateNode
@end

@class MPSNNFilterNode;
@class MPSNNGradientFilterNode;

/*! @abstract   Block callback for customizing gradient nodes as they are constructed
 *  @discussion Example code for copying handles from inference image nodes to corresponding gradient nodes:
 *          @code
 *              MPSNodeCustomizationBlock myCopyHandleBlock = ^( MPSNNFilterNode * __nonnull gradientNode,
 *                                                               MPSNNFilterNode * __nonnull inferenceNode,
 *                                                               MPSNNImageNode * __nonnull inferenceSource )
 *              {
 *                  gradientNode.resultImage.handle = inferenceSource.handle;
 *              }
 *          @endcode
 *
 *  @param      gradientNode    The new gradient node created to mirror inferenceNode
 *  @param      inferenceNode   The preexisting inference node mirrored by gradient node.
 *                              If nil, an extra node was automatically inserted into the graph.
 *                              An MPSNNAdditionNode may be inserted at junctions
 *                              where multiple inference MPSNNFilterNodes read from the
 *                              same MPSNNImageNode.
 *  @param      inferenceSource The  source image argument to the inference node to which the gradient result corresponds
 *  @param      gradientSource  The source gradient argument to the new gradient node. 
 */
typedef void (^MPSGradientNodeBlock)( MPSNNFilterNode * __nonnull gradientNode,
                                      MPSNNFilterNode * __nonnull inferenceNode,
                                      MPSNNImageNode * __nonnull inferenceSource,
                                      MPSNNImageNode * __nonnull gradientSource );



/*! @class      MPSNNFilterNode
 *  @abstract   A placeholder node denoting a neural network filter stage
 *  @discussion There are as many MPSNNFilterNode subclasses as there are
 *              MPS neural network filter objects. Make one of those. 
 *              This class defines an polymorphic interface for them.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNFilterNode : NSObject

/* This is a virtual base class. Make MPSNNFilterNode subclass objects instead. */
-(nonnull instancetype) init    NS_UNAVAILABLE;


/*! @abstract   Get the node representing the image result of the filter 
 *  @discussion Except where otherwise noted, the precision used for the 
 *              result image (see format property) is copied from the precision 
 *              from the first input image node.
 */
@property (nonatomic, readonly, nonnull) MPSNNImageNode * resultImage;

/*! @abstract   convenience method for resultStates[0]
 *  @discussion  If resultStates is nil, returns nil */
@property (nonatomic, readonly, nullable) MPSNNStateNode * resultState;

/*! @abstract   Get the node representing the state result of the filter
 @discussion If more than one, see description of subclass for ordering. */
@property (nonatomic, readonly, nullable) NSArray<MPSNNStateNode*> * resultStates;

/*! @abstract   The padding method used for the filter node
 *  @discussion The padding policy configures how the filter centers
 *              the region of interest in the source image. It principally
 *              is responsible for setting the MPSCNNKernel.offset and
 *              the size of the image produced, and sometimes will also
 *              configure .sourceFeatureChannelOffset, .sourceFeatureChannelMaxCount,
 *              and .edgeMode.  It is permitted to set any other filter properties
 *              as needed using a custom padding policy. The default padding
 *              policy varies per filter to conform to consensus expectation for
 *              the behavior of that filter.  In some cases, pre-made padding
 *              policies are provided to match the behavior of common neural
 *              networking frameworks with particularly complex or unexpected
 *              behavior for specific nodes. See MPSNNDefaultPadding class methods
 *              in MPSNeuralNetworkTypes.h for more.
 *
 *              BUG: MPS doesn't provide a good way to reset the MPSKernel properties
 *              in the context of a MPSNNGraph after the kernel is finished encoding.
 *              These values carry on to the next time the graph is used. Consequently,
 *              if your custom padding policy modifies the property as a function of the
 *              previous value, e.g.:
 *
 *                  kernel.someProperty += 2;
 *
 *              then the second time the graph runs, the property may have an inconsistent
 *              value, leading to unexpected behavior. The default padding computation
 *              runs before the custom padding method to provide it with a sense of
 *              what is expected for the default configuration and will reinitialize the value
 *              in the case of the .offset. However, that computation usually doesn't reset
 *              other properties. In such cases, the custom padding policy may need to keep
 *              a record of the original value to enable consistent behavior.
 */
@property (nonatomic, readwrite, retain, nonnull) id <MPSNNPadding> paddingPolicy;

/*!
 @property label
 @abstract A string to help identify this object.
 */
@property (nullable, copy, atomic) NSString *label;

/*! @abstract Return the gradient (backwards) version of this filter.
 *  @discussion The backwards training version of the filter will be returned.
 *              The non-gradient image and state arguments for the filter are automatically
 *              obtained from the target.
 *  @param  gradientImage The gradient images corresponding with the resultImage
 *                        of the target */
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSource: (MPSNNImageNode* __nonnull) gradientImage;

/*! @abstract Return the gradient (backwards) version of this filter.
 *  @discussion The backwards training version of the filter will be returned.
 *              The non-gradient image and state arguments for the filter are automatically
 *              obtained from the target.
 *  @param  gradientImages The gradient images corresponding with the resultImage
 *                        of the target */
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages;

/*! @abstract Return multiple gradient versions of the filter
 *  @discussion     MPSNNFilters that consume multiple inputs generally result in
 *                  multiple conjugate filters for the gradient computation at
 *                  the end of training. For example, a single concatenation operation
 *                  that concatenates multple images will result in an array of slice
 *                  operators that carve out subsections of the input gradient image. */
-(NSArray <MPSNNGradientFilterNode*> * __nonnull) gradientFiltersWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages;

/*! @abstract Return multiple gradient versions of the filter
 *  @discussion     MPSNNFilters that consume multiple inputs generally result in
 *                  multiple conjugate filters for the gradient computation at
 *                  the end of training. For example, a single concatenation operation
 *                  that concatenates multple images will result in an array of slice
 *                  operators that carve out subsections of the input gradient image. */
-(NSArray <MPSNNGradientFilterNode*> * __nonnull) gradientFiltersWithSource: (MPSNNImageNode* __nonnull) gradientImage;


/*! @abstract       Build training graph from inference graph
 *  @discussion     This method will iteratively build the training potion of a graph based
 *                  on an inference graph. Self should be the last node in the
 *                  inference graph. It is typically a loss layer, but can be anything.
 *                  Typically, the "inference graph" used here is the desired inference
 *                  graph with a dropout node and a loss layer node appended.
 *
 *                  BUG: This method can not follow links to regions of the graph that are
 *                  connected to the rest of the graph solely via MPSNNStateNodes. A gradient
 *                  image input is required to construct a MPSNNGradientFilterNode from a
 *                  inference filter node.
 *
 *  @param          gradientImage   The input gradient image for the first gradient
 *                                  node in the training section of the graph. If nil,
 *                                  self.resultImage is used. This results in a standard monolithic
 *                                  training graph. If the graph is instead divided into multiple
 *                                  subgraphs (potentially to allow for your custom code to appear
 *                                  inbetween MPSNNGraph segments) a new MPSImageNode*
 *                                  may be substituted.
 *  @param          nodeHandler     An optional block to allow for customization of gradient
 *                                  nodes and intermediate images as the graph is constructed.
 *                                  It may also be used to prune braches of the developing
 *                                  training graph. If nil, the default handler is used. It builds
 *                                  the full graph, and assigns any inferenceNodeSources[i].handle
 *                                  to their gradient counterparts.
 *  @return         The list of new MPSNNFilterNode training graph termini. These MPSNNFilterNodes
 *                  are not necessarily all MPSNNGradientFilterNodes. To build a full list of nodes
 *                  created, use a custom nodeHandler. If no nodes are created nil is returned.
 */
-(NSArray <MPSNNFilterNode*> * __nullable) trainingGraphWithSourceGradient: (MPSNNImageNode* __nullable ) gradientImage
                                                               nodeHandler: (__nullable MPSGradientNodeBlock) nodeHandler
        MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

@end

/*! @class MPSNNGradientFilterNode
 *  @discussion For each MPSNNFilterNode, there is a corresponding MPSNNGradientFilterNode
 *              used for training that back propagates image gradients to refine the
 *              various parameters in each node. Generally, it takes as input a gradient
 *              corresponding to the result image from the MPSNNFilterNode and returns
 *              a gradient image corresponding to the source image of the MPSNNFilterNode.
 *              In addition, there is generally a MPSNNState produced by the MPSNNFilterNode
 *              that is consumed by the MPSNNGradientNode and the MPSNNGradientNode generally
 *              needs to look at the MPSNNFilterNode source image.
 *
 *              If you have a simple method to traverse your inference graph backwards, then
 *              -[MPSNNFilterNode gradientFilterWithSource:] is a simple way to construct
 *              these.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNGradientFilterNode : MPSNNFilterNode

-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources: (NSArray<MPSNNImageNode*> * __nonnull) sourceGradient NS_UNAVAILABLE;
-(NSArray <MPSNNGradientFilterNode*> * __nonnull) gradientFiltersWithSources: (NSArray<MPSNNImageNode*> * __nonnull) sourceGradient NS_UNAVAILABLE;
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSource: (MPSNNImageNode*__nonnull) sourceGradient NS_UNAVAILABLE;
-(NSArray <MPSNNGradientFilterNode*> * __nonnull) gradientFiltersWithSource: (MPSNNImageNode*__nonnull) sourceGradient NS_UNAVAILABLE;

@end


#pragma mark -
#pragma mark Convolution Nodes

@class MPSCNNConvolutionDescriptor;
@protocol MPSCNNConvolutionDataSource;

/*! @abstract   A MPSNNFilterNode representing a MPSCNNConvolution kernel   */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNConvolutionNode : MPSNNFilterNode

/*! @abstract   Set the floating-point precision used by the convolution accumulator
 *  @discussion Default:  MPSNNConvolutionAccumulatorPrecisionOptionFloat */
@property (readwrite, nonatomic) MPSNNConvolutionAccumulatorPrecisionOption accumulatorPrecision
    MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract   Init an autoreleased not representing a MPSCNNConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases. If it is used for training, it may not
 *                                      have a neuron embedded in the convolution descriptor.
 *  @return     A new MPSNNFilter node for a MPSCNNConvolution kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights;

/*! @abstract   Init a node representing a MPSCNNConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases. If it is used for training, it may not
 *                                      have a neuron embedded in the convolution descriptor.
 *  @return     A new MPSNNFilter node for a MPSCNNConvolution kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights;

/*! @abstract   A node to represent a MPSCNNConvolutionGradientState object
 *  @discussion  Use this if the convolution is mirrored by a convolution transpose node
 *               later on in the graph to make sure that the size of the image returned
 *               from the convolution transpose matches the size of the image passed in
 *               to this node. */
@property (nonatomic, readonly, nullable) MPSCNNConvolutionGradientStateNode * convolutionGradientState
    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A MPSNNFilterNode representing a MPSCNNFullyConnected kernel   */
@interface MPSCNNFullyConnectedNode : MPSCNNConvolutionNode
/*! @abstract   Init an autoreleased not representing a MPSCNNFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @return     A new MPSNNFilter node for a MPSCNNConvolution kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights;

/*! @abstract   Init a node representing a MPSCNNFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @return     A new MPSNNFilter node for a MPSCNNFullyConnected kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A MPSNNFilterNode representing a MPSCNNBinaryConvolution kernel   */
@interface MPSCNNBinaryConvolutionNode : MPSCNNConvolutionNode
/*! @abstract   Init an autoreleased node representing a MPSCNNBinaryConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      scaleValue              A floating point value used to scale the entire convolution.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryConvolution kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;

/*! @abstract   Init a node representing a MPSCNNBinaryConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      scaleValue              A floating point value used to scale the entire convolution.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryConvolution kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;


/*! @abstract   Init an autoreleased node representing a MPSCNNBinaryConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      outputBiasTerms         A pointer to bias terms to be applied to the convolution output.
 *                                      See MPSCNNBinaryConvolution for more details.
 *  @param      outputScaleTerms        A pointer to scale terms to be applied to binary convolution
 *                                      results per output feature channel. See MPSCNNBinaryConvolution for more details.
 *  @param      inputBiasTerms          A pointer to offset terms to be applied to the input before convolution and
 *                                      before input scaling. See MPSCNNBinaryConvolution for more details.
 *  @param      inputScaleTerms         A pointer to scale terms to be applied to the input before convolution,
 *                                      but after input biasing. See MPSCNNBinaryConvolution for more details.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryConvolution kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags
                MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Init a node representing a MPSCNNBinaryConvolution kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      outputBiasTerms         A pointer to bias terms to be applied to the convolution output.
 *                                      See MPSCNNBinaryConvolution for more details.
 *  @param      outputScaleTerms        A pointer to scale terms to be applied to binary convolution
 *                                      results per output feature channel. See MPSCNNBinaryConvolution for more details.
 *  @param      inputBiasTerms          A pointer to offset terms to be applied to the input before convolution and
 *                                      before input scaling. See MPSCNNBinaryConvolution for more details.
 *  @param      inputScaleTerms         A pointer to scale terms to be applied to the input before convolution,
 *                                      but after input biasing. See MPSCNNBinaryConvolution for more details.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryConvolution kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags
            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @abstract unavailable */
@property (nonatomic, readonly, nullable) MPSCNNConvolutionGradientStateNode * convolutionGradientState NS_UNAVAILABLE;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A MPSNNFilterNode representing a MPSCNNBinaryFullyConnected kernel   */
@interface MPSCNNBinaryFullyConnectedNode : MPSCNNBinaryConvolutionNode
/*! @abstract   Init an autoreleased node representing a MPSCNNBinaryFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      scaleValue              A floating point value used to scale the entire convolution.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryFullyConnected kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;

/*! @abstract   Init a node representing a MPSCNNBinaryFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      scaleValue              A floating point value used to scale the entire convolution.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryFullyConnected kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;

/*! @abstract   Init an autoreleased node representing a MPSCNNBinaryFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      outputBiasTerms         A pointer to bias terms to be applied to the convolution output.
 *                                      See MPSCNNBinaryConvolution for more details.
 *  @param      outputScaleTerms        A pointer to scale terms to be applied to binary convolution
 *                                      results per output feature channel. See MPSCNNBinaryConvolution for more details.
 *  @param      inputBiasTerms          A pointer to offset terms to be applied to the input before convolution and
 *                                      before input scaling. See MPSCNNBinaryConvolution for more details.
 *  @param      inputScaleTerms         A pointer to scale terms to be applied to the input before convolution,
 *                                      but after input biasing. See MPSCNNBinaryConvolution for more details.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryFullyConnected kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags
            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Init a node representing a MPSCNNBinaryFullyConnected kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @param      outputBiasTerms         A pointer to bias terms to be applied to the convolution output.
 *                                      See MPSCNNBinaryConvolution for more details.
 *  @param      outputScaleTerms        A pointer to scale terms to be applied to binary convolution
 *                                      results per output feature channel. See MPSCNNBinaryConvolution for more details.
 *  @param      inputBiasTerms          A pointer to offset terms to be applied to the input before convolution and
 *                                      before input scaling. See MPSCNNBinaryConvolution for more details.
 *  @param      inputScaleTerms         A pointer to scale terms to be applied to the input before convolution,
 *                                      but after input biasing. See MPSCNNBinaryConvolution for more details.
 *  @param      type                    What kind of binarization strategy is to be used.
 *  @param      flags                   See documentation of MPSCNNBinaryConvolutionFlags.
 *  @return     A new MPSNNFilter node for a MPSCNNBinaryFullyConnected kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags
            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));



@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A MPSNNFilterNode representing a MPSCNNConvolutionTranspose kernel   */
@interface MPSCNNConvolutionTransposeNode : MPSCNNConvolutionNode
/*! @abstract   Init an autoreleased not representing a MPSCNNConvolutionTransposeNode kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      convolutionGradientState        When the convolution transpose is used to 'undo' an earlier convolution
 *                                      in the graph, it is generally desired that the output image be the same
 *                                      size as the input image to the earlier convolution. You may optionally 
 *                                      specify this size identity by passing in the MPSNNConvolutionGradientStateNode
 *                                      created by the convolution node here.
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @return     A new MPSNNFilter node for a MPSCNNConvolutionTransposeNode kernel.  */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
              convolutionGradientState: (MPSCNNConvolutionGradientStateNode * __nullable) convolutionGradientState
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   Init a node representing a MPSCNNConvolutionTransposeNode kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      convolutionGradientState        When the convolution transpose is used to 'undo' an earlier convolution
 *                                      in the graph, it is generally desired that the output image be the same
 *                                      size as the input image to the earlier convolution. You may optionally
 *                                      specify this size identity by passing in the MPSCNNConvolutionGradientState node
 *                                      here.
 *  @param      weights                 A pointer to a valid object conforming to the MPSCNNConvolutionDataSource
 *                                      protocol. This object is provided by you to encapsulate storage for
 *                                      convolution weights and biases.
 *  @return     A new MPSNNFilter node for a MPSCNNConvolutionTransposeNode kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
              convolutionGradientState: (MPSCNNConvolutionGradientStateNode * __nullable) convolutionGradientState
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract unavailable */
@property (nonatomic, readonly, nullable) MPSCNNConvolutionGradientStateNode * convolutionGradientState NS_UNAVAILABLE;
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNConvolutionGradientNode : MPSNNGradientFilterNode <MPSNNTrainableNode>
/*! @abstract   A node to represent the gradient calculation for convolution training.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter. Often
 *                          that is a neuron gradient filter node.
 *  @param sourceImage      The input image from the forward convolution node
 *  @param gradientState    The gradient state from the forward convolution
 *  @param weights          The data source from the forward convolution. It may not contain
 *                          an integrated neuron. Similary, any normalization should be
 *                          broken out into a separate node. Pass nil to use the weights
 *                          from the forward convolution pass.
 *  @return  A MPSCNNConvolutionGradientNode    */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                      convolutionGradientState: (MPSCNNConvolutionGradientStateNode*__nonnull) gradientState
                                       weights: (nullable id <MPSCNNConvolutionDataSource>) weights;

/*! @abstract   A node to represent the gradient calculation for convolution training.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter. Often
 *                          that is a neuron gradient filter node.
 *  @param sourceImage      The input image from the forward convolution node
 *  @param gradientState    The gradient state from the forward convolution
 *  @param weights          The data source from the forward convolution. It may not contain
 *                          an integrated neuron. Similary, any normalization should be
 *                          broken out into a separate node. Pass nil to use the weights
 *                          from the forward convolution pass.
 *  @return  A MPSCNNConvolutionGradientNode    */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                      convolutionGradientState: (MPSCNNConvolutionGradientStateNode*__nonnull) gradientState
                                       weights: (nullable id <MPSCNNConvolutionDataSource>) weights;
@end

#pragma mark -
#pragma mark Neuron Nodes

@class MPSNNNeuronDescriptor;

/*! @abstract virtual base class for MPSCNNNeuron nodes 
 *  @discussion This is a virtual base class only. Please create a
 *              subclass using +newNeuronNodeWithSouce:descriptor or
 *              by making one of the subclasses directly. Better yet, skip
 *              the node entirely and specify the neuron function directly in
 *              your MPSCNNConvolutionDataSource.descriptor.neuronDescriptor.
 *
 *              MPSCNNNeuronNodes are provided as a representational convenience.
 *              However, you are usually better off incorporating your neuron
 *              into the MPSCNNConvolutionDataSource when possible. The MPSNNGraph
 *              will attempt to optimize away the neuron pass by fusing it with a 
 *              preceeding convolution, but it might be prevented from doing so 
 *              if the neuron pass has a custom padding method or more than one 
 *              node reads from the convolution result. The graph -debugDescription
 *              should reveal what happened.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNNeuronNode : MPSNNFilterNode

/*! @abstract Create a neuron node of the appropriate type with a MPSNNNeuronDescriptor */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode* __nonnull) sourceNode
                            descriptor: (MPSNNNeuronDescriptor * __nonnull) descriptor
    MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract filter parameter a */
@property (nonatomic, readonly)  float a;

/*! @abstract filter parameter b */
@property (nonatomic, readonly)  float b;

/*! @abstract filter parameter c */
@property (nonatomic, readonly)  float c;

-(nonnull instancetype) init NS_UNAVAILABLE;

@end


MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronAbsolute kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = fabs(x)
 *  @endcode */
@interface MPSCNNNeuronAbsoluteNode : MPSCNNNeuronNode
/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronELU kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = a * exp(x) - 1, x <  0
 *             x             , x >= 0
 *  @endcode    */
@interface MPSCNNNeuronELUNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a;

/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronReLUN kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = min((x >= 0 ? x : a * x), b)
 *  @endcode    */
@interface MPSCNNNeuronReLUNNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Create an autoreleased node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end


MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronLinear kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = a * x + b
 *  @endcode    */
@interface MPSCNNNeuronLinearNode : MPSCNNNeuronNode

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

/*! @abstract   Init a node representing a MPSCNNNeuronLinear kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronLinear kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;


/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronReLU kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = x            if x >= 0
 *           = a * x        if x < 0
 *  @endcode        */
@interface MPSCNNNeuronReLUNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a;

/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronSigmoid kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = 1 / (1 + e^-x)
 *  @endcode        */
@interface MPSCNNNeuronSigmoidNode : MPSCNNNeuronNode
/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronHardSigmoid kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = clamp((a * x) + b, 0, 1)
 *  @endcode        */
@interface MPSCNNNeuronHardSigmoidNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

/*! @abstract   Init a node representing a MPSCNNNeuronHardSigmoid kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronHardSigmoid kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;


/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronSoftPlus kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = a * log(1 + e^(b * x))
 *  @endcode            */
@interface MPSCNNNeuronSoftPlusNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

/*! @abstract   Init a node representing a MPSCNNNeuronSoftPlus kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronSoftPlus kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;


/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronSoftSign kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = x / (1 + abs(x))
 *  @endcode        */
@interface MPSCNNNeuronSoftSignNode : MPSCNNNeuronNode

/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;
@end



MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A node representing a MPSCNNNeuronTanH kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = a * tanh(b * x)
 *  @endcode                */
@interface MPSCNNNeuronTanHNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;

/*! @abstract   Init a node representing a MPSCNNNeuronTanH kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = a * tanh(b * x)
 *  @endcode
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronTanH kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b;


/*! @abstract Create an autoreleased node with default values for parameters a & b */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a & b */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract   A ReLU node with parameter a provided independently for each feature channel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = x                if x >= 0
 *           = aData[i] * x     if x < 0,  i is the index of the feature channel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      aData                   An array of single precision floating-point alpha values to use
 *  @endcode                */
@interface MPSCNNNeuronPReLUNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                 aData: (NSData* __nonnull) aData;

/*! @abstract   Init a node representing a MPSCNNNeuronTanH kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = x                if x >= 0
 *           = aData[i] * x     if x < 0,  i is the index of the feature channel
 *  @endcode
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      aData                   An array of single precision floating-point alpha values to use
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronTanH kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                 aData: (NSData * __nonnull) aData;


+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_UNAVAILABLE;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_UNAVAILABLE;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract   A node representing a MPSCNNNeuronPower kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = (a * x + b) ^ c
 *  @endcode                */
@interface MPSCNNNeuronPowerNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;

/*! @abstract   Init a node representing a MPSCNNNeuronPower kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = (a * x + b) ^ c
 *  @endcode
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @param      c                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronPower kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;


/*! @abstract Create an autoreleased node with default values for parameters a, b, and c */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a, b, and c */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract   A node representing a MPSCNNNeuronExponential kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = c ^ (a * x + b)
 *  @endcode                */
@interface MPSCNNNeuronExponentialNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;

/*! @abstract   Init a node representing a MPSCNNNeuronExponential kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = c ^ (a * x + b)
 *  @endcode
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @param      c                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronExponential kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;


/*! @abstract Create an autoreleased node with default values for parameters a, b, and c */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a, b, and c */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract   A node representing a MPSCNNNeuronLogarithm kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = log_c(a * x + b)
 *  @endcode                */
@interface MPSCNNNeuronLogarithmNode : MPSCNNNeuronNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;

/*! @abstract   Init a node representing a MPSCNNNeuronLogarithm kernel
 *  @discussion For each pixel, applies the following function:
 *  @code
 *      f(x) = log_c(a * x + b)
 *  @endcode
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      a                       See discussion above.
 *  @param      b                       See discussion above.
 *  @param      c                       See discussion above.
 *  @return     A new MPSNNFilter node for a MPSCNNNeuronLogarithm kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c;


/*! @abstract Create an autoreleased node with default values for parameters a, b, and c */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract Init a node with default values for parameters a, b, and c */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

#pragma mark -
#pragma mark Neuron Gradient Nodes

/*! @abstract A node representing a MPSCNNNeuronGradient
 *  @discussion We use one generic neuron gradient node
 *              instead of having dozens of subclasses. */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNNeuronGradientNode : MPSNNGradientFilterNode

/*! @abstract create a new neuron gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodeWithSources:]
 *              for an easier way to do this */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    descriptor: (MPSNNNeuronDescriptor*__nonnull) descriptor;

/*! @abstract create a new neuron gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodeWithSources:]
 *              for an easier way to do this */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    descriptor: (MPSNNNeuronDescriptor*__nonnull) descriptor;

/*! @abstract The neuron descriptor */
@property (nonatomic, readonly)  MPSNNNeuronDescriptor * __nonnull descriptor;

-(nonnull instancetype) init NS_UNAVAILABLE;

@end


#pragma mark -
#pragma mark Pooling Nodes

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract  A node for a MPSCNNPooling kernel
 *  @discussion This is an abstract base class that does not correspond with any
 *              particular MPSCNNKernel. Please make one of the MPSCNNPooling
 *              subclasses instead. */
@interface MPSCNNPoolingNode : MPSNNFilterNode
@property (readonly, nonatomic)  NSUInteger kernelWidth      MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));
@property (readonly, nonatomic)  NSUInteger kernelHeight     MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));
@property (readonly, nonatomic)  NSUInteger strideInPixelsX  MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));
@property (readonly, nonatomic)  NSUInteger strideInPixelsY  MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract Convenience initializer for MPSCNNPooling nodes with square non-overlapping kernels
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = strideInPixelsX = strideInPixelsY = size
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size;

/*! @abstract Convenience initializer for MPSCNNPooling nodes with square non-overlapping kernels and a different stride
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = size
 *  @param      stride          strideInPixelsX = strideInPixelsY = stride
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size
                                stride: (NSUInteger) stride;

/*! @abstract   Init a node representing a MPSCNNPooling kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      kernelWidth             The width of the max filter window
 *  @param      kernelHeight            The height of the max filter window
 *  @param      strideInPixelsX         The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY         The output stride (downsampling factor) in the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY;


/*! @abstract Convenience initializer for MPSCNNPooling nodes with square kernels
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = size
 *  @param      stride          strideInPixelsX = strideInPixelsY = stride
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size
                                stride: (NSUInteger) stride;

/*! @abstract Convenience initializer for MPSCNNPooling nodes with square non-overlapping kernels
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = strideInPixelsX = strideInPixelsY = size
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract  A node representing a MPSCNNPoolingAverage kernel
 *  @discussion The default edge mode is MPSImageEdgeModeClamp  */
@interface MPSCNNPoolingAverageNode : MPSCNNPoolingNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract  A node representing a MPSCNNPoolingL2Norm kernel
 *  @discussion The default edge mode is MPSImageEdgeModeClamp  */
@interface MPSCNNPoolingL2NormNode : MPSCNNPoolingNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract  A node representing a MPSCNNPoolingMax kernel
 *  @discussion The default edge mode is MPSImageEdgeModeClamp  */
@interface MPSCNNPoolingMaxNode : MPSCNNPoolingNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract  A node for a MPSCNNDilatedPooling kernel
 *  @discussion This class corresponds to the MPSCNNDilatedPooling class. */
@interface MPSCNNDilatedPoolingMaxNode : MPSNNFilterNode

@property (nonatomic, readonly)    NSUInteger  dilationRateX;
@property (nonatomic, readonly)    NSUInteger  dilationRateY;


/*! @abstract Convenience initializer for MPSCNNDilatedPooling nodes with square non-overlapping kernels
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = strideInPixelsX = strideInPixelsY = dilationRateX = dilationRateY = size
 *  @return     A new MPSNNFilter node for a MPSCNNDilatedPooling kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size;

/*! @abstract Convenience initializer for MPSCNNDilatedPooling nodes with square kernels and equal dilation factors
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = size
 *  @param      stride          strideInPixelsX = strideInPixelsY = stride
 *  @param      dilationRate    dilationRateX = dilationRateY = stride
 *  @return     A new MPSNNFilter node for a MPSCNNDilatedPooling kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size
                                stride: (NSUInteger) stride
                          dilationRate: (NSUInteger) dilationRate;

/*! @abstract   Init a node representing a MPSCNNPooling kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      kernelWidth             The width of the max filter window
 *  @param      kernelHeight            The height of the max filter window
 *  @param      strideInPixelsX         The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY         The output stride (downsampling factor) in the y dimension.
 *  @param      dilationRateX           The dilation factor in the x dimension.
 *  @param      dilationRateY           The dilation factor in the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY
                         dilationRateX: (NSUInteger) dilationRateX
                         dilationRateY: (NSUInteger) dilationRateY;

/*! @abstract Convenience initializer for MPSCNNDilatedPooling nodes with square kernels and equal dilation factors
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = size
 *  @param      stride          strideInPixelsX = strideInPixelsY = stride
 *  @param      dilationRate    dilationRateX = dilationRateY = stride
 *  @return     A new MPSNNFilter node for a MPSCNNDilatedPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size
                                stride: (NSUInteger) stride
                          dilationRate: (NSUInteger) dilationRate;

/*! @abstract Convenience initializer for MPSCNNDilatedPooling nodes with square non-overlapping kernels
 *  @param      sourceNode      The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      size            kernelWidth = kernelHeight = strideInPixelsX = strideInPixelsY = dilationRateX = dilationRateY = size
 *  @return     A new MPSNNFilter node for a MPSCNNDilatedPooling kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            filterSize: (NSUInteger) size;

@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNPoolingGradientNode : MPSNNGradientFilterNode
/*! @abstract make a pooling gradient node
 *  @discussion  It would be much easier to use [inferencePoolingNode gradientNodeForSourceGradient:] instead.
 *  @param      sourceGradient  The gradient from the downstream gradient filter.
 *  @param      sourceImage     The input image to the inference pooling filter
 *  @param      gradientState   The gradient state produced by the inference poolin filter
 *  @param      kernelWidth     The kernel width of the inference filter
 *  @param      kernelHeight    The kernel height of the inference filter
 *  @param      strideInPixelsX The X stride from the inference filter
 *  @param      strideInPixelsY The Y stride from the inference filter
 */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight
                               strideInPixelsX: (NSUInteger) strideInPixelsX
                               strideInPixelsY: (NSUInteger) strideInPixelsY
                                 paddingPolicy: (nullable id <MPSNNPadding>) paddingPolicy;

/*! @abstract make a pooling gradient node
 *  @discussion  It would be much easier to use [inferencePoolingNode gradientNodeForSourceGradient:] instead.
 *  @param      sourceGradient  The gradient from the downstream gradient filter.
 *  @param      sourceImage     The input image to the inference pooling filter
 *  @param      gradientState   The gradient state produced by the inference poolin filter
 *  @param      kernelWidth     The kernel width of the inference filter
 *  @param      kernelHeight    The kernel height of the inference filter
 *  @param      strideInPixelsX The X stride from the inference filter
 *  @param      strideInPixelsY The Y stride from the inference filter
 */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight
                               strideInPixelsX: (NSUInteger) strideInPixelsX
                               strideInPixelsY: (NSUInteger) strideInPixelsY
                                 paddingPolicy: (nullable id <MPSNNPadding>) paddingPolicy;

@property (readonly, nonatomic) NSUInteger  kernelWidth;
@property (readonly, nonatomic) NSUInteger  kernelHeight;
@property (readonly, nonatomic) NSUInteger  strideInPixelsX;
@property (readonly, nonatomic) NSUInteger  strideInPixelsY;

@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNPoolingMaxGradientNode : MPSCNNPoolingGradientNode
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNPoolingAverageGradientNode : MPSCNNPoolingGradientNode
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNPoolingL2NormGradientNode : MPSCNNPoolingGradientNode
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDilatedPoolingMaxGradientNode : MPSCNNPoolingGradientNode
/*! @abstract make a pooling gradient node
 *  @discussion  It would be much easier to use [inferencePoolingNode gradientNodeForSourceGradient:] instead.
 *  @param      sourceGradient  The gradient from the downstream gradient filter.
 *  @param      sourceImage     The input image to the inference pooling filter
 *  @param      gradientState   The gradient state produced by the inference poolin filter
 *  @param      kernelWidth     The kernel width of the inference filter
 *  @param      kernelHeight    The kernel height of the inference filter
 *  @param      strideInPixelsX The X stride from the inference filter
 *  @param      strideInPixelsY The Y stride from the inference filter
 */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight
                               strideInPixelsX: (NSUInteger) strideInPixelsX
                               strideInPixelsY: (NSUInteger) strideInPixelsY
                                 dilationRateX: (NSUInteger) dilationRateX
                                 dilationRateY: (NSUInteger) dilationRateY;

/*! @abstract make a pooling gradient node
 *  @discussion  It would be much easier to use [inferencePoolingNode gradientNodeForSourceGradient:] instead.
 *  @param      sourceGradient  The gradient from the downstream gradient filter.
 *  @param      sourceImage     The input image to the inference pooling filter
 *  @param      gradientState   The gradient state produced by the inference poolin filter
 *  @param      kernelWidth     The kernel width of the inference filter
 *  @param      kernelHeight    The kernel height of the inference filter
 *  @param      strideInPixelsX The X stride from the inference filter
 *  @param      strideInPixelsY The Y stride from the inference filter
 */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight
                               strideInPixelsX: (NSUInteger) strideInPixelsX
                               strideInPixelsY: (NSUInteger) strideInPixelsY
                                 dilationRateX: (NSUInteger) dilationRateX
                                 dilationRateY: (NSUInteger) dilationRateY;

@property (readonly, nonatomic) NSUInteger  dilationRateX;
@property (readonly, nonatomic) NSUInteger  dilationRateY;

@end

#pragma mark - 
#pragma mark Normalization Nodes

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract virtual base class for CNN normalization nodes */
@interface MPSCNNNormalizationNode : MPSNNFilterNode

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_DESIGNATED_INITIALIZER;

@end

/*! @abstract Node representing MPSCNNSpatialNormalization 
 *  @discussion  For each feature channel, the function computes the sum of squares of X inside each rectangle, N2(i,j).
 *               It then divides each element of X as follows:
 *                  Y(i,j) = X(i,j) / (delta + alpha/(kw*kh) * N2(i,j))^beta,
 *               where kw and kh are the kernelWidth and the kernelHeight.
 *
 *      @code
 *        Defaults:
 *             alpha = 1.0f
 *             beta  = 5.0f
 *             delta = 1.0f
 *             kernelHeight = kernelWidth = kernelSize
 *      @endcode
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNSpatialNormalizationNode : MPSCNNNormalizationNode

@property (readwrite, nonatomic) NSUInteger  kernelWidth;
@property (readwrite, nonatomic) NSUInteger  kernelHeight;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize NS_DESIGNATED_INITIALIZER;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_DESIGNATED_INITIALIZER;

@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNSpatialNormalizationGradientNode : MPSNNGradientFilterNode

@property (readwrite, nonatomic) NSUInteger  kernelWidth;
@property (readwrite, nonatomic) NSUInteger  kernelHeight;

+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    kernelSize: (NSUInteger) kernelSize;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    kernelSize: (NSUInteger) kernelSize;

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

@end


/*! @abstract Node representing MPSCNNLocalContrastNormalization
 *  @discussion  The result is computed for each element of X as follows:
 *
 *                  Y(i,j) = pm + ps * ( X(i,j) - p0 * M(i,j)) / pow((delta + alpha * variance(i,j)), beta),
 *
 *              where kw and kh are the kernelWidth and the kernelHeight and pm, ps and p0 are parameters that
 *              can be used to offset and scale the result in various ways. *
 *      @code
 *        Defaults:
 *             alpha = 1.0f
 *             beta  = 0.5f
 *             delta = 2^-10
 *             pm = 0
 *             ps = 1
 *             p0 = 1
 *             kernelHeight = kernelWidth = kernelSize
 *      @endcode
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNLocalContrastNormalizationNode : MPSCNNNormalizationNode

@property (readwrite, nonatomic) float       pm;
@property (readwrite, nonatomic) float       ps;
@property (readwrite, nonatomic) float       p0;
@property (readwrite, nonatomic) NSUInteger  kernelWidth;
@property (readwrite, nonatomic) NSUInteger  kernelHeight;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize NS_DESIGNATED_INITIALIZER;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_DESIGNATED_INITIALIZER;

@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLocalContrastNormalizationGradientNode : MPSNNGradientFilterNode
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                   kernelWidth: (NSUInteger) kernelWidth
                                  kernelHeight: (NSUInteger) kernelHeight;

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 0.0
 *  @discussion The default value 0.0 is not recommended and is
 *              preserved for backwards compatibility. With alpha 0,
 *              it performs a local mean subtraction. The
 *              MPSCNNLocalContrastNormalizationNode used with
 *              the MPSNNGraph uses 1.0 as a default.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 0.5
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1/1024
 */
@property (readwrite, nonatomic) float   delta;

/*! @property   p0
 *  @abstract   The value of p0.  Default is 1.0
 */
@property (readwrite, nonatomic) float   p0;

/*! @property   pm
 *  @abstract   The value of pm.  Default is 0.0
 */
@property (readwrite, nonatomic) float   pm;

/*! @property   ps
 *  @abstract   The value of ps.  Default is 1.0
 */
@property (readwrite, nonatomic) float   ps;

@property (readonly, nonatomic) NSUInteger kernelWidth;
@property (readonly, nonatomic) NSUInteger kernelHeight;

@end

/*! @abstract Node representing MPSCNNCrossChannelNormalization
 *  @discussion  The normalized output is given by:
 *                  Y(i,j,k) = X(i,j,k) / L(i,j,k)^beta,
 *               where the normalizing factor is:
 *                  L(i,j,k) = delta + alpha/N * (sum_{q in Q(k)} X(i,j,q)^2, where
 *               N is the kernel size. The window Q(k) itself is defined as:
 *                  Q(k) = [max(0, k-floor(N/2)), min(D-1, k+floor((N-1)/2)], where
 *
 *              k is the feature channel index (running from 0 to D-1) and
 *              D is the number of feature channels, and alpha, beta and delta are paremeters.
 *      @code
 *        Defaults:
 *             alpha = 1.0f
 *             beta  = 5.0f
 *             delta = 1.0f
 *             kernelHeight = kernelWidth = kernelSize
 *      @endcode
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNCrossChannelNormalizationNode : MPSCNNNormalizationNode

@property (readwrite, nonatomic) NSUInteger  kernelSizeInFeatureChannels;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            kernelSize: (NSUInteger) kernelSize NS_DESIGNATED_INITIALIZER;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode NS_DESIGNATED_INITIALIZER;
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNCrossChannelNormalizationGradientNode : MPSNNGradientFilterNode
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    kernelSize: (NSUInteger) kernelSize;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                    kernelSize: (NSUInteger) kernelSize;

@property (readonly, nonatomic) NSUInteger kernelSize;

@end

@protocol MPSCNNInstanceNormalizationDataSource;

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNInstanceNormalizationNode : MPSNNFilterNode

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                            dataSource: (nonnull id <MPSCNNInstanceNormalizationDataSource>) dataSource;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                            dataSource: (nonnull id <MPSCNNInstanceNormalizationDataSource>) dataSource;
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNInstanceNormalizationGradientNode : MPSNNGradientFilterNode <MPSNNTrainableNode>

+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;
@end

@protocol MPSCNNBatchNormalizationDataSource;

/*! @class MPSCNNBatchNormalizationNode
 *  @abstract   A node representing batch normalization for inference or training
 *  @discussion Batch normalization operates differently for inference and training.
 *              For inference, the normalization is done according to a static statistical
 *              representation of data saved during training. For training, this representation
 *              is ever evolving.  In the low level MPS batch normalization interface,
 *              during training, the batch normalization is broken up into two steps:
 *              calculation of the statistical representation of input data, followed
 *              by normalization once the statistics are known for the entire batch.
 *              These are MPSCNNBatchNormalizationStatistics and MPSCNNBatchNormalization,
 *              respectively.
 *
 *              When this node appears in a graph and is not required to produce a
 *              MPSCNNBatchNormalizationState -- that is, MPSCNNBatchNormalizationNode.resultState
 *              is not used within the graph -- then it operates in inference mode
 *              and new batch-only statistics are not calculated. When this state node
 *              is consumed, then the node is assumed to be in training mode and
 *              new statistics will be calculated and written to the MPSCNNBatchNormalizationState
 *              and passed along to the MPSCNNBatchNormalizationGradient and
 *              MPSCNNBatchNormalizationStatisticsGradient as necessary. This should
 *              allow you to construct an identical sequence of nodes for inference
 *              and training and expect the right thing to happen.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNBatchNormalizationNode : MPSNNFilterNode

/*! @abstract Options controlling how batch normalization is calculated
 *  @discussion     Default: MPSCNNBatchNormalizationFlagsDefault */
@property (readwrite, nonatomic)  MPSCNNBatchNormalizationFlags   flags;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                            dataSource: (nonnull id <MPSCNNBatchNormalizationDataSource>) dataSource;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                            dataSource: (nonnull id <MPSCNNBatchNormalizationDataSource>) dataSource;

@end

/*! @class MPSCNNBatchNormalizationGradientNode
 *  @abstract   A node representing batch normalization gradient for training
 *  @discussion This filter encapsulates the MPSCNNBatchNormalizationStatisticsGradient
 *              and MPSCNNBatchNormalizationGradient low level filters as a single
 *              node. They will be called in sequence: statistics gradient until the
 *              batch is complete, then batch normalization gradient on the result.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNBatchNormalizationGradientNode : MPSNNGradientFilterNode <MPSNNTrainableNode>
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;
@end

#pragma mark -
#pragma mark Image resizing
@protocol MPSImageTransformProvider  <NSSecureCoding, NSObject>
-(MPSScaleTransform) transformForSourceImage: (MPSImage * __nonnull) image
                                      handle: (__nullable id <MPSHandle>) handle;
@end

/*! @abstract Abstract Node representing a image resampling operation
 *  @discussion  Please make a MPSNNBilinearScale or MPSNNLanczosScale object instead
 */
 MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNScaleNode : MPSNNFilterNode
/*! @abstract create an autoreleased node to convert a MPSImage to the desired size
 *  @param  sourceNode    A valid MPSNNImageNode
 *  @param  size          The size of the output image {width, height, depth}    */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            outputSize: (MTLSize) size;

/*! @abstract create an autoreleased node to convert a MPSImage to the desired size for a region of interest
 *  @param  sourceNode            A valid MPSNNImageNode
 *  @param  transformProvider    If non-nil, a valid MPSImageTransformProvider that provides the region of interest
 *  @param  size              The size of the output image {width, height, depth}    */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                     transformProvider: (__nullable id <MPSImageTransformProvider>) transformProvider
                            outputSize: (MTLSize) size;

/*! @abstract init a node to convert a MPSImage to the desired size
 *  @param  sourceNode    A valid MPSNNImageNode
 *  @param  size          The size of the output image {width, height, depth}    */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                            outputSize: (MTLSize) size;

/*! @abstract init a node to convert a MPSImage to the desired size for a region of interest
 *  @param  sourceNode           A valid MPSNNImageNode
 *  @param  transformProvider    If non-nil, a valid MPSImageTransformProvider that provides the region of interest
 *  @param  size                 The size of the output image {width, height, depth}    */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                     transformProvider: (__nullable id <MPSImageTransformProvider>) transformProvider
                            outputSize: (MTLSize) size;

@end

/*!    @abstract A MPSNNScale object that uses bilinear interpolation for resampling
 *     @discussion    Caution: bilinear downscaling by more than a factor of
 *                    two in any dimension causes loss of information if a
 *                    low pass filter is not run over the image first. Details
 *                    may be omitted.    */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNBilinearScaleNode : MPSNNScaleNode
@end

/*!    @abstract A MPSNNScale object that uses the Lanczos resampling filter
 *     @discussion    This method does not require a low pass filter for downsampling
 *                    by more than a factor of two. Caution: may cause ringing, which
 *                    could prove distracting to a neural network unused to seeing it.
 *                    You should use the resampling method that was used to train the
 *                    network. */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNLanczosScaleNode : MPSNNScaleNode
@end


#pragma mark -
#pragma mark Arithmetic

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract virtual base class for basic arithmetic nodes */
@interface MPSNNBinaryArithmeticNode : MPSNNFilterNode

/*! @abstract create an autoreleased arithemtic node with an array of sources
 *  @param  sourceNodes     A valid NSArray containing two sources  */
+(nonnull instancetype) nodeWithSources: (NSArray <MPSNNImageNode *> * __nonnull) sourceNodes;

/*! @abstract create an autoreleased arithemtic node with two sources
 *  @param  left    the left operand
 *  @param  right   the right operand */
+(nonnull instancetype) nodeWithLeftSource: (MPSNNImageNode * __nonnull) left
                               rightSource: (MPSNNImageNode * __nonnull) right;

/*! @abstract init an arithemtic node with an array of sources
 *  @param  sourceNodes     A valid NSArray containing two sources  */
-(nonnull instancetype) initWithSources: (NSArray <MPSNNImageNode *> * __nonnull) sourceNodes;

/*! @abstract init an arithemtic node with two sources
 *  @param  left    the left operand
 *  @param  right   the right operand */
-(nonnull instancetype) initWithLeftSource: (MPSNNImageNode * __nonnull) left
                               rightSource: (MPSNNImageNode * __nonnull) right;

-(nonnull Class) gradientClass;

-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources:(NSArray<MPSNNImageNode *> * __nonnull) gradientImages NS_UNAVAILABLE;

/*! @abstract create new arithmetic gradient nodes
 *  @discussion Create two new arithmetic gradient nodes - one that computes the gradient for the primary
 *  source image and one that computes the gradient for the secondary sourcefrom the inference pass.
 */
-(NSArray <MPSNNGradientFilterNode*> * __nonnull) gradientFiltersWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

@property (readwrite, nonatomic) float primaryScale;
@property (readwrite, nonatomic) float secondaryScale;
@property (readwrite, nonatomic) float bias;
@property (readwrite, nonatomic) NSUInteger primaryStrideInPixelsX;
@property (readwrite, nonatomic) NSUInteger primaryStrideInPixelsY;
@property (readwrite, nonatomic) NSUInteger primaryStrideInFeatureChannels;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInPixelsX;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInPixelsY;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInFeatureChannels;
@property (readwrite, nonatomic) float minimumValue;
@property (readwrite, nonatomic) float maximumValue;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract returns elementwise sum of left + right */
@interface MPSNNAdditionNode : MPSNNBinaryArithmeticNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract returns elementwise difference of left - right */
@interface MPSNNSubtractionNode : MPSNNBinaryArithmeticNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract returns elementwise product of left * right */
@interface MPSNNMultiplicationNode : MPSNNBinaryArithmeticNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! @abstract returns elementwise quotient of left / right */
@interface MPSNNDivisionNode : MPSNNBinaryArithmeticNode
@end

#pragma mark -
#pragma mark Arithmetic Gradient

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNArithmeticGradientNode : MPSNNGradientFilterNode
/*! @abstract create a new arithmetic gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodesWithSources:]
 *              for an easier way to do this.
 *  @param      sourceGradient          The input gradient from the 'downstream' gradient filter.
 *  @param      sourceImage             The source input image from the forward pass (primary or secondary).
 *  @param      gradientState           The gradient state produced by the concatenation filter, consumed by this filter.
 */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNBinaryGradientStateNode*__nonnull) gradientState
                       isSecondarySourceFilter: (BOOL) isSecondarySourceFilter;

/*! @abstract create a new arithmetic gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodesWithSources:]
 *              for an easier way to do this.
 *  @param      sourceGradient          The input gradient from the 'downstream' gradient filter.
 *  @param      sourceImage             The source input image from the forward pass (primary or secondary).
 *  @param      gradientState           The gradient state produced by the concatenation filter, consumed by this filter.
 */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNBinaryGradientStateNode*__nonnull) gradientState
                       isSecondarySourceFilter: (BOOL) isSecondarySourceFilter;

/*! @abstract create a new arithmetic gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodesWithSources:]
 *              for an easier way to do this.
 *  @param      gradientImages          The input gradient from the 'downstream' gradient filter and the source input image
 *                                      from the forward pass (primary or secondary).
 *  @param      filter                  The matching filter node from the forward pass.
 *  @param      isSecondarySourceFilter The isSecondarySourceFilter property is used to indicate whether the arithmetic
 *                                      gradient filter is operating on the primary or secondary source image from the
 *                                      forward pass.
 */
-(nonnull instancetype) initWithGradientImages: (NSArray<MPSNNImageNode *> *__nonnull)gradientImages
                                 forwardFilter: (MPSNNFilterNode *__nonnull)filter
                       isSecondarySourceFilter: (BOOL) isSecondarySourceFilter;

@property (readwrite, nonatomic) float primaryScale;
@property (readwrite, nonatomic) float secondaryScale;
@property (readwrite, nonatomic) float bias;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInPixelsX;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInPixelsY;
@property (readwrite, nonatomic) NSUInteger secondaryStrideInFeatureChannels;
@property (readwrite, nonatomic) float minimumValue;
@property (readwrite, nonatomic) float maximumValue;
@property (readonly, nonatomic) BOOL isSecondarySourceFilter;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract returns gradient for either primary or secondary source image from the inference pass.
 *  Use the isSecondarySourceFilter property to indicate whether this filter is computing the gradient
 *  for the primary or secondary source image from the inference pass.
 */
@interface MPSNNAdditionGradientNode : MPSNNArithmeticGradientNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract returns gradient for either primary or secondary source image from the inference pass.
 *  Use the isSecondarySourceFilter property to indicate whether this filter is computing the gradient
 *  for the primary or secondary source image from the inference pass.
 */
@interface MPSNNSubtractionGradientNode : MPSNNArithmeticGradientNode
@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @abstract returns gradient for either primary or secondary source image from the inference pass.
 *  Use the isSecondarySourceFilter property to indicate whether this filter is computing the gradient
 *  for the primary or secondary source image from the inference pass.
 */
@interface MPSNNMultiplicationGradientNode : MPSNNArithmeticGradientNode
@end

#pragma mark -
#pragma mark Dropout

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDropoutNode : MPSNNFilterNode
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source;
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                       keepProbability: (float) keepProbability;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                       keepProbability: (float) keepProbability;

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                       keepProbability: (float) keepProbability
                                  seed: (NSUInteger) seed
                    maskStrideInPixels: (MTLSize) maskStrideInPixels;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                       keepProbability: (float) keepProbability
                                  seed: (NSUInteger) seed
                    maskStrideInPixels: (MTLSize) maskStrideInPixels NS_DESIGNATED_INITIALIZER;

@property (readonly, nonatomic) float keepProbability;
@property (readonly, nonatomic) NSUInteger seed;
@property (readonly, nonatomic) MTLSize maskStrideInPixels;

@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDropoutGradientNode : MPSNNGradientFilterNode
/*! @abstract create a new dropout gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodeWithSources:]
 *              for an easier way to do this */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                               keepProbability: (float) keepProbability
                                          seed: (NSUInteger) seed
                            maskStrideInPixels: (MTLSize) maskStrideInPixels;

/*! @abstract create a new dropout gradient node
 *  @discussion See also -[MPSCNNNeuronNode gradientFilterNodeWithSources:]
 *              for an easier way to do this */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                               keepProbability: (float) keepProbability
                                          seed: (NSUInteger) seed
                            maskStrideInPixels: (MTLSize) maskStrideInPixels;

@property (readonly, nonatomic) float keepProbability;
@property (readonly, nonatomic) NSUInteger seed;
@property (readonly, nonatomic) MTLSize maskStrideInPixels;

@end

#pragma mark -
#pragma mark Loss

@class MPSCNNLossDescriptor;

/*! @class MPSNNLabelsNode
 *  @discussion  The labels and weights for each MPSImage are passed in
 *               separately to the graph in a MPSNNLabels object. If
 *               the batch interface is used then there will be a
 *               MPSStateBatch of these of the same size as the MPSImageBatch
 *               that holds the images.  The MPSNNLabelsNode is a place
 *               holder in the graph for these nodes. The MPSNNLabels node
 *               is taken as an input to the Loss node*/
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNLabelsNode : MPSNNStateNode
@end

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
/*! @class MPSCNNLossNode
 *  @discussion  This node calculates loss information during training
 *               typically immediately after the inference portion
 *               of network evaluation is performed. The result image
 *               of the loss operations is typically the first gradient
 *               image to be comsumed by the gradient passes that work
 *               their way back up the graph. In addition, the node will
 *               update the loss image in the MPSNNLabels with the
 *               desired estimate of correctness. */
@interface MPSCNNLossNode : MPSNNFilterNode

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                        lossDescriptor: (MPSCNNLossDescriptor * __nonnull) descriptor;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                        lossDescriptor: (MPSCNNLossDescriptor * __nonnull) descriptor;

/*! @abstract Get the input node for labes and weights, for example to set the handle */
@property (nonnull, readonly, retain, nonatomic) MPSNNLabelsNode * inputLabels;

/*! @abstract The loss filter is its own gradient filter and doesn't provide a corresponding gradient node.
 *  @discussion The image returned by the loss filter is the gradient image to be consumed by
 *              the gradient filters corresponding to preceeding inference nodes.   */
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages NS_UNAVAILABLE;

@end


#pragma mark -
#pragma mark YOLOLoss

@class MPSCNNYOLOLossDescriptor;

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
/*! @class MPSCNNYOLOLossNode
 *  @discussion  This node calculates loss information during training
 *               typically immediately after the inference portion
 *               of network evaluation is performed. The result image
 *               of the loss operations is typically the first gradient
 *               image to be comsumed by the gradient passes that work
 *               their way back up the graph. In addition, the node will
 *               update the loss image in the MPSNNLabels with the
 *               desired estimate of correctness. */
@interface MPSCNNYOLOLossNode : MPSNNFilterNode

+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) source
                        lossDescriptor: (MPSCNNYOLOLossDescriptor * __nonnull) descriptor;

-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) source
                        lossDescriptor: (MPSCNNYOLOLossDescriptor * __nonnull) descriptor;

/*! @abstract Get the input node for labes and weights, for example to set the handle */
@property (nonnull, readonly, retain, nonatomic) MPSNNLabelsNode * inputLabels;

/*! @abstract The loss filter is its own gradient filter and doesn't provide a corresponding gradient node.
 *  @discussion The image returned by the loss filter is the gradient image to be consumed by
 *              the gradient filters corresponding to preceeding inference nodes.   */
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages NS_UNAVAILABLE;

@end


#pragma mark -
#pragma mark Other Nodes

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! Node representing a the concatenation (in the feature channel dimension) of the results from one or more kernels */
@interface MPSNNConcatenationNode : MPSNNFilterNode
/*! @abstract   Init a autoreleased node that concatenates feature channels from multiple images
 *  @discussion In some neural network designs, it is necessary to append feature channels
 *              from one neural network filter to the results of another. If we have three
 *              image nodes with M, N and O feature channels in them, passed to -initWithSources:
 *              as @[imageM, imageN, imageO], then feature channels [0,M-1] will be drawn from
 *              image M,  feature channels [M, M+N-1] will be drawn from image N and feature channels
 *              [M+N, M+N+O-1] will be drawn from image O.
 *
 *              As all images are padded out to a multiple of four feature channels,
 *              M, N and O here are also multiples of four, even when the MPSImages
 *              are not. That is, if the image is 23 feature channels and one channel
 *              of padding, it takes up 24 feature channels worth of space in the
 *              concatenated result.
 *
 *              Performance Note:  Generally, concatenation is free as long as all
 *              of the sourceNodes are produced by filters in the same MPSNNGraph.
 *              Most MPSCNNKernels have the ability to write their results  at a
 *              feature channel offset within a target MPSImage. However, if the
 *              MPSNNImageNode source nodes come from images external to the MPSNNGraph,
 *              then we have to do a copy operation to assemble the concatenated node.
 *              As a result, when deciding where to break a large logical graph into
 *              multiple smaller MPSNNGraphs, it is better for concatenations to
 *              appear at the ends of subgraphs when possible rather than at the start,
 *              to the extent that all the images used in the concatenation are
 *              produced by that subgraph.
 *
 *  @param      sourceNodes              The MPSNNImageNode representing the source MPSImages for the filter
 *  @return     A new MPSNNFilter node that concatenates its inputs.
 */
+(nonnull instancetype) nodeWithSources: (NSArray <MPSNNImageNode *> * __nonnull) sourceNodes;

/*! @abstract   Init a node that concatenates feature channels from multiple images
 *  @discussion In some neural network designs, it is necessary to append feature channels
 *              from one neural network filter to the results of another. If we have three
 *              image nodes with M, N and O feature channels in them, passed to -initWithSources:
 *              as @[imageM, imageN, imageO], then feature channels [0,M-1] will be drawn from
 *              image M,  feature channels [M, M+N-1] will be drawn from image N and feature channels
 *              [M+N, M+N+O-1] will be drawn from image O.
 *
 *              As all images are padded out to a multiple of four feature channels,
 *              M, N and O here are also multiples of four, even when the MPSImages
 *              are not. That is, if the image is 23 feature channels and one channel
 *              of padding, it takes up 24 feature channels worth of space in the
 *              concatenated result.
 *
 *              Performance Note:  Generally, concatenation is free as long as all
 *              of the sourceNodes are produced by filters in the same MPSNNGraph.
 *              Most MPSCNNKernels have the ability to write their results  at a
 *              feature channel offset within a target MPSImage. However, if the
 *              MPSNNImageNode source nodes come from images external to the MPSNNGraph,
 *              then we have to do a copy operation to assemble the concatenated node. 
 *              As a result, when deciding where to break a large logical graph into 
 *              multiple smaller MPSNNGraphs, it is better for concatenations to 
 *              appear at the ends of subgraphs when possible rather than at the start,
 *              to the extent that all the images used in the concatenation are 
 *              produced by that subgraph.
 *
 *  @param      sourceNodes              The MPSNNImageNode representing the source MPSImages for the filter
 *  @return     A new MPSNNFilter node that concatenates its inputs.
 */
-(nonnull instancetype) initWithSources: (NSArray <MPSNNImageNode *> * __nonnull) sourceNodes;


/*! @abstract Concatenation returns multiple gradient filters. Use -gradientFiltersWithSources: instead. */
-(MPSNNGradientFilterNode*__nonnull) gradientFilterWithSources: (NSArray<MPSNNImageNode*> * __nonnull) gradientImages NS_UNAVAILABLE;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! @class MPSNNConcatenationGradientNode
 *  @abstract  A MPSNNSlice filter that operates as the conjugate computation for concatentation operators during training
 *  @discussion As concatenation is formally just a copy and not a computation, there isn't a lot of arithmetic for
 *              the slice operator to do, but we still need to extract out the relevant portion
 *              of the gradient of the input signal that went into the corresponding concatenation
 *              destination image. */
@interface MPSNNConcatenationGradientNode : MPSNNGradientFilterNode

/*! @abstract       create a MPSNNConcatenationGradientNode
 *  @discussion     Generally you should use [MPSNNConcatenationNode gradientFiltersWithSources:] instead.
 *  @param          gradientSourceNode  The gradient image functioning as input for the operator
 *  @param          sourceImage         The particular input image to the concatentation, if any, that the slice corresponds with
 *  @param          gradientState       The gradient state produced by the concatenation filter, consumed by this filter */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode * __nonnull) gradientSourceNode
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

/*! @abstract       Init a MPSNNConcatenationGradientNode
 *  @discussion     Generally you should use [MPSNNConcatenationNode gradientFiltersWithSources:] instead.
 *  @param          gradientSourceNode  The gradient image functioning as input for the operator
 *  @param          sourceImage         The particular input image to the concatentation, if any, that the slice corresponds with
 *  @param          gradientState       The gradient state produced by the concatenation filter, consumed by this filter */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode * __nonnull) gradientSourceNode
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

@end


MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! Node representing a MPSCNNSoftMax kernel */
@interface MPSCNNSoftMaxNode : MPSNNFilterNode
/*! @abstract   Init a node representing a autoreleased MPSCNNSoftMax kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @return     A new MPSNNFilter node for a MPSCNNSoftMax kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract   Init a node representing a MPSCNNSoftMax kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @return     A new MPSNNFilter node for a MPSCNNSoftMax kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! Node representing a MPSCNNSoftMaxGradient kernel */
@interface MPSCNNSoftMaxGradientNode : MPSNNGradientFilterNode
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! Node representing a MPSCNNLogSoftMax kernel */
@interface MPSCNNLogSoftMaxNode : MPSNNFilterNode
/*! @abstract   Init a node representing a autoreleased MPSCNNLogSoftMax kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @return     A new MPSNNFilter node for a MPSCNNLogSoftMax kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode;

/*! @abstract   Init a node representing a MPSCNNLogSoftMax kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @return     A new MPSNNFilter node for a MPSCNNLogSoftMax kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! Node representing a MPSCNNLogSoftMaxGradient kernel */
@interface MPSCNNLogSoftMaxGradientNode : MPSNNGradientFilterNode
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;

-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState;
@end


MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! Node representing a MPSCNNUpsamplingNearest kernel */
@interface MPSCNNUpsamplingNearestNode : MPSNNFilterNode
/*! @abstract Convenience initializer for an autoreleased MPSCNNUpsamplingNearest nodes
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingNearest kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY;

/*! @abstract   Init a node representing a MPSCNNUpsamplingNearest kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingNearest kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY;

@property (nonatomic, readonly)    double  scaleFactorX;
@property (nonatomic, readonly)    double  scaleFactorY;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
/*! Node representing a MPSCNNUpsamplingBilinear kernel */
@interface MPSCNNUpsamplingBilinearNode : MPSNNFilterNode
/*! @abstract   Init a autoreleased node representing a MPSCNNUpsamplingBilinear kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingBilinear kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY;

/*! @abstract   Init a autoreleased node representing a MPSCNNUpsamplingBilinear kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @param      alignCorners            Specifier whether the centers of the 4 corner pixels of the input and output regions are aligned,
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingBilinear kernel.
 */
+(nonnull instancetype) nodeWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY
                          alignCorners: (BOOL) alignCorners;

/*! @abstract   Init a node representing a MPSCNNUpsamplingBilinear kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingBilinear kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY;


/*! @abstract   Init a node representing a MPSCNNUpsamplingBilinear kernel
 *  @param      sourceNode              The MPSNNImageNode representing the source MPSImage for the filter
 *  @param      integerScaleFactorX     The upsampling factor for the x dimension.
 *  @param      integerScaleFactorY     The upsampling factor for the y dimension.
 *  @param      alignCorners            Specifier whether the centers of the 4 corner pixels of the input and output regions are aligned,
 *  @return     A new MPSNNFilter node for a MPSCNNUpsamplingBilinear kernel.
 */
-(nonnull instancetype) initWithSource: (MPSNNImageNode * __nonnull) sourceNode
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY
                          alignCorners: (BOOL) alignCorners;

@property (nonatomic, readonly)    double  scaleFactorX;
@property (nonatomic, readonly)    double  scaleFactorY;
@property (nonatomic, readonly)    BOOL    alignCorners;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! Node representing a MPSCNNUpsamplingNearest kernel */
@interface MPSCNNUpsamplingNearestGradientNode : MPSNNGradientFilterNode

/*! @abstract   A node to represent the gradient calculation for nearest upsampling training.
 *  @discussion [forwardFilter gradientFilterWithSources:] is a more convient way to do this.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter.
 *  @param sourceImage      The input image from the forward filter node
 *  @param gradientState    The gradient state from the forward filter
 *  @param scaleFactorX     The X scale factor from the forward pass
 *  @param scaleFactorY     The Y scale factor from the forward pass
 *  @return  A MPSCNNUpsamplingNearestGradientNode    */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                  scaleFactorX: (double) scaleFactorX
                                  scaleFactorY: (double) scaleFactorY;

/*! @abstract   A node to represent the gradient calculation for nearest upsampling training.
 *  @discussion [forwardFilter gradientFilterWithSources:] is a more convient way to do this.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter.
 *  @param sourceImage      The input image from the forward filter node
 *  @param gradientState    The gradient state from the forward filter
 *  @param scaleFactorX     The X scale factor from the forward pass
 *  @param scaleFactorY     The Y scale factor from the forward pass
 *  @return  A MPSCNNUpsamplingNearestGradientNode    */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                  scaleFactorX: (double) scaleFactorX
                                  scaleFactorY: (double) scaleFactorY;


@property (nonatomic, readonly)    double  scaleFactorX;
@property (nonatomic, readonly)    double  scaleFactorY;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
/*! Node representing a MPSCNNUpsamplingBilinear kernel */
@interface MPSCNNUpsamplingBilinearGradientNode : MPSNNGradientFilterNode

/*! @abstract   A node to represent the gradient calculation for nearest upsampling training.
 *  @discussion [forwardFilter gradientFilterWithSources:] is a more convient way to do this.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter.
 *  @param sourceImage      The input image from the forward filter node
 *  @param gradientState    The gradient state from the forward filter
 *  @param scaleFactorX     The X scale factor from the forward pass
 *  @param scaleFactorY     The Y scale factor from the forward pass
 *  @return  A MPSCNNUpsamplingBilinearGradientNode    */
+(nonnull instancetype) nodeWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                  scaleFactorX: (double) scaleFactorX
                                  scaleFactorY: (double) scaleFactorY;

/*! @abstract   A node to represent the gradient calculation for nearest upsampling training.
 *  @discussion [forwardFilter gradientFilterWithSources:] is a more convient way to do this.
 *  @param sourceGradient   The input gradient from the 'downstream' gradient filter.
 *  @param sourceImage      The input image from the forward filter node
 *  @param gradientState    The gradient state from the forward filter
 *  @param scaleFactorX     The X scale factor from the forward pass
 *  @param scaleFactorY     The Y scale factor from the forward pass
 *  @return  A MPSCNNUpsamplingBilinearGradientNode    */
-(nonnull instancetype) initWithSourceGradient: (MPSNNImageNode*__nonnull) sourceGradient
                                   sourceImage: (MPSNNImageNode*__nonnull) sourceImage
                                 gradientState: (MPSNNGradientStateNode*__nonnull) gradientState
                                  scaleFactorX: (double) scaleFactorX
                                  scaleFactorY: (double) scaleFactorY;

@property (nonatomic, readonly)    double  scaleFactorX;
@property (nonatomic, readonly)    double  scaleFactorY;


@end


#endif /* MPSNNGraphNodes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNNormalizationWeights.h
//
//  MPSCNNNormalizationWeights.h
//  MPSNeuralNetwork
//
//  Created by Justin Voo on 2/4/18.
//  Copyright © 2018 Apple. All rights reserved.
//

#ifndef MPSCNNNormalizationWeights_h
#define MPSCNNNormalizationWeights_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif
    
/*!
 *  @class  MPSCNNNormalizationGammaAndBetaState
 *  @description A state which contains gamma and beta terms used to apply a scale
 *               and bias in either an MPSCNNInstanceNormalization or MPSCNNBatchNormalization
 *               operation.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNNormalizationGammaAndBetaState : MPSState

/*! @property   gamma
 *  @abstract   A MTLBuffer containing the gamma terms.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> gamma;

/*! @property   beta
 *  @abstract   A MTLBuffer containing the beta terms.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> beta;

/*!
 *  @abstract   Initialize a MPSCNNNormalizationGammaAndBetaState object using values
 *              contained in MTLBuffers.
 *
 *  @param      gamma       The MTLBuffer containing gamma terms.
 *
 *  @param      beta        The MTLBuffer containing beta terms.
 */
- (nonnull instancetype) initWithGamma: (__nonnull id<MTLBuffer>) gamma
                                  beta: (__nonnull id<MTLBuffer>) beta;

/*!
 *  @abstract   Create a temporary MPSCNNNormalizationGammaAndBetaState suitable
 *              for a normalization operation on images containing no more than
 *              the specified number of feature channels.
 *
 *  @param      commandBuffer           The command buffer on which the temporary state will
 *                                      be used.
 *
 *  @param      numberOfFeatureChannels The number of feature channels used to size the
 *                                      state.
 */
+ (nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                 numberOfFeatureChannels: (NSUInteger) numberOfFeatureChannels;

@end    // MPSCNNNormalizationGammaAndBetaState
    
#ifdef __cplusplus
}
#endif
#endif /* MPSCNNNormalizationWeights_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNMath.h
//
//  MPSCNNMath.h
//  MPS
//
//  Created by Anna Tikhonova on 11/17/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNMath_h
#define MPSCNNMath_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif

    
#pragma mark -
#pragma mark MPSCNNArithmeticGradientState

/*!
 *  @class      MPSCNNArithmeticGradientState
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNArithmeticGradientState is used to hold the clamp mask used by both
 *              MPSCNNArithmetic forward filter and MPSCNNArithmeticGradient backward filter.
 *              The MPSCNNArithmetic forward filter populates the MPSCNNArithmeticGradientState
 *              object and the MPSCNNArithmeticGradient backward filter consumes the state
 *              object.
 *
 *              The clamp mask is stored internally and is not accessible by the user.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNArithmeticGradientState : MPSNNBinaryGradientState

/*
 * Use [MPSCNNDropout resultStateForSourceImage:...] or other variants instead
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

@end


#pragma mark -
#pragma mark MPSCNNArithmeticGradientStateBatch

typedef NSArray<MPSCNNArithmeticGradientState*> MPSCNNArithmeticGradientStateBatch
MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));


#pragma mark -
#pragma mark MPSCNNArithmetic

/*!
 *  @class      MPSCNNArithmetic
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNArithmetic filter takes two source images, a primary source image and a
 *              secondary source image, and outputs a single destination image. It applies an
 *              element-wise arithmetic operator to each pixel in a primary source image and a
 *              corresponding pixel in a secondary source image over a specified region.
 *
 *              The supported arithmetic operators are the following:
 *              - Addition
 *              - Subtraction
 *              - Multiplication
 *              - Division
 *
 *              This filter takes additional parameters: primaryScale, secondaryScale, and bias. The default
 *              value for primaryScale and secondaryScale is 1.0f. The default value for bias is 0.0f. This
 *              filter applies primaryScale, secondaryScale, and bias to the primary source pixel (x) and
 *              secondary source pixel (y) in the following way:
 *              - Addition:         result = ((primaryScale * x) + (secondaryScale * y)) + bias
 *              - Subtraction:      result = ((primaryScale * x) - (secondaryScale * y)) + bias
 *              - Multiplicaton:    result = ((primaryScale * x) * (secondaryScale * y)) + bias
 *              - Division:         result = ((primaryScale * x) / (secondaryScale * y)) + bias
 *
 *              To clamp the result of an arithmetic operation, where
 *              result = clamp(result, minimumValue, maximumValue),
 *              set the minimumValue and maximumValue appropriately. The default value of minimumValue
 *              is -FLT_MAX. The default value of maximumValue is FLT_MAX.
 *
 *              This filter also takes the following additional parameters:
 *              - primaryStrideInPixelsX, primaryStrideInPixelsY, primaryStrideInFeatureChannels
 *              - secondaryStrideInPixelsX, secondaryStrideInPixelsY, secondaryStrideInFeatureChannels
 *              These parameters can be used to control broadcasting for the data stored in the primary and
 *              secondary source images. For example, setting all strides for the primary source image to 0
 *              will result in the primarySource image being treated as a scalar value. The only supported
 *              values are 0 or 1. The default value of these parameters is 1.
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 *
 *              You must use one of the sub-classes of MPSImageArithmetic.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNArithmetic : MPSCNNBinaryKernel

@property (readwrite, nonatomic) float primaryScale;
@property (readwrite, nonatomic) float secondaryScale;
@property (readwrite, nonatomic) float bias;

/*! @property   primaryStrideInPixels
 *  @abstract   The primarySource stride in the feature channel dimension. The only supported values are 0 or 1.
 *              The default value for each dimension is 1.
 */
@property(readwrite, nonatomic) NSUInteger      primaryStrideInFeatureChannels;

/*! @property   secondaryStrideInPixels
 *  @abstract   The secondarySource stride in the feature channel dimension. The only supported values are 0 or 1.
 *              The default value for each dimension is 1.
 */
@property(readwrite, nonatomic) NSUInteger      secondaryStrideInFeatureChannels;

/*! @property   minimumValue
 *  @abstract   minimumValue is to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of minimumValue is -FLT_MAX.
 */
@property (readwrite, nonatomic) float          minimumValue;

/*! @property   maximumValue
 *  @abstract   maximumValue is used to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of maximumValue is FLT_MAX.
 */
@property (readwrite, nonatomic) float          maximumValue;

/*
 * You must use one of the sub-classes of MPSCNNArithmetic.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*! @abstract       Encode call that operates on a state for later consumption by a gradient kernel in training
 *  @discussion     This is the older style of encode which reads the offset, doesn't change it,
 *                  and ignores the padding method.
 *  @param          commandBuffer       The command buffer
 *  @param          primaryImage        A MPSImage to use as the source images for the filter.
 *  @param          secondaryImage      A MPSImage to use as the source images for the filter.
 *  @param          destinationState    MPSCNNArithmeticGradientState to be consumed by the gradient layer
 *  @param          destinationImage    A valid MPSImage to be overwritten by result image. destinationImage
 *                                      may not alias primarySourceImage or secondarySourceImage.
 */
-(void) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                 primaryImage: (MPSImage * __nonnull) primaryImage
               secondaryImage: (MPSImage * __nonnull) secondaryImage
             destinationState: (MPSCNNArithmeticGradientState * __nonnull) destinationState
             destinationImage: (MPSImage * __nonnull) destinationImage
MPS_SWIFT_NAME( encode(commandBuffer:primaryImage:secondaryImage:destinationState:destinationImage:));


/*!
 *  @abstract   Encode call that operates on a state for later consumption by a gradient kernel in training
 *  @discussion This is the older style of encode which reads the offset, doesn't change it,
 *              and ignores the padding method. Multiple images are processed concurrently.
 *              All images must have MPSImage.numberOfImages = 1.
 *  @param      commandBuffer         A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryImages         An array of MPSImage objects containing the primary source images.
 *  @param      secondaryImages       An array MPSImage objects containing the secondary source images.
 *  @param      destinationStates     An array of MPSCNNArithmeticGradientStateBatch to be consumed by the gradient layer
 *  @param      destinationImages     An array of MPSImage objects to contain the result images.
 *                                    destinationImages may not alias primarySourceImages or secondarySourceImages
 *                                    in any manner.
 */
-(void) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                     primaryImages: (MPSImageBatch * __nonnull) primaryImages
                   secondaryImages: (MPSImageBatch * __nonnull) secondaryImages
                 destinationStates: (MPSCNNArithmeticGradientStateBatch * __nonnull) destinationStates
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages
MPS_SWIFT_NAME( encodeBatch(commandBuffer:primaryImages:secondaryImages:destinationStates:destinationImages:));



@end /* MPSCNNArithmetic */


#pragma mark -
#pragma mark MPSCNNAdd
    
/*!
 *  @class      MPSCNNAdd
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the addition operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) + (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNAdd : MPSCNNArithmetic

/*!
 *  @abstract  Initialize the addition operator.
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSCNNAdd object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNAdd */


#pragma mark -
#pragma mark MPSCNNSubtract

/*!
 *  @class      MPSCNNSubtract
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the subtraction operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) - (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNSubtract : MPSCNNArithmetic

/*!
 *  @abstract  Initialize the subtraction operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSCNNSubtract object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNSubtract */


#pragma mark -
#pragma mark MPSCNNMultiply

/*!
 *  @class      MPSCNNMultiply
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the multiplication operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) * (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNMultiply : MPSCNNArithmetic

/*!
 *  @abstract  Initialize the multiplication operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSCNNMultiply object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNMultiply */
    

#pragma mark -
#pragma mark MPSCNNDivide

/*!
 *  @class      MPSCNNDivide
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the division operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) / (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNDivide : MPSCNNArithmetic

/*!
 *  @abstract  Initialize the division operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSCNNDivide object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNDivide */


#pragma mark -
#pragma mark MPSCNNArithmeticGradient

/*!
 *  @class      MPSCNNArithmeticGradient
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNArithmeticGradient filter is the backward filter for the MPSCNNArithmetic
 *              forward filter.
 *
 *              The forward filter takes two inputs, primary and secondary source images, and produces
 *              a single output image. Thus, going backwards requires two separate filters (one for
 *              the primary source image and one for the secondary source image) that take multiple
 *              inputs and produce a single output. The secondarySourceFilter property is used to
 *              indicate whether the filter is operating on the primary or secondary source image from
 *              the forward pass.
 *
 *              All the arithmetic gradient filters require the following inputs: gradient image from
 *              the previous layer (going backwards) and all the applicable input source images from
 *              the forward pass.
 *
 *              The forward filter takes the following additional parameters:
 *              - primaryStrideInPixelsX, primaryStrideInPixelsY, primaryStrideInFeatureChannels
 *              - secondaryStrideInPixelsX, secondaryStrideInPixelsY, secondaryStrideInFeatureChannels
 *              These parameters can be used in the forward filter to control broadcasting for the data
 *              stored in the primary and secondary source images. For example, setting all strides for
 *              the primary source image to 0 will result in the primarySource image being treated as a
 *              single pixel. The only supported values are 0 or 1. The default value of these parameters
 *              is 1.
 *
 *              The first input to the backward filter is the gradient image from the previous layer
 *              (going backwards), so there are no broadcasting parameters for this input. For the
 *              backward filter, the broadcasting parameters for the second input must match the
 *              broadcasting parameters set for the same image in the forward filter.
 *
 *              In the backward pass, broadcasting results in a reduction operation (sum) across all of the
 *              applicable broadcasting dimensions (rows, columns, feature channels, or any combination
 *              thereof) to produce the destination image of the size that matches the primary/secondary
 *              input images used in the forward pass.
 *
 *              In the case of no broadcasting, the following arithmetic gradient operations are copy
 *              operations (that can be optimized away by the graph interface):
 *              - Add (primarySource, secondarySource)
 *              - Subtract (primarySource)
 *
 *              Similarly to the forward filter, this backward filter takes additional parameters:
 *              primaryScale, secondaryScale, and bias. The default value for primaryScale and secondaryScale
 *              is 1.0f. The default value for bias is 0.0f. This filter applies primaryScale to the primary
 *              source image, applies the secondaryScale to the secondary source image, where appropriate,
 *              and applies bias to the result, i.e.:
 *              result = ((primaryScale * x) [insert operation] (secondaryScale * y)) + bias.
 *
 *              The subtraction gradient filter for the secondary source image requires that the primaryScale
 *              property is set to -1.0f (for x - y, d/dy(x - y) = -1).
 *
 *              In the forward filter, there is support for clamping the result of the available operations,
 *              where result = clamp(result, minimumValue, maximumValue). The clamp backward operation is
 *              not supported in the arithmetic gradient filters. If you require this functionality, it can
 *              be implemented by performing a clamp backward operation before calling the arithmetic gradient
 *              filters. You would need to apply the following function on the incomping gradient input image:
 *              f(x) = ((minimumValue < x) && (x < maximumValue)) ? 1 : 0, where x is the original result
 *              (before clamping) of the forward arithmetic filter.
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 *
 *              You must use one of the sub-classes of MPSImageArithmeticGradient.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNArithmeticGradient : MPSCNNGradientKernel

@property (readwrite, nonatomic) float primaryScale;
@property (readwrite, nonatomic) float secondaryScale;
@property (readwrite, nonatomic) float bias; /* bias is ignored in the backward pass */

/*! @property   secondaryStrideInPixels
 *  @abstract   The secondarySource stride in the feature channel dimension. The only supported values are 0 or 1.
 *              The default value for each dimension is 1.
 */
@property(readwrite, nonatomic) NSUInteger      secondaryStrideInFeatureChannels;

/*! @property   minimumValue
 *  @abstract   minimumValue is to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of minimumValue is -FLT_MAX.
 */
@property (readwrite, nonatomic) float          minimumValue;

/*! @property   maximumValue
 *  @abstract   maximumValue is used to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of maximumValue is FLT_MAX.
 */
@property (readwrite, nonatomic) float          maximumValue;

/*! @property   isSecondarySourceFilter
 *  @abstract   The isSecondarySourceFilter property is used to indicate whether the arithmetic gradient
 *              filter is operating on the primary or secondary source image from the forward pass.
 */
@property (readonly, nonatomic) BOOL            isSecondarySourceFilter;

/*
 * You must use one of the sub-classes of MPSCNNArithmetic.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*
 * You must use one of the sub-classes of MPSCNNArithmetic.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
               isSecondarySourceFilter: (BOOL) isSecondarySourceFilter NS_UNAVAILABLE;

@end /* MPSCNNArithmetic */


#pragma mark -
#pragma mark MPSCNNAddGradient

/*!
 *  @class      MPSCNNAddGradient
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the addition gradient operator.
 *              This arithmetic gradient filter requires the following inputs: gradient image from
 *              the previous layer (going backwards) and either the primary or the secondary source
 *              image from the forward pass. You will need a separate filter for the primary and
 *              secondary source images.
 *
 *              Without broadcasting, the arithmetic add gradient operation is a copy operation on
 *              the input gradient image. It is the same operation for both the primary and secondary
 *              source images (for x + y, d/dx(x + y) = 1, d/dy(x + y) = 1). This copy operation can
 *              be optimized away by the graph interface.
 *
 *              Setting the broadcasting parameters results in a reduction operation (sum) across all
 *              of the applicable broadcasting dimensions (rows, columns, feature channels, or any
 *              combination thereof) to produce the destination image of the size that matches the
 *              primary/secondary input images used in the forward pass.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNAddGradient : MPSCNNArithmeticGradient

/*!
 *  @abstract  Initialize the addition gradient operator.
 *  @param     device                   The device the filter will run on.
 *  @param     isSecondarySourceFilter  A boolean indicating whether the arithmetic gradient
 *             filter is operating on the primary or secondary source image from the forward pass.
 *  @return    A valid MPSCNNAddGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
               isSecondarySourceFilter: (BOOL) isSecondarySourceFilter NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNAddGradient */
    
    
#pragma mark -
#pragma mark MPSCNNSubtractGradient

/*!
 *  @class      MPSCNNSubtractGradient
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the subtraction gradient operator.
 *              This arithmetic gradient filter requires the following inputs: gradient image from
 *              the previous layer (going backwards) and either the primary or the secondary source
 *              image from the forward pass. You will need a separate filter for the primary and
 *              secondary source images.
 *
 *              Without broadcasting, the arithmetic subtract gradient operation for the primary
 *              source image is a copy operation on the input gradient image (for x - y, d/dx(x - y) = 1).
 *              This copy operation can be optimized away by the graph interface.
 *
 *              For the secondary source image, the result is a negation of the gradient image from
 *              the previous layer (for x - y, d/dy(x - y) = -1).
 *
 *              Setting the broadcasting parameters results in a reduction operation (sum) across all
 *              of the applicable broadcasting dimensions (rows, columns, feature channels, or any
 *              combination thereof) to produce the destination image of the size that matches the
 *              primary/secondary input images used in the forward pass.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNSubtractGradient : MPSCNNArithmeticGradient

/*!
 *  @abstract  Initialize the subtraction gradient operator.
 *  @param     device                   The device the filter will run on.
 *  @param     isSecondarySourceFilter  A boolean indicating whether the arithmetic gradient
 *             filter is operating on the primary or secondary source image from the forward pass.
 *  @return    A valid MPSCNNSubtractGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
               isSecondarySourceFilter: (BOOL) isSecondarySourceFilter NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNSubtractGradient */


#pragma mark -
#pragma mark MPSCNNMultiplyGradient

/*!
 *  @class      MPSCNNMultiplyGradient
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the multiplication gradient operator.
 *              This arithmetic gradient filter requires the following inputs: gradient image from
 *              the previous layer (going backwards) and either the primary or the secondary source
 *              image from the forward pass. You will need a separate filter for the primary and
 *              secondary source images.
 *
 *              Without broadcasting, the arithmetic multiply gradient operation is an element-wise
 *              multiplication operation between the gradient image from the previous layer (going
 *              backwards) and:
 *              - The secondary source image from the forward pass for the primary source filter
 *                (for x * y, d/dx(x * y) = y).
 *              - The primary source image from the forward pass for the secondary source filter
 *                (for x * y, d/dy(x * y) = x).
 *
 *              Setting the broadcasting parameters results in a reduction operation (sum) across all
 *              of the applicable broadcasting dimensions (rows, columns, feature channels, or any
 *              combination thereof) to produce the destination image of the size that matches the
 *              primary/secondary input images used in the forward pass.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNMultiplyGradient : MPSCNNArithmeticGradient

/*!
 *  @abstract  Initialize the multiplication gradient operator.
 *  @param     device                   The device the filter will run on.
 *  @param     isSecondarySourceFilter  A boolean indicating whether the arithmetic gradient
 *             filter is operating on the primary or secondary source image from the forward pass.
 *  @return    A valid MPSCNNMultiplyGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
               isSecondarySourceFilter: (BOOL) isSecondarySourceFilter NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNMultiplyGradient */


#ifdef __cplusplus
}
#endif

#endif /* MPSCNNMath_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNGradientState.h
//
//  MPSNNGradientState.h
//  MPSNeuralNetwork
//
//  Created by Ian Ollmann on 11/9/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSNNGradientState_h
#define MPSNNGradientState_h

#include <MPSCore/MPSState.h>

@class MPSCNNKernel;
@class MPSCNNBinaryKernel;

/*! @class  A state created to record a MPSCNNKernel properties
 *          at the time an -encode call was made. The contents are opaque.
 *
 *          Gradient states must be created with [MPSCNNKernel resultStateForSourceImage:sourceStates:destinationImage:]
 *          or analogous interfaces.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNGradientState : MPSState

@end

typedef NSArray<MPSNNGradientState *>  MPSNNGradientStateBatch
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @class  A state created to record MPSCNNBinaryKernel properties
 *          at the time an -encode call was made. The contents are opaque.
 *
 *          Gradient states must be created with [MPSCNNBinaryKernel resultStateForPrimaryImage:secondaryImage:sourceStates:destinationImage:]
 *          or analogous interfaces.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNBinaryGradientState : MPSState

@end

typedef NSArray<MPSNNBinaryGradientState *>  MPSNNBinaryGradientStateBatch
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

#endif /* MPSNNGradientState_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSMatrixLayer.h
/*!
 *  @header MPSMatrixLayer.h
 *  @framework MPSNeuralNetwork
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 */

#ifndef MPSMatrixLayer_h
#define MPSMatrixLayer_h

#import <MPSNeuralNetwork/MPSMatrixNeuron.h>
#import <MPSNeuralNetwork/MPSMatrixFullyConnected.h>
#import <MPSNeuralNetwork/MPSMatrixSum.h>
#import <MPSNeuralNetwork/MPSMatrixBatchNormalization.h>

#endif /* MPSMatrixLayer_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNUpsampling.h
//
//  MPSCNNUpsampling.h
//  MPS
//
//  Created by Anna Tikhonova on 9/1/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNUpsampling_h
#define MPSCNNUpsampling_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif

#pragma mark -
#pragma mark MPSCNNUpsampling

/*!
 *  @class      MPSCNNUpsampling
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNUpsampling filter can be used to resample an existing MPSImage
 *              using a different sampling frequency for the x and y dimensions with the purpose of
 *              enlarging the size of an image.
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 *
 *              The scaleFactor must be an integer value >= 1. The default value is 1.
 *              If scaleFactor == 1, the filter acts as a copy kernel.
 *              
 *              Nearest and bilinear variants are supported.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNUpsampling : MPSCNNKernel

/*! @property   scaleFactorX
 *  @abstract   The upsampling scale factor for the x dimension. The default value is 1.
 */
@property(readonly, nonatomic) double      scaleFactorX;

/*! @property   scaleFactorY
 *  @abstract   The upsampling scale factor for the y dimension. The default value is 1.
 */
@property(readonly, nonatomic) double      scaleFactorY;

/*! @property   alignCorners
 *  @abstract   If YES, the centers of the 4 corner pixels of the input and output regions are aligned,
 *              preserving the values at the corner pixels.
 *              The default is NO.
 */
@property(readonly, nonatomic) BOOL      alignCorners;

/*
 * You must use initWithDevice:scaleFactorX:scaleFactorY instead.
 * You must use one of the sub-classes of MPSCNNUpsampling.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end /* MPSCNNUpsampling */


#pragma mark -
#pragma mark MPSCNNUpsamplingNearest

/*!
 *  @class      MPSCNNUpsamplingNearest
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the nearest spatial upsampling filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNUpsamplingNearest : MPSCNNUpsampling

/*!
 *  @abstract  Initialize the nearest spatial upsampling filter.
 *  @param     device                   The device the filter will run on.
 *  @param     integerScaleFactorX      The upsampling factor for the x dimension.
 *  @param     integerScaleFactorY      The upsampling factor for the y dimension.
 *  @return    A valid MPSCNNUpsamplingNearest object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY NS_DESIGNATED_INITIALIZER;

@end /* MPSCNNUpsamplingNearest */


#pragma mark -
#pragma mark MPSCNNUpsamplingBilinear

/*!
 *  @class      MPSCNNUpsamplingBilinear
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the bilinear spatial upsampling filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNUpsamplingBilinear : MPSCNNUpsampling

/*!
 *  @abstract  Initialize the bilinear spatial upsampling filter.
 *  @param     device                   The device the filter will run on.
 *  @param     integerScaleFactorX      The upsampling factor for the x dimension.
 *  @param     integerScaleFactorY      The upsampling factor for the y dimension.
 *  @return    A valid MPSCNNUpsamplingBilinear object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY;

/*!
 *  @abstract  Initialize the bilinear spatial upsampling filter.
 *  @param     device                   The device the filter will run on.
 *  @param     integerScaleFactorX      The upsampling factor for the x dimension.
 *  @param     integerScaleFactorY      The upsampling factor for the y dimension.
 *  @param     alignCorners             Specifier whether the centers of the 4 corner pixels of the input and output regions are aligned,
 *                                      preserving the values at the corner pixels.
 *  @return    A valid MPSCNNUpsamplingBilinear object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY
                          alignCorners: (BOOL) alignCorners NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNUpsamplingBilinear */


#pragma mark -
#pragma mark MPSCNNUpsamplingGradient
    
/*!
 *  @class      MPSCNNUpsamplingGradient
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNUpsamplingGradient filter is used for training. It is the backward
 *              filter for the MPSCNNUpsampling filter. It operates on the gradient input,
 *              specifically, it reduces the size of the gradient input in the x and y dimensions.
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 *
 *              The scaleFactor must be an integer value >= 1. The default value is 1.
 *              If scaleFactor == 1, the filter acts as a copy kernel.
 *
 *              Nearest and bilinear variants are supported.
 *
 *              For example, for the nearest variant with scaleFactorX = scaleFactorY = 2, the
 *              forward pass produced the following output:
 *
 *              Input:	    Output:
 *                          a a b b
 *              a b         a a b b
 *              c d         c c d d
 *                          c c d d
 *
 *              To upsample the image, the input data is replicated.
 *
 *              And, the backward pass for the above froward pass is computed in the following
 *              way:
 *
 *              Input:		    Output:
 *              a1 a2 b1 b2
 *              a2 a3 b3 b4	    x y
 *              c1 c2 d1 d2	    z w
 *              c3 c4 d3 d4
 *
 *              where	x = a1 + a2 + a3 + a4
 *                      y = b1 + b2 + b3 + b4
 *                      z = c1 + c2 + c3 + c4
 *                      w = d1 + d2 + d3 + d4
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNUpsamplingGradient : MPSCNNGradientKernel

/*! @property   scaleFactorX
 *  @abstract   The downsampling scale factor for the x dimension. The default value is 1.
 */
@property(readonly, nonatomic) double      scaleFactorX;

/*! @property   scaleFactorY
 *  @abstract   The downsampling scale factor for the y dimension. The default value is 1.
 */
@property(readonly, nonatomic) double      scaleFactorY;

/*
 * You must use initWithDevice:scaleFactorX:scaleFactorY instead.
 * You must use one of the sub-classes of MPSCNNUpsamplingGradient.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


@end /* MPSCNNUpsamplingGradient */
    
    
#pragma mark -
#pragma mark MPSCNNUpsamplingNearestGradient
    
/*!
 *  @class      MPSCNNUpsamplingNearestGradient
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the nearest spatial downsampling filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNUpsamplingNearestGradient : MPSCNNUpsamplingGradient

/*!
 *  @abstract  Initialize the nearest spatial upsampling filter.
 *  @param     device                   The device the filter will run on.
 *  @param     integerScaleFactorX      The downsampling factor for the x dimension.
 *  @param     integerScaleFactorY      The downsampling factor for the y dimension.
 *  @return    A valid MPSCNNUpsamplingNearestGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY NS_DESIGNATED_INITIALIZER;

@end /* MPSCNNUpsamplingNearestGradient */
    
    
#pragma mark -
#pragma mark MPSCNNUpsamplingBilinearGradient
    
/*!
 *  @class      MPSCNNUpsamplingBilinearGradient
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the bilinear spatial downsampling filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNUpsamplingBilinearGradient : MPSCNNUpsamplingGradient

/*!
 *  @abstract  Initialize the bilinear spatial downsampling filter.
 *  @param     device                   The device the filter will run on.
 *  @param     integerScaleFactorX      The downsampling factor for the x dimension.
 *  @param     integerScaleFactorY      The downsampling factor for the y dimension.
 *  @return    A valid MPSCNNUpsamplingBilinearGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                   integerScaleFactorX: (NSUInteger) integerScaleFactorX
                   integerScaleFactorY: (NSUInteger) integerScaleFactorY NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNUpsamplingBilinearGradient */


#ifdef __cplusplus
}
#endif

#endif /* MPSCNNUpsampling_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNConvolution.h
//
//  MPSCNNConvolution.h
//  MPS
//
//  Created by Ian Ollmann on 8/21/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNConvolution_h
#define MPSCNNConvolution_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>
#include <MPSNeuralNetwork/MPSCNNNormalization.h>
#include <MPSNeuralNetwork/MPSCNNNeuronType.h>
#include <MPSNeuralNetwork/MPSCNNNeuron.h>
#include <MPSCore/MPSState.h>
#include <MPSNeuralNetwork/MPSNNGradientState.h>
#include <simd/simd.h>

#ifdef __cplusplus
extern "C" {
#endif


#pragma mark -
#pragma mark MPSCNNConvolutionDescriptor

@class MPSNNFilterNode;

/*!
 *  @class      MPSCNNConvolutionDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNConvolutionDescriptor specifies a convolution descriptor
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNConvolutionDescriptor : NSObject <NSSecureCoding, NSCopying>

/*! @property   kernelWidth
 *  @abstract   The width of the filter window.  The default value is 3.
 *              Any positive non-zero value is valid, including even values.
 *              The position of the left edge of the filter window is given
 *              by offset.x - (kernelWidth>>1)
 */
@property(readwrite, nonatomic) NSUInteger       kernelWidth;

/*! @property   kernelHeight
 *  @abstract   The height of the filter window.  The default value is 3.
 *              Any positive non-zero value is valid, including even values.
 *              The position of the top edge of the filter window is given
 *              by offset.y - (kernelHeight>>1)
 */
@property(readwrite, nonatomic) NSUInteger       kernelHeight;

/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image.
 */
@property(readwrite, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the output image.
 */
@property(readwrite, nonatomic) NSUInteger       outputFeatureChannels;

/*! @property   strideInPixelsX
 *  @abstract   The output stride (downsampling factor) in the x dimension. The default value is 1.
 */
@property(readwrite, nonatomic) NSUInteger      strideInPixelsX;

/*! @property   strideInPixelsY
 *  @abstract   The output stride (downsampling factor) in the y dimension. The default value is 1.
 */
@property(readwrite, nonatomic) NSUInteger      strideInPixelsY;

/*! @property   groups
 *  @abstract   Number of groups input and output channels are divided into. The default value is 1.
 *              Groups lets you reduce the parameterization. If groups is set to n, input is divided into n
 *              groups with inputFeatureChannels/n channels in each group. Similarly output is divided into
 *              n groups with outputFeatureChannels/n channels in each group. ith group in input is only
 *              connected to ith group in output so number of weights (parameters) needed is reduced by factor
 *              of n. Both inputFeatureChannels and outputFeatureChannels must be divisible by n and number of
 *              channels in each group must be multiple of 4.
 */
@property(readwrite, nonatomic) NSUInteger      groups;

/*! @property      dilationRateX
 *  @discussion    dilationRateX property can be used to implement dilated convolution as described in
 *                          https://arxiv.org/pdf/1511.07122v3.pdf
 *                 to aggregate global information in dense prediction problems.
 *                 Default value is 1. When set to value > 1, original kernel width, kW is dilated to
 *
 *                       kW_Dilated = (kW-1)*dilationRateX + 1
 *
 *                 by inserting d-1 zeros between consecutive entries in each row of the original kernel. 
 *                 The kernel is centered based on kW_Dilated.
 */
@property(readwrite, nonatomic) NSUInteger      dilationRateX;

/*! @property      dilationRateY
 *  @discussion    dilationRateY property can be used to implement dilated convolution as described in
 *                          https://arxiv.org/pdf/1511.07122v3.pdf
 *                 to aggregate global information in dense prediction problems.
 *                 Default value is 1. When set to value > 1, original kernel height, kH is dilated to
 *
 *                       kH_Dilated = (kH-1)*dilationRateY + 1
 *
 *                 by inserting d-1 rows of zeros between consecutive row of the original kernel.
 *                 The kernel is centered based on kH_Dilated.
 */
@property(readwrite, nonatomic) NSUInteger      dilationRateY;

/*!
 *  @property   fusedNeuronDescriptor
 *  @discussion This mathod can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A and B that are shared across all output channels.
 *              Neuron activation fucntion is applied to output of convolution. This is a per-pixel
 *              operation that is fused with convolution kernel itself for best performance.
 *              Note that this method can only be used to fuse neuron of kind for which parameters
 *              A and B are shared across all channels of convoution output. It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. For those kind of neuron activation functions,
 *              use appropriate setter functions. Default is descriptor with neuronType MPSCNNNeuronTypeNone.
 *
 *              Note: in certain cases the neuron descriptor will be cached by the MPSNNGraph or the
 *              MPSCNNConvolution. If the neuron type changes after either is made, behavior is undefined.
 */
@property(readwrite, nonatomic, retain) MPSNNNeuronDescriptor* __nonnull fusedNeuronDescriptor
                                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   neuron
 *  @abstract   MPSCNNNeuron filter to be applied as part of convolution. This is applied after BatchNormalization in the end.
 *              Default is nil.
 *              This is deprecated. You dont need to create MPSCNNNeuron object to fuse with convolution. Use neuron properties
 *              in this descriptor.
 */
@property(readwrite, nonatomic, retain) const MPSCNNNeuron * __nullable  neuron
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "A MPSCNNNeuron:MPSKernel is much too heavy an object to\n"
                                                "represent what is a type code and two floats. It is deprecated.\n"
                                                "Please set fusedNeuronDescriptor property instead.",
                                                ios(10.0, 11.0), tvos(10.0, 11.0));

/*! @abstract <NSSecureCoding> support */
@property (class, readonly) BOOL supportsSecureCoding
            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
/*! @abstract <NSSecureCoding> support */
- (void)encodeWithCoder:(NSCoder *__nonnull)aCoder
            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
/*! @abstract <NSSecureCoding> support */
- (nullable instancetype)initWithCoder:(NSCoder *__nonnull)aDecoder
            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)) NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   This method is deprecated. Please use neuronType, neuronParameterA and neuronParameterB properites to fuse
 *              neuron with convolution.
 *  @param      kernelWidth             The width of the filter window.  Must be > 0. Large values will take a long time.
 *  @param      kernelHeight            The height of the filter window.   Must be > 0. Large values will take a long time.
 *  @param      inputFeatureChannels    The number of feature channels in the input image. Must be >= 1.
 *  @param      outputFeatureChannels   The number of feature channels in the output image. Must be >= 1.
 *  @param      neuronFilter            An optional neuron filter that can be applied to the output of convolution.
 *  @return     A valid MPSCNNConvolutionDescriptor object or nil, if failure.
 */
+(nonnull instancetype) cnnConvolutionDescriptorWithKernelWidth: (NSUInteger) kernelWidth
                                                   kernelHeight: (NSUInteger) kernelHeight
                                           inputFeatureChannels: (NSUInteger) inputFeatureChannels
                                          outputFeatureChannels: (NSUInteger) outputFeatureChannels
                                                   neuronFilter: (const MPSCNNNeuron * __nullable) neuronFilter
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use neuronType, neuronParameterA and neuronParameterB properties instead.",
                                            ios(10.0, 11.0), tvos(10.0, 11.0));

/*!
 *  @abstract   Creates a convolution descriptor.
 *  @param      kernelWidth             The width of the filter window.  Must be > 0. Large values will take a long time.
 *  @param      kernelHeight            The height of the filter window.   Must be > 0. Large values will take a long time.
 *  @param      inputFeatureChannels    The number of feature channels in the input image. Must be >= 1.
 *  @param      outputFeatureChannels   The number of feature channels in the output image. Must be >= 1.
 *  @return     A valid MPSCNNConvolutionDescriptor object or nil, if failure.
 */
+(nonnull instancetype) cnnConvolutionDescriptorWithKernelWidth: (NSUInteger) kernelWidth
                                                   kernelHeight: (NSUInteger) kernelHeight
                                           inputFeatureChannels: (NSUInteger) inputFeatureChannels
                                          outputFeatureChannels: (NSUInteger) outputFeatureChannels
    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );



/*!
 *  @abstract   Adds batch normalization for inference, it copies all the float arrays provided, expecting 
 *              outputFeatureChannels elements in each.
 *
 *  @discussion This method will be used to pass in batch normalization parameters to the convolution during the
 *              init call. For inference we modify weights and bias going in convolution or Fully Connected layer to combine
 *              and optimize the layers.
 *
 *
 *              w: weights for a corresponding output feature channel
 *              b: bias for a corresponding output feature channel
 *              W: batch normalized weights for a corresponding output feature channel
 *              B: batch normalized bias for a corresponding output feature channel
 *
 *
 *              I = gamma / sqrt(variance + epsilon), J = beta - ( I * mean )
 *
 *              W = w * I
 *              B = b * I + J
 *
 *              Every convolution has (OutputFeatureChannel * kernelWidth * kernelHeight * InputFeatureChannel) weights
 *
 *              I, J are calculated, for every output feature channel separately to get the corresponding weights and bias
 *              Thus, I, J are calculated and then used for every (kernelWidth * kernelHeight * InputFeatureChannel)
 *              weights, and this is done OutputFeatureChannel number of times for each output channel.
 *
 *              thus, internally, batch normalized weights are computed as:
 *
 *              W[no][i][j][ni] = w[no][i][j][ni] * I[no]
 *
 *              no: index into outputFeatureChannel
 *              i : index into kernel Height
 *              j : index into kernel Width
 *              ni: index into inputFeatureChannel
 *
 *              One usually doesn't see a bias term and batch normalization together as batch normalization potentially cancels
 *              out the bias term after training, but in MPS if the user provides it, batch normalization will use the above 
 *              formula to incorporate it, if user does not have bias terms then put a float array of zeroes in the convolution
 *              init for bias terms of each output feature channel.
 *
 *
 *              this comes from:
 *              https://arxiv.org/pdf/1502.03167v3.pdf
 *
 *              Note: in certain cases the batch normalization parameters will be cached by the MPSNNGraph
 *              or the MPSCNNConvolution. If the batch normalization parameters change after either is made,
 *              behavior is undefined.
 *
 *  @param      mean                        Pointer to an array of floats of mean for each output feature channel
 *  @param      variance                    Pointer to an array of floats of variance for each output feature channel
 *  @param      gamma                       Pointer to an array of floats of gamma for each output feature channel
 *  @param      beta                        Pointer to an array of floats of beta for each output feature channel
 *  @param      epsilon                     A small float value used to have numerical stability in the code
 */
-(void) setBatchNormalizationParametersForInferenceWithMean: (const float * __nullable) mean
                                                   variance: (const float * __nullable) variance
                                                      gamma: (const float * __nullable) gamma
                                                       beta: (const float * __nullable) beta
                                                    epsilon: (const float) epsilon
                                                                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Adds a neuron activation function to convolution descriptor.
 *
 *  @discussion This mathod can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A and B that are shared across all output channels.
 *              Neuron activation fucntion is applied to output of convolution. This is a per-pixel
 *              operation that is fused with convolution kernel itself for best performance.
 *              Note that this method can only be used to fuse neuron of kind for which parameters
 *              A and B are shared across all channels of convoution output. It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. For those kind of neuron activation functions,
 *              use appropriate setter functions.
 *
 *              Note: in certain cases, the neuron descriptor will be cached by the MPSNNGraph or the
 *              MPSCNNConvolution. If the neuron type changes after either is made, behavior is undefined.
 *
 *  @param      neuronType      type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all channels of convolution output.
 *  @param      parameterB      parameterB of neuron activation that is shared across all channels of convolution output.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
                                        MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "set fusedNeuronDescriptor property instead",
                                              macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0, 11.3) );

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB method
 */
-(MPSCNNNeuronType) neuronType
                    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "use fusedNeuronDescriptor property instead",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0, 11.3) );

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB method
 */
-(float) neuronParameterA
                    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "use fusedNeuronDescriptor property instead",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0, 11.3) );

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB method
 */
-(float) neuronParameterB
                    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "use fusedNeuronDescriptor property instead",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0, 11.3) );

/*!
 *  @abstract   Add per-channel neuron parameters A for PReLu neuron activation functions.
 *
 *  @discussion This method sets the neuron to PReLU, zeros parameters A and B and sets the per-channel
 *              neuron parameters A to an array containing a unique value of A for each output feature
 *              channel.
 *
 *              If the neuron function is f(v,a,b), it will apply
 *
 *                     OutputImage(x,y,i) = f( ConvolutionResult(x,y,i), A[i], B[i] ) where i in [0,outputFeatureChannels-1]
 *
 *              See https://arxiv.org/pdf/1502.01852.pdf for details.
 *
 *              All other neuron types, where parameter A
 *              and parameter B are shared across channels must be set using
 *              -setNeuronOfType:parameterA:parameterB:
 *
 *              If batch normalization parameters are set, batch normalization will occur before
 *              neuron application i.e. output of convolution is first batch normalized followed
 *              by neuron activation. This function automatically sets neuronType to MPSCNNNeuronTypePReLU.
 *
 *              Note: in certain cases the neuron descriptor will be cached by the MPSNNGraph or the
 *              MPSCNNConvolution. If the neuron type changes after either is made, behavior is undefined.
 *
 *  @param      A       An array containing per-channel float values for neuron parameter A.
 *                      Number of entries must be equal to outputFeatureChannels.
 */
-(void) setNeuronToPReLUWithParametersA: (NSData* __nonnull) A
                            MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "use fusedNeuronDescriptor property instead",
                                      macos(10.13, 10.13.4), ios(11.0, 11.3), tvos(11.0, 11.3) );

@end    /* MPSCNNConvolutionDescriptor */

/*! @abstract         MPSCNNSubPixelConvolutionDescriptor can be used to create MPSCNNConvolution object that does sub pixel upsamling
 *                    and reshaping opeartion as described in
 *                        http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf
 *  @discussion
 *                   Conceptually MPSCNNConvolution with subPixelScaleFactor > 1 can be thought of as filter performing regular CNN convolution producing N output feature channels at each pixel of
 *                   an intermediate MPSImage followed by a kernel that rearranges/reshapes these N channels at each pixel of intermediate MPSImage into a pixel block of
 *                   size subPixelScaleFactor x subPixelScaleFactor with N/(subPixelScaleFactor * subPixelScaleFactor) featureChannels at each pixel of this pixel block. Thus each pixel in intermedaite
 *                   MPSImage with N channels map to subPixelScaleFactor x subPixelScaleFactor pixel block in final destination MPSImage with N/(subPixelScaleFactor * subPixelScaleFactor) featureChannels.
 *                   MPSCNNConvolution with subPixelScaleFactor > 1 fuses the convolution and reshaping operation into single compute kernel thus not only saving DRAM roundtrip but also memory
 *                   needed for intermediate MPSImage had these operation done separately.
 *                   Let N be the value of outputFeatureChannels property and let r = subPixelScaleFactor.
 *                   Conceptually Convolution will produce intermedaite image Io of dimensions (treated as 3D tensor) width x height x N where
 *                              width = (clipRect.size.width + r - 1) / r
 *                              height = (clipRect.size.height + r -1) / r
 *                   Reshaping happens as follows
 *                   @code
 *                   Destination[clipRect.origin.x+x][clipRect.origin.y+y][c] = Io[ floor(x/r) ][ floor(y/r) ][ (N/r^2) * ( r * mod(y,r) + mod(x,r) ) + c ]
 *                   where x in [0,clipRect.size.width-1], y in [0,clipRect.size.height-1], c in [0,N/r^2 - 1]
 *                   @endcode
 *
 *                   The following conditions must be met:
 *                   1) N (outputFeatureChannels) must be multiple of r^2 (subPixelScaleFactor * subPixelScaleFactor).
 *                   2) The destination MPSImage to encode call must have at least N/r^2 + destinationFeatureChannelOffset channels.
 *                   3) Number of feature channels in reshaped output image (N/r^2) can be any value when groups = 1 but must be multiple of 4 when groups > 1.
*/
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNSubPixelConvolutionDescriptor : MPSCNNConvolutionDescriptor

/*! @property      subPixelScaleFactor
 *  @discussion    Upsampling scale factor. Each pixel in input is upsampled into a subPixelScaleFactor x subPixelScaleFactor pixel block by rearranging
 *                 the outputFeatureChannels as described above. Default value is 1.
 */
@property(readwrite, nonatomic) NSUInteger      subPixelScaleFactor;

@end
    
/*! @abstract         MPSCNNDepthWiseConvolutionDescriptor can be used to create MPSCNNConvolution object that does depthwise convolution
 *  @discussion
 *                    Depthwise convolution applies different filter to each input feature channel i.e. no cross channel mixing.
 *                    Number of outputFeatureChannels can be greater than number of inputFeatureChannels, in which case convolution
 *                    expects channelMultipler = outputFeactureChannels/inputFeatureChannels number of filters for each input channel.
 *                    This means channelMultipler filters are applied to each input feature channel producing channelMultipler output feature channels.
 *                    All channelMultipler output feature channels produced by single input feature channel are stored togather in output image i.e.
 *                              output[x,y,k*channelMultiplier + q] = input[x,y,k] * filter[k,q]
 *                    where * here denotes convolution.
 *                    group must be 1.
 *                    Weights array returned by MPSCNNConvolutionDataProvier is interpreted as
 *                              Weights [inputFeatureChannels] [channelMultiplier] [kH] [kW]
 *                            = Weights [ inputFeatureChannels * channelMultiplier ] [kH] [kW]
 *                            = Weights [ outputFeatureChannels ] [kH] [kW]
 *
 *                    Currently only channel multipler of 1 is supported i.e. inputFeatureChannels == outputFeatureChannels
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface MPSCNNDepthWiseConvolutionDescriptor : MPSCNNConvolutionDescriptor

/*! @property      channelMultiplier
 *  @discussion    Ratio of outputFeactureChannel to inputFeatureChannels for depthwise convolution i.e. how many output feature channels are
 *                 produced by each input channel.
 */
@property(readonly, nonatomic) NSUInteger      channelMultiplier;

@end
    
/*! @enum        MPSCNNWeightsQuantizationType
 *  @discussion A value to specify a type of quantization used to generate quantized UInt weights.
 *              Same scheme will be used to dequantize weights to fp16 for CNN convolution.
 *
 *  @constant   MPSCNNWeightsQuantizationTypeNone       No quantization. Weights are fp16 or fp32
 *  @constant   MPSCNNWeightsQuantizationTypeLinear     Linear quantization of range in which weights lie in 2^b bins where b
                                                        is bit depth of quantized coefficients.
 *  @constant   MPSCNNWeightsQuantizationTypeLookupTable A lookup table of 2^b entries is used to map b-bit quantized weight to floating point value.
 */
#if defined(DOXYGEN)
    typedef enum MPSCNNWeightsQuantizationType
#else
    typedef NS_ENUM(uint32_t, MPSCNNWeightsQuantizationType)
#endif
    {
        MPSCNNWeightsQuantizationTypeNone           MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(none) = 0,
        MPSCNNWeightsQuantizationTypeLinear         MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(none) = 1,
        MPSCNNWeightsQuantizationTypeLookupTable    MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(none) = 2,
    }
#if defined(DOXYGEN)
    MPSCNNWeightsQuantizationType
#endif
    ;

    
@class MPSCNNConvolution;
@protocol MPSCNNConvolutionDataSource;
/*!
 *  @class      MPSCNNConvolutionGradientState
 *  @discussion The MPSCNNConvolutionGradientState is returned by resultStateForSourceImage:sourceStates method on MPSCNNConvolution object.
 *              Note that resultStateForSourceImage:sourceStates:destinationImage creates the object on autoreleasepool.
 *              It will be consumed by MPSCNNConvolutionGradient. This used by MPSCNNConvolutionTranspose encode call
 *              that returns MPSImage on left hand side to correctly size the destination.
 *              Note that state objects are not usable across batches i.e. when batch is done you should nuke the state object and create
 *              new one for next batch.
 *
 *              This state exposes the gradient with respect to weights and biases, as computed by the MPSCNNConvolutionGradient kernel, as a metal buffer to be used
 *              during weights and biases update. The standard weights and biases update formula is:
 *
 *                        weights(t+1) = f(weights(t), gradientForWeights(t)) and
 *                        biases(t+1) = f(biases(t), gradientForBiases(t)),
 *
 *              where the weights(t)/biases(t) are the wegihts and the biases at step t that are provided by data source provider used to create MPSCNNConvolution and
 *              MPSCNNConvoltuionGradient objects. There are multiple ways user can update weights and biases as described below:
 *
 *              1) For check pointing, i.e. updating weights/biases and storing:
 *                   once the command buffer on which MPSCNNConvolutionGradient is enqueued is done (e.g. in command
 *                 buffer completion callback), the application can simply use
 *                                    float* delta_w = (float*)((char*)[gradientForWeights contents]);
 *                                    float* delta_b = (float*)((char*)[gradientForBiases contents]);
 *                  to update the weights and biases in the data provider directly.
 *                  The application can instead provide a metal kernel that reads from gradientForWeights and gradientForBiases buffer and the buffer created using data provided by the data source
 *                  to do any kind of update it will like to do, then read back the updated weights/biases and store to the data source. Note that lifetime of the
 *                  gradientForWeights and gradientForBiases buffer is the same as the MPSCNNConvolutionGradientState. So it's the applications's responsibility to make sure the buffer is alive
 *                  (retained) when the update kernel is running if the command buffer doesn't retain the buffer. Also, in order to gaurantee that the buffer is correctly
 *                  synchronized for CPU side access, it is the application's responsibility to call
 *                                     [gradientState synchronizeOnCommandBuffer:]
 *                  before accessing data from the buffer.
 *
 *              2) For a CPU side update, once the weights and biases in the data source provider are updated as above, the original MPSCNNConvolution and
 *                 MPSCNNConvolutionGradient objects need to be updated with the new weigths and biases by calling the
 *                       -(void) reloadWeightsAndBiasesFromDataSource
 *                 method. Again application needs to call [gradientState synchronizeOnCommandBuffer:] before touching data on CPU side.
 *
 *              3) The above CPU side update requires command buffer to be done. If the application doesn't want to update its data source provider object and would prefer to directly
 *                 enqueue an update of the internal MPSCNNConvolution and MPSCNNConvolutionGradient weights/biases buffers on the GPU without CPU side involvement, it needs to do
 *                 following:
 *                     i) get gradientForWeights and gradientForBiases buffers from this gradient state object and set it as source of update kernel
 *                    ii) create a temporary buffer, dest, of same size and set it as destination of update kernel
 *                   iii) enqueue update kernel on command buffer
 *                    iv) call reloadWeightsAndBiasesWithCommandBuffer:dest:weightsOffset:biasesOffset on MPSCNNConvolution and MPSCNNConvolutionGradient objects. This
 *                        will reload the weights from application's update kernel in dest on GPU without CPU side involvement.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNConvolutionGradientState : MPSNNGradientState <MPSImageSizeEncodingState>

/*! @property   gradientForWeights
 *  @abstract   A buffer that contains the loss function gradients with respect to weights.
 *              Each value in the buffer is a float. The layout of the gradients with respect to the weights is the same as
 *              the weights layout provided by data source i.e. it can be interpreted as 4D array
 *
 *                   gradientForWeights[outputFeatureChannels][kernelHeight][kernelWidth][inputFeatureChannels/groups]
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> gradientForWeights;

/*! @property   gradientForBiases
 *  @abstract   A buffer that contains the loss function gradients with respect to biases.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> gradientForBiases;

/*! @property   convolution
 *  @abstract   The convolution filter that produced the state. */
@property (readonly, nonatomic, retain, nonnull) MPSCNNConvolution * convolution;

@end
    
typedef NSArray<MPSCNNConvolutionGradientState*>  MPSCNNConvolutionGradientStateBatch
        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*!
 *  @class      MPSCNNConvolutionWeightsAndBiasesState
 *  @discussion The MPSCNNConvolutionWeightsAndBiasesState is returned by exportWeightsAndBiasesWithCommandBuffer: method on MPSCNNConvolution object.
 *              This is mainly used for GPU side weights/biases update process.
 *              During training, application can keep a copy of weights, velocity, momentum MTLBuffers in its data source, update the weights (in-place or out of place)
 *              with gradients obtained from MPSCNNConvolutionGradientState and call [MPSCNNConvolution reloadWeightsAndBiasesWithCommandBuffer] with resulting updated
 *              MTLBuffer. If application does not want to keep a copy of weights/biases, it can call [MPSCNNConvolution exportWeightsAndBiasesWithCommandBuffer:] to get
 *              the current weights from convolution itself, do the updated and call reloadWithCommandBuffer.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNConvolutionWeightsAndBiasesState : MPSState

/*! @property   weights
 *  @abstract   A buffer that contains the weights.
 *              Each value in the buffer is a float. The layout of the weights with respect to the weights is the same as
 *              the weights layout provided by data source i.e. it can be interpreted as 4D array
 *
 *                   weights[outputFeatureChannels][kernelHeight][kernelWidth][inputFeatureChannels/groups]
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> weights;

/*! @property   biases
 *  @abstract   A buffer that contains the biases. Each value is float and there are ouputFeatureChannels values.
 */
@property (readonly, nonatomic) __nullable id<MTLBuffer> biases;

- (nonnull instancetype) initWithWeights: (__nonnull id<MTLBuffer>) weights
                                  biases: (__nullable id<MTLBuffer>) biases;

- (nonnull instancetype) initWithDevice: (__nonnull id<MTLDevice>) device
               cnnConvolutionDescriptor: (MPSCNNConvolutionDescriptor* __nonnull) descriptor;

+ (nonnull instancetype) temporaryCNNConvolutionWeightsAndBiasesStateWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                                              cnnConvolutionDescriptor: (MPSCNNConvolutionDescriptor* __nonnull) descriptor;

@end
    
    
/*! @protocol   MPSCNNConvolutionDataSource
 *  @abstract   Provides convolution filter weights and bias terms
 *  @discussion The MPSCNNConvolutionDataSource protocol declares the methods that an
 *              instance of MPSCNNConvolution uses to obtain the weights and bias terms
 *              for the CNN convolution filter.
 *              
 *              Why? CNN weights can be large. If multiple copies of all the weights
 *              for all the convolutions are available unpacked in memory at the same
 *              time, some devices can run out of memory. The MPSCNNConvolutionDataSource
 *              is used to encapsulate a reference to the weights such as a file path,
 *              so that unpacking can be deferred until needed, then purged soon thereafter
 *              so that not all of the data must be in memory at the same time.
 *              MPS does not provide a class that conforms to this protocol. It is up to
 *              the developer to craft his own to encapsulate his data.
 *              
 *              Batch normalization and the neuron activation function are handled using the
 *              -descriptor method.
 *
 *              Thread safety: The MPSCNNConvolutionDataSource object can be called by
 *              threads that are not the main thread. If you will be creating multiple 
 *              MPSNNGraph objects concurrently in multiple threads and these share 
 *              MPSCNNConvolutionDataSources, then the data source objects may be called 
 *              reentrantly.
 */
@protocol MPSCNNConvolutionDataSource <NSCopying, NSObject>

@required
    
    /*! @abstract   Alerts MPS what sort of weights are provided by the object
     *  @discussion For MPSCNNConvolution, MPSDataTypeUInt8, MPSDataTypeFloat16
     *              and MPSDataTypeFloat32 are supported for normal convolutions
     *              using MPSCNNConvolution. MPSCNNBinaryConvolution assumes weights to be
     *              of type MPSDataTypeUInt32 always.
     */
    -(MPSDataType)  dataType;
    
    /*! @abstract   Return a MPSCNNConvolutionDescriptor as needed 
     *  @discussion MPS will not modify this object other than perhaps to retain it.
     *              User should set the appropriate neuron in the creation of convolution descriptor
     *              and for batch normalization use:
     *  @code       
     *              -setBatchNormalizationParametersForInferenceWithMean:variance:gamma:beta:epsilon:
     *  @endcode
     *
     *  @return     A MPSCNNConvolutionDescriptor that describes the kernel housed by this object.
     */
    -(MPSCNNConvolutionDescriptor * __nonnull) descriptor;
    
    /*! @abstract   Returns a pointer to the weights for the convolution.
     *  @discussion The type of each entry in array is given by -dataType. The number
     *              of entries is equal to:
     *              @code
     *                  inputFeatureChannels * outputFeatureChannels * kernelHeight * kernelWidth
     *              @endcode
     *              The layout of filter weight is as a 4D tensor (array)
     *              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]
     *
     *              Frequently, this function is a single line of code to return
     *              a pointer to memory allocated in -load.
     *
     *              Batch normalization parameters are set using -descriptor.
     *
     *              Note: For binary-convolutions the layout of the weights are:
     *                  weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ floor((inputChannels/groups)+31) / 32 ]
     *              with each 32 sub input feature channel index specified in machine byte order, so that for example
     *              the 13th feature channel bit can be extracted using bitmask = (1U << 13).
     */
    -(void * __nonnull) weights;
 
    /*! @abstract   Returns a pointer to the bias terms for the convolution.
     *  @discussion Each entry in the array is a single precision IEEE-754 float
     *              and represents one bias. The number of entries is equal
     *              to outputFeatureChannels.
     *
     *              Frequently, this function is a single line of code to return
     *              a pointer to memory allocated in -load. It may also just
     *              return nil.
     *
     *              Note: bias terms are always float, even when the weights are not.
     */
    -(float * __nullable) biasTerms;

    
    /*! @abstract   Alerts the data source that the data will be needed soon
     *  @discussion Each load alert will be balanced by a purge later, when MPS
     *              no longer needs the data from this object.
     *              Load will always be called atleast once after initial construction
     *              or each purge of the object before anything else is called.
     *              Note: load may be called to merely inspect the descriptor.
     *              In some circumstances, it may be worthwhile to postpone
     *              weight and bias construction until they are actually needed
     *              to save touching memory and keep the working set small.
     *              The load function is intended to be an opportunity to open
     *              files or mark memory no longer purgeable. 
     *  @return     Returns YES on success.  If NO is returned, expect MPS
     *              object construction to fail.
     */
    -(BOOL) load;
    
    /*! @abstract   Alerts the data source that the data is no longer needed
     *  @discussion Each load alert will be balanced by a purge later, when MPS
     *              no longer needs the data from this object.
     */
    -(void) purge;
    
    
    /*! @abstract   A label that is transferred to the convolution at init time
     *  @discussion Overridden by a MPSCNNConvolutionNode.label if it is non-nil.
     */
    -(NSString*__nullable) label;
    
    /* MPSDataTypeUInt8 weight containers must implement one of the following optional methods.
       If quantizationMode method returns linear, rangesForUInt8Kernel must be implemented.
       If quantizationMode method returns lookupTable, lookupTableForUInt8Kernel should be returned.
     */
@optional
    /*! @abstract       A list of per-output channel limits that describe the 8-bit range
     *  @discussion     This returns a pointer to an array of vector_float2[outputChannelCount] 
     *                  values. The first value in the vector is the minimum value in the range.
     *                  The second value in the vector is the maximum value in the range. 
     *
     *                  The 8-bit weight value is interpreted as:
     *                  @code
     *                      float unorm8_weight = uint8_weight / 255.0f;    // unorm8_weight has range [0,1.0]
     *                      float max = range[outputChannel].y;
     *                      float min = range[outputChannel].x;
     *                      float weight = unorm8_weight * (max - min) + min
     *                  @endcode
     */
    -(vector_float2 * __nonnull) rangesForUInt8Kernel;
    
    
    /*! @abstract       A pointer to a 256 entry lookup table containing the values to use for the weight range [0,255]
     */
    -(float * __nonnull) lookupTableForUInt8Kernel;
    
    /*! @abstract       Quantizaiton type of weights. If it returns MPSCNNWeightsQuantizationTypeLookupTable,
     *                  lookupTableForUInt8Kernel method must be implmented. if it returns MPSCNNWeightsQuantizationTypeLookupLinear,
     *                  rangesForUInt8Kernel method must be implemented.
     */
    -(MPSCNNWeightsQuantizationType) weightsQuantizationType;
    
    /*! @abstract   Callback for the MPSNNGraph to update the convolution weights on GPU.
     *  @discussion It is the resposibility of this method to decrement the read count of both the gradientState
     *              and the sourceState before returning.  BUG: prior to macOS 10.14, ios/tvos 12.0, the MPSNNGraph
     *              incorrectly decrements the readcount of the gradientState after this method is called.
     *
     *  @param      commandBuffer   The command buffer on which to do the update.
     *                              MPSCNNConvolutionGradientNode.MPSNNTrainingStyle controls where you want your update
     *                              to happen. Provide implementation of this function for GPU side update.
     *  @param      gradientState   A state object produced by the MPSCNNConvolution and updated by MPSCNNConvolutionGradient
     *                              containing weight gradients.
     *  @param      sourceState     A state object containing the convolution weights
     *  @return     If NULL, no update occurs. If nonnull, the result will be used to update the
     *              weights in the MPSNNGraph  */
    -(MPSCNNConvolutionWeightsAndBiasesState* __nullable) updateWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                                                    gradientState: (MPSCNNConvolutionGradientState* __nonnull) gradientState
                                                                      sourceState: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) sourceState
                                                                                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));
    
    /*! @abstract   Callback for the MPSNNGraph to update the convolution weights on CPU.
     *                               MPSCNNConvolutionGradientNode.MPSNNTrainingStyle controls where you want your update
     *                              to happen. Provide implementation of this function for CPU side update.
     *  @param      gradientState   A state object produced by the MPSCNNConvolution and updated by MPSCNNConvolutionGradient
     *                              containing weight gradients. MPSNNGraph is responsible for calling [gradientState synchronizeOnCommandBuffer:]
     *                              so that application get correct gradients for CPU side update.
     *  @param      sourceState     A state object containing the convolution weights used. MPSCNNConvolution and MPSCNNConvolutionGradient reloadWeightsWithDataSource
     *                              will be called right after this method is called. Note that the weights returned here may not match the weights
     *                              in your data source due to conversion loss. These are the weights actually used, and should
     *                              be what you use to calculate the new weights. Your copy may be incorrect. Write the new weights
     *                              to your copy and return them out the left hand side.
     * @return                      TRUE if success/no error, FALSE in case of failure.
     */
    - (BOOL) updateWithGradientState: (MPSCNNConvolutionGradientState* __nonnull) gradientState
                         sourceState: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) sourceState
                                         MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

    /*! @abstract  When copyWithZone:device on convolution is called, data source copyWithZone:device
     *             will be called if data source object responds to this selector. If not, copyWithZone:
     *             will be called if data source responds to it. Otherwise, it is simply retained.
     *             This is to allow application to make a separate copy of data source in convolution
     *             when convolution itself is coplied, for example when copying training graph for running
     *             on second GPU so that weights update on two different GPUs dont end up stomping same
     *             data source.
     */

    -(nonnull instancetype) copyWithZone: (nullable NSZone*) zone
                                  device: (nullable id <MTLDevice>) device MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));


@end
    
#pragma mark -
#pragma mark MPSCNNConvolution

/*!
 *  @class      MPSCNNConvolution
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNConvolution specifies a convolution.
 *              The MPSCNNConvolution convolves the input image with a set of filters, each producing one feature map in the output image.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNConvolution : MPSCNNKernel

/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image.
 */
@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the output image.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;

/*! @property   groups
 *  @abstract   Number of groups input and output channels are divided into.
 */
@property(readonly, nonatomic) NSUInteger      groups;

/*! @property   dataSource
 *  @abstract   dataSource with which convolution object was created
 */
@property(readonly, nonatomic, retain, nonnull) id<MPSCNNConvolutionDataSource>      dataSource;

/*! @property   subPixelScaleFactor
 *  @abstract   Sub pixel scale factor which was passed in as part of MPSCNNConvolutionDescriptor when creating this MPSCNNConvolution object.
 */
@property(readonly, nonatomic) NSUInteger      subPixelScaleFactor;

/*! @property   neuron
 *  @abstract   MPSCNNNeuron filter to be applied as part of convolution.
 *              Can be nil in wich case no neuron activation fuction is applied.
 */
@property(readonly, nonatomic) const MPSCNNNeuron * __nullable  neuron
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "A MPSCNNNeuron is much too heavy for this purpose. Please set fusedNeuronDescriptor property of convolution descriptor instead.",
                                            ios(10.0, 11.0), tvos(10.0, 11.0) );

/*! @abstract   The type of neuron to append to the convolution
 *  @discussion Please see class description for a full list. Default is MPSCNNNeuronTypeNone. */
@property   (readonly, nonatomic) MPSCNNNeuronType     neuronType
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Use fusedNeuronDesciptor instead.",
                                        macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0) );

/*! @abstract   Parameter "a" for the neuron.  Default: 1.0f
 *  @discussion Please see class description for interpretation of a. */
@property   (readonly, nonatomic) float                neuronParameterA
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Use fusedNeuronDesciptor instead.",
                                        macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0) );

/*! @abstract   Parameter "b" for the neuron.  Default: 1.0f
 *  @discussion Please see class description for interpretation of b. */
@property   (readonly, nonatomic) float                neuronParameterB
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Use fusedNeuronDesciptor instead.",
                                        macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0) );

/*! @abstract   Parameter "c" for the neuron.  Default: 1.0f
 *  @discussion Please see class description for interpretation of c. */
@property   (readonly, nonatomic) float                neuronParameterC
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Use fusedNeuronDesciptor instead.",
                                        macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0) );

/*! @abstract   Fused neuron descritor passed in convolution descriptor for fusion with convolution.
 *  @discussion Please see class description for interpretation of c. */
@property (readonly, nonatomic) MPSNNNeuronDescriptor* __nullable fusedNeuronDescriptor
                                MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract   Channel multiplier.
 *  @discussion For convolution created with MPSCNNDepthWiseConvolutionDescriptor, it is the number of
 *              output feature channels for each input channel. See MPSCNNDepthWiseConvolutionDescriptor for more details.
 *              Default is 0 which means regular CNN convolution.
 */
@property   (readonly, nonatomic) NSUInteger           channelMultiplier;

/*! @abstract    Precision of accumulator used in convolution.
 *  @discussion  See MPSNeuralNetworkTypes.h for discussion. Default is MPSNNConvolutionAccumulatorPrecisionOptionFloat.
 */
@property   (readwrite, nonatomic) MPSNNConvolutionAccumulatorPrecisionOption accumulatorPrecisionOption
                                                                      MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*!
 *  @abstract   Initializes a convolution kernel
 *  @param      device                          The MTLDevice on which this MPSCNNConvolution filter will be used
 *  @param      weights                         A pointer to a object that conforms to the MPSCNNConvolutionDataSource
 *                                              protocol. The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNConvolution uses to obtain the weights and bias terms 
 *                                              for the CNN convolution filter.
 *
 *  @return     A valid MPSCNNConvolution object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights NS_DESIGNATED_INITIALIZER
                                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Initializes a convolution kernel
 *              WARNING:                        This API is depreated and will be removed in the future. It cannot be used
 *                                              when training. Also serialization/unserialization wont work for MPSCNNConvolution
 *                                              objects created with this init. Please move onto using initWithDevice:weights:.
 *  @param      device                          The MTLDevice on which this MPSCNNConvolution filter will be used
 *  @param      convolutionDescriptor           A pointer to a MPSCNNConvolutionDescriptor.
 *  @param      kernelWeights                   A pointer to a weights array.  Each entry is a float value. The number of entries is =
 *                                              inputFeatureChannels * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]
 *                                              Weights are converted to half float (fp16) internally for best performance.
 *  @param      biasTerms                       A pointer to bias terms to be applied to the convolution output.  Each entry is a float value.
 *                                              The number of entries is = numberOfOutputFeatureMaps
 *  @param      flags                           Currently unused. Pass MPSCNNConvolutionFlagsNone
 *
 *  @return     A valid MPSCNNConvolution object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                 convolutionDescriptor: (const MPSCNNConvolutionDescriptor * __nonnull) convolutionDescriptor
                         kernelWeights: (const float * __nonnull) kernelWeights
                             biasTerms: (const float * __nullable) biasTerms
                                 flags: (MPSCNNConvolutionFlags) flags  NS_DESIGNATED_INITIALIZER
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use  -initWithDevice:convolutionDescriptor:weights: instead.",
                                                ios(10.0, 11.0), tvos(10.0, 11.0) );

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*
 * Use initWithDevice:weights instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract   Allocate a MPCNNConvolutionGradientSState to hold the results from a -encodeBatchToCommandBuffer... operation
 *
 *  @param      sourceImage         The MPSImage consumed by the associated -encode call.
 *  @param      sourceStates        The list of MPSStates consumed by the associated -encode call,
 *                                  for a batch size of 1.
 *  @return     The list of states produced by the -encode call for batch size of 1.
 *              -isResultStateReusedAcrossBatch returns YES for MPSCNNConvolution so same
 *              state is used across entire batch. State object is not reusasable across batches.
 */
-(MPSCNNConvolutionGradientState * __nullable) resultStateForSourceImage: (MPSImage *__nonnull) sourceImage
                                                            sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                        destinationImage: (MPSImage *__nonnull) destinationImage
                                                                                MPS_SWIFT_NAME( resultState(sourceImage:sourceStates:destinationImage:))
                                                                                MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/* To be used with batch encode call. Since same state is used across entire batch, it will return copy of same state in returned NSArray*/
-(MPSCNNConvolutionGradientStateBatch * __nullable) resultStateBatchForSourceImage: (MPSImageBatch * __nonnull) sourceImage
                                                                      sourceStates: (NSArray<MPSStateBatch *> * __nullable) sourceStates
                                                                  destinationImage:(MPSImageBatch * _Nonnull)destinationImage
                                                                    MPS_SWIFT_NAME( resultStateBatch(sourceImage:sourceStates:destinationImage:))
                                                                    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

-(MPSCNNConvolutionGradientState * __nullable) temporaryResultStateForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                                        sourceImage: (MPSImage *__nonnull) sourceImage
                                                                       sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                                   destinationImage: (MPSImage * __nonnull)destinationImage
                                                                    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                                                                    MPS_SWIFT_NAME( temporaryResultState(commandBuffer:sourceImage:sourceStates:destinationImage:));

-(MPSCNNConvolutionGradientStateBatch * __nullable) temporaryResultStateBatchForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                                                  sourceImage: (MPSImageBatch *__nonnull) sourceImage
                                                                                 sourceStates: (NSArray <MPSStateBatch *> *__nullable) sourceStates
                                                                             destinationImage: (MPSImageBatch *__nonnull) destinationImage
                                                                    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                                                                    MPS_SWIFT_NAME( temporaryResultStateBatch(commandBuffer:sourceImage:sourceStates:destinationImage:));

/*! @abstract   CPU side reload. Reload the updated weights and biases from data provider into internal weights and bias buffers. Weights and biases
 *              gradients needed for update are obtained from MPSCNNConvolutionGradientState object. Data provider passed in init call is used for this purpose.
 */
-(void) reloadWeightsAndBiasesFromDataSource
            MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*! Deprecated. dataSource will be ignored. */
-(void) reloadWeightsAndBiasesWithDataSource: (__nonnull id<MPSCNNConvolutionDataSource>) dataSource
            MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -reloadWeightsAndBiasesFromDataSource instead. ", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3, 12.0));


/*! @abstract   GPU side reload. Reload the updated weights and biases from update buffer produced by application enqueued metal kernel into internal weights
 *              and biases buffer. Weights and biases gradients needed for update are obtained from MPSCNNConvolutionGradientState object's gradientForWeights and gradientForBiases metal buffer.
 *
 *  @param      commandBuffer      Metal command buffer on which application update kernel was enqueued consuming MPSCNNConvolutionGradientState's gradientForWeights and gradientForBiases buffers
 *                                 and producing updateBuffer metal buffer.
 *  @param      state              MPSCNNConvolutionWeightsAndBiasesState containing weights and biases buffers which have updated weights produced by application's update kernel.
 *                                 The state readcount will be decremented.
 *
 */
-(void) reloadWeightsAndBiasesWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                          state: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) state
                                    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract   GPU side export. Enqueue a kernel to export current weights and biases stored in MPSCNNConvoltion's internal buffers into weights and biases MTLBuffer
 *              returned in MPSCNNConvolutionWeightsAndBiasesState.
 *
 *  @param      commandBuffer              Metal command buffer on which export kernel is enqueued.
 *  @param      resultStateCanBeTemporary  If FALSE, state returned will be non-temporary. If TRUE, returned state may or may not be temporary.
 *  @return     MPSCNNConvolutionWeightsAndBiasesState containing weights and biases buffer to which weights got exported. This state and be
                temporary or non-temporary depending on the flag resultStateCanBeTemporary
 */
- (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) exportWeightsAndBiasesWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                                                    resultStateCanBeTemporary: (BOOL) resultStateCanBeTemporary
                                                                     MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

@end    /* MPSCNNConvolution */
    
/*! @enum       MPSCNNConvolutionGradientOption
 *  @memberof   MPSCNNConvolutionGradient
 *  @abstract   Options used to control which gradient to compute
 */
#if defined(DOXYGEN)
    typedef enum MPSCNNConvolutionGradientOption
#else
    typedef NS_OPTIONS(NSUInteger, MPSCNNConvolutionGradientOption)
#endif
{
    // Only compute gradient with respect to data
    MPSCNNConvolutionGradientOptionGradientWithData              MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(gradientWithData)           = 1U,
    
    // Only compute gradient with respect to weights and bias
    MPSCNNConvolutionGradientOptionGradientWithWeightsAndBias    MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(gradientWithWeightsAndBias) = 2U,
    
    // Compute both gradients
    MPSCNNConvolutionGradientOptionAll                           MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(gradientWithWeightsAndBias) = MPSCNNConvolutionGradientOptionGradientWithData | MPSCNNConvolutionGradientOptionGradientWithWeightsAndBias
};
    
/*!
 *  @class      MPSCNNConvolutionGradient
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNConvolutionGradient implementents backward propagation of gradient i.e. it computes the gradient of loss function
 *              with respect input data of corresonding forward convolution and gradient of loss function with respect to weights and bias
 *              of corresponding convolution in forward pass.
 *
 *              Gradient with respect to data
 *              ==============================
 *              Gradient with respect to input data of corresponding forward convolution will be written in destination image passed to
 *              encode call of MPSCNNConvolutionGradient.
 *              This step is similar to convolution transpose in that the strided convolution in forward pass become zero filled convolution in
 *              backward propagation of gradients. The difference between MPSCNNConvolutionTranspose and gradient wrt data is how the
 *              weights, that are provided by data source, are interpreted. MPSCNNConvolution and MPSCNNConvolutionTranspose interpret weights
 *              provided by data source as
 *                                          weights[outputFeatureChannels][kernelWidth][kernelHeight][inputFeatureChannels]
 *              whereas convoution gradient with respect to data interpret the weights as
 *                                          weights[inputFeatureChannels][kernelWidth][kernelHeight][outputFeatureChannels]
 *              i.e. weights are transposed in inputFeatureChannels/outputFeatureChannels dimension and also rotated 180 degress in spatial dimension
 *
 *              User should use the same data source provider to initialize MPSCNNConvolutionGradient as is used to initialize corresponding
 *              forward MPSCNNConvolution. Implementation will do the transposition/shuffling needed.
 *              Thus, while the forward MPSCNNConvolution takes sourceImage of inputFeatureChannels and convolves it with
 *              Wt[outputFeatureChannels][kernelHeight][kernelWidth][inputFeatureChannels] to produce destinationImage of outputFeatureChannels,
 *              MPSConvolutionGradient takes sourceGradient of outputFeatureChannels which is out of previous layer (nomally neuron backward layer),
 *              convolves it with transposed and rotated weights and produces destinationGradient of inputFeatureChannels.
 *              If the user decide to double buffer data source provider i.e. different data source providers are passed to forward MPSCNNConvolution object and
 *              corresponding MPSCNNConvolutionGradient object, it is user responsibility to make sure both data source providers provide same weights/bias data
 *              and have same properties in convolution descriptor else behavior is undefined.
 *
 *              Gradient with respect to weights and bias
 *              =========================================
 *              Gradient with respect to weights and bias are returned in MPSCNNConvolutionGradientState object to be used in weights update functions.
 *              If I denotes the input image to corresponding MPSCNNConvolution in forward pass and E denoates the loss gradient from previous layer
 *              (normally neuron backward layer) in backward pass, gradient of E with respect to weights is
 *
 *              delta_E/delta_Wkpqc = sum_i sum_j [ E(i - primaryOffset.x,j - primaryOffset.y, k) * I( secondaryStrideInPixelX*i + secondaryOffset.x - secondaryDilationRateX*secondaryKernelWidth/2 + secondaryDilationRateX*p,
 *                                                                                                     secondaryStrideinPixelY*i + secondaryOffset.y - secondaryDilationRateY*secondaryKernelHeight/2 + secondaryDilationRateY*q, c) ]
 *
 *              where i goes over 0..W-1 and j goes over 0..H-1, (W,H) being width and height of E.
 *              p in [0, secondaryKernelWidth-1]
 *              q in [0, secondaryKernelHeight-1]
 *              c in [0, inputeFeatureChannels/groups - 1]
 *              k in [0, outputFeatureChannels]
 *
 *              and gradient with respect to bias
 *
 *              delta_E/delta_bk = sum_i sum_j [ E(i - primaryOffset.x,j - primaryOffset.y, k) ]
 *
 *              These gradients with respect to weights and bias are returned as buffers in MPSCNNConvolutionGradientState object passed in the encode call.
 *              These are consumed by MPSCNNConvolution object's -updateWeightsAndBias:MPSCNNConvolutionGradientState* method for CPU side update and
 *              encodeWeightsAndBiasUpdate:commandBuffer:MPSCNNConvolutionGradientState* method of MPSCNNConvolution object for GPU side update.
 *              UPdated weights and biases are computed as
 *
 *                         Wkpqc_new = Wkpqc_old + delta_E/delta_Wkpqc
 *                         bk_new = bk_old + delta_E/delta_bk
 *
 *              Note that MPSCNNConvolutionGradientState objects's buffers that contain gradients, for CPU side update, will only contain
 *              valid data after command buffer is complete so
 *              its only makes sense to call -updateWeightsAndBias method on MPSCNNConvolution objects after command bufer is
 *              complete. One can achieve this by enqueueing a command buffer completion handler block that make this call.
 *              Since MPSCNNConvolutionGradientState is used across command buffers i.e. its created in forward pass, consumed by  MPSCNNConvolutionGradient in backward pass in same command buffer and passed onto MPSCNNConvolution updateWeightsAndBias method
 *              after completion of command buffer, it cannot be a temporary state.
 *
 *              In order to gaurantee consistency between forward pass (MPSCNNConvolution) and weights gradient computation in this filter, certain requirements
 *              must be met.
 *              1) Dimensions of loss gradient E from previous layer in backward pass must be equal to clipRect.size of corresponding MPSCNNConvolution in forward pass.
 *                 This is to gaurantee that only those pixels for which weights/bias contributed in destination of forward pass end up contributing to weights/bias gradient update.
 *                 If the dimension of loss gradient E from previous layer is not equal to clipRect.size of corresponding forward MPSCNNConvolution,
 *                    i) one can insert a slice operation to extract out the region of size clipRect.size from appropriate offset in E and set primaryOffset = 0 Or
 *                   ii) set primatryOffset to offset in E at which valid data starts and make sure data outside is zeroed.
 *              2) secondaryOffset should be set to what offset property of MPSCNNConvolution was set to in forward pass.
 *
 *              Currently back propagation for gradients is only supported for regualar convolution and depthwise convolution. Back propagation
 *              sub-pixel convolution are not supported. So channelMultiplier and subPixelScaleFactor must be one.
*/
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNConvolutionGradient : MPSCNNGradientKernel

/*! @property   sourceGradientFeatureChannels
 *  @abstract   The number of feature channels per pixel in the gradient image (primarySource) of encode call. This is same is outputFeatureChannels
 *              or the feature channels of destination image in forward convolution i.e. dataSource.descriptor.outputFeatureChannels
 */
@property(readonly, nonatomic) NSUInteger       sourceGradientFeatureChannels;

/*! @property   sourceImageFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image to forward convolution which is used here as secondarySource.
 *              This is same as dataSource.descriptor.inputFeatureChannels. This is also the number of feature channels in destinatin image
 *              here i.e. gradient with respect to data.
 */
@property(readonly, nonatomic) NSUInteger       sourceImageFeatureChannels;

/*! @property   groups
 *  @abstract   Number of groups input and output channels are divided into.
 */
@property(readonly, nonatomic) NSUInteger      groups;

/*! @abstract   Channel multiplier.
 *  @discussion For convolution created with MPSCNNDepthWiseConvolutionDescriptor, it is the number of
 *              output feature channels for each input channel. See MPSCNNDepthWiseConvolutionDescriptor for more details.
 *              Default is 0 which means regular CNN convolution. Currently only channelMultiplier of 1 is supported i.e. inputChannels == outputChannels
 */
@property   (readonly, nonatomic) NSUInteger           channelMultiplier;

/*! @property   dataSource
 *  @abstract   dataSource with which gradient object was created
 */
@property(readonly, nonatomic, retain, nonnull) id<MPSCNNConvolutionDataSource>      dataSource;


/*! @property   gradientOption
 *  @abstract   Option to control which gradient to compute. Default is MPSCNNConvolutionGradientOptionAll
 *              which means both gradient with respect to data and gradient with respect to weight and bias are computed.
 */
@property(readwrite, nonatomic) MPSCNNConvolutionGradientOption      gradientOption;

/*! @abstract    Property to control serialization of weights and bias.
 *  @discussion  During serialization of convolution object in -encodeWithCoder call, weights and biases are saved so that convolution
 *               object can be properly unserialized/restored in -initWithCoder call. If data source provied is NSSecureCoding compliant,
 *               data source is serialized else weights and biases are serialized.
 *               As weights/biases data may be several MB and these are same for both gradient and forward convolution object,
 *               application may already have weights/biases on disk through convolution, it can
 *               save disk space by setting this property false so convolution gradient object does not end up storing another copy of weights/biases.
 *               Default is NO. When application decides to set it to NO, it MUST call
 *                              -(void) reloadWeightsAndBiasesFromDataSource
 *               after initWithCoder has initialized convolution object.
 */
@property   (readwrite, nonatomic) BOOL serializeWeightsAndBiases
                            MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "This is deprecated and doesn't do anything. It is here for backward compatibility. MPSCNNConvolutionGradient doesn't serialize weights. It gets weight from state.convolution.dataSource on first use i.e. first encodeToCommandBuffer call",
                                      macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3, 12.0) );

/*!
 *  @abstract   Initializes a convolution gradient (with respect to weights and bias) object.
 *  @param      device                          The MTLDevice on which this MPSCNNConvolutionGradient filter will be used
 *  @param      weights                         A pointer to a object that conforms to the MPSCNNConvolutionDataSource
 *                                              protocol. Note that same data source as provided to forward convolution should be used.
 *
 *  @return     A valid MPSCNNConvolutionGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*
 * Use initWithDevice:weights instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract   CPU side reload. Reload the updated weights and biases from data provider into internal weights and bias buffers. Weights and biases
 *              gradients needed for update are obtained from MPSCNNConvolutionGradientState object. Data provider passed in init call is used for this purpose.
 */
-(void) reloadWeightsAndBiasesFromDataSource;

/*! @abstract   GPU side reload. Reload the updated weights and biases from update buffer produced by application enqueued metal kernel into internal weights
 *              and biases buffer. Weights and biases gradients needed for update are obtained from MPSCNNConvolutionGradientState object's gradientForWeights and gradientForBiases metal buffer.
 *
 *  @param      commandBuffer      Metal command buffer on which application update kernel was enqueued consuming MPSCNNConvolutionGradientState's gradientForWeights and gradientForBiases buffer
 *                                 and producing updateBuffer metal buffer.
 *  @param      state              MPSCNNConvolutionWeightsAndBiasesState containing weights and biases buffers which have updated weights produced by application's update kernel.
 *
 */
-(void) reloadWeightsAndBiasesWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                          state: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) state
                                                MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

@end    /* MPSCNNConvolutionGradient */

#pragma mark -
#pragma mark MPSCNNFullyConnected layer

/*!
 *  @class      MPSCNNFullyConnected
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNFullyConnected specifies a fully connected convolution layer a.k.a. Inner product
 *              layer. A fully connected CNN layer is one where every input channel is connected
 *              to every output channel. The kernel width is equal to width of source image
 *              and the kernel height is equal to the height of source image. Width and height of the output
 *              is 1x1. Thus, it takes a srcW x srcH x Ni MPSCNNImage, convolves it with Weights[No][SrcW][srcH][Ni]
 *              and produces a 1 x 1 x No output. The following must be true:
 *@code
 *                         kernelWidth  == source.width
 *                         kernelHeight == source.height
 *                         clipRect.size.width == 1
 *                         clipRect.size.height == 1
 *@endcode
 *              One can think of a fully connected layer as a matrix multiplication that flattens an image into a vector of length
 *              srcW*srcH*Ni. The weights are arragned in a matrix of dimension No x (srcW*srcH*Ni) for product output vectors
 *              of length No. The strideInPixelsX, strideInPixelsY, and group must be 1. Offset is not applicable and is ignored.
 *              Since clipRect is clamped to the destination image bounds, if the destination is 1x1, one doesn't need to set the
 *              clipRect.
 *
 *              Note that one can implement an inner product using MPSCNNConvolution by setting
 *@code
 *                     offset = (kernelWidth/2,kernelHeight/2)
 *                     clipRect.origin = (ox,oy), clipRect.size = (1,1)
 *                     strideX = strideY = group = 1
 *@endcode
 *              However, using the MPSCNNFullyConnected for this is better for performance as it lets us choose the most
 *              performant method which may not be possible when using a general convolution. For example,
 *              we may internally use matrix multiplication or special reduction kernels for a specific platform.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNFullyConnected : MPSCNNConvolution

/*!
 *  @abstract   Initializes a fully connected kernel
 *  @param      device                          The MTLDevice on which this MPSCNNFullyConnected filter will be used
 *  @param      weights                         A pointer to a object that conforms to the MPSCNNConvolutionDataSource
 *                                              protocol. The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNFullyConnected uses to obtain the weights and bias terms
 *                                              for the CNN fully connected filter.
 *
 *  @return     A valid MPSCNNFullyConnected object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights NS_DESIGNATED_INITIALIZER
                                                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Initializes a convolution kernel
 *              WARNING:                        This API is depreated and will be removed in the future. It cannot be used
 *                                              when training. Also serialization/unserialization wont work for MPSCNNConvolution
 *                                              objects created with this init. Please move onto using initWithDevice:weights:.
 *  @param      device                          The MTLDevice on which this MPSCNNConvolution filter will be used
 *  @param      convolutionDescriptor           A pointer to a MPSCNNConvolutionDescriptor.
 *  @param      kernelWeights                   A pointer to a weights array.  Each entry is a float value. The number of entries is =
 *                                              inputFeatureChannels * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ inputChannels / groups ]
 *                                              Weights are converted to half float (fp16) internally for best performance.
 *  @param      biasTerms                       A pointer to bias terms to be applied to the convolution output.  Each entry is a float value.
 *                                              The number of entries is = numberOfOutputFeatureMaps
 *  @param      flags                           Currently unused. Pass MPSCNNConvolutionFlagsNone
 *
 *  @return     A valid MPSCNNConvolution object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                 convolutionDescriptor: (const MPSCNNConvolutionDescriptor * __nonnull) convolutionDescriptor
                         kernelWeights: (const float * __nonnull) kernelWeights
                             biasTerms: (const float * __nullable) biasTerms
                                 flags: (MPSCNNConvolutionFlags) flags  NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use  -initWithDevice:convolutionDescriptor:weights: instead.",
                                                            ios(10.0, 11.0), tvos(10.0, 11.0) );

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                                                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*
 * Use initWithDevice:weights instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 * NOTE:    The encodeToCommandBuffer API in MPSCNNKernel can be used to encode a inner product kernel to a MTLCommandBuffer.
 *          The source and destination must be MPSImage.
 */

@end    /* MPSCNNFullyConnected */
    
/*!
 *  @class      MPSCNNFullyConnectedGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Compute the gradient for fully connected layer.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNFullyConnectedGradient : MPSCNNConvolutionGradient

/*!
 *  @abstract   Initializes a convolution gradient (with respect to weights and bias) object.
 *  @param      device                          The MTLDevice on which this MPSCNNConvolutionGradient filter will be used
 *  @param      weights                         A pointer to a object that conforms to the MPSCNNConvolutionDataSource
 *                                              protocol. Note that same data source as provided to forward convolution should be used.
 *
 *  @return     A valid MPSCNNConvolutionGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*
 * Use initWithDevice:weights instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*
 *  call base class (MPSCNNConvolutionGradient) reload methods for reloading weights/biases during training
 */

@end    /* MPSCNNConvolutionGradient */
    
#pragma mark -
#pragma mark MPSCNNConvolutionTranspose
    
/*!
 *  @class      MPSCNNConvolutionTranspose
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNConvolutionTranspose specifies a transposed convolution.
 *              The MPSCNNConvolutionTranspose convolves the input image with a set of filters, each producing one feature map in the output image.
 *
 *              Some third-party frameworks may rotate the weights spatially by 180 degrees for Convolution Transpose. MPS uses the weights
 *              specified by the developer as-is and does not perform any rotation. The developer may need to rotate the weights appropriately
 *              in case this rotation is needed before the convolution transpose is applied.
 *
 *              When the stride in any dimension is greater than 1, the convolution transpose puts (stride - 1) zeroes in-between the source 
 *              image pixels to create an expanded image. Then a convolution is done over the expanded image to generate the output of the 
 *              convolution transpose.
 *
 *              Intermediate image size = (srcSize - 1) * Stride + 1
 *
 *              Examples:
 *
 *
 *  @code
 *              So in case of sride == 2 (this behaves same in both dimensions)
 *
 *              Source image:
 *               _______________
 *              |   |   |   |   |
 *              | 1 | 2 | 3 | 4 |
 *              |   |   |   |   |
 *               ---------------
 *
 *              Intermediate Image:
 *               ___________________________
 *              |   |   |   |   |   |   |   |
 *              | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
 *              |   |   |   |   |   |   |   |
 *               ---------------------------
 *
 *
 *              NOTE on Offset:
 *              There are 2 types of offsets defined:
 *              1) The Offset defined in MPSCNNKernel from which MPSCNNConvolutionTranspose inherits. This offset is applied to from where
 *                 the kernel will be applied on the source.
 *              2) The kernelOffsetX and kernelOffsetY which is the offset applied to the kernel when it is finally applied on the intermediate
 *                 image.
 *
 *              So totalOffset = Offset * stride + kernelOffset
 *
 *              The offset defined by user refers to the coordinate frame of the expanded image
 *              (we are showing only 1 dimension X it can be extended to Y dimension as well) :
 *
 *              X indicates where the convolution transpose begins:
 *
 *              Intermediate Image:  Offset = 0, kernelOffset = 0
 *               ___________________________
 *              |   |   |   |   |   |   |   |
 *              | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
 *              | X |   |   |   |   |   |   |
 *               ---------------------------
 *
 *
 *              X indicates where the convolution transpose begins:
 *
 *              Intermediate Image:  Offset = 0, kernelOffset = 1
 *               ___________________________
 *              |   |   |   |   |   |   |   |
 *              | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
 *              |   | X |   |   |   |   |   |
 *               ---------------------------
 *
 *
 *              X indicates where the convolution transpose begins:
 *
 *              Intermediate Image:  Offset = 0, kernelOffset = -1
 *                 ___________________________
 *                |   |   |   |   |   |   |   |
 *              X | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
 *                |   |   |   |   |   |   |   |
 *                 ---------------------------
 *
 *
 *
 *
 *              So if the user wanted to apply an offset of 2 on the source image of convolution transpose:
 *
 *              Source image:
 *               _______________
 *              |   |   |   |   |
 *              | 1 | 2 | 3 | 4 |
 *              |   |   | X |   |
 *               ---------------
 *
 *              offset = 2, kernelOffset = 0
 *
 *              Intermediate Image:
 *               ___________________________
 *              |   |   |   |   |   |   |   |
 *              | 1 | 0 | 2 | 0 | 3 | 0 | 4 |
 *              |   |   |   |   | X |   |   |
 *               ---------------------------
 *
 * @endcode
 *
 */


MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNConvolutionTranspose : MPSCNNKernel

/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image.
 */
@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the output image.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;

/*! @property   kernelOffsetX
 *  @abstract   Offset in X from which the kernel starts sliding
 */
@property(readwrite, nonatomic) NSInteger      kernelOffsetX;

/*! @property   kernelOffsetY
 *  @abstract   Offset in Y from which the kernel starts sliding
 */
@property(readwrite, nonatomic) NSInteger      kernelOffsetY;

/*! @property   groups
 *  @abstract   Number of groups input and output channels are divided into.
 */
@property(readonly, nonatomic) NSUInteger      groups;

/*! @abstract    Precision of accumulator used in convolution.
 *  @discussion  See MPSNeuralNetworkTypes.h for discussion. Default is MPSNNConvolutionAccumulatorPrecisionOptionFloat.
 */
@property   (readwrite, nonatomic) MPSNNConvolutionAccumulatorPrecisionOption accumulatorPrecisionOption
                                                    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*!
 *  @abstract   Initializes a convolution transpose kernel
 *  @param      device                          The MTLDevice on which this MPSCNNConvolutionTranspose filter will be used
 *  @param      weights                         A pointer to a object that conforms to the MPSCNNConvolutionDataSource
 *                                              protocol. The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNConvolutionTranspose uses to obtain the weights and bias terms
 *                                              for the CNN convolutionTranspose filter. Currently we support only Float32 weights.
 *
 *  @return     A valid MPSCNNConvolutionTranspose object.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                               weights: (nonnull id <MPSCNNConvolutionDataSource>) weights NS_DESIGNATED_INITIALIZER;

/*
 * Use initWithDevice:convolutionDescriptor:kernelWeights:biasTerms instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract <NSSecureCoding> support */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract       Encode a MPSCNNKernel into a command Buffer. Create a texture to hold the result and return it.
 *  @discussion     In the first iteration on this method, encodeToCommandBuffer:sourceImage:destinationImage:
 *                  some work was left for the developer to do in the form of correctly setting the offset property
 *                  and sizing the result buffer. With the introduction of the padding policy (see padding property)
 *                  the filter can do this work itself. If you would like to have some input into what sort of MPSImage
 *                  (e.g. temporary vs. regular) or what size it is or where it is allocated, you may set the
 *                  destinationImageAllocator to allocate the image yourself.
 *
 *                  This method uses the MPSNNPadding padding property to figure out how to size
 *                  the result image and to set the offset property. See discussion in MPSNeuralNetworkTypes.h.
 *
 *                  Note: the regular encodeToCommandBuffer:sourceImage: method may be used when no state is needed,
 *                  such as when the convolution transpose operation is not balanced by a matching convolution object upstream.
 *
 *  @param          commandBuffer       The command buffer
 *  @param          sourceImage         A MPSImage to use as the source images for the filter.
 *  @param          convolutionGradientState    A valid MPSCNNConvolutionGradientState from the MPSCNNConvoluton counterpart to this MPSCNNConvolutionTranspose.
 *                                      If there is no forward convolution counterpart, pass NULL here. This state affects the sizing
 *                                      the result.
 *  @result         A MPSImage or MPSTemporaryImage allocated per the destinationImageAllocator containing the output of the graph.
 *                  The offset property will be adjusted to reflect the offset used during the encode.
 *                  The returned image will be automatically released when the command buffer completes. If you want to
 *                  keep it around for longer, retain the image. (ARC will do this for you if you use it later.)
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                  sourceImage: (MPSImage *  __nonnull) sourceImage
                     convolutionGradientState: (MPSCNNConvolutionGradientState * __nullable) convolutionGradientState
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:convolutionGradientState:));

-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                           sourceImages: (MPSImageBatch *  __nonnull) sourceImage
                              convolutionGradientStates: (MPSCNNConvolutionGradientStateBatch * __nullable) convolutionGradientState
                        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME( encodeBatch(commandBuffer:sourceImages:convolutionGradientStates:));

-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage *  __nonnull) sourceImage
     convolutionGradientState: (MPSCNNConvolutionGradientState * __nullable) convolutionGradientState
             destinationImage: (MPSImage * __nonnull) destinationImage
                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME(encode(commandBuffer:sourceImage:convolutionGradientState:destinationImage:));

-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch *  __nonnull) sourceImage
         convolutionGradientStates: (MPSCNNConvolutionGradientStateBatch * __nullable) convolutionGradientState
                 destinationImages: (MPSImageBatch * __nonnull) destinationImage
                        MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
                        MPS_SWIFT_NAME(encodeBatch(commandBuffer:sourceImages:convolutionGradientStates:destinationImages:));


@end    /* MPSCNNConvolutionTranspose */
    


#pragma mark -
#pragma mark MPSCNNBinaryConvolution

/*!
 *  @class      MPSCNNBinaryConvolution
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNBinaryConvolution specifies a convolution with binary weights and an input image using binary approximations.
 *              The MPSCNNBinaryConvolution optionally first binarizes the input image and then convolves the result with a set of
 *              binary-valued filters, each producing one feature map in the output image (which is a normal image)
 *
 *              The output is computed as follows:
 *
 *                  out[i, x, y, c] = ( sum_{dx,dy,f} in[i,x+dx, y+dy, f] x B[c,dx,dy,f] )
 *                                      * scale[c] * beta[i,x,y] + bias[c], where
 *
 *              the sum over dx,dy is over the spatial filter kernel window defined by 'kernelWidth' and 'KernelHeight',
 *              sum over 'f' is over the input feature channel indices within group, 'B' contains the binary weights, interpreted as
 *              {-1,1} or { 0, 1 } and scale[c] is the 'outputScaleTerms' array and bias is the 'outputBiasTerms' array. Above 'i' is
 *              the image index in batch the sum over input channels 'f' runs through the group indices.
 *
 *              The convolution operator 'x' is defined by MPSCNNBinaryConvolutionType passed in at initialization time of the filter
 *              (@see initWithDevice).
 *              In case 'type' = MPSCNNBinaryConvolutionTypeBinaryWeights, the input image is not binarized at all
 *                  and the convolution is computed interpreting the weights as [ 0, 1 ] -> { -1, 1 } with the given scaling terms.
 *              In case 'type' = MPSCNNBinaryConvolutionTypeXNOR the convolution is computed by first binarizing the input image
 *                  using the sign function 'bin(x) = x < 0 ? -1 : 1' and the convolution multiplication is done with the
 *                  XNOR-operator !(x ^ y) = delta_xy = { (x==y) ? 1 : 0 },
 *                  and scaled according to the optional scaling operations. Note that we output the values of the bitwise convolutions
 *                  to interval { -1, 1 }, which means that the output of the XNOR-operator is scaled implicitly as follows:
 *                      r = 2 * ( !(x ^ y) ) - 1 = { -1, 1 }.
 *                  This means that for a dot-product of two 32-bit words the result is:
 *                      r = 2 * popcount(!(x ^ y) ) - 32 = 32 - 2 * popcount( x ^ y ) = { -32, -30, ..., 30, 32 }.
 *              In case 'type' = MPSCNNBinaryConvolutionTypeAND the convolution is computed by first binarizing the input image
 *                  using the sign function 'bin(x) = x < 0 ? -1 : 1' and the convolution multiplication is done with the
 *                  AND-operator (x & y) = delta_xy * delta_x1 = { (x==y==1) ? 1 : 0 }.
 *                  and scaled according to the optional scaling operations. Note that we output the values of the AND-operation is
 *                  assumed to lie in { 0, 1 } interval and hence no more implicit scaling takes place.
 *                  This means that for a dot-product of two 32-bit words the result is:
 *                      r = popcount(x & y) = { 0, ..., 31, 32 }.
 *
 *              The input data can be pre-offset and scaled by providing the 'inputBiasTerms' and 'inputScaleTerms' parameters for the
 *              initialization functions and this can be used for example to accomplish batch normalization of the data. The scaling of
 *              input values happens before possible beta-image computation.
 *
 *              The parameter 'beta' above is an optional image which is used to compute scaling factors for each spatial position and image index.
 *              For the XNOR-Net based networks this is computed as follows: beta[i,x,y] = sum_{dx,dy} A[i, x+dx, y+dy] / (kx * ky), where
 *              (dx,dy) are summed over the convolution filter window [ -kx/2, (kx-1)/2], [ -ky/2, (ky-1)/2 ] and
 *              A[i,x,y] = sum_{c} abs( in[i,x,y,c] ) / Nc, where 'in' is the original input image (in full precision) and Nc is the
 *              number of input channels in the input image. Parameter 'beta' is not passed as input and to enable beta-scaling the user can
 *              provide 'MPSCNNBinaryConvolutionFlagsUseBetaScaling' in the flags parameter in the initialization functions.
 *
 *              Finally the normal activation neuron is applied and the result is written to the output image.
 *
 *              NOTE: MPSCNNBinaryConvolution does not currently support groups > 1.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNBinaryConvolution : MPSCNNKernel

@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the output image.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;


/*!
 *  @abstract   Initializes a binary convolution kernel with binary weights and a single scaling term.
 *  @param      device                          The MTLDevice on which this MPSCNNBinaryConvolution filter will be used
 *  @param      convolutionData                 A pointer to a object that conforms to the MPSCNNConvolutionDataSource protocol.
 *                                              The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNBinaryConvolution uses to obtain the weights and bias terms as
 *                                              well as the convolution descriptor.
 *                                              Each entry in the convolutionData:weights array is a 32-bit unsigned integer value
 *                                              and each bit represents one filter weight (given in machine byte order).
 *                                              The featurechannel indices increase from the least significant bit within the 32-bits.
 *                                              The number of entries is =
 *                                              ceil( inputFeatureChannels/32.0 ) * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as a 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ ceil( inputChannels / 32.0 ) ]
 *                                              (The ordering of the reduction from 4D tensor to 1D is per C convention. The index based on
 *                                              inputchannels varies most rapidly, followed by kernelWidth, then kernelHeight and finally
 *                                              outputChannels varies least rapidly.)
 *  @param      scaleValue                      A floating point value used to scale the entire convolution.
 *  @param      type                            What kind of binarization strategy is to be used.
 *  @param      flags                           See documentation above and documentation of MPSCNNBinaryConvolutionFlags.
 *
 *  @return     A valid MPSCNNBinaryConvolution object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       convolutionData: (nonnull id <MPSCNNConvolutionDataSource>) convolutionData
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;




/*!
 *  @abstract   Initializes a binary convolution kernel with binary weights as well as both pre and post scaling terms.
 *  @param      device                          The MTLDevice on which this MPSCNNBinaryConvolution filter will be used
 *  @param      convolutionData                 A pointer to a object that conforms to the MPSCNNConvolutionDataSource protocol.
 *                                              The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNBinaryConvolution uses to obtain the weights and the convolution descriptor.
 *                                              Each entry in the convolutionData:weights array is a 32-bit unsigned integer value
 *                                              and each bit represents one filter weight (given in machine byte order).
 *                                              The featurechannel indices increase from the least significant bit within the 32-bits.
 *                                              The number of entries is =
 *                                              ceil( inputFeatureChannels/32.0 ) * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as a 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ ceil( inputChannels / 32.0 ) ]
 *                                              (The ordering of the reduction from 4D tensor to 1D is per C convention. The index based on
 *                                              inputchannels varies most rapidly, followed by kernelWidth, then kernelHeight and finally
 *                                              outputChannels varies least rapidly.)
 *  @param      outputBiasTerms                 A pointer to bias terms to be applied to the convolution output.  Each entry is a float value.
 *                                              The number of entries is = numberOfOutputFeatureMaps. If nil then 0.0 is used for bias.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      outputScaleTerms                A pointer to scale terms to be applied to binary convolution results per output feature channel.
 *                                              Each entry is a float value. The number of entries is = numberOfOutputFeatureMaps. If nil then 1.0 is used.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      inputBiasTerms                  A pointer to offset terms to be applied to the input before convolution and before input scaling.
 *                                              Each entry is a float value. The number of entries is 'inputFeatureChannels'. If NULL then 0.0 is used for bias.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      inputScaleTerms                 A pointer to scale terms to be applied to the input before convolution, but after input biasing.
 *                                              Each entry is a float value. The number of entries is 'inputFeatureChannels'. If nil then 1.0 is used.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      type                            What kind of binarization strategy is to be used.
 *  @param      flags                           See documentation above and documentation of MPSCNNBinaryConvolutionFlags.
 *
 *  @return     A valid MPSCNNBinaryConvolution object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       convolutionData: (nonnull id <MPSCNNConvolutionDataSource>) convolutionData
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


/*
 * Use initWithDevice:convolutionDescriptor:kernelWeights:biasTerms:scaleTerms:betaImage:flags instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 * NOTE:    The encodeToCommandBuffer API in MPSCNNKernel can be used to encode a XNOR convolution kernel to a MTLCommandBuffer.
 */


@end    /* MPSCNNBinaryConvolution */



#pragma mark -
#pragma mark MPSCNNBinaryFullyConnected

/*!
 *  @class      MPSCNNBinaryFullyConnected
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSCNNBinaryFullyConnected specifies a fully connected convolution layer with binary weights
 *              and optionally binarized input image.
 *              See @ref MPSCNNFullyConnected for details on the fully connected layer and
 *              MPSCNNBinaryConvolution for binary convolutions.
 *
 *              The default padding policy for MPSCNNBinaryConvolution is different from most 
 *              filters. It uses MPSNNPaddingMethodSizeValidOnly instead of MPSNNPaddingMethodSizeSame.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNBinaryFullyConnected : MPSCNNBinaryConvolution


/*!
 *  @abstract   Initializes a binary fully connected kernel with binary weights and a single scaling term.
 *
 *  @param      device                          The MTLDevice on which this MPSCNNBinaryFullyConnected filter will be used
 *  @param      convolutionData                 A pointer to a object that conforms to the MPSCNNConvolutionDataSource protocol.
 *                                              The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNBinaryFullyConnected uses to obtain the weights and bias terms as
 *                                              well as the convolution descriptor.
 *                                              Each entry in the convolutionData:weights array is a 32-bit unsigned integer value
 *                                              and each bit represents one filter weight (given in machine byte order).
 *                                              The featurechannel indices increase from the least significant bit within the 32-bits.
 *                                              The number of entries is =
 *                                              ceil( inputFeatureChannels/32.0 ) * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as a 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ ceil( inputChannels / 32.0 ) ]
 *                                              (The ordering of the reduction from 4D tensor to 1D is per C convention. The index based on
 *                                              inputchannels varies most rapidly, followed by kernelWidth, then kernelHeight and finally
 *                                              outputChannels varies least rapidly.)
 *  @param      scaleValue                      A single floating point value used to scale the entire convolution.
 *                                              Each entry is a float value. The number of entries is 'inputFeatureChannels'. If nil then 1.0 is used.
 *  @param      type                            What kind of binarization strategy is to be used.
 *  @param      flags                           See documentation above and documentation of MPSCNNBinaryConvolutionFlags.
 *
 *  @return     A valid MPSCNNBinaryFullyConnected object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       convolutionData: (nonnull id <MPSCNNConvolutionDataSource>) convolutionData
                            scaleValue: (float) scaleValue
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;



/*!
 *  @abstract   Initializes a binary fully connected kernel with binary weights as well as both pre and post scaling terms.
 *
 *  @param      device                          The MTLDevice on which this MPSCNNBinaryFullyConnected filter will be used
 *  @param      convolutionData                 A pointer to a object that conforms to the MPSCNNConvolutionDataSource protocol.
 *                                              The MPSCNNConvolutionDataSource protocol declares the methods that an
 *                                              instance of MPSCNNBinaryFullyConnected uses to obtain the weights and the convolution descriptor.
 *                                              Each entry in the convolutionData:weights array is a 32-bit unsigned integer value
 *                                              and each bit represents one filter weight (given in machine byte order).
 *                                              The featurechannel indices increase from the least significant bit within the 32-bits.
 *                                              The number of entries is =
 *                                              ceil( inputFeatureChannels/32.0 ) * outputFeatureChannels * kernelHeight * kernelWidth
 *                                              The layout of filter weight is so that it can be reinterpreted as a 4D tensor (array)
 *                                              weight[ outputChannels ][ kernelHeight ][ kernelWidth ][ ceil( inputChannels / 32.0 ) ]
 *                                              (The ordering of the reduction from 4D tensor to 1D is per C convention. The index based on
 *                                              inputchannels varies most rapidly, followed by kernelWidth, then kernelHeight and finally
 *                                              outputChannels varies least rapidly.)
 *
 *  @param      outputBiasTerms                 A pointer to bias terms to be applied to the convolution output.  Each entry is a float value.
 *                                              The number of entries is = numberOfOutputFeatureMaps. If nil then 0.0 is used for bias.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      outputScaleTerms                A pointer to scale terms to be applied to binary convolution results per output feature channel.
 *                                              Each entry is a float value. The number of entries is = numberOfOutputFeatureMaps. If nil then 1.0 is used.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      inputBiasTerms                  A pointer to offset terms to be applied to the input before convolution and before input scaling.
 *                                              Each entry is a float value. The number of entries is 'inputFeatureChannels'. If NULL then 0.0 is used for bias.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      inputScaleTerms                 A pointer to scale terms to be applied to the input before convolution, but after input biasing.
 *                                              Each entry is a float value. The number of entries is 'inputFeatureChannels'. If nil then 1.0 is used.
 *                                              The values stored in the pointer are copied in and the array can be freed after this function returns.
 *  @param      type                            What kind of binarization strategy is to be used.
 *  @param      flags                           See documentation above and documentation of MPSCNNBinaryConvolutionFlags.
 *
 *  @return     A valid MPSCNNBinaryFullyConnected object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       convolutionData: (nonnull id <MPSCNNConvolutionDataSource>) convolutionData
                       outputBiasTerms: (const float * __nullable) outputBiasTerms
                      outputScaleTerms: (const float * __nullable) outputScaleTerms
                        inputBiasTerms: (const float * __nullable) inputBiasTerms
                       inputScaleTerms: (const float * __nullable) inputScaleTerms
                                  type: (MPSCNNBinaryConvolutionType) type
                                 flags: (MPSCNNBinaryConvolutionFlags) flags;



/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


/*
 * Use initWithDevice:convolutionDescriptor:kernelWeights:biasTerms:scaleTerms:betaImage:flags instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 * NOTE:    The encodeToCommandBuffer API in MPSCNNKernel can be used to encode a inner product kernel to a MTLCommandBuffer.
 */

@end    /* MPSCNNBinaryFullyConnected */






#ifdef __cplusplus
}
#endif


#endif /* MPSCNNConvolution_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNSlice.h
//
//  MPSNNSlice.h
//  MPSNeuralNetwork
//
//  Created by Aaftab Munshi on 11/27/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSNNSlice_h
#define MPSNNSlice_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif

/*
 *  @class      MPSNNSlice
 *  @dependency This depends on Metal.framework
 *  @abstract   Describes a slice operation
 *  @discussion The slice kernel is used to extract a slice from a source MPSImage.  The extracted slice is copied
 *              to a destination MPSImage.  The offset and sourceFeatureChannelOffset specify the following:
 *                  - the (x, y) location in the source image
 *                  - the starting feature channel offset in the source image
 *
 *              The clipRect specifies the starting (x, y) position in the destination image to copy the slice and
 *              the size (width, height) in pixels of the slice.  The featureChannelsInSlice specifies the number of
 *              feature channels to be extracted from the source image for the slice.  The featureChannels extracted
 *              from the slice are copied to the destination MPSImage starting at feature channel offset 0.
 *
 *              Some examples of slice operations can be found at:
 *                   http://mxnet.incubator.apache.org/api/python/ndarray.html?highlight=slice#mxnet.ndarray.slice
 *                   https://www.tensorflow.org/api_docs/python/tf/slice
 */

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNSlice : MPSCNNKernel

/*!
 *  @abstract Initialize a MPSNNSlice kernel
 *  @param    device            The device the filter will run on
 *  @return   A valid MPSNNSlice object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;


-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(__nonnull id<MTLDevice>)device    NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNSlice */


#ifdef __cplusplus
}
#endif

#endif /* MPSNNSlice_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNeuralNetworkTypes.h
//
//  MPSNeuralNetworkTypes.h
//  MPS
//
//  Created by Ian Ollmann on 8/20/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSNeuralNetworkTypes_h
#define MPSNeuralNetworkTypes_h

#import <MPSCore/MPSCoreTypes.h>


#ifdef __cplusplus
extern "C" {
#endif
    
@class MPSImage;
@class MPSImageDescriptor;
@class MPSState;
@class MPSKernel;
@class MPSCNNKernel;
    
/*! @enum       MPSCNNConvolutionFlags
 *  @abstract   Options used to control how kernel weights are stored and used in the CNN kernels.
 *              For future expandability.
 */
#if defined(DOXYGEN)
typedef enum MPSCNNConvolutionFlags
#else
typedef NS_ENUM(NSUInteger, MPSCNNConvolutionFlags)
#endif
{
    /*! Use default options */
    MPSCNNConvolutionFlagsNone      MPS_ENUM_AVAILABLE_STARTING_BUT_DEPRECATED( "Use the Convolution -init method with a MPSCNNConvolutionDataSource instead.", ios(10.0, 11.0), tvos(10.0, 11.0)) MPS_SWIFT_NAME(none)  = 0,
}
#if defined(DOXYGEN)
    MPSCNNConvolutionFlags
#endif
;

#pragma mark -
#pragma mark MPSCNNBinaryConvolutionFlags
    
    /*! @enum       MPSCNNBinaryConvolutionFlags
     *  @abstract   Options used to control CNN Binary convolution kernels.
     */
#if defined(DOXYGEN)
typedef enum MPSCNNBinaryConvolutionFlags
#else
typedef NS_ENUM(NSUInteger, MPSCNNBinaryConvolutionFlags)
#endif
{
    /*! Use default in binary convolution options */
    MPSCNNBinaryConvolutionFlagsNone            MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  MPS_SWIFT_NAME(none)  = 0,
    /*! Scale the binary convolution operation using the beta-image option as detailed in MPSCNNBinaryConvolution */
    MPSCNNBinaryConvolutionFlagsUseBetaScaling  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))   = 1 << 0,
}
#if defined(DOXYGEN)
    MPSCNNBinaryConvolutionFlags
#endif
    ;
    
    
    /*! @enum       MPSCNNBinaryConvolutionType
     *  @abstract   Defines what operations are used to perform binary convolution
     */
#if defined(DOXYGEN)
typedef enum MPSCNNBinaryConvolutionType
#else
typedef NS_ENUM(NSUInteger, MPSCNNBinaryConvolutionType)
#endif
{
    /*! Otherwise a normal convolution operation, except that the weights are binary values */
    MPSCNNBinaryConvolutionTypeBinaryWeights    MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  MPS_SWIFT_NAME(binaryWeights)  = 0,
    /*! Use input image binarization and the XNOR-operation to perform the actual convolution - See MPSCNNBinaryConvolution for details  */
    MPSCNNBinaryConvolutionTypeXNOR  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)),
    /*! Use input image binarization and the AND-operation to perform the actual convolution - See MPSCNNBinaryConvolution for details */
    MPSCNNBinaryConvolutionTypeAND  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)),
}
#if defined(DOXYGEN)
    MPSCNNBinaryConvolutionType
#endif
;
    
/*! @enum        MPSNNConvolutionAccumulatorPrecisionOption
 *  @memberof    MPSCNNConvolution
 *  @abstract    Options set on MPSCNNConvolution objects to control precision of accumulator used.
 *  @discussion  Convolution operation involves sequence of multiply-accumulate operation i.e.
 *                         accumulator = imageData * weight + accumulator
 *               Default MPSDataType for both imageData and weight is MPSDataTypeFloat16 i.e. half precision float.
 *               Multiply-accumulate result (imageData * weight + accumulator) needs to be rounded to precision of accumulator.
 *               MPS allows either half float or full float accumulator type using appropriate flags.
 *               The choice of accmulator precision should be based on how much precision loss application can sustain
 *               without significanly affecting accuracy of network.
 *               The default accumulator precision is half (MPSNNConvolutionAccumulatorPrecisionOptionFloat).
 *               If accumulation of computational rounding error in the result is excessive,
 *               user can specify MPSNNConvolutionAccumulatorPrecisionOptionFloat for full float accumulator.
 *               Note that on some devices, which do not provide IEEE compliant half arithmetic (A10 and older), half precision
 *               accumulator can cause excessive loss of precision causing severe loss in accuracy. MPS automatically
 *               ignores this option on those hardware and uses full float accumulator. On hardware that does support IEEE
 *               compliant half arithmetic and half accumulator does meet applications accuracy requirements, it can provide
 *               significant performance benefits.
 */
    
#if defined(DOXYGEN)
typedef enum MPSNNConvolutionAccumulatorPrecisionOption
#else
typedef NS_OPTIONS(NSUInteger, MPSNNConvolutionAccumulatorPrecisionOption)
#endif
{
    /*! Set accumulator type to half precision float. */
    MPSNNConvolutionAccumulatorPrecisionOptionHalf        MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(half) = 0U,
        
    /*! Set accumulator type to single precision float. */
    MPSNNConvolutionAccumulatorPrecisionOptionFloat        MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(float)  = 1U << 0,
        
};

/*! @enum       MPSNNTrainingStyle
 *  @abstract   Options that control how the MPSNNGraph nodes are trained
 *  @discussion Nodes tha are trainable conform to the MPSNNTrainableNode protocol.
 *              This adds a MPSNNTrainingStyle property to the node that may be used
 *              to influence when and where the neural network training paramers such as
 *              convolution weights are updated. */
#if defined(DOXYGEN)
typedef enum MPSNNTrainingStyle
#else
typedef NS_OPTIONS(NSUInteger, MPSNNTrainingStyle)
#endif
{
    /*! Do not train this node, for example in transfer learning */
    MPSNNTrainingStyleUpdateDeviceNone  MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))  MPS_SWIFT_NAME(UpdateDeviceNone) = 0,

    /*! The weight update pass will be called in a command buffer completion callback, with a nil command buffer */
    MPSNNTrainingStyleUpdateDeviceCPU   MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) = 1,

    /*! The weight update pass will be called immediately after the gradient pass is encoded, with a nonnull command buffer */
    MPSNNTrainingStyleUpdateDeviceGPU   MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) = 2,
}
#if defined(DOXYGEN)
MPSNNTrainingStyle
#endif
;

#if defined(DOXYGEN)
    typedef enum MPSCNNBatchNormalizationFlags
#else
    typedef NS_OPTIONS(NSUInteger, MPSCNNBatchNormalizationFlags)
#endif
    {
        /*! Default Settings */
        MPSCNNBatchNormalizationFlagsDefault  MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))  MPS_SWIFT_NAME(Default) = 0,
        
        /*! Statistics are calculated if another node consumes the gradient node (training). The data source is used otherwise. */
        MPSCNNBatchNormalizationFlagsCalculateStatisticsAutomatic   MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(CalculateStatisticsAutomatic) = MPSCNNBatchNormalizationFlagsDefault,

        /*! Statistics are calculated always */
        MPSCNNBatchNormalizationFlagsCalculateStatisticsAlways      MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) = 1,

        /*! Statistics are never calculated. Predefined values from the data source are used instead */
        MPSCNNBatchNormalizationFlagsCalculateStatisticsNever       MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) = 2,

        /*! Bits used for  MPSCNNBatchNormalizationFlagsCalculateStatistics*/
        MPSCNNBatchNormalizationFlagsCalculateStatisticsMask        MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) = 3,
    }
#if defined(DOXYGEN)
    MPSCNNBatchNormalizationFlags
#endif
    ;

    
/*! @enum       MPSNNPaddingMethod
 *  @abstract   How to pad MPSNNGraph image nodes
 *  @discussion The MPSNNGraph must make automatic decisions about how big to make the result
 *              of each filter node. This is typically determined by a combination of input
 *              image size, size of the filter window (e.g. convolution weights), filter stride,
 *              and a description of how much extra space beyond the edges of the image to allow 
 *              the filter read. By knowing the properties of the filter, we can then infer 
 *              the size of the result image.  Most of this information is known to the MPSNNGraph
 *              as part of its normal operation. However, the amount of padding to add and where
 *              to add it is a matter of choice left to you, the developer. Different neural network
 *              frameworks such as TensorFlow and Caffe make different choices here.  Depending on where
 *              your network was trained, you will need to adjust the policies used by MPS during inference. 
 *              In the event that the padding method is not simply described by this enumeration, you may
 *              provide you own custom policy definition by overriding the -destinationImageDescriptorForSourceImages:
 *              sourceStates:forKernel:suggestedDescriptor: method in a custom MPSNNPadding child 
 *              class.
 *
 *              Common values that influence the size of the result image by adjusting the amount of 
 *              padding added to the source images:
 *                  - MPSNNPaddingMethodSizeValidOnly  Result values are only produced for the area that
 *                                                     is guaranteed to have all of its input values
 *                                                     defined (i.e. not off the edge).  This produces
 *                                                     the smallest result image.
 *                  - MPSNNPaddingMethodSizeSame       The result image is the same size as the input
 *                                                     image. If the stride is not 1, then the result
 *                                                     is scaled accordingly.  
 *                  - MPSNNPaddingMethodSizeFull       Result values are produced for any position for which
 *                                                     at least one input value is defined (i.e. not off
 *                                                     the edge)
 *                  - MPSNNPaddingMethodCustom     The sizing and centering policy is given by the
 *                                                     [MPSNNPadding destinationImageDescriptorForSourceImages:
 *                                                     sourceStates:forKernel:suggestedDescriptor:]
 *
 *              Except possibly when MPSNNPaddingMethodCustom is used, the area within the source 
 *              image that is read will be centered on the source image. Even so, at times the area can
 *              not be perfectly centered because the source image has odd size and the region read
 *              has even size, or vice versa. In such cases, you may use the following values to 
 *              select where to put the extra padding:
 *
 *                  - MPSNNPaddingMethodAddRemainderToTopLeft     Leftover padding is added to the top or left
 *                                                                side of image as appropriate.
 *                  - MPSNNPaddingMethodAddRemainderToBottomRight Leftover padding is added to the bottom or right
 *                                                                side of image as appropriate.
 *
 *              Here again, different external frameworks may use different policies. 
 *
 *              In some cases, Caffe intoduces the notion of a region beyond the padding which is invalid. 
 *              This can happen when the padding is set to a width narrower than what is needed for a destination
 *              size. In such cases, MPSNNPaddingMethodExcludeEdges is used to adjust normalization factors
 *              for filter weights (particularly in pooling) such that invalid regions beyond the padding are 
 *              not counted towards the filter area.  Currently, only pooling supports this feature. Other filters
 *              ignore it.
 *
 *              The MPSNNPaddingMethodSize and a MPSNNPaddingMethodAddRemainder policy always appear together
 *              in the MPSNNPaddingMethod. There is no provision for a MPSNNPaddingMethodSize without a remainder
 *              policy or vice versa. It is in practice used as a bit field.
 *
 *              Most MPSNN filters are considered forward filters. Some (e.g. convolution transpose)
 *              are considered reverse filters. For the reverse filters, the image stride is measured in destination
 *              values rather than source values and has the effect of enlarging the image rather than reducing it.
 *              When a reverse filter is used to "undo" the effects of a forward filter, the MPSNNPaddingMethodSize should
 *              be the opposite of the forward MPSNNPaddingMethod.  For example, if the forward filter used 
 *              MPSNNPaddingMethodSizeValidOnly | MPSNNPaddingMethodAddRemainderToTopLeft, the reverse filter should use
 *              MPSNNPaddingMethodSizeFull | MPSNNPaddingMethodAddRemainderToTopLeft. Some consideration of the geometry 
 *              of inputs and outputs will reveal why this is so. It is usually not important to adjust the centering
 *              method because the size of the reverse result generally doesn't suffer from centering asymmetries. That is:
 *              the size would usually be given by:
 *              @code
 *              static int DestSizeReverse( int sourceSize, int stride, int filterWindowSize, Style style ) {
 *                  return (sourceSize-1) * stride + 1 + style  * (filterWindowSize-1);  // style = {-1,0,1} for valid-only, same, full
 *              }
 *              @endcode
 *              so the result size is exactly the one needed for the source size and there are no centering problems.
 *              In some cases where the reverse pass is intended to completely reverse a forward pass, the MPSState
 *              object produced by the forward pass should be used to determine the size of the reverse pass result
 *              image.
 *
 *              Tensorflow does not appear to provide a full padding method, but instead appears to use its valid-only
 *              padding mode for reverse filters to in effect achieve what is called MPSNNPaddingMethodSizeFull here.
 *
 *              MPSGetPaddingPolicy() is provided as a convenience to make shorter work of MPSNNPaddingMethods and policies.
 *
 *              Walkthrough of operation of padding policy:
 *              ===========================================
 *                  Most MPSCNNKernels have two types of -encode calls. There is one for which you must pass in
 *              a preallocated MPSImage to receive the results.  This is for manual configuration. It assumes you know
 *              what you are doing, and asks you to correctly set a diversity of properties to correctly position image
 *              inputs and size results. It does not use the padding policy. You must size the result correctly, set the
 *              clipRect, offset and other properties as needed yourself.
 *                  Layered on top of that is usually another flavor of -encode call that returns a destination image 
 *              instead from the left hand side of the function. It is designed to automatically configure itself based
 *              on the MPSCNNKernel.paddingPolicy. When this more automated -encode... method is called, it invokes a 
 *              method in the MPSKernel that looks at the MPSNNPaddingMethod bitfield of the policy. Based on the 
 *              information therein and the size of the input images and other filter properties, it determines the size 
 *              of the output, sets the offset property, and returns an appropriate MPSImageDescriptor for the destination 
 *              image.
 *                  If you set the MPSNNPaddingMethodCustom bit in the MPSNNPaddingMethod, then the MPSNNPadding
 *              -destinationImageDescriptorForSourceImages:sourceStates:forKernel:suggestedDescriptor: method
 *              is called. The MPSImageDescriptor prepared earlier is passed in as the last parameter. You can
 *              use this descriptor or modify as needed.  In addition, you can adjust any properties of the MPSKernel
 *              with which it will be used. If, for example, the descriptor is not the right MPSFeatureChannelFormat, you
 *              can change it, or make your own MPSImageDescriptor based on the one handed to you. This is your opportunity
 *              to customize the configuration of the MPSKernel. In some cases (e.g. MPSNNDefaultPadding.paddingForTensorflowAveragePooling
 *              you might change other properties such as the filter edging mode, or adjust the offset that was already
 *              set for you. When the kernel is fully configured, return the MPSImageDescriptor.
 *                  The MPSImageDescriptor is then passed to the MPSCNNKernel.destinationImageAllocator to allocate
 *              the image. You might provide such an allocator if you want to use your own custom MTLHeap rather 
 *              than the MPS internal heap. The allocator can be set either directly in the MPSCNNKernel or through the 
 *              MPSNNImageNode.allocator property. 
 *                  It is intended that most of the time, default values for padding method and destination image allocator
 *              should be good enough. Only minimal additional configuration should be required, apart from occasional
 *              adjustments to set the MPSNNPaddingMethod when something other than default padding for the object is
 *              needed.  If you find yourself encumbered by frequent adjustments of this kind, you might find it 
 *              to your advantage to subclass MPSNNFilterNodes or MPSCNNKernels to adjust the default padding policy and
 *              allocator at initialization time.
 *
 *              tensorFlowSame = MPSNNPaddingMethodAddRemainderToBottomRight | MPSNNPaddingMethodAlignCentered | MPSNNPaddingMethodSizeSame
 *
 *              In some cases, a custom padding policy may be preferable to a custom MPSImageAllocator for adjusting MPSImage
 *              characteristics. The MPS provided MPSImageAllocators are more efficient at allocating image batches than
 *              naive code, and should be used when possible. However, custom padding policies may prevent the MPSNNGraph
 *              from fusing away nodes involving images created by the padding policy because it doesn't know what the
 *              custom padding policy does. If the changes made by the padding policy modify to the MPSImageDescriptor
 *              alone (and not, critically, the MPSKernel properties or image dimensions) then it may be acceptable to pass
 *              MPSNNPaddingMethodCustomWhitelistForNodeFusion, which allows the fusion to proceed even with a custom
 *              padding policy. In usage of MPSNNPaddingMethodCustomWhitelistForNodeFusion, you are guaranteeing to MPS
 *              that it can fuse the node with an adjacent filter node if it can. It makes no further checks.   You can
 *              get a detailed printout of graph optimizations including reasons why they didn't happen by setting the
 *              MPS_LOG_INFO environment variable.
 *
 *              What happens when my node is fused?
 *                  In this case, the image described by the padding policy is never made, the custom padding policy method
 *              -destinationImageDescriptorForSourceImages:sourceStates:forKernel:suggestedDescriptor: is never called
 *              and data is passed directy from one MPSKernel to another without precision loss. This generally only happens
 *              for MPSKernels that do not change the size of the image, for example batch normalization, neuron filters,
 *              and gradients of the same.  For such filters, all padding method size, remainder and alignment options
 *              produce identical results and so can be ignored. Only the custom padding method has the capability of changing
 *              the results, for example by changing the destination size or MPSCNNKernel.offset, which is why it must be
 *              explicitly whitelisted away if fusion is to occur.
 */
    
    
#if defined(DOXYGEN)
    typedef enum MPSNNPaddingMethod
#else
    typedef NS_OPTIONS(NSUInteger, MPSNNPaddingMethod)
#endif
    {
        // The first two bits describe how to position the region read within the source image
        MPSNNPaddingMethodAlignCentered                  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(centered)  = 0,    ///< Extra padding pixels are distributed as evenly as possible to all sides
        MPSNNPaddingMethodAlignTopLeft                   MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 1,    ///< Extra padding pixels appear on top and left sides
        MPSNNPaddingMethodAlignBottomRight               MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 2,    ///< Extra padding pixels appear on the bottom and right sides
        MPSNNPaddingMethodAlign_reserved                 MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 3,    ///< Extra padding pixels are not defined.
        MPSNNPaddingMethodAlignMask = MPSNNPaddingMethodAlign_reserved,

        
        // The next two bits describe which sides the extra padding goes in the case where the total amount of padding
        // in a dimension is not an even number and the alignment is centered. This typically happens when the filter window size is even.
        MPSNNPaddingMethodAddRemainderToTopLeft          MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(topLeft) = 0UL << 2,     ///< Extra padding pixels are accumulated to top and left sides
        MPSNNPaddingMethodAddRemainderToTopRight         MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 1UL << 2,
        MPSNNPaddingMethodAddRemainderToBottomLeft       MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 2UL << 2,
        MPSNNPaddingMethodAddRemainderToBottomRight      MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 3UL << 2,     ///< Extra padding pixels are accumulated to bottom and right sides
        MPSNNPaddingMethodAddRemainderToMask = MPSNNPaddingMethodAddRemainderToBottomRight,
        
        // The next thirteen bits describe the size of the padding area
        MPSNNPaddingMethodSizeValidOnly                 MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(validOnly) = 0,               ///< The result is the largest image for which *all* source pixels are valid for result pixels
        MPSNNPaddingMethodSizeSame                      MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 1UL << 4,        ///< The result is the same size as the input image (before strides)
        MPSNNPaddingMethodSizeFull                      MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 2UL << 4,        ///< The result is the largest image for which *any* source pixel is valid for result pixels
        MPSNNPaddingMethodSize_reserved                 MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 3UL << 4,
        MPSNNPaddingMethodCustomWhitelistForNodeFusion  MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))  = (1UL << 13),   ///< By itself, MPSNNPaddingMethodCustom will inhibit automatic fusion between nodes producing and consuming the image described by the padding policy. MPSNNPaddingMethodCustomWhitelistForNodeFusion signals that the custom method is benign and fusion may go ahead.
        MPSNNPaddingMethodCustom                        MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = (1UL << 14),   ///< Use destinationImageDescriptorForSourceImages:sourceStates:forKernel:suggestedDescriptor: to calculate padding and offset.
        MPSNNPaddingMethodSizeMask                      MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = 0x7f0,

        /*! The caffe framework constrains the average pooling area to the limits of the padding area in cases
         * where a pixel would read beyond the padding area. Set this bit for Caffe emulation with average pooling.
         */
        MPSNNPaddingMethodExcludeEdges     MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))  = (1UL << 15),
    }
#if defined(DOXYGEN)
    MPSNNPaddingMethod
#endif
;

    
/*! @protocol   MPSNNPadding
 *  @abstract   A method to describe how MPSCNNKernels should pad images when data outside the image is needed
 *  @discussion Different (non-Apple) CNN frameworks have different policies for how to size the result
 *              of a CNN filter and what padding to add around the edges.  Some filters such
 *              as pooling and convolution read from neighboring feature channel (pixel) values.
 *              Four predefined MPSPaddingMethods are available: MPSNNPaddingMethodValidOnly,
 *              MPSNNPaddingMethodFull, MPSNNPaddingMethodSameTL, MPSNNPaddingMethodSameBR. You
 *              may also implement your own padding definition with a block that conforms
 *              to this prototype.
 */
@protocol MPSNNPadding <NSObject, NSSecureCoding>
    
    @required
    /*! @abstract   Get the preferred padding method for the node */
    -(MPSNNPaddingMethod) paddingMethod;
    
    @optional
    /*! A human readable string that describes the padding policy. Useful for verbose debugging support. */
    -(NSString*__nonnull) label;        // FIXME:  to be made @required
    
    /*! @abstract       Determine padding and sizing of result images
     *  @discussion     A MPSNNPaddingMethod must both return a valid MPSImageDescriptor
     *                  and set the MPSKernel.offset to the correct value.  This is a 
     *                  required feature if the MPSNNPaddingMethodCustom bit is set in
     *                  the paddingMethod.
     *
     *                  Some code that may prove helpful:
     *
     *                  @code
     *                  const int centeringPolicy = 0;  // When kernelSize is even: 0 pad bottom right. 1 pad top left.    Centers the kernel for even sized kernels.
     *
     *                  typedef enum Style{
     *                      StyleValidOnly = -1,
     *                      StyleSame = 0,
     *                      StyleFull = 1
     *                  }Style;
     *
     *                  // Typical destination size in one dimension for forward filters (most filters)
     *                  static int DestSize( int sourceSize, int stride, int filterWindowSize, Style style ){
     *                      sourceSize += style * (filterWindowSize - 1);       // adjust how many pixels we are allowed to read
     *                      return (sourceSize + stride - 1) / stride;          // sourceSize / stride, round up
     *                  }
     *
     *                  // Typical destination size in one dimension for reverse filters (e.g. convolution transpose)
     *                  static int DestSizeReverse( int sourceSize, int stride, int filterWindowSize, Style style ){
     *                      return (sourceSize-1) * stride +        // center tap for the last N-1 results. Take stride into account
     *                              1 +                             // center tap for the first result
     *                              style * (filterWindowSize-1);   // add or subtract (or ignore) the filter extent
     *                  }
     *
     *                  // Find the MPSOffset in one dimension
     *                  static int Offset( int sourceSize, int stride, int filterWindowSize, Style style ){
     *                      // The correction needed to adjust from position of left edge to center per MPSOffset definition
     *                      int correction = filterWindowSize / 2;
     *
     *                      // exit if all we want is to start consuming pixels at the left edge of the image.
     *                      if( 0 )
     *                          return correction;
     *
     *                      // Center the area consumed in the source image:
     *                      // Calculate the size of the destination image
     *                      int destSize = DestSize( sourceSize, stride, filterWindowSize, style ); // use DestSizeReverse here instead as appropriate
     *
     *                      // calculate extent of pixels we need to read in source to populate the destination
     *                      int readSize = (destSize-1) * stride + filterWindowSize;
     *
     *                      // calculate number of missing pixels in source
     *                      int extraSize = readSize - sourceSize;
     *
     *                      // number of missing pixels on left side
     *                      int leftExtraPixels = (extraSize + centeringPolicy) / 2;
     *
     *                      // account for the fact that the offset is based on the center pixel, not the left edge
     *                      return correction - leftExtraPixels;
     *                  }
     *                  @endcode
     *
     *  @param          sourceImages        The list of source images to be used
     *  @param          sourceStates        The list of source states to be used
     *  @param          kernel              The MPSKernel the padding method will be applied to. Set the kernel.offset
     *  @param          inDescriptor        MPS will prepare a starting guess based on the padding policy (exclusive of
     *                                      MPSNNPaddingMethodCustom) set for the object. You should adjust the offset 
     *                                      and image size accordingly. It is on an autoreleasepool.
     *
     *  @return         The MPSImageDescriptor to use to make a MPSImage to capture the results from the filter. 
     *                  The MPSImageDescriptor is assumed to be on an autoreleasepool. Your method must also set the
     *                  kernel.offset property.
     */
    -(MPSImageDescriptor * __nonnull) destinationImageDescriptorForSourceImages: (NSArray <MPSImage *> *__nonnull) sourceImages
                                                                   sourceStates: (NSArray <MPSState *> * __nullable) sourceStates
                                                                      forKernel: (MPSKernel * __nonnull) kernel
                                                            suggestedDescriptor: (MPSImageDescriptor * __nonnull) inDescriptor;

    /*! @abstract   Make a "inverted" padding policy suitable for a training gradient pass.  */
    -(instancetype __nullable) inverse MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

@end

/*! @abstract   This class provides some pre-rolled padding policies for common tasks
 *  @discussion You are, of course, welcome to write your own class that conforms to
 *              The MPSNNPadding protocol and use that instead.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSNNDefaultPadding : NSObject <MPSNNPadding>

/*! @abstract   Fetch a well known object that implements a non-custom padding method
 *  @discussion For custom padding methods, you will need to implement an object that conforms
 *              to the full MPSNNPadding protocol, including NSSecureCoding.
 *  @param      method  A MPSNNPaddingMethod
 *  @return     An object that implements <MPSNNPadding> for use with MPSNNGraphNodes.
 */
+ (instancetype __nonnull) paddingWithMethod: (MPSNNPaddingMethod) method;

/*! @abstract       A padding policy that attempts to reproduce TensorFlow behavior for average pooling
 *  @discussion     Most TensorFlow padding is covered by the standard MPSNNPaddingMethod encodings.
 *                  You can use +paddingWithMethod to get quick access to MPSNNPadding objects, when
 *                  default filter behavior isn't enough. (It often is.)  However, the edging for
 *                  max pooling in TensorFlow is a bit unusual.
 *
 *                  This padding method attempts to reproduce TensorFlow padding for average pooling.
 *                  In addition to setting MPSNNPaddingMethodSizeSame | MPSNNPaddingMethodAlignCentered |
 *                  MPSNNPaddingMethodAddRemainderToBottomRight, it also configures the filter to run with
 *                  MPSImageEdgeModeClamp, which (as a special case for average pooling only), normalizes the
 *                  sum of contributing samples to the area of valid contributing pixels only.
 *
 *                  @code
 *                      // Sample implementation for the tensorflowPoolingPaddingPolicy returned
 *                       -(MPSNNPaddingMethod) paddingMethod{ return MPSNNPaddingMethodCustom | MPSNNPaddingMethodSizeSame; }
 *
 *                       -(MPSImageDescriptor * __nonnull) destinationImageDescriptorForSourceImages: (NSArray <MPSImage *> *__nonnull) sourceImages
 *                                                                                      sourceStates: (NSArray <MPSState *> * __nullable) sourceStates
 *                                                                                         forKernel: (MPSKernel * __nonnull) kernel
 *                                                                               suggestedDescriptor: (MPSImageDescriptor * __nonnull) inDescriptor
 *                       {
 *
 *                          ((MPSCNNKernel *)kernel).edgeMode = MPSImageEdgeModeClamp;
 *
 *                          return inDescriptor;
 *                       }
 *                  @endcode
 */
+ (instancetype __nonnull) paddingForTensorflowAveragePooling MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));  // same size centering mode

/*! @abstract Typical pooling padding policy for valid only mode */
+ (instancetype __nonnull) paddingForTensorflowAveragePoolingValidOnly  MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract  Human readable description of what the padding policy does */
-(NSString * __nonnull) label;

@end
   
/*! @abstract   MPSStates conforming to this protocol contain information about a image size elsewhere in the graph
 *  @discussion In some graphs a sequence of operations are done, then they are undone ins a series of 'reverse' 
 *              operations. Examples might be pooling vs pooling gradient / upsampling,  or convolution vs. convolution transpose.
 *              In such cases, the 'reverse' pass generally is converting from a smaller image to a larger image,
 *              and there is insufficient information to do this correctly. Several answers exist and we don't know
 *              which is correct.
 *
 *              As an example, consider trying to 'undo' integer division with a multiplication. The expression c = a/b 
 *              is incomplete because there is also a remainder, which may constitute information lost. If we want to 
 *              reconstitute a based on c and b, we need to use a = c * b + remainder, not just a = c*b.  Similarly, when
 *              undoing a downsizing operation, we need the original size to find which answer in the range of
 *              a = c*b + [0,b-1] is the right one. */
@protocol MPSImageSizeEncodingState <NSObject>
    /*! @abstract   The width of the source image passed to MPSCNNConvolution encode call. */
    @property(readonly, nonatomic) NSUInteger       sourceWidth;
    
    /*! @abstract   The height of the source image passed to MPSCNNConvolution encode call. */
    @property(readonly, nonatomic) NSUInteger       sourceHeight;
    
@end
    
#ifdef __cplusplus
}
#endif

#endif /* MPSNeuralNetworkTypes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNSoftMax.h
//
//  MPSCNNSoftMax.h
//  MPS
//
//  Created by Ian Ollmann on 8/21/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNSoftMax_h
#define MPSCNNSoftMax_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif

#pragma mark - MPSCNNSoftMax


/*!
 *  @class      MPSCNNSoftMax
 *  @dependency This depends on Metal.framework
 *  @discussion The softMax filter is a neural transfer function and is useful for classification tasks.
 *              The softMax filter is applied across feature channels and in a convolutional manner at all
 *              spatial locations. The softMax filter can be seen as the combination of an
 *              activation function (exponential) and a normalization operator.
 *              For each feature channel per pixel in an image in a feature map, the softMax filter computes the following:
 *                  result channel in pixel = exp(pixel(x,y,k))/sum(exp(pixel(x,y,0)) ... exp(pixel(x,y,N-1))
 *                      where N is the number of feature channels
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNSoftMax : MPSCNNKernel

@end    /* MPSCNNSoftMax */

/*!
 *  @class      MPSCNNSoftMaxGradient
 *  @dependency This depends on Metal.framework
 *  @discussion The softMax gradient filter calculates the gradient to be backpropagated.
 *              The softMax gradient just as the softMax filter, is applied across feature channels and at all spatial locations.
 *              It computes the gradient for a given output generated by the corresponding softMax (i.e. MPSCNNSoftMax) layer and
 *              the gradient computed by the previous layer in the back-propagation pass.
 *              For each feature channel in an image in a feature map, the softMax gradient filter computes the following:
 *                  result gradient channel in pixel
 *                      outputGradient(x,y,k) = softMax(x,y,k) * (inputGradient(x,y,k) -
 *                                               sum(inputGradient(x,y,0) * softMax(x,y,0) ... inputGradient(x,y,N-1) * softMax(x,y,N-1)))
 *                      where N is the number of feature channels
 *
 *              The incoming gradient is the primary source.
 *              The original output of corresponding softMax is the secondary source.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNSoftMaxGradient : MPSCNNGradientKernel

/*!
 *  @abstract   Initializes a MPSCNNSoftMaxGradient function
 *  @param      device                          The MTLDevice on which this MPSCNNSoftMaxGradient filter will be used
 *
 *  @return     A valid MPSCNNSoftMaxGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNSoftMaxGradient */

#pragma mark - MPSCNNLogSoftMax

/*!
 *  @class      MPSCNNLogSoftMax
 *  @dependency This depends on Metal.framework
 *  @discussion The logarithmic softMax filter can be achieved by taking the natural logarithm of the
 *              the result of the softMax filter. The results are often used to construct a loss function to be
 *              minimized when training neural networks.
 *              For each feature channel per pixel in an image in a feature map, the logarithmic softMax filter
 *              computes the following:
 *                  result channel in pixel = pixel(x,y,k)) - ln{sum(exp(pixel(x,y,0)) ... exp(pixel(x,y,N-1))}
 *                      where N is the number of feature channels and y = ln{x} satisfies e^y = x.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNLogSoftMax : MPSCNNKernel

@end    /* MPSCNNLogSoftMax */


/*!
 *  @class      MPSCNNLogSoftMaxGradient
 *  @dependency This depends on Metal.framework
 *  @discussion The logSoftMax gradient filter calculates the gradient to be backpropagated.
 *              The logSoftMax gradient just as the log softMax filter, is applied across feature channels and at all spatial locations.
 *              It computes the gradient for a given output generated by the corresponding logSoftMax (i.e. MPSCNNLogSoftMax) layer and
 *              the gradient computed by the previous layer in the back-propagation pass.
 *              For each feature channel per pixel in an image in a feature map, the logSoftMax gradient filter computes the following:
 *                  result gradient channel in pixel
 *                      outputGradient(x,y,k) = inputGradient(x,y,k) - exp(logSoftMax(x,y,k)) * sum(inputGradient(x,y,0) ... inputGradient(x,y,N-1))
 *                      where N is the number of feature channels
 *
 *              The incoming gradient is the primary source.
 *              The original output of corresponding logSoftMax is the secondary source.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLogSoftMaxGradient : MPSCNNGradientKernel

/*!
 *  @abstract   Initializes a MPSCNNLogSoftMaxGradient function
 *  @param      device                          The MTLDevice on which this MPSCNNLogSoftMaxGradient filter will be used
 *
 *  @return     A valid MPSCNNLogSoftMaxGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNLogSoftMaxGradient */

#ifdef __cplusplus
}
#endif

    
#endif /* MPSCNNSoftMax_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNResize.h
//
//  MPSResize.h
//  MPS
//
//  Copyright © 2018 Apple. All rights reserved.
//

#ifndef MPSNNResize_h
#define MPSNNResize_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif
    
#pragma mark -
#pragma mark MPSNNResizeBilinear

/*!
 *  @class      MPSNNResizeBilinear
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSNNResizeBilinear filter resizes the source image  using bilinear interpolation to
 *              a destination whose dimensions are given by resizeWidth and resizeHeight
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSNNResizeBilinear : MPSCNNKernel

/*! @property   resizeWidth
 *  @abstract   The resize width.
 */
@property(readonly, nonatomic) NSUInteger   resizeWidth;

/*! @property   resizeHeight
 *  @abstract   The resize height.
 */
@property(readonly, nonatomic) NSUInteger   resizeHeight;

/*! @property   alignCorners
 *  @abstract   If YES, the centers of the 4 corner pixels of the input and output regions are aligned,
 *              preserving the values at the corner pixels.
 *              The default is NO.
 */
@property(readonly, nonatomic) BOOL      alignCorners;

/*
 * You must use initWithDevice:resizeWidth:resizeHeight instead.
 * You must use one of the sub-classes of MPSCNNUpsampling.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract  Initialize the resize bilinear filter.
 *  @param     device                   The device the filter will run on.
 *  @param     resizeWidth              The destination resize width in pixels
 *  @param     resizeHeight             The destination resize height in pixels
 *  @param     alignCorners             Specifier whether the centers of the 4 corner pixels of the input and output regions are aligned,
 *                                      preserving the values at the corner pixels.
 *  @return    A valid MPSNNResizeBilinear object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           resizeWidth: (NSUInteger) resizeWidth
                          resizeHeight: (NSUInteger) resizeHeight
                          alignCorners: (BOOL) alignCorners NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSNNResizeBilinear
 *  @param      device      The MTLDevice on which to make the MPSNNResizeBilinear
 *  @return     A new MPSNNResizeBilinear object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


@end /* MPSNNResizeBilinear */
    

/*!
 *  @class      MPSNNCropAndResizeBilinear
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSNNCropAndResizeBilinear filter resizes the source image  using bilinear interpolation to
 *              a destination whose dimensions are given by resizeWidth and resizeHeight
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSNNCropAndResizeBilinear : MPSCNNKernel

/*! @property   resizeWidth
 *  @abstract   The resize width.
 */
@property(readonly, nonatomic) NSUInteger   resizeWidth;

/*! @property   resizeHeight
 *  @abstract   The resize height.
 */
@property(readonly, nonatomic) NSUInteger   resizeHeight;

/*! @property   numberOfRegions
 *  @abstract   the number of bounding box i.e. regions to resize.
 */
@property(readonly, nonatomic) NSUInteger   numberOfRegions;

/*! @property   regions
 *  @abstract   This is a pointer to "numberOfRegions" boxes which specify the locations in the
 *              source image to use for each box/region to perform the resize operation.
 *              The coordinates specified are normalized values.  A normalized region outside the
 *              [0, 1] range is allowed, in which case we use extrapolation_value to extrapolate
 *              the input image values.
 */
@property (readonly, nonatomic, nonnull) const MPSRegion *regions;

/*
 * You must use initWithDevice:resizeWidth:resizeHeight instead.
 * You must use one of the sub-classes of MPSCNNUpsampling.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract  Initialize the crop and resize bilinear filter.
 *  @param     device                   The device the filter will run on.
 *  @param     resizeWidth              The destination resize width in pixels
 *  @param     resizeHeight             The destination resize height in pixels
 *  @param     numberOfRegions          Specifies the number of bounding box i.e. regions to resize
 *  @param     regions                  This is a pointer to "numberOfRegions" boxes which specify the locations in the
 *                                      source image to use for each box/region to perform the resize operation.
 *  @return    A valid MPSNNCropAndResizeBilinear object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           resizeWidth: (NSUInteger) resizeWidth
                          resizeHeight: (NSUInteger) resizeHeight
                       numberOfRegions: (NSUInteger) numberOfRegions
                               regions: (const MPSRegion * __nonnull) regions NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSNNCropAndResizeBilinear
 *  @param      device      The MTLDevice on which to make the MPSNNCropAndResizeBilinear
 *  @return     A new MPSNNResizeBilinear object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


@end /* MPSNNCropAndResizeBilinear */

#ifdef __cplusplus
}
#endif

#endif /* MPSNNResize_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNBatchNormalization.h
//
//  MPSCNNBatchNormalization.h
//  MPS
//
//  Created by Justin Voo on 10/23/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNBatchNormalization_h
#define MPSCNNBatchNormalization_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>
#include <MPSNeuralNetwork/MPSCNNNormalizationWeights.h>
#include <MPSNeuralNetwork/MPSCNNNeuron.h>
#include <MPSCore/MPSState.h>

#ifdef __cplusplus
extern "C" {
#endif // __cplusplus
    
@class MPSNNFilterNode;
@class MPSState;
@class MPSCNNBatchNormalization;
    
/*!
 *  @class      MPSCNNBatchNormalizationState
 *  @discussion MPSCNNBatchNormalizationState encapsulates the data necessary
 *              to execute batch normalization.
 *
 *              MPSCNNBatchNormalizationState cannot initialize the size of its own
 *              underlying resources.  Use [MPSCNNBatchNormalizationStatistics resultStateForSourceImages:]
 *              or [MPSCNNBatchNormalizationStatistics temporaryResultStateForCommandBuffer:sourceImages:].
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNBatchNormalizationState : MPSNNGradientState

@property (readonly, nonnull, retain, nonatomic) MPSCNNBatchNormalization * batchNormalization;

/*!
 *  Unavailable.  Use MPSCNNBatchNormalizationStatistics methods to initialize the state object.
 */
-(nonnull instancetype) initWithResource: (__nullable id<MTLResource>) resource NS_UNAVAILABLE;

/*!
 *  Unavailable.  Use MPSCNNBatchNormalizationStatistics methods to create the temporary state object.
 */
+(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                             bufferSize: (size_t) bufferSize NS_UNAVAILABLE;

/*
 *  Unavailable.  Use MPSCNNBatchNormalizationStatistics methods to create the temporary state object.
 */
+(__nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) cmdBuf
                                        textureDescriptor: (MTLTextureDescriptor* __nonnull) descriptor NS_UNAVAILABLE;

/*!
 *  @abstract   Reset any accumulated state data to its initial values.
 */
-(void) reset;

/*!
 *  @abstract   Return an MTLBuffer object with the state's current gamma values.
 */
-(nullable id<MTLBuffer>) gamma;

/*!
 *  @abstract   Return an MTLBuffer object with the state's current beta values..
 */
-(nullable id<MTLBuffer>) beta;

/*!
 *  @abstract   Return an MTLBuffer object with the most recently computed batch mean values.
 */
-(nullable id<MTLBuffer>) mean;

/*!
 *  @abstract   Return an MTLBuffer object with the most recently computed batch variance values.
 */
-(nullable id<MTLBuffer>) variance;

/*!
 *  @abstract   Return an MTLBuffer object containing the values of the gradient of the loss function
 *              with respect to the scale factors.  If a MPSCNNBatchNormalizationGradient kernel
 *              has not successfully generated these values nil will be returned.
 */
-(nullable id<MTLBuffer>) gradientForGamma;

/*!
 *  @abstract   Return an MTLBuffer object containing the values of the gradient of the loss function
 *              with respect to the bias terms.  If a MPSCNNBatchNormalizationGradient kernel
 *              has not successfully generated these values nil will be returned.
 */
-(nullable id<MTLBuffer>) gradientForBeta;

@end    // MPSCNNBatchNormalizationState
    
/*!
 *  @class  MPSCNNNormalizationMeanAndVarianceState
 *  @description A state which contains mean and variance terms used to apply a
 *               normalization in a MPSCNNBatchNormalization operation.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSCNNNormalizationMeanAndVarianceState : MPSState

/*! @property   mean
 *  @abstract   A MTLBuffer containing the mean terms.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> mean;

/*! @property   variance
 *  @abstract   A MTLBuffer containing the variance terms.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> variance;

/*!
 *  @abstract   Initialize a MPSCNNNormalizationMeanAndVarianceState object using values
 *              contained in MTLBuffers.
 *
 *  @param      mean        The MTLBuffer containing mean terms.
 *
 *  @param      variance    The MTLBuffer containing variance terms.
 */
- (nonnull instancetype) initWithMean: (__nonnull id<MTLBuffer>) mean
                             variance: (__nonnull id<MTLBuffer>) variance;

/*!
 *  @abstract   Create a temporary MPSCNNNormalizationMeanAndVarianceState suitable
 *              for a normalization operation on images containing no more than
 *              the specified number of feature channels.
 *
 *  @param      commandBuffer           The command buffer on which the temporary state will
 *                                      be used.
 *
 *  @param      numberOfFeatureChannels The number of feature channels used to size the
 *                                      state.
 */
+ (nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                 numberOfFeatureChannels: (NSUInteger) numberOfFeatureChannels;

@end    // MPSCNNNormalizationMeanAndVarianceState
   
#pragma mark -
#pragma mark MPSCNNBatchNormalizationDataSource
    
/*! @protocol   MPSCNNBatchNormalizationDataSource
 *  @abstract   The MPSCNNBatchNormalizationDataSource protocol declares the methods that an
 *              instance of MPSCNNBatchNormalizationState uses to initialize the
 *              scale factors, bias terms, and batch statistics.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@protocol MPSCNNBatchNormalizationDataSource <NSObject, NSCopying>

@required
    
    /*! @abstract   Returns the number of feature channels within images to be normalized
     *              using the supplied parameters.
     */
    -(NSUInteger) numberOfFeatureChannels;
    
    /*! @abstract   Returns a pointer to the scale factors for the batch normalization.
     */
    -(float * __nullable) gamma;
 
    /*! @abstract   Returns a pointer to the bias terms for the batch normalization.
     *              If NULL then no bias is to be applied.
     */
    -(float * __nullable) beta;

    /*! @abstract   Returns a pointer to batch mean values with which to initialize
     *              the state for a subsequent batch normalization.
     */
    -(float * __nullable) mean;
    
    /*! @abstract   Returns a pointer to batch variance values with which to initialize
     *              the state for a subsequent batch normalization.
     */
    -(float * __nullable) variance;

    /*! @abstract   Alerts the data source that the data will be needed soon
     *  @discussion Each load alert will be balanced by a purge later, when MPS
     *              no longer needs the data from this object.
     *              Load will always be called atleast once after initial construction
     *              or each purge of the object before anything else is called.
     *  @return     Returns YES on success.  If NO is returned, expect MPS
     *              object construction to fail.
     */
    -(BOOL) load;
    
    /*! @abstract   Alerts the data source that the data is no longer needed
     *  @discussion Each load alert will be balanced by a purge later, when MPS
     *              no longer needs the data from this object.
     */
    -(void) purge;
    
    /*! @abstract   A label that is transferred to the batch normalization filter at init time
     *  @discussion Overridden by a MPSCNNBatchNormalizationNode.label if it is non-nil.
     */
    -(NSString* __nullable) label;
   
@optional
    /*! @abstract       Compute new gamma and beta values using current values and gradients contained within a
     *                  MPSCNNBatchNormalizationState.  Perform the update using a GPU.
     *  @discussion     This operation is expected to also decrement the read count of batchNormalizationState by 1.
     *
     *  @param          commandBuffer               The command buffer on which to encode the update.
     *
     *  @param          batchNormalizationState     The MPSCNNBatchNormalizationState object containing the current gamma and
     *                                              beta values and the gradient values.
     *
     *  @return         A MPSCNNNormalizationMeanAndVarianceState object containing updated mean and variance values.  If NULL, the MPSNNGraph
     *                  batch normalization filter gamma and beta values will remain unmodified.
     */
    -(MPSCNNNormalizationGammaAndBetaState * __nullable) updateGammaAndBetaWithCommandBuffer: (nonnull id<MTLCommandBuffer>) commandBuffer
                                                              batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState;
    
    /*! @abstract       Compute new mean and variance values using current batch statistics contained within a
     *                  MPSCNNBatchNormalizationState.  Perform the update using a GPU.
     *  @discussion     This operation is expected to also decrement the read count of batchNormalizationState by 1.
     *
     *  @param          commandBuffer               The command buffer on which to encode the update.
     *
     *  @param          batchNormalizationState     The MPSCNNBatchNormalizationState object containing the current batch statistics.
     *
     *  @return         A MPSCNNNormalizationMeanAndVarianceState object containing updated mean and variance values.  If NULL, the MPSNNGraph
     *                  batch normalization filter mean and variance values will remain unmodified.
     */
    -(MPSCNNNormalizationMeanAndVarianceState * __nullable) updateMeanAndVarianceWithCommandBuffer: (nonnull id<MTLCommandBuffer>) commandBuffer
                                                                            batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
    MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));
    
    /*! @abstract       Compute new gamma and beta values using current values and gradients contained within a
     *                  MPSCNNBatchNormalizationState.  Perform the update using the CPU.
     *
     *  @param          batchNormalizationState     The MPSCNNBatchNormalizationState object containing the current gamma and
     *                                              beta values and the gradient values.
     *
     *  @return         A boolean value indicating if the update was performed.
     */
    -(BOOL) updateGammaAndBetaWithBatchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState;
    
    /*! @abstract       Compute new mean and variance values using current batch statistics contained within a
     *                  MPSCNNBatchNormalizationState.  Perform the update using the CPU.
     *
     *  @param          batchNormalizationState     The MPSCNNBatchNormalizationState object containing the current batch statistics.
     *
     *  @return         A boolean value indicating if the update was performed.
     */
    -(BOOL) updateMeanAndVarianceWithBatchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
    MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));
    
    /*! @abstract       An optional tiny number to use to maintain numerical stability.
     *  @discussion     output_image = (input_image - mean[c]) * gamma[c] / sqrt(variance[c] + epsilon) + beta[c];
     *                  Defalt value if method unavailable: FLT_MIN   */
    -(float) epsilon;
    
    /*! @abstract       NSSecureCoding compatibility.
     */
    - (void)encodeWithCoder:(NSCoder * __nonnull)aCoder;
    
    /*! @abstract       NSSecureCoding compatibility.
     */
    - (nullable instancetype)initWithCoder:(NSCoder * __nonnull)aDecoder;
    
    /*! @abstract       NSSecureCoding compatibility.
     */
    @property (class, readonly) BOOL supportsSecureCoding;
    
    /*!
     *  @abstract   Optional copy method to create a copy of the data source for use with a new device.
     *
     *  @param      zone    The NSZone on which to allocate.
     *  @param      device  The device where the kernel which uses this data source will be used.
     *
     *  @result     A pointer to a copy of this data source.
     */
    - (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                                   device:(nullable id <MTLDevice>) device MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));
    
@end    // MPSCNNBatchNormalizationDataSource
    
/*!
 *  @class      MPSCNNBatchNormalization
 *  @dependency This depends on Metal.framework
 *  @discussion MPSCNNBatchNormalization normalizes input images using per-channel
 *              means and variances.
 *
 *              for (c = 0; c < numberOfFeatureChannels; ++c)
 *              {
 *                  input_image = in(:,:,c,:);
 *                  output_image = (input_image - mean[c]) * gamma[c] / sqrt(variance[c] + epsilon) + beta[c];
 *                  out(:,:,c,:) = output_image;
 *              }
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNBatchNormalization : MPSCNNKernel

/*! @property   numberOfFeatureChannels
 *  @abstract   The number of feature channels in an image to be normalized.
 */
@property(readonly, nonatomic) NSUInteger       numberOfFeatureChannels;

/*! @property   epsilon
 *  @abstract   The epsilon value used in the batch normalization formula to
 *              bias the variance when normalizing.
 */
@property(readwrite, nonatomic) float       epsilon;

/*! @abstract   The data source the batch normalization was initialized with */
@property (readonly, nonatomic, retain, nonnull) id <MPSCNNBatchNormalizationDataSource> dataSource;

/*!
 *  @abstract   Initializes a batch normalization kernel using a data source.
 *  @param      device                          The MTLDevice on which this filter will be used
 *  @param      dataSource                      A pointer to a object that conforms to the MPSCNNBatchNormalizationDataSource
 *                                              protocol.  The data source provides filter weights and bias terms and, optionally,
 *                                              image statistics which may be used to perform the normalization.
 *
 *  @return     A valid MPSCNNBatchNormalization object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            dataSource: (nonnull id <MPSCNNBatchNormalizationDataSource>) dataSource;

/*!
 *  @abstract   Initializes a batch normalization kernel using a data source and a neuron descriptor.
 *  @param      device                          The MTLDevice on which this filter will be used
 *  @param      dataSource                      A pointer to a object that conforms to the MPSCNNBatchNormalizationDataSource
 *                                              protocol.  The data source provides filter weights and bias terms and, optionally,
 *                                              image statistics which may be used to perform the normalization.
 *  @param      fusedNeuronDescriptor           A MPSNNNeuronDescriptor object which specifies a neuron activation function to
 *                                              be applied to the result of the batch normalization.
 *
 *  @return     A valid MPSCNNBatchNormalization object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            dataSource: (nonnull id <MPSCNNBatchNormalizationDataSource>) dataSource
                 fusedNeuronDescriptor: (MPSNNNeuronDescriptor* __nullable) fusedNeuronDescriptor NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*!
 * Use initWithDevice:dataSource instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use a subclass of NSCoder that
 *              implements the <MPSDeviceProvider> protocol  to
 *              tell MPS the MTLDevice to use.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSCNNBatchNormalization object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode this kernel to a command buffer for a single image using
 *              a batch normalization state.
 *
 *  @param      commandBuffer               A valid command buffer to receive the kernel.
 *  @param      sourceImage                 The source MPSImage.
 *  @param      batchNormalizationState     A MPSCNNBatchNormalizationState containing weights and/or
 *                                          statistics to use for the batch normalization. If the state
 *                                          is temporary its read count will be decremented.
 *  @param      destinationImage            An MPSImage to contain the resulting normalized and scaled
 *                                          image.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
      batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
             destinationImage: (MPSImage * __nonnull) destinationImage;

/*!
 *  @abstract   Encode this kernel to a command buffer for a batch of images using
 *              a batch normalization state.
 *
 *  @param      commandBuffer               A valid command buffer to receive the kernel.
 *  @param      sourceImages                The batch of source images.
 *  @param      batchNormalizationState     A MPSCNNBatchNormalizationState containing weights and/or
 *                                          statistics to use for the batch normalization. If the state
 *                                          is temporary its read count will be decremented.
 *  @param      destinationImages           The batch of images to contain the normalized and scaled
 *                                          result images.
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
           batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages;

/*
 *  Unavailable.  Destination states not supported at encode time.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
             destinationState: (MPSState * __nonnull) destinationState
             destinationImage: (MPSImage * __nonnull) destinationImage NS_UNAVAILABLE;

/*
 *  Unavailable.  Destination states not supported at encode time.
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                  sourceImage: (MPSImage *  __nonnull) sourceImage
                             destinationState: (__autoreleasing MPSState * __nullable * __nonnull) outState
                  destinationStateIsTemporary: (BOOL) isTemporary NS_UNAVAILABLE;

/*
 *  Unavailable.  DestinationStates not supported at encode time.
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
                 destinationStates: (MPSStateBatch * __nullable) destinationStates
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages NS_UNAVAILABLE;

/*
 *  Unavailable.  Destination states not supported at encode time.
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                           sourceImages: (MPSImageBatch *  __nonnull) sourceImages
                                      destinationStates: (__autoreleasing MPSStateBatch * __nullable * __nonnull) outStates
                            destinationStateIsTemporary: (BOOL) isTemporary NS_UNAVAILABLE;

/*! @abstract       Return an MPSCNNBatchNormalizationState object which may be used with a MPSCNNBatchNormalization filter.
 */
-(MPSCNNBatchNormalizationState * __nullable) resultStateForSourceImage: (MPSImage *__nonnull) sourceImage
                                                           sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                       destinationImage: (MPSImage *__nonnull) destinationImage;

/*! @abstract       Return a temporary MPSCNNBatchNormalizationState object which may be used with
 *                  a MPSCNNBatchNormalization filter.
 */
-(MPSCNNBatchNormalizationState * __nullable) temporaryResultStateForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                                       sourceImage: (MPSImage *__nonnull) sourceImage
                                                                      sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                                  destinationImage: (MPSImage *__nonnull) destinationImage;
/*!
 *  @abstract   Reinitialize the filter using a data source.
 *
 *  @param      dataSource  The data source which will provide the weights and, optionally, the
 *                          image batch statistics with which to normalize.
 */
-(void) reloadDataSource: (__nonnull id<MPSCNNBatchNormalizationDataSource>) dataSource
MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use -reloadGammaAndBetaFromDataSource and/or -relaodMeanAndVarianceFromDataSource instead.",
                                      macos(10.13.4, 10.14), ios(11.3,12.0), tvos(11.3, 12.0));

/*!
 *  @abstract   Reinitialize the filter's gamma and beta values using the data source provided at kernel initialization.
 */
-(void) reloadGammaAndBetaFromDataSource MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*!
 *  @abstract   Reinitialize the filter's mean and variance values using the data source provided at kernel initialization.
 */
-(void) reloadMeanAndVarianceFromDataSource MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*!
 *  @abstract   Reload data using new gamma and beta terms contained within an
 *              MPSCNNNormalizationGammaAndBetaState object.
 *
 *  @param      commandBuffer               The command buffer on which to encode the reload.
 *
 *  @param      gammaAndBetaState           The state containing the updated weights which are to
 *                                          be reloaded.
 */
-(void) reloadGammaAndBetaWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                          gammaAndBetaState: (MPSCNNNormalizationGammaAndBetaState* __nonnull) gammaAndBetaState;

/*!
 *  @abstract   Reload data using new mean and variance terms contained within an
 *              MPSCNNNormalizationMeanAndVarianceState object.
 *
 *  @param      commandBuffer               The command buffer on which to encode the reload.
 *
 *  @param      meanAndVarianceState        The state containing the updated statistics which are to
 *                                          be reloaded.
 */
-(void) reloadMeanAndVarianceWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                          meanAndVarianceState: (MPSCNNNormalizationMeanAndVarianceState* __nonnull) meanAndVarianceState
 MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));
@end    // MPSCNNBatchNormalization
    
/*!
 *  @class      MPSCNNBatchNormalizationStatistics
 *  @dependency This depends on Metal.framework
 *  @discussion MPSCNNBatchNormalizationStatistics updates a MPSCNNBatchNormalizationState
 *              with the batch statistics necessary to perform a batch normalization.
 *              MPSCNNBatchNormalizationStatistics may be executed multiple times with
 *              multiple images to accumulate all the statistics necessary to perform
 *              a batch normalization as described in  https://arxiv.org/pdf/1502.03167v3.pdf.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNBatchNormalizationStatistics : MPSCNNKernel

/*!
 * @abstract    Initialize this kernel on a device.
 *
 *  @param      device      The MTLDevice on which to initialize the kernel.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSCNNBatchNormalizationStatistics object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract       Encode this operation to a command buffer.
 *  @param          commandBuffer           The command buffer.
 *  @param          sourceImages            An MPSImageBatch containing the source images.
 *  @param          batchNormalizationState A valid MPSCNNBatchNormalizationState object which
 *                                          will be updated with the image batch statistics.
 */
-(void) encodeBatchToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch* __nonnull) sourceImages
           batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState;

/*
 * encodeBatchToCommandBuffer:sourceImages:destinationImages: is unavailable.
 * Use encodeToCommandBuffer:sourceImages:state
 */
-(void) encodeBatchToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch* __nonnull) sourceImages
                 destinationImages: (MPSImageBatch* __nonnull) destinationImages NS_UNAVAILABLE;
/*
 * encodeToCommandBuffer:sourceImage:destinationImage: is unavailable.
 * Use encodeToCommandBuffer:sourceImages:state
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage* __nonnull) sourceImage
             destinationImage: (MPSImage* __nonnull) destinationImage NS_UNAVAILABLE;
/*
 * encodeToCommandBuffer:sourceImage: is unavailable.
 * Use encodeToCommandBuffer:sourceImages:state
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                                  sourceImage: (MPSImage*  __nonnull) sourceImage NS_UNAVAILABLE;
/*
 * encodeBatchToCommandBuffer:sourceImages: is unavailable.
 * Use encodeToCommandBuffer:sourceImages:state
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id<MTLCommandBuffer>) commandBuffer
                                           sourceImages: (MPSImageBatch*  __nonnull) sourceImages NS_UNAVAILABLE;

@end    // MPSCNNBatchNormalizationStatistics

/*!
 *  @class      MPSCNNBatchNormalizationGradient
 *  @dependency This depends on Metal.framework
 *
 *  @discussion MPSCNNBatchNormalizationGradient computes the gradients of a
 *              loss function resulting from a network containing a corresponding
 *              MPSCNNBatchNormalization kernel.
 *
 *              Two sets of values are computed: the gradient of the loss function
 *              with respect to the batch normalization source images, and the
 *              gradient of the loss function with respect to the scale and bias
 *              terms used to compute the batch normalization.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNBatchNormalizationGradient : MPSCNNGradientKernel
/*!
 *  @abstract   Initializes a batch normalization gradient kernel using a device and neuron descriptor.
 *  @param      device                          The MTLDevice on which this filter will be used
 *  @param      fusedNeuronDescriptor           A MPSNNNeuronDescriptor object which specifies a neuron activation function whose
 *                                              gradient should be applied prior to computing the resulting gradient.
 *                                              This neuron descriptor should match that used in the corresponding forward batch
 *                                              normalization kernel as well as the preceeding batch normalization statistics gradient
 *                                              kernel.
 *
 *  @return     A valid MPSCNNBatchNormalizationGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                 fusedNeuronDescriptor: (MPSNNNeuronDescriptor* __nullable) fusedNeuronDescriptor NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use a subclass of NSCoder that
 *              implements the <MPSDeviceProvider> protocol  to
 *              tell MPS the MTLDevice to use.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSCNNBatchNormalizationGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract       Encode this operation to a command buffer for a single image.
 *  @param          commandBuffer               The command buffer.
 *  @param          sourceGradient              An MPSImage containing the gradient of the loss function with
 *                                              respect to the results of batch normalization on the source image.
 *  @param          sourceImage                 An MPSImage containing the source image for batch normalization.
 *  @param          batchNormalizationState     A valid MPSCNNBatchNormalizationState object which
 *                                              has been previously updated using a MPSCNNBatchNormalizationStatisticsGradient
 *                                              kernel and the source images. If the state is temporary its read count will be decremented.
 *  @param          destinationGradient         An MPSImage which contains the gradient of the loss function with respect to the source image.
 */
-(void) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
               sourceGradient: (MPSImage * __nonnull ) sourceGradient
                  sourceImage: (MPSImage * __nonnull ) sourceImage
      batchNormalizationState: (MPSCNNBatchNormalizationState * __nonnull ) batchNormalizationState
          destinationGradient: (MPSImage * __nonnull ) destinationGradient;

/*! @abstract       Encode this operation to a command buffer.
 *  @param          commandBuffer               The command buffer.
 *  @param          sourceGradients             An MPSImageBatch containing the gradient of the
 *                                              loss function with respect to the results of batch normalization
 *                                              on the source images.
 *  @param          sourceImages                An MPSImageBatch containing the source images for
 *                                              batch normalization.
 *  @param          batchNormalizationState     A valid MPSCNNBatchNormalizationState object which
 *                                              has been previously updated using a MPSCNNBatchNormalizationStatisticsGradient
 *                                              kernel and the source images. If the state is temporary its read count will be decremented.
 *  @param          destinationGradients        An MPSImageBatch whose images will contain the gradient
 *                                              of the loss function with respect to the source images.
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                   sourceGradients: (MPSImageBatch * __nonnull) sourceGradients
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
           batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
              destinationGradients: (MPSImageBatch * __nonnull) destinationGradients;

/*! @abstract       Encode this operation to a command buffer.  Create an MPSImage to contain
 *                  the result and return it.
 *                  See encodeToCommandBuffer:sourceImage:sourceGradient:sourceImage:batchNormalizationState:destinationGradient
 *                  for further details.
 */
-(MPSImage*__nonnull) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                             sourceGradient: (MPSImage * __nonnull ) sourceGradient
                                sourceImage: (MPSImage * __nonnull ) sourceImage
                    batchNormalizationState: (MPSCNNBatchNormalizationState * __nonnull ) batchNormalizationState;

/*! @abstract       Encode this operation to a command buffer.  Create an MPSImageBatch to contain
 *                  the result and return it.
 *                  See encodeBatchToCommandBuffer:sourceGradients:sourceImages:batchNormalizationState:destinationGradients
 *                  for further details.
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                        sourceGradients: (MPSImageBatch * __nonnull) sourceGradients
                                           sourceImages: (MPSImageBatch *  __nonnull) sourceImages
                                batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState;
/*
 * Unavailable.  Use encodeBatchToCommandBuffer:sourceImages:lossGradientForDestination:lossGradientForSource:batchNormalizationState:
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 primaryImage: (MPSImage * __nonnull) primaryImage
               secondaryImage: (MPSImage * __nonnull) secondaryImage
             destinationImage: (MPSImage * __nonnull) destinationImage NS_UNAVAILABLE;
/*
 * Unavailable.  Use encodeBatchToCommandBuffer:sourceImages:lossGradientForDestination:lossGradientForSource:batchNormalizationState:
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                     primaryImages: (MPSImageBatch * __nonnull) primaryImages
                   secondaryImages: (MPSImageBatch * __nonnull) secondaryImages
                 destinationImages: (MPSImageBatch * __nonnull) destinationImages NS_UNAVAILABLE;
/*
 * Unavailable.  Use -(MPSImageBatch*)encodeBatchToCommandBuffer:sourceImages:lossGradientForDestination:batchNormalizationState:
 */
-(MPSImage * __nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                 primaryImage: (MPSImage * __nonnull) primaryImage
                               secondaryImage: (MPSImage * __nonnull) secondaryImage NS_UNAVAILABLE;
/*
 * Unavailable.  Use -(MPSImageBatch*)encodeBatchToCommandBuffer:sourceImages:lossGradientForDestination:batchNormalizationState:
 */
-(MPSImageBatch * __nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                          primaryImages: (MPSImageBatch * __nonnull) primaryImage
                                        secondaryImages: (MPSImageBatch * __nonnull) secondaryImage NS_UNAVAILABLE;
@end    // MPSCNNBatchNormalizationGradient
    
    
/*!
 *  @class      MPSCNNBatchNormalizationStatisticsGradient
 *  @dependency This depends on Metal.framework
 *  @discussion MPSCNNBatchNormalizationStatisticsGradient updates a MPSCNNBatchNormalizationState
 *              with the gradient of the loss function with respect to the batch statistics and
 *              batch normalization weights used to perform a batch normalization.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNBatchNormalizationStatisticsGradient : MPSCNNGradientKernel
/*!
 *  @abstract   Initializes a batch normalization statistics gradient kernel using a device and neuron descriptor.
 *  @param      device                          The MTLDevice on which this filter will be used
 *  @param      fusedNeuronDescriptor           A MPSNNNeuronDescriptor object which specifies a neuron activation function whose
 *                                              gradient should be applied prior to computing the statistics of the input gradient.
 *                                              This neuron descriptor should match that used in the corresponding forward batch
 *                                              normalization kernel.
 *
 *  @return     A valid MPSCNNBatchNormalizationStatisticsGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                 fusedNeuronDescriptor: (MPSNNNeuronDescriptor* __nullable) fusedNeuronDescriptor NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use a subclass of NSCoder that
 *              implements the <MPSDeviceProvider> protocol  to
 *              tell MPS the MTLDevice to use.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSCNNBatchNormalizationStatisticsGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract       Encode this operation to a command buffer.
 *  @param          commandBuffer               The command buffer.
 *  @param          sourceGradients             An MPSImageBatch containing the gradient of the
 *                                              loss function with respect to the results of batch normalization
 *                                              on the source images.
 *  @param          sourceImages                An MPSImageBatch containing the source images for
 *                                              batch normalization.
 *  @param          batchNormalizationState     A valid MPSCNNBatchNormalizationState object which
 *                                              has been previously updated using a MPSCNNBatchNormalizationStatistics
 *                                              kernel and the source images.  Upon completion of the
 *                                              command buffer, will contain the (possibly partially updated)
 *                                              gradients for the loss function with respect to the scale and
 *                                              bias parameters used to compute the batch normalization.  The
 *                                              state will be considered to be completely updated when all
 *                                              MPSImages in the training batch have been processed.  If the state
 *                                              is temporary its read count will be decremented.
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                   sourceGradients: (MPSImageBatch * __nonnull) sourceGradients
                      sourceImages: (MPSImageBatch * __nonnull) sourceImages
           batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState;

/*
 *  Unavailable. Use encodeBatchToCommandBuffer:sourceGradients:sourceImages:batchNormalizationState:
 */
-(MPSImage*__nonnull) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                             sourceGradient: (MPSImage * __nonnull ) sourceGradient
                                sourceImage: (MPSImage * __nonnull ) sourceImage
                              gradientState: (MPSState * __nonnull ) gradientState NS_UNAVAILABLE;

/*
 *  Unavailable.  Use encodeBatchToCommandBuffer:sourceGradients:sourceImages:batchNormalizationState:
 */
-(void) encodeToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
               sourceGradient: (MPSImage * __nonnull ) sourceGradient
                  sourceImage: (MPSImage * __nonnull ) sourceImage
                gradientState: (MPSState * __nonnull ) gradientState
          destinationGradient: (MPSImage * __nonnull ) destinationGradient NS_UNAVAILABLE;

/*
 *  Unavailable.  Use encodeBatchToCommandBuffer:sourceGradients:sourceImages:batchNormalizationState:
 */
-(MPSImageBatch*__nonnull) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                                      sourceGradients: (MPSImageBatch * __nonnull ) sourceGradients
                                         sourceImages: (MPSImageBatch * __nonnull ) sourceImages
                                       gradientStates: (MPSStateBatch * __nonnull ) gradientStates NS_UNAVAILABLE;
/*
 *  Unavailable.  Use encodeBatchToCommandBuffer:sourceGradients:sourceImages:batchNormalizationState:
 */
-(void) encodeBatchToCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                   sourceGradients: (MPSImageBatch * __nonnull ) sourceGradients
                      sourceImages: (MPSImageBatch * __nonnull ) sourceImages
                    gradientStates: (MPSStateBatch * __nonnull ) gradientStates
              destinationGradients: (MPSImageBatch * __nonnull ) destinationGradients NS_UNAVAILABLE;

@end    // MPSCNNBatchNormalizationStatisticsGradient
#ifdef __cplusplus
}
#endif // __cplusplus

#endif /* MPSCNNBatchNormalization_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNLoss.h
//
//  MPSCNNLoss.h
//  MPS
//
//  Created by Anna Tikhonova on 10/6/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNLoss_h
#define MPSCNNLoss_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>
#import <MPSNeuralNetwork/MPSCNNTypes.h>
#import <MPSCore/MPSState.h>

#ifdef __cplusplus
extern "C" {
#endif

#pragma mark -
#pragma mark MPSCNNLossDataDescriptor
    
/*!
 *  @class      MPSCNNLossDataDescriptor
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNLossDataDescriptor specifies a loss data descriptor.
 *              The same descriptor can be used to initialize both the
 *              labels and the optional weights data.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLossDataDescriptor : NSObject <NSCopying>

/*! @property   layout
 *  @abstract   Data layout of loss data. See MPSImage.h for more information.
 *  @discussion This parameter specifies the layout of loss data.
 */
@property (readonly, nonatomic) MPSDataLayout layout;

/*! @property   size
 *  @abstract   Size of loss data: (width, height, feature channels}.
 *  @discussion This parameter specifies the size of loss data.
 */
@property (readonly, nonatomic) MTLSize size;

/*! @property   bytesPerRow
 *  @abstract   Row bytes of loss data.
 *  @discussion This parameter specifies the row bytes of loss data.
 */
@property (readwrite, nonatomic) NSUInteger bytesPerRow;

/*! @property   bytesPerImage
 *  @abstract   Slice bytes of loss data.
 *  @discussion This parameter specifies the slice bytes of loss data.
 */
@property (readwrite, nonatomic) NSUInteger bytesPerImage;

/*
 * You must use the cnnLossDataDescriptorWithDataLayout interface instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract   Make a descriptor loss data. The bytesPerRow and bytesPerImage
 *              are automatically calculated assuming a dense array. If it is
 *              not a dense array, adjust bytesPerRow and bytesPerImage to the
 *              right value by changing properties.
 *  @param      data                        The per-element loss data. The data must be in floating point format.
 *  @param      layout                      The data layout of loss data.
 *  @param      size                        The size of loss data.
 *  @return     A valid MPSCNNLossDataDescriptor object or nil, if failure.
 */
+(nullable MPSCNNLossDataDescriptor*) cnnLossDataDescriptorWithData: (NSData*__nonnull) data
                                                             layout: (MPSDataLayout) layout
                                                               size: (MTLSize) size;

@end /* MPSCNNLossDataDescriptor */

    
#pragma mark -
#pragma mark MPSCNNLossLabels

/*!
 *  @class      MPSCNNLossLabels
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNLossLabels is used to hold the per-element weights buffer
 *              used by both MPSCNNLoss forward filter and MPSCNNLossGradient backward filter.
 *              The MPSCNNLoss forward filter populates the MPSCNNLossLabels object
 *              and the MPSCNNLossGradient backward filter consumes the state object.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLossLabels : MPSState

/*!
 *  Use one of the interfaces below instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract   Set labels (aka targets, ground truth) for the MPSCNNLossLabels object. 
 *  @discussion The labels and weights data are copied into internal storage. The computed loss can either be a
 *                                      scalar value (in batch mode, a single value per image in a batch) or it
 *                                      can be one value per feature channel. Thus, the size of the loss image
 *                                      must either match the size of the input source image or be {1, 1, 1},
 *                                      which results in a scalar value. In this convinience initializer, the
 *                                      assumed size of the loss image is {1, 1, 1}.
 *  @param      device                  Device the state resources will be created on.
 *  @param      labelsDescriptor        Describes the labels data. This includes:
 *                                          - The per-element labels data. The data must be in floating point format.
 *                                          - Data layout of labels data. See MPSImage.h for more information.
 *                                          - Size of labels data: (width, height, feature channels}.
 *                                          - Optionally, row bytes of labels data.
 *                                          - Optionally, slice bytes of labels data.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>)device
                      labelsDescriptor: (MPSCNNLossDataDescriptor* _Nonnull) labelsDescriptor;

/*!
 *  @abstract   Set labels (aka targets, ground truth) and weights for the MPSCNNLossLabels object.
 *              Weights are optional.
 *  @discussion The labels and weights data are copied into internal storage.
 *  @param      device                  Device the state resources will be created on.
 *  @param      lossImageSize           The size of the resulting loss image: { width, height, featureChannels }.
 *                                      The computed loss can either be a scalar value (in batch mode, a single
 *                                      value per image in a batch) or it can be one value per feature channel.
 *                                      Thus, the size of the loss image must either match the size of the input
 *                                      source image or be {1, 1, 1}, which results in a scalar value.
 *  @param      labelsDescriptor        Describes the labels data. This includes:
 *                                          - The per-element labels data. The data must be in floating point format.
 *                                          - Data layout of labels data. See MPSImage.h for more information.
 *                                          - Size of labels data: (width, height, feature channels}.
 *                                          - Optionally, row bytes of labels data.
 *                                          - Optionally, slice bytes of labels data.
 *  @param      weightsDescriptor       Describes the weights data. This includes:
 *                                          - The per-element weights data. The data must be in floating point format.
 *                                          - Data layout of weights data. See MPSImage.h for more information.
 *                                          - Size of weights data: (width, height, feature channels}.
 *                                          - Optionally, row bytes of weights data.
 *                                          - Optionally, slice bytes of weights data.
 *                                      This parameter is optional. If you are using a single weight, please use the
 *                                      weight property of the MPSCNNLossDescriptor object.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>)device
                         lossImageSize: (MTLSize) lossImageSize
                      labelsDescriptor: (MPSCNNLossDataDescriptor* _Nonnull) labelsDescriptor
                     weightsDescriptor: (MPSCNNLossDataDescriptor* _Nullable) weightsDescriptor NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Loss image accessor method.
 *  @return     An autoreleased MPSImage object, containing the loss data.
 *              The loss data is populated in the -encode call, thus the contents
 *              are undefined until you -encode the filter.
 *
 *              In order to gaurantee that the image is correctly synchronized for CPU side access,
 *              it is the application's responsibility to call the [gradientState synchronizeOnCommandBuffer:]
 *              method before accessing the data in the image.
 */
-(nonnull MPSImage*) lossImage;

/*!
 *  @abstract   Labels image accessor method.
 *  @return     An autoreleased MPSImage object, containing the labels data.
 *              The labels data is populated in the -initWithDevice call.
 *
 *              In order to gaurantee that the image is correctly synchronized for CPU side access,
 *              it is the application's responsibility to call the [gradientState synchronizeOnCommandBuffer:]
 *              method before accessing the data in the image.
 */
-(nonnull MPSImage*) labelsImage;

/*!
 *  @abstract   Weights image accessor method.
 *  @return     An autoreleased MPSImage object, containing the weights data.
 *              The weights data is populated in the -initWithDevice call.
 *
 *              In order to gaurantee that the image is correctly synchronized for CPU side access,
 *              it is the application's responsibility to call the [gradientState synchronizeOnCommandBuffer:]
 *              method before accessing the data in the image.
 */
-(nonnull MPSImage*) weightsImage;

@end /* MPSCNNLossLabels */
    

typedef NSArray <MPSCNNLossLabels*>  MPSCNNLossLabelsBatch MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

#pragma mark -
#pragma mark MPSCNNLossDescriptor
    
/*!
 *  @class      MPSCNNLossDescriptor
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNLossDescriptor specifies a loss filter descriptor.
 *              The same descriptor can be used to initialize both the
 *              MPSCNNLoss and the MPSCNNLossGradient filters.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLossDescriptor : NSObject <NSCopying>

/*! @property   lossType
 *  @abstract   The type of a loss filter.
 *  @discussion This parameter specifies the type of a loss filter.
 */
@property (readwrite, nonatomic) MPSCNNLossType lossType;

/*! @property   reductionType
 *  @abstract   The type of a reduction operation performed in the loss filter.
 *  @discussion This parameter specifies the type of a reduction operation
 *              performed in the loss filter.
 */
@property (readwrite, nonatomic) MPSCNNReductionType reductionType;

/*! @property   weight
 *  @abstract   The scale factor to apply to each element of a result.
 *  @discussion Each element of a result is multiplied by the weight value.
 *              The default value is 1.0f.
 */
@property (readwrite, nonatomic) float weight;

/*!
 * @property    labelSmoothing
 * @abstract    The label smoothing parameter. The default value is 0.0f.
 * @discussion  This parameter is valid only for the loss functions of the following type(s):
 *              MPSCNNLossFunctionTypeSoftmaxCrossEntropy, MPSCNNLossFunctionTypeSigmoidCrossEntropy.
 *
 *              MPSCNNLossFunctionTypeSoftmaxCrossEntropy: given labels (ground truth), it is applied in the following way:
 *              labels = labelSmoothing > 0 ? labels * (1 - labelSmoothing) + labelSmoothing / numberOfClasses : labels
 *
 *              MPSCNNLossFunctionTypeSigmoidCrossEntropy: given labels (ground truth), it is applied in the following way:
 *              labels = labelSmoothing > 0 ? labels * (1 - labelSmoothing) + 0.5 * labelSmoothing : labels
 */
@property (readwrite, nonatomic) float labelSmoothing;

/*!
 * @property    numberOfClasses
 * @abstract    The number of classes parameter. The default value is 1.
 * @discussion  This parameter is valid only for the loss functions of the following type(s):
 *              MPSCNNLossFunctionTypeSoftmaxCrossEntropy.
 *
 *              Given labels (ground truth), it is applied in the following way:
 *              labels = labelSmoothing > 0 ? labels * (1 - labelSmoothing) + labelSmoothing / numberOfClasses : labels
 */
@property (readwrite, nonatomic) NSUInteger numberOfClasses;

/*!
 * @property    epsilon
 * @abstract    The epsilon parameter. The default value is 1e-7.
 * @discussion  This parameter is valid only for the loss functions of the following type(s):
 *              MPSCNNLossTypeLog.
 *
 *              Given predictions and labels (ground truth), it is applied in the following way:
 *              -(labels * log(predictions + epsilon)) - ((1 - labels) * log(1 - predictions + epsilon))
 */
@property (readwrite, nonatomic) float epsilon;

/*!
 * @property    delta
 * @abstract    The delta parameter. The default value is 1.0f.
 * @discussion  This parameter is valid only for the loss functions of the following type(s):
 *              MPSCNNLossTypeHuber.
 *
 *              Given predictions and labels (ground truth), it is applied in the following way:
 *              if (|predictions - labels| <= delta, loss = 0.5f * predictions^2
 *              if (|predictions - labels| >  delta, loss = 0.5 * delta^2 + delta * (|predictions - labels| - delta)
 */
@property (readwrite, nonatomic) float delta;

/*
 * You must use one of the interfaces below instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract   Make a descriptor for a MPSCNNLoss or MPSCNNLossGradient object.
 *  @param      lossType                    The type of a loss filter.
 *  @param      reductionType               The type of a reduction operation to apply.
 *                                          This argument is ignored in the MPSCNNLossGradient filter.
 *  @return     A valid MPSCNNLossDescriptor object or nil, if failure.
 */
+(nonnull MPSCNNLossDescriptor*) cnnLossDescriptorWithType:(MPSCNNLossType) lossType
                                             reductionType:(MPSCNNReductionType) reductionType;

@end /* MPSCNNLossDescriptor */


#pragma mark -
#pragma mark MPSCNNLoss

/*!
 *  @class      MPSCNNLoss
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNLoss filter is only used for training. This filter performs both the forward and
 *              backward pass computations. Specifically, it computes the loss between the input (predictions)
 *              and target data (labels) and the loss gradient. The loss value can be a 1 x 1 x 1 image containing
 *              a scalar loss value or an image (of the same size as the input source image) with per feature
 *              channel losses. The loss value is used to determine whether to continue the training operation or
 *              to terminate it, once satisfactory results are achieved. The loss gradient is the first gradient
 *              computed for the backward pass and serves as input to the next gradient filter (in the backward
 *              direction).
 *
 *              The MPSCNNLoss filter is created with a MPSCNNLossDescriptor describing the type of a loss filter
 *              and the type of a reduction to use for computing the overall loss.
 *
 *              The MPSCNNLoss filter takes the output of the inference pass (predictions) as input. It also
 *              requires the target data (labels) and optionally, weights for the labels. If per-label weights
 *              are not supplied, there is an option to use a single weight value by setting the 'weight' properly
 *              on the MPSCNNLossDescriptor object. The labels and optional weights need to be supplied by the user
 *              using the MPSCNNLossLabels object. The labels and weights are described via the MPSCNNLossDataDescriptor
 *              objects, which are in turn used to initialize the MPSCNNLossLabels object.
 *
 *              If the specified reduction operation is MPSCNNReductionTypeNone, the destinationImage should be
 *              at least as large as the specified clipRect. The detinationImage will then contain per-element
 *              losses. Otherse, a reduction operation will be performed, according to the specified reduction
 *              type, and the filter will return a scalar value containing the overall loss. For more information
 *              on the available reduction types, see MPSCNNTypes.h. Also see MPSCNNLossDescriptor for the
 *              description of optional parameters.
 *
 *              Here is a code example:
 *
 *              // Setup
 *              MPSCNNLossDataDescriptor* labelsDescriptor =
 *                  [MPSCNNLossDataDescriptor cnnLossDataDescriptorWithData: labelsData
 *                                                                   layout: MPSDataLayoutHeightxWidthxFeatureChannels
 *                                                                     size: labelsDataSize];
 *              MPSCNNLossLabels* labels = [[MPSCNNLossLabels alloc] initWithDevice: device
 *                                                                 labelsDescriptor: labelsDescriptor];
 *              MPSCNNLossDescriptor *lossDescriptor =
 *                  [MPSCNNLossDescriptor cnnLossDescriptorWithType: (MPSCNNLossType)MPSCNNLossTypeMeanAbsoluteError
 *                                                    reductionType: (MPSCNNReductionType)MPSCNNReductionTypeSum];
 *              MPSCNNLoss* lossFilter = [[MPSCNNLoss alloc] initWithDevice: device lossDescriptor: lossDescriptor];
 *
 *              // Encode loss filter.
 *              // The sourceImage is the output of a previous layer, for example, the SoftMax layer. The lossGradientsImage
 *              // is the sourceGradient input image to the first gradient layer (in the backward direction), for example,
 *              // the SoftMax gradient filter.
 *              [lossFilter encodeToCommandBuffer: commandBuffer sourceImage: sourceImage
 *                                                                    labels: labels
 *                                                          destinationImage: lossGradientsImage];
 *
 *              // In order to gaurantee that the loss image data is correctly synchronized for CPU side access,
 *              // it is the application's responsibility to call the [labels synchronizeOnCommandBuffer:]
 *              // method before accessing the loss image data.
 *              [labels synchronizeOnCommandBuffer:commandBuffer];
 *              MPSImage* lossImage = [labels lossImage];
 *
 *
 *
 *              For predictions (y) and labels (t), the available loss filter types are the following:
 *
 *              Mean Absolute Error loss filter. This filter measures the absolute error of the element-wise
 *              difference between the predictions and labels.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = |y - t|
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Mean Squared Error loss filter. This filter measures the squared error of the element-wise
 *              difference between the predictions and labels.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = (y - t)^2
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 * 
 *              SoftMax Cross Entropy loss filter. This loss filter is applied element-wise.
 *              This loss filter combines the LogSoftMax and Negative Log Likelihood operations in a
 *              single filter. It is useful for training a classification problem with multiple classes.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = -t * LogSoftMax(y)
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *                                           If reductionType is MPSCNNReductionTypeMean, the accumulated
 *                                           loss value is divided by width * height instead of
 *                                           width * height * featureChannels.
 *
 *              Sigmoid Cross Entropy loss filter. This loss filter is applied element-wise.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = max(y, 0) - y * t + log(1 + exp(-|y|))
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Categorical Cross Entropy loss filter. This loss filter is applied element-wise.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = -t * log(y)
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Hinge loss filter. This loss filter is applied element-wise.
 *              The labels are expected to be 0.0 or 1.0.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = max(1 - (t * y), 0.0f)
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Huber loss filter. This loss filter is applied element-wise.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          if (|y - t| <= delta, losses = 0.5 * y^2
 *                                           if (|y - t| >  delta, losses = 0.5 * delta^2 + delta * (|y - t| - delta)
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Cosine Distance loss filter. This loss filter is applied element-wise.
 *              The only valid reduction type for this loss filter is MPSCNNReductionTypeSum.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          loss = 1 - reduce_sum(y * t)
 *                  Compute overall loss:    weighted_loss = weight * loss
 *
 *              Log loss filter. This loss filter is applied element-wise.
 *              This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = -(t * log(y + epsilon)) - ((1 - t) * log(1 - y + epsilon))
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *              Kullback-Leibler Divergence loss filter. This loss filter is applied element-wise.
 *              The input (predictions) is expected to contain log-probabilities.
 *                  This loss function is computed according to the following formulas:
 *                  Compute losses:          losses = t * (log(t) - y)
 *                  Compute weighted losses: weighted_losses = weight(s) * losses
 *                  Compute overall loss:    loss = reduce(weighted_losses, reductionType)
 *
 *
 *
 *              For predictions (y) and labels (t), the loss gradient for each available loss filter type
 *              is computed as follows:
 *
 *              Mean Absolute Error loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = (y - t) / |y - t|
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Mean Squared Error loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = 2 * (y - t)
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              SoftMax Cross Entropy loss.
 *              The loss gradient is computed according to the following formulas:
 *                  First, apply the same label smoothing as in the MPSCNNLoss filter.
 *                  Compute gradient:          d/dy = y - t
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Sigmoid Cross Entropy loss.
 *              The loss gradient is computed according to the following formulas:
 *              First, apply the same label smoothing as in the MPSCNNLoss filter.
 *                  Compute gradient:          d/dy = (1 / (1 + exp(-y)) - t
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Categorical Cross Entropy loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = -t / y
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Hinge loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = ((1 + ((1 - (2 * t)) * y)) > 0) ? 1 - (2 * t) : 0
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Huber loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = |y - t| > delta ? delta : y - t
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Cosine Distance loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = -t
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Log loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = (-2 * epsilon * t - t + y + epsilon) / (y * (1 - y) + epsilon * (epsilon + 1))
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              Kullback-Leibler Divergence loss.
 *              The loss gradient is computed according to the following formulas:
 *                  Compute gradient:          d/dy = -t / y
 *                  Compute weighted gradient: weighted_gradient = weight(s) * gradient
 *
 *              The number of output feature channels remains the same as the number of input feature
 *              channels.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLoss : MPSCNNKernel

/*!
 * See MPSCNNLossDescriptor for information about the following properties.
 */
@property (readonly, nonatomic) MPSCNNLossType lossType;
@property (readonly, nonatomic) MPSCNNReductionType reductionType;
@property (readonly, nonatomic) float weight;
@property (readonly, nonatomic) float labelSmoothing;
@property (readonly, nonatomic) NSUInteger numberOfClasses;
@property (readonly, nonatomic) float epsilon;
@property (readonly, nonatomic) float delta;

/*
 * You must use initWithDevice:lossDescriptor instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract   Initialize the loss filter with a loss descriptor.
 *  @param      device                   The device the filter will run on.
 *  @param      lossDescriptor           The loss descriptor.
 *  @return     A valid MPSCNNLoss object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        lossDescriptor: (MPSCNNLossDescriptor*_Nonnull) lossDescriptor NS_DESIGNATED_INITIALIZER;

/*! @abstract <NSSecureCoding> support */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract   Encode a MPSCNNLoss filter and return a gradient in the destinationImage.
 *  @discussion This filter consumes the output of a previous layer, for example, the SoftMax layer containing
 *              predictions, and the MPSCNNLossLabels object containing the target data (labels) and optionally,
 *              weights for the labels. The destinationImage contains the computed gradient for the loss layer.
 *              It serves as a source gradient input image to the first gradient layer (in the backward direction),
 *              in our example, the SoftMax gradient layer.
 *
 *  @param      commandBuffer       The MTLCommandBuffer on which to encode.
 *  @param      sourceImage         The source image from the previous filter in the graph (in the inference direction).
 *  @param      labels              The object containing the target data (labels) and optionally, weights for the labels.
 *  @param      destinationImage    The MPSImage into which to write the gradient result.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
                       labels: (MPSCNNLossLabels * __nonnull) labels
             destinationImage: (MPSImage * __nonnull) destinationImage
            MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:labels:destinationImage:));

/*! @abstract   Encode a MPSCNNLoss filter and return a gradient.
 *  @discussion This -encode call is similar to the encodeToCommandBuffer:sourceImage:labels:destinationImage: above,
 *              except that it creates and returns the MPSImage with the loss gradient result.
 *
 *  @param      commandBuffer       The MTLCommandBuffer on which to encode.
 *  @param      sourceImage         The source image from the previous filter in the graph (in the inference direction).
 *  @param      labels              The object containing the target data (labels) and optionally, weights for the labels.
 *  @return     The MPSImage containing the gradient result.
 */
-(MPSImage*__nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                sourceImage: (MPSImage * __nonnull) sourceImage
                                     labels: (MPSCNNLossLabels * __nonnull) labels
            MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:labels:));


-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImage
                            labels: (MPSCNNLossLabelsBatch * __nonnull) labels
                 destinationImages: (MPSImageBatch * __nonnull) destinationImage
            MPS_SWIFT_NAME( encode(commandBuffer:sourceImages:labels:destinationImages:));

-(MPSImageBatch*__nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                         sourceImages: (MPSImageBatch * __nonnull) sourceImage
                                               labels: (MPSCNNLossLabelsBatch * __nonnull) labels
            MPS_SWIFT_NAME( encode(commandBuffer:sourceImages:labels:));

@end /* MPSCNNLoss */




#pragma mark -
#pragma mark MPSCNNYOLOLossDescriptor

/*!
 *  @class      MPSCNNYOLOLossDescriptor
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNYOLOLossDescriptor specifies a loss filter descriptor
 *              that is used to create a MPSCNNLoss filter. The MPSCNNYOLOLoss is a filter that
 *              has been specialized for object detection tasks and follows a specific layout
 *              for the feature-channels of the input, output, weight and label data.
 *
 *              The layout of the data within the feature-channels is as follows:
 *
 *                  Each anchorbox uses ( 2+2+1 + numberOfClasses = 5 + numberOfClasses ) feature channels.
 *
 *              Therefore the total number of feature channels used is: (5 + numberOfClasses) * numberOfAnchorBoxes.
 *              The first feature channel for anchorbox index 'anchorIdx' is at fcIndex = (5 + numberOfClasses) * anchorIdx,
 *              and the feature channels within each anchorbox are stored in the layout: 'XYWHCFFFFFF...', where (XY) are
 *              the so-called raw x and y coordinates of the bounding box within each gridcell and (WH) are the corresponding
 *              width and height. 'C' signifies a confidence for having an object in the cell and FFFFF... are the feature channel
 *              values for each class of object to be classified in the object detector.
 *
 *              The YOLO-loss filter works by operating mostly independently on each anchorbox:
 *                  *   The XY-channels of the inputs are first transformed to relative XY-values by applying the sigmoid-neuron on them,
 *                      after which they are passed through the loss function defined by @ref XYLossDescriptor, which is typically chosen
 *                      to be the @ref MPSCNNLossTypeMeanSquaredError type loss function.
 *                  *   The WH-channels contain the raw width and height of the bounding box and they are operated with the
 *                      loss function defined by @ref WHLossDescriptor, which is typically of type @ref MPSCNNLossTypeHuber.
 *                  *   The C-channel contains the confidence value of having an object in the bounding box and it is operated
 *                      by the loss function defined by @ref confidenceLossDescriptor, which is typically chosen to be
 *                      @ref MPSCNNLossTypeSigmoidCrossEntropy.
 *                  *   The FFFFF... (number of channels is number of classes) channels contains the raw feature channels for
 *                      object classes, used to identify which objects are the most probable ones in the bounding box and
 *                      these channels are passed through the loss function defined by @ref classesLossDescriptor, which in
 *                      typical cases is of the type @ref MPSCNNLossTypeSoftMaxCrossEntropy.
 *
 *              For details on how to set up the label values and anchorboxes see https://arxiv.org/abs/1612.08242
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12), tvos(12))
@interface MPSCNNYOLOLossDescriptor : NSObject <NSCopying>

/*! @property   XYLossDescriptor
 *  @abstract   The type of a loss filter.
 *  @discussion This parameter specifies the type of a loss filter.
 */
@property (readwrite, nonatomic, nonnull, retain) MPSCNNLossDescriptor *XYLossDescriptor;

/*! @property   WHLossDescriptor
 *  @abstract   The type of a loss filter.
 *  @discussion This parameter specifies the type of a loss filter.
 */
@property (readwrite, nonatomic, nonnull, retain) MPSCNNLossDescriptor *WHLossDescriptor;

/*! @property   confidenceLossDescriptor
 *  @abstract   The type of a loss filter.
 *  @discussion This parameter specifies the type of a loss filter.
 */
@property (readwrite, nonatomic, nonnull, retain) MPSCNNLossDescriptor *confidenceLossDescriptor;

/*! @property   classesLossDescriptor
 *  @abstract   The type of a loss filter.
 *  @discussion This parameter specifies the type of a loss filter.
 */
@property (readwrite, nonatomic, nonnull, retain) MPSCNNLossDescriptor *classesLossDescriptor;

/*! @property   reductionType
 *  @abstract   ReductionType shared accross all losses (so they may generate same sized output)
 */
@property (readwrite, nonatomic) MPSCNNReductionType reductionType;

/*! @property   rescore
 *  @abstract   Rescore pertains to multiplying the confidence groundTruth with IOU (intersection over union)
 *              of predicted bounding box and the groundTruth boundingBox. Default is YES
 */
@property (readwrite, nonatomic) BOOL rescore;

/*! @property   scaleXY
 *  @abstract   scale factor for XY loss and loss gradient default is 10.0
 */
@property (readwrite, nonatomic) float scaleXY;

/*! @property   scaleWH
 *  @abstract   scale factor for WH loss and loss gradient default is 10.0
 */
@property (readwrite, nonatomic) float scaleWH;

/*! @property   scaleNoObject
 *  @abstract   scale factor for no object confidence loss and loss gradient default is 5.0
 */
@property (readwrite, nonatomic) float scaleNoObject;

/*! @property   scaleObject
 *  @abstract   scale factor for no object confidence loss and loss gradient default is 100.0
 */
@property (readwrite, nonatomic) float scaleObject;

/*! @property   scaleClass
 *  @abstract   scale factor for no object classes loss and loss gradient default is 2.0
 */
@property (readwrite, nonatomic) float scaleClass;

/*! @property   pos_iou
 *  @abstract   If the prediction IOU with groundTruth is higher than this
 *              value we consider it a confident object presence, default is 0.7
 */
@property (readwrite, nonatomic) float minIOUForObjectPresence;

/*! @property   neg_iou
 *  @abstract   If the prediction IOU with groundTruth is lower than this
 *              value we consider it a confident object absence, default is 0.3
 */
@property (readwrite, nonatomic) float maxIOUForObjectAbsence;

/*! @property   numberOfAnchorBoxes
 *  @abstract   number of anchor boxes used to detect object per grid cell
 */
@property (readwrite, nonatomic) NSUInteger numberOfAnchorBoxes;

/*! @property   anchorBoxes
 *  @abstract   NSData containing the width and height for numberOfAnchorBoxes anchor boxes
 *              This NSData should have 2 float values per anchor box which represent the width
 *              and height of the anchor box.
 *  @code
 *              typedef struct anchorBox{
 *                  float width;
 *                  float height;
 *              }anchorBox;
 *
 *
 *              anchorBox_t gAnchorBoxes[MAX_NUM_ANCHOR_BOXES] = {
 *                  {.width = 1.f, .height = 2.f},
 *                  {.width = 1.f, .height = 1.f},
 *                  {.width = 2.f, .height = 1.f},
 *              };
 *              NSData* labelsInputData = [NSData dataWithBytes: gAnchorBoxes length: MAX_NUM_ANCHOR_BOXES * sizeof(anchorBox)];
 *  @endcode
 *
 */
@property (readwrite, nonatomic, nonnull, retain) NSData *anchorBoxes;

/*
 * You must use one of the interfaces below instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract   Make a descriptor for a MPSCNNYOLOLoss object.
 *  @param      XYLossType                  The type of spatial position loss filter.
 *  @param      WHLossType                  The type of spatial size loss filter.
 *  @param      confidenceLossType          The type of confidence filter.
 *  @param      classesLossType             The type of classes filter.
 *  @param      reductionType               The type of a reduction operation to apply.
 *  @param      anchorBoxes                 This is an NSData which has an array of anchorBoxes defined as a struct{ float width; float height; };
 *  @return     A valid MPSCNNYOLOLossDescriptor object or nil, if failure.
 */
+(nonnull MPSCNNYOLOLossDescriptor*) cnnLossDescriptorWithXYLossType:(MPSCNNLossType) XYLossType
                                                          WHLossType:(MPSCNNLossType) WHLossType
                                                  confidenceLossType:(MPSCNNLossType) confidenceLossType
                                                     classesLossType:(MPSCNNLossType) classesLossType
                                                       reductionType:(MPSCNNReductionType) reductionType
                                                         anchorBoxes:(NSData*__nonnull) anchorBoxes
                                                 numberOfAnchorBoxes:(NSUInteger) numberOfAnchorBoxes;

@end /* MPSCNNYOLOLossDescriptor */


MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface MPSCNNYOLOLoss : MPSCNNKernel

/*! @property   lossXY
 *  @abstract   loss filter for prediction of bounding box position
 */
@property (readonly, nonatomic, retain, nonnull) MPSCNNLoss *lossXY;

/*! @property   lossWH
 *  @abstract   loss filter for prediction of bounding box size
 */
@property (readonly, nonatomic, retain, nonnull) MPSCNNLoss *lossWH;

/*! @property   lossConfidence
 *  @abstract   loss filter for prediction of bounding box probability of presence of object
 */
@property (readonly, nonatomic, retain, nonnull) MPSCNNLoss *lossConfidence;

/*! @property   lossClasses
 *  @abstract   loss filter for prediction of bounding box predicted class of the detected object
 */
@property (readonly, nonatomic, retain, nonnull) MPSCNNLoss *lossClasses;

/*!
 * See MPSCNNYOLOLossDescriptor for information about the following properties.
 */
@property (readonly, nonatomic) float scaleXY;
@property (readonly, nonatomic) float scaleWH;
@property (readonly, nonatomic) float scaleNoObject;
@property (readonly, nonatomic) float scaleObject;
@property (readonly, nonatomic) float scaleClass;
@property (readonly, nonatomic) float minIOUForObjectPresence;
@property (readonly, nonatomic) float maxIOUForObjectAbsence;
@property (readonly, nonatomic) MPSCNNReductionType reductionType;
@property (readonly, nonatomic) NSUInteger numberOfAnchorBoxes;
@property (readonly, nonatomic, nonnull, retain) NSData *anchorBoxes;

/*
 * You must use initWithDevice:lossDescriptor instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract   Initialize the loss filter with a loss descriptor.
 *  @param      device                   The device the filter will run on.
 *  @param      lossDescriptor           The loss descriptor.
 *  @return     A valid MPSCNNLoss object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        lossDescriptor: (MPSCNNYOLOLossDescriptor*_Nonnull) lossDescriptor NS_DESIGNATED_INITIALIZER;

/*! @abstract <NSSecureCoding> support */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract   Encode a MPSCNNYOLOLoss filter and return a gradient in the destinationImage.
 *  @discussion This filter consumes the output of a previous layer and the MPSCNNLossLabels object containing
 *              the target data (labels) and optionally, weights for the labels.
 *              The destinationImage contains the computed gradient for the loss layer.
 *              It serves as a source gradient input image to the first gradient layer (in the backward direction).
 *              For information on the data-layout see @ref MPSCNNYOLOLossDescriptor.
 *
 *  @param      commandBuffer       The MTLCommandBuffer on which to encode.
 *  @param      sourceImage         The source image from the previous filter in the graph (in the inference direction).
 *  @param      labels              The object containing the target data (labels) and optionally, weights for the labels.
 *  @param      destinationImage    The MPSImage into which to write the gradient result.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (MPSImage * __nonnull) sourceImage
                       labels: (MPSCNNLossLabels * __nonnull) labels
             destinationImage: (MPSImage * __nonnull) destinationImage
MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:labels:destinationImage:));

/*! @abstract   Encode a MPSCNNLoss filter and return a gradient.
 *  @discussion This -encode call is similar to the encodeToCommandBuffer:sourceImage:labels:destinationImage: above,
 *              except that it creates and returns the MPSImage with the loss gradient result.
 *
 *  @param      commandBuffer       The MTLCommandBuffer on which to encode.
 *  @param      sourceImage         The source image from the previous filter in the graph (in the inference direction).
 *  @param      labels              The object containing the target data (labels) and optionally, weights for the labels.
 *  @return     The MPSImage containing the gradient result.
 */
-(MPSImage*__nonnull) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                sourceImage: (MPSImage * __nonnull) sourceImage
                                     labels: (MPSCNNLossLabels * __nonnull) labels
MPS_SWIFT_NAME( encode(commandBuffer:sourceImage:labels:));


-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (MPSImageBatch * __nonnull) sourceImage
                            labels: (MPSCNNLossLabelsBatch * __nonnull) labels
                 destinationImages: (MPSImageBatch * __nonnull) destinationImage
MPS_SWIFT_NAME( encode(commandBuffer:sourceImages:labels:destinationImages:));

-(MPSImageBatch*__nonnull) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                         sourceImages: (MPSImageBatch * __nonnull) sourceImage
                                               labels: (MPSCNNLossLabelsBatch * __nonnull) labels
MPS_SWIFT_NAME( encode(commandBuffer:sourceImages:labels:));

@end /* MPSCNNYOLOLoss */


#ifdef __cplusplus
}
#endif

#endif /* MPSCNNLoss_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNReshape.h
//
//  MPSNNReshape.h
//  MPSNeuralNetwork
//
//  Created by Aaftab Munshi on 12/11/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSNNReshape_h
#define MPSNNReshape_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif

/*
 *  @class      MPSNNReshape
 *  @dependency This depends on Metal.framework
 *  @abstract   Describes a reshape operation
 *  @discussion This functions copies data from source MPSImage intot the new shape in the destination MPSImage
 *
 */

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNReshape : MPSCNNKernel

/*!
 *  @abstract Initialize a MPSNNReshape kernel
 *  @param    device    The device the filter will run on
 *  @return   A valid MPSNNReshape object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;


-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(__nonnull id<MTLDevice>)device    NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReshape_h */


#ifdef __cplusplus
}
#endif

#endif /* MPSNNReshape_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSRNNLayer.h
//
//  MPSRNNLayer.h
//  MPS
//
//  Created by Teemu Rantalaiho on 12/13/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSRNNLayer_h
#define MPSRNNLayer_h


#include <MPSNeuralNetwork/MPSCNNConvolution.h>
#include <MPSMatrix/MPSMatrix.h>

#ifdef __cplusplus
extern "C" {
#endif


/*! @enum       MPSRNNSequenceDirection
 *  @abstract   Defines the direction in which a sequence of inputs is processed by a RNN Layer.
 *              @see MPSRNNImageInferenceLayer and @see MPSRNNMatrixInferenceLayer.
 */
#if defined(DOXYGEN)
typedef enum MPSRNNSequenceDirection
#else
typedef NS_ENUM(NSUInteger, MPSRNNSequenceDirection)
#endif
{
    /*! The input sequence is processed from index zero to array length minus one */
    MPSRNNSequenceDirectionForward    MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(forward)  = 0,
    /*! The input sequence is processed from index array length minus one to zero */
    MPSRNNSequenceDirectionBackward  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)),
}
#if defined(DOXYGEN)
MPSRNNSequenceDirection
#endif
;

/*! @enum       MPSRNNBidirectionalCombineMode
 *  @abstract   Defines the way in which two images or matrices are combined together, or if the results are to be kept separate.
 *              @see MPSRNNImageInferenceLayer and @see MPSRNNMatrixInferenceLayer.
 */
#if defined(DOXYGEN)
typedef enum MPSRNNBidirectionalCombineMode
#else
typedef NS_ENUM(NSUInteger, MPSRNNBidirectionalCombineMode)
#endif
{
    /*! The two sequences are kept separate */
    MPSRNNBidirectionalCombineModeNone    MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(none)  = 0,
    /*! The two sequences are summed together to form a single output */
    MPSRNNBidirectionalCombineModeAdd  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)),
    /*! The two sequences are concatenated together along the feature channels to form a single output */
    MPSRNNBidirectionalCombineModeConcatenate  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)),
}
#if defined(DOXYGEN)
MPSRNNBidirectionalCombineMode
#endif
;




#pragma mark -
#pragma mark MPSRNNDescriptor

/*!
 *  @class      MPSRNNDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSRNNDescriptor specifies a Recursive neural network block/layer descriptor.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSRNNDescriptor : NSObject


/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image or number of rows in the input matrix.
 */
@property(readwrite, nonatomic) NSUInteger      inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the destination image or number of rows in the destination matrix.
 */
@property(readwrite, nonatomic) NSUInteger      outputFeatureChannels;

/*! @property   useLayerInputUnitTransformMode
 *  @abstract   if YES then use identity transformation for all weights (W, Wr, Wi, Wf, Wo, Wc) affecting input x_j in this layer,
 *              even if said weights are specified as nil.
 *              For example 'W_ij * x_j' is replaced by 'x_j' in formulae defined in @ref MPSRNNSingleGateDescriptor. Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL            useLayerInputUnitTransformMode;

/*! @property   useFloat32Weights
 *  @abstract   If YES, then @ref MPSRNNMatrixInferenceLayer uses 32-bit floating point numbers internally for weights when
 *              computing matrix transformations. If NO, then 16-bit, half precision floating point numbers are used.
 *              Currently @ref MPSRNNImageInferenceLayer ignores this property and the convolution operations always
 *              convert FP32 weights into FP16 for better performance.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL            useFloat32Weights;



/*! @property   layerSequenceDirection
 *  @abstract   When the layer specified with this descriptor is used to process a sequence of inputs
 *              by calling @see encodeBidirectionalSequenceToCommandBuffer then this parameter defines
 *              in which direction the sequence is processed. The operation of the layer is:
 *                  (yt, ht, ct) = f(xt,ht-1,ct-1) for MPSRNNSequenceDirectionForward
 *              and
 *                  (yt, ht, ct) = f(xt,ht+1,ct+1) for MPSRNNSequenceDirectionBackward, where
 *              xt is the output of the previous layer that encodes in the same direction as this layer,
 *              (or the input image or matrix if this is the first layer in stack with this direction).
 *              @see MPSRNNImageInferenceLayer and @see MPSRNNMatrixInferenceLayer.
 */
@property(readwrite, nonatomic) MPSRNNSequenceDirection      layerSequenceDirection;

@end    /* MPSRNNDescriptor */



#pragma mark -
#pragma mark MPSRNNSingleGateDescriptor

/*!
 *  @class      MPSRNNSingleGateDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSRNNSingleGateDescriptor specifies a simple recurrent block/layer descriptor.
 *              The RNN layer initialized with a MPSRNNSingleGateDescriptor transforms the input data (image or matrix),
 *              and previous output with a set of filters, each producing one feature map in the new output data.
 *              The user may provide the RNN unit a single input or a sequence of inputs.
 *
 *                  Description of operation:
 *
 *              Let x_j be the input data (at time index t of sequence,
 *                          j index containing quadruplet: batch index, x,y and feature index (x=y=0 for matrices)).
 *              Let h0_j be the recurrent input (previous output) data from previous time step (at time index t-1 of sequence).
 *              Let h1_i be the output data produced at this time step.
 *
 *              Let W_ij, U_ij be the weights for input and recurrent input data respectively
 *              Let b_i be a bias term
 *
 *              Let gi(x) be a neuron activation function
 *
 *              Then the new output image h1_i data is computed as follows:
 *
 *                  h1_i = gi( W_ij * x_j + U_ij * h0_j  + b_i )
 *
 *              The '*' stands for convolution (see @ref MPSRNNImageInferenceLayer) or matrix-vector/matrix multiplication
 *              (see @ref MPSRNNMatrixInferenceLayer).
 *              Summation is over index j (except for the batch index), but there is no summation over
 *              repeated index i - the output index.
 *              Note that for validity all intermediate images have to be of same size and the U matrix has to be square
 *              (ie. outputFeatureChannels == inputFeatureChannels in those). Also the bias terms are scalars wrt. spatial dimensions.
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSRNNSingleGateDescriptor : MPSRNNDescriptor


// Input parameters

/*! @property   inputWeights
 *  @abstract   Contains weights 'W_ij', bias 'b_i' and neuron 'gi' from the simple RNN layer formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping). Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputWeights;

/*! @property   recurrentWeights
 *  @abstract   Contains weights 'U_ij' from the simple RNN layer formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> recurrentWeights;
/*!
 *  @abstract   Creates a MPSRNNSingleGateDescriptor
 *  @param      inputFeatureChannels    The number of feature channels in the input image/matrix. Must be >= 1.
 *  @param      outputFeatureChannels   The number of feature channels in the output image/matrix. Must be >= 1.
 *  @return     A valid MPSRNNSingleGateDescriptor object or nil, if failure.
 */
+(nonnull instancetype) createRNNSingleGateDescriptorWithInputFeatureChannels: (NSUInteger) inputFeatureChannels
                                                        outputFeatureChannels: (NSUInteger) outputFeatureChannels
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

@end    /* MPSRNNSingleGateDescriptor */



#pragma mark -
#pragma mark MPSGRUDescriptor

/*!
 *  @class      MPSGRUDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSGRUDescriptor specifies a GRU (Gated Recurrent Unit) block/layer descriptor.
 *              The RNN layer initialized with a MPSGRUDescriptor transforms the input data (image or matrix),
 *              and previous output with a set of filters, each producing one feature map in
 *              the output data according to the Gated unit formulae detailed below.
 *              The user may provide the GRU unit a single input or a sequence of inputs. The layer also supports
 *              p-norm gating (Detailed in: https://arxiv.org/abs/1608.03639 ).
 *
 *                  Description of operation:
 *
 *              Let x_j be the input data (at time index t of sequence,
 *                          j index containing quadruplet: batch index, x,y and feature index (x=y=0 for matrices)).
 *              Let h0_j be the recurrent input (previous output) data from previous time step (at time index t-1 of sequence).
 *              Let h_i be the proposed new output.
 *              Let h1_i be the output data produced at this time step.
 *
 *              Let Wz_ij, Uz_ij, be the input gate weights for input and recurrent input data respectively
 *              Let bi_i be the bias for the input gate
 *
 *              Let Wr_ij, Ur_ij be the recurrent gate weights for input and recurrent input data respectively
 *              Let br_i be the bias for the recurrent gate
 *
 *              Let Wh_ij, Uh_ij, Vh_ij, be the output gate weights for input, recurrent gate and input gate respectively
 *              Let bh_i be the bias for the output gate
 *
 *              Let gz(x), gr(x), gh(x) be the neuron activation function for the input, recurrent and output gates
 *              Let p > 0 be a scalar variable (typicall p >= 1.0) that defines the p-norm gating norm value.
 *
 *              Then the output of the Gated Recurrent Unit layer is computed as follows:
 *
 *                      z_i = gz(  Wz_ij * x_j  +  Uz_ij * h0_j  +  bz_i  )
 *                      r_i = gr(  Wr_ij * x_j  +  Ur_ij * h0_j  +  br_i  )
 *                      c_i =      Uh_ij * (r_j h0_j)  +  Vh_ij * (z_j h0_j)
 *                      h_i = gh(  Wh_ij * x_j  + c_i + bh_i  )
 *
 *                  h1_i = ( 1 - z_i ^ p)^(1/p) h_i + z_i h0_i
 *
 *
 *              The '*' stands for convolution (see @ref MPSRNNImageInferenceLayer) or matrix-vector/matrix multiplication
 *              (see @ref MPSRNNMatrixInferenceLayer).
 *              Summation is over index j (except for the batch index), but there is no summation over
 *              repeated index i - the output index.
 *              Note that for validity all intermediate images have to be of same size and all U and V matrices have to be square
 *              (ie. outputFeatureChannels == inputFeatureChannels in those). Also the bias terms are scalars wrt. spatial dimensions.
 *              The conventional GRU block is achieved by setting Vh = 0 (nil) and the so-called Minimal Gated Unit is achieved with Uh = 0.
 *              (The Minimal Gated Unit is detailed in: https://arxiv.org/abs/1603.09420 and there they call z_i the value of the forget gate).
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSGRUDescriptor : MPSRNNDescriptor


// Input gate parameters

/*! @property   inputGateInputWeights
 *  @abstract   Contains weights 'Wz_ij', bias 'bz_i' and neuron 'gz' from the GRU formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping). Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputGateInputWeights;
/*! @property   inputGateRecurrentWeights
 *  @abstract   Contains weights 'Uz_ij' from the GRU formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputGateRecurrentWeights;




// Recurrent gate parameters

/*! @property   recurrentGateInputWeights
 *  @abstract   Contains weights 'Wr_ij', bias 'br_i' and neuron 'gr' from the GRU formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping).Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> recurrentGateInputWeights;
/*! @property   recurrentGateRecurrentWeights
 *  @abstract   Contains weights 'Ur_ij' from the GRU formula.
 *              If nil then assumed zero weights.Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> recurrentGateRecurrentWeights;




// Proposed output gate parameters

/*! @property   outputGateInputWeights
 *  @abstract   Contains weights 'Wh_ij', bias 'bh_i' and neuron 'gh' from the GRU formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping).Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateInputWeights;
/*! @property   outputGateRecurrentWeights
 *  @abstract   Contains weights 'Uh_ij' from the GRU formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateRecurrentWeights;
/*! @property   outputGateInputGateWeights
 *  @abstract   Contains weights 'Vh_ij' - can be used to implement the "Minimally Gated Unit".
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateInputGateWeights;


/*! @property   gatePnormValue
 *  @abstract   The p-norm gating norm value as specified by the GRU formulae. Defaults to 1.0f.
 */
@property(readwrite, nonatomic) float       gatePnormValue;

/*! @property   flipOutputGates
 *  @abstract   If YES then the GRU-block output formula is changed to:
 *                  h1_i = ( 1 - z_i ^ p)^(1/p) h0_i + z_i h_i.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL        flipOutputGates;

/*!
 *  @abstract   Creates a GRU descriptor.
 *  @param      inputFeatureChannels    The number of feature channels in the input image/matrix. Must be >= 1.
 *  @param      outputFeatureChannels   The number of feature channels in the output image/matrix. Must be >= 1.
 *  @return     A valid MPSGRUDescriptor object or nil, if failure.
 */
+(nonnull instancetype) createGRUDescriptorWithInputFeatureChannels: (NSUInteger) inputFeatureChannels
                                              outputFeatureChannels: (NSUInteger) outputFeatureChannels
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

@end    /* MPSGRUDescriptor */





#pragma mark -
#pragma mark MPSLSTMDescriptor

/*!
 *  @class      MPSLSTMDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSLSTMDescriptor specifies a LSTM block/layer descriptor.
 *              The RNN layer initialized with a MPSLSTMDescriptor transforms the input data (image or matrix),
 *              the memory cell data and previous output with a set of filters, each producing one feature map in
 *              the output data and memory cell, according to the LSTM formulae detailed below.
 *              The user may provide the LSTM unit a single input or a sequence of inputs.
 *
 *                  Description of operation:
 *
 *              Let x_j be the input data (at time index t of sequence,
 *                          j index containing quadruplet: batch index, x,y and feature index (x=y=0 for matrices)).
 *              Let h0_j be the recurrent input (previous output) data from previous time step (at time index t-1 of sequence).
 *              Let h1_i be the output data produced at this time step.
 *              Let c0_j be the previous memory cell data (at time index t-1 of sequence).
 *              Let c1_i be the new memory cell data (at time index t-1 of sequence).
 *
 *              Let Wi_ij, Ui_ij, Vi_ij, be the input gate weights for input, recurrent input and memory cell (peephole) data respectively
 *              Let bi_i be the bias for the input gate
 *
 *              Let Wf_ij, Uf_ij, Vf_ij, be the forget gate weights for input, recurrent input and memory cell data respectively
 *              Let bf_i be the bias for the forget gate
 *
 *              Let Wo_ij, Uo_ij, Vo_ij, be the output gate weights for input, recurrent input and memory cell data respectively
 *              Let bo_i be the bias for the output gate
 *
 *              Let Wc_ij, Uc_ij, Vc_ij, be the memory cell gate weights for input, recurrent input and memory cell data respectively
 *              Let bc_i be the bias for the memory cell gate
 *
 *              Let gi(x), gf(x), go(x), gc(x) be neuron activation function for the input, forget, output gate and memory cell gate
 *              Let gh(x) be the activation function applied to result memory cell data
 *
 *              Then the new memory cell data c1_j and output image h1_i are computed as follows:
 *
 *                      I_i = gi(  Wi_ij * x_j  +  Ui_ij * h0_j  +  Vi_ij * c0_j  + bi_i  )
 *                      F_i = gf(  Wf_ij * x_j  +  Uf_ij * h0_j  +  Vf_ij * c0_j  + bf_i  )
 *                      C_i = gc(  Wc_ij * x_j  +  Uc_ij * h0_j  +  Vc_ij * c0_j  + bc_i  )
 *
 *                  c1_i = F_i c0_i  +  I_i C_i
 *
 *                      O_i = go(  Wo_ij * x_j  +  Uo_ij * h0_j  +  Vo_ij * c1_j  + bo_i  )
 *
 *                  h1_i = O_i gh( c1_i )
 *
 *              The '*' stands for convolution (see @ref MPSRNNImageInferenceLayer) or matrix-vector/matrix multiplication
 *              (see @ref MPSRNNMatrixInferenceLayer).
 *              Summation is over index j (except for the batch index), but there is no summation over
 *              repeated index i - the output index.
 *              Note that for validity all intermediate images have to be of same size and all U and V matrices have to be square
 *              (ie. outputFeatureChannels == inputFeatureChannels in those). Also the bias terms are scalars wrt. spatial dimensions.
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSLSTMDescriptor : MPSRNNDescriptor


/*! @property   memoryWeightsAreDiagonal
 *  @abstract   If YES, then the 'peephole' weight matrices will be diagonal matrices represented as
 *              vectors of length the number of features in memory cells, that will be multiplied pointwise
 *              with the peephole matrix or image in order to achieve the diagonal (nonmixing) update.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic)     BOOL      memoryWeightsAreDiagonal;



// Input gate parameters

/*! @property   inputGateInputWeights
 *  @abstract   Contains weights 'Wi_ij', bias 'bi_i' and neuron 'gi' from the LSTM formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping). Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputGateInputWeights;
/*! @property   inputGateRecurrentWeights
 *  @abstract   Contains weights 'Ui_ij' from the LSTM formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputGateRecurrentWeights;
/*! @property   inputGateMemoryWeights
 *  @abstract   Contains weights 'Vi_ij' - the 'peephole' weights - from the LSTM formula.
 *              if YES == memoryWeightsAreDiagonal, then the number of weights used is the number of features
 *                  in the memory cell image/matrix.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> inputGateMemoryWeights;




// Forget gate parameters

/*! @property   forgetGateInputWeights
 *  @abstract   Contains weights 'Wf_ij', bias 'bf_i' and neuron 'gf' from the LSTM formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping).Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> forgetGateInputWeights;
/*! @property   forgetGateRecurrentWeights
 *  @abstract   Contains weights 'Uf_ij' from the LSTM formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> forgetGateRecurrentWeights;
/*! @property   forgetGateMemoryWeights
 *  @abstract   Contains weights 'Vf_ij' - the 'peephole' weights - from the LSTM formula.
 *              if YES == memoryWeightsAreDiagonal, then the number of weights used is the number of features
 *                  in the memory cell image/matrix.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> forgetGateMemoryWeights;




// Output gate parameters

/*! @property   outputGateInputWeights
 *  @abstract   Contains weights 'Wo_ij', bias 'bo_i' and neuron 'go' from the LSTM formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping). Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateInputWeights;
/*! @property   outputGateRecurrentWeights
 *  @abstract   Contains weights 'Uo_ij' from the LSTM formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateRecurrentWeights;
/*! @property   outputGateMemoryWeights
 *  @abstract   Contains weights 'Vo_ij' - the 'peephole' weights - from the LSTM.
 *              if YES == memoryWeightsAreDiagonal, then the number of weights used is the number of features
 *                  in the memory cell image/matrix.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> outputGateMemoryWeights;




// Memory cell gate parameters

/*! @property   cellGateInputWeights
 *  @abstract   Contains weights 'Wc_ij', bias 'bc_i' and neuron 'gc' from the LSTM formula.
 *              If nil then assumed zero weights, bias and no neuron (identity mapping). Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> cellGateInputWeights;
/*! @property   cellGateRecurrentWeights
 *  @abstract   Contains weights 'Uc_ij' from the LSTM formula.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> cellGateRecurrentWeights;
/*! @property   cellGateMemoryWeights
 *  @abstract   Contains weights 'Vc_ij' - the 'peephole' weights - from the LSTM formula.
 *              if YES == memoryWeightsAreDiagonal, then the number of weights used is the number of features
 *                  in the memory cell image/matrix.
 *              If nil then assumed zero weights. Defaults to nil.
 */
@property (readwrite, retain, nonatomic, nullable)  id <MPSCNNConvolutionDataSource> cellGateMemoryWeights;



/*! @property   cellToOutputNeuronType
 *  @abstract   Neuron type definition for 'gh', see @ref MPSCNNNeuronType. Defaults to MPSCNNNeuronTypeTanH.
 */
@property(readwrite, nonatomic) MPSCNNNeuronType        cellToOutputNeuronType;

/*! @property   cellToOutputNeuronParamA
 *  @abstract   Neuron parameter A for 'gh'. Defaults to 1.0f.
 */
@property(readwrite, nonatomic) float                   cellToOutputNeuronParamA;

/*! @property   cellToOutputNeuronParamB
 *  @abstract   Neuron parameter B for 'gh'. Defaults to 1.0f.
 */
@property(readwrite, nonatomic) float                   cellToOutputNeuronParamB;

/*! @property   cellToOutputNeuronParamC
 *  @abstract   Neuron parameter C for 'gh'. Defaults to 1.0f.
 */
@property(readwrite, nonatomic) float                   cellToOutputNeuronParamC;



/*!
 *  @abstract   Creates a LSTM descriptor.
 *  @param      inputFeatureChannels    The number of feature channels in the input image/matrix. Must be >= 1.
 *  @param      outputFeatureChannels   The number of feature channels in the output image/matrix. Must be >= 1.
 *  @return     A valid MPSNNLSTMDescriptor object or nil, if failure.
 */
+(nonnull instancetype) createLSTMDescriptorWithInputFeatureChannels: (NSUInteger) inputFeatureChannels
                                               outputFeatureChannels: (NSUInteger) outputFeatureChannels
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

@end    /* MPSNNLSTMDescriptor */





/*!
 *  @class      MPSRNNRecurrentImageState
 *  @dependency This depends on Metal.framework
 *  @discussion This class holds all the data that is passed from one sequence iteration of the image-based RNN layer (stack) to the next.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSRNNRecurrentImageState : MPSState

/*!
 *  @abstract   Access the stored recurrent image data.
 *  @param      layerIndex      Index of the layer whose to get - belongs to { 0, 1,...,@see numberOfLayers - 1 }
 *  @return     For valid layerIndex the recurrent output image data, otherwise nil.
 */
-(nullable MPSImage*)   getRecurrentOutputImageForLayerIndex: (NSUInteger) layerIndex;

/*!
 *  @abstract   Access the stored memory cell image data (if present).
 *  @param      layerIndex      Index of the layer whose to get - belongs to { 0, 1,...,@see numberOfLayers - 1 }
 *  @return     For valid layerIndex the memory cell image data, otherwise nil.
 */
-(nullable MPSImage*)   getMemoryCellImageForLayerIndex: (NSUInteger) layerIndex;



@end    /* MPSRNNRecurrentImageState */



#pragma mark -
#pragma mark MPSRNNImageInferenceLayer

/*!
 *  @class      MPSRNNImageInferenceLayer
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSRNNImageInferenceLayer specifies a recurrent neural network layer for inference on MPSImages.
 *              Currently two types of recurrent layers are supported: ones that operate with convolutions on
 *              images: @ref MPSRNNImageInferenceLayer and one that operates on matrices: @ref MPSRNNMatrixInferenceLayer.
 *              The former can be often used to implement the latter by using 1x1-images, but due to
 *              image size restrictions and performance, it is advisable to use @ref MPSRNNMatrixInferenceLayer for
 *              linear recurrent layers.
 *              A MPSRNNImageInferenceLayer is initialized using a @ref MPSRNNLayerDescriptor, which further specifies the
 *              recurrent network layer, or an array of @ref MPSRNNLayerDescriptors, which specifies a stack
 *              of recurrent layers, that can operate in parallel a subset of the inputs in a sequence of inputs and
 *              recurrent outputs. Note that currently stacks with bidirectionally traversing encode functions do not support starting
 *              from a previous set of recurrent states, but this can be achieved quite easily by defining two separate
 *              unidirectional stacks of layers, and running the same input sequence on them separately (one forwards and one backwards)
 *              and ultimately combining the two result sequences as desired with auxiliary functions.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSRNNImageInferenceLayer : MPSCNNKernel


/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the input image.
 */
@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels per pixel in the output image.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;



/*! @property   numberOfLayers
 *  @abstract   Number of layers in the filter-stack. This will be one when using initWithDevice:rnnDescriptor to initialize
 *                  this filter and the number of entries in the array 'rnnDescriptors' when initializing this filter with
 *                  initWithDevice:rnnDescriptors.
 */
@property(readonly, nonatomic) NSUInteger   numberOfLayers;


/*! @property   recurrentOutputIsTemporary
 *  @abstract   How output states from @ref encodeSequenceToCommandBuffer are constructed.
 *              Defaults to NO. For reference @see MPSState.
 */
@property(readwrite, nonatomic) BOOL   recurrentOutputIsTemporary;

/*! @property   storeAllIntermediateStates
 *  @abstract   If YES then calls to @ref encodeSequenceToCommandBuffer return every recurrent state
 *              in the array: recurrentOutputStates.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL   storeAllIntermediateStates;



/*! @property   bidirectionalCombineMode
 *  @abstract   Defines how to combine the output-results, when encoding bidirectional layers using
 *              @ref encodeBidirectionalSequenceToCommandBuffer.
 *              Defaults to @ref MPSRNNBidirectionalCombineModeNone.
 */
@property(readwrite, nonatomic) MPSRNNBidirectionalCombineMode   bidirectionalCombineMode;


/*!
 *  @abstract   Initializes a convolutional RNN kernel
 *  @param      device                          The MTLDevice on which this MPSRNNImageLayer filter will be used
 *  @param      rnnDescriptor                   The descriptor that defines the RNN layer
 *  @return     A valid MPSRNNImageInferenceLayer object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         rnnDescriptor: (nonnull const MPSRNNDescriptor*) rnnDescriptor
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Initializes a kernel that implements a stack of convolutional RNN layers
 *  @param      device                          The MTLDevice on which this MPSRNNImageLayer filter will be used
 *  @param      rnnDescriptors                  An array of RNN descriptors that defines a stack of RNN layers, starting at index zero.
 *                                                  The number of layers in stack is the number of entries in the array.
 *                                                  All entries in the array must be valid MPSRNNDescriptors.
 *  @return     A valid MPSRNNImageInferenceLayer object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        rnnDescriptors: (NSArray<const MPSRNNDescriptor*>  * __nonnull) rnnDescriptors
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*
 * Use initWithDevice:rnnDescriptor instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*!
 *  @abstract   Encode an MPSRNNImageInferenceLayer kernel (stack) for a sequence of inputs into a command buffer.
 *                  Note that when encoding using this function the @see layerSequenceDirection is ignored and the layer stack operates as
 *                  if all layers were forward feeding layers. In order to run bidirectional sequences
 *                  use @ref encodeBidirectionalSequenceToCommandBuffer:sourceSequence: or alternatively run two layer stacks and combine
 *                  results at the end using utility functions.
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImages                    An array of valid MPSImage objects containing the sequence of source images.
 *  @param      destinationImages               An array valid MPSImages to be overwritten by result image sequence. destinationImages may not alias sourceImages.
 *  @param      recurrentInputState             An optional state containing the output images and memory cells (for LSTMs)
 *                                                  of the layer obtained from the previous input images in a sequence of inputs.
 *                                                  Has to be the output of a previous call to this function or nil (assumed zero).
 *                                                  Note: can be one of the states returned in @ref recurrentOutputStates.
 *  @param      recurrentOutputStates            An optional array that will contain the recurrent output states. If nil then
 *                                                  the recurrent output state is discarded.
 *                                                  If @ref storeAllIntermediateStates is YES, then all intermediate states of the sequence
 *                                                  are returned in the array, the first one corresponding to the first input in the sequence,
 *                                                  otherwise only the last recurrent output state is returned.
 *                                                  If recurrentOutputIsTemporary is YES and then all returned recurrent states
 *                                                  will be temporary. @see MPSState:isTemporary.
 *                                                  Example: In order to get a new state one can do the following:
 *                                                  @code
 *                                                      MPSRNNRecurrentImageState* recurrent0 = nil;
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                        sourceImage: source0
 *                                                                   destinationImage: destination0
 *                                                                recurrentInputState: nil
 *                                                               recurrentOutputState: &recurrent0];
 *                                                  @endcode
 *                                                  Then use it for the next input in sequence:
 *                                                  @code
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                        sourceImage: source1
 *                                                                   destinationImage: destination1
 *                                                                recurrentInputState: recurrent0
 *                                                               recurrentOutputState: &recurrent0];
 *                                                  @endcode
 *                                                  And discard recurrent output of the third input:
 *                                                  @code
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                        sourceImage: source2
 *                                                                   destinationImage: destination2
 *                                                                recurrentInputState: recurrent0
 *                                                               recurrentOutputState: nil];
 *                                                  @endcode
 */

-(void)  encodeSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                          sourceImages:(NSArray<MPSImage*> * __nonnull)sourceImages
                     destinationImages:(NSArray<MPSImage*> * __nonnull)destinationImages
                   recurrentInputState:(MPSRNNRecurrentImageState * __nullable)recurrentInputState
                 recurrentOutputStates:(NSMutableArray<MPSRNNRecurrentImageState*>  * __nullable)recurrentOutputStates

MPS_SWIFT_NAME( encodeSequence(commandBuffer:sourceImages:destinationImages:recurrentInputState:recurrentOutputStates:));


/*!
 *  @abstract   Encode an MPSRNNImageInferenceLayer kernel stack for an input image sequences into a command buffer bidirectionally.
 *                  The operation proceeds as follows: The first source image x0 is passed through all forward traversing layers in the stack,
 *                  ie. those that were initialized with MPSRNNSequenceDirectionForward, recurrent input is assumed zero.
 *                  This produces forward output yf0 and recurrent states hf00, hf01, hf02, ... hf0n, one for each forward layer.
 *                  Then x1 is passed to forward layers together with recurrent state hf00, hf01, ..., hf0n, which produces yf1, and hf10,...
 *                  This procedure is iterated until the last image in the input sequence x_(N-1), which produces forward output yf(N-1).
 *                  The backwards layers iterate the same sequence backwards, starting from input x_(N-1) (recurrent state zero),
 *                  that produces yb(N-1) and recurrent output hb(N-1)0, hf(N-1)1, ... hb(N-1)m, one for each backwards traversing layer.
 *                  Then the backwards layers handle input x_(N-2) using recurrent state hb(N-1)0, ..., et cetera, until the
 *                  first image of the sequence is computed, producing output yb0. The result of the operation is either pair of sequences
 *                  ({yf0, yf1, ... , yf(N-1)},  {yb0, yb1, ... , yb(N-1)}) or a combined sequence, {(yf0 + yb0), ... , (yf(N-1) + yb(N-1)) },
 *                  where '+' stands either for sum, or concatenation along feature channels, as specified by @ref bidirectionalCombineMode.
 *
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceSequence                  An array of valid MPSImage objects containing the source image sequence (x0, x1, ... x_n-1).
 *  @param      destinationForwardImages        An array of valid MPSImages to be overwritten by result from forward input images. If bidirectionalCombineMode
 *                                                  is either MPSRNNBidirectionalCombineModeAdd or MPSRNNBidirectionalCombineModeConcatenate, then will
 *                                                  contain the combined results. destinationForwardImage may not alias with any of the source images.
 *  @param      destinationBackwardImages       If bidirectionalCombineMode is MPSRNNBidirectionalCombineModeNone, then must be a valid MPSImage
 *                                                  that will be  overwritten by result from backward input image. Otherwise this parameter is ignored
 *                                                  and can be nil. destinationBackwardImages may not alias to any of the source images.
 */

-(void) encodeBidirectionalSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                                    sourceSequence:(NSArray<MPSImage*> * __nonnull)sourceSequence
                          destinationForwardImages:(NSArray<MPSImage*> * __nonnull)destinationForwardImages
                         destinationBackwardImages:(NSArray<MPSImage*> * __nullable)destinationBackwardImages
MPS_SWIFT_NAME( encodeBidirectionalSequence(commandBuffer:sourceSequence:destinationForwardImages:destinationBackwardImages:));

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSRNNImageInferenceLayer
 *  @param      device      The MTLDevice on which to make the MPSRNNImageInferenceLayer
 *  @return     A new MPSRNNImageInferenceLayer object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end    /* MPSRNNImageInferenceLayer */



/* MPSRNNRecurrentMatrixState */


/*!
 *  @class      MPSRNNRecurrentMatrixState
 *  @dependency This depends on Metal.framework
 *  @discussion This class holds all the data that is passed from one sequence iteration of the matrix-based RNN layer to the next.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSRNNRecurrentMatrixState : MPSState
/*!
 *  @abstract   Access the stored recurrent matrix data.
 *  @param      layerIndex      Index of the layer whose to get - belongs to { 0, 1,...,@see numberOfLayers - 1 }
 *  @return     For valid layerIndex the recurrent output matrix data, otherwise nil.
 */
-(nullable MPSMatrix*)   getRecurrentOutputMatrixForLayerIndex: (NSUInteger) layerIndex;

/*!
 *  @abstract   Access the stored memory cell matrix data (if present).
 *  @param      layerIndex      Index of the layer whose to get - belongs to { 0, 1,...,@see numberOfLayers - 1 }
 *  @return     For valid layerIndex the memory cell image matrix, otherwise nil.
 */
-(nullable MPSMatrix*)   getMemoryCellMatrixForLayerIndex: (NSUInteger) layerIndex;

@end



#pragma mark -
#pragma mark MPSRNNMatrixInferenceLayer

/*!
 *  @class      MPSRNNMatrixInferenceLayer
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSRNNMatrixInferenceLayer specifies a recurrent neural network layer for inference on MPSMatrices.
 *              Currently two types of recurrent layers are supported: ones that operate with convolutions on
 *              images: @ref MPSRNNImageInferenceLayer and one that operates on matrices: @ref MPSRNNMatrixInferenceLayer.
 *              The former can be often used to implement the latter by using 1x1-matrices, but due to
 *              image size restrictions and performance, it is advisable to use @ref MPSRNNMatrixInferenceLayer for
 *              linear recurrent layers.
 *              A MPSRNNMatrixInferenceLayer is initialized using a @ref MPSRNNLayerDescriptor, which further specifies the
 *              recurrent network layer, or an array of @ref MPSRNNLayerDescriptors, which specifies a stack
 *              of recurrent layers, that can operate in parallel a subset of the inputs in a sequence of inputs and
 *              recurrent outputs. Note that currently stacks with bidirectionally traversing encode functions do not support starting
 *              from a previous set of recurrent states, but this can be achieved quite easily by defining two separate
 *              unidirectional stacks of layers, and running the same input sequence on them separately (one forwards and one backwards)
 *              and ultimately combining the two result sequences as desired with auxiliary functions.
 *              The input and output vectors in encode calls are stored as rows of the input and output matrices and
 *              MPSRNNMatrixInferenceLayer supports matrices with decreasing number of rows: The row-indices identify the different
 *              sequences that may be of different lengths - for example if we have three sequences:
 *                  ( x1, x2, x3 ), ( y1, y2, y3, y4 ) and ( z1, z2 )
 *              of vectors xi, yi and zi, then these can be inserted together as a batch to the sequence encoding kernel by
 *              using the matrices:
 *                  @code
 *                           ( y1 )        ( y2 )        ( y3 )        ( y4 )
 *                      m1 = ( x1 ),  m2 = ( x2 ),  m3 = ( x3 ),  m4 =
 *                           ( z1 )        ( z2 )
 *                  @endcode
 *              If a recurrent output state is requested then it will contain the state corresponding to last inputs to each
 *              sequence and if all the intermediate states are requested (see storeAllIntermediateStates),
 *              then the shorter sequences will be propagated by copying the state of the previous output if the
 *              input vector is not present in the sequence - in the example above the output states would be:
 *                  @code
 *                           ( s_y1 )        ( s_y2 )        ( s_y3 )        ( s_y4 )
 *                      s1 = ( s_x1 ),  s2 = ( s_x2 ),  s3 = ( s_x3 ),  s4 = ( s_x3 )
 *                           ( s_z1 )        ( s_z2 )        ( s_z2 )        ( s_z2 )
 *                  @endcode
 *              The mathematical operation described in the linear transformations of @ref MPSRNNSingleGateDescriptor
 *              @ref MPSLSTMDescriptor and @ref MPSGRUDescriptor are y^T = W x^T  <=> y = x W^T, where x is the matrix containing
 *              the input vectors as rows, y is the matrix containing the output vectors as rows and W is the weight matrix.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSRNNMatrixInferenceLayer : MPSKernel


/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels input vector/matrix.
 */
@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels in the output vector/matrix.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;



/*! @property   numberOfLayers
 *  @abstract   Number of layers in the filter-stack. This will be one when using initWithDevice:rnnDescriptor to initialize
 *                  this filter and the number of entries in the array 'rnnDescriptors' when initializing this filter with
 *                  initWithDevice:rnnDescriptors.
 */
@property(readonly, nonatomic) NSUInteger   numberOfLayers;

/*! @property   recurrentOutputIsTemporary
 *  @abstract   How output states from @ref encodeSequenceToCommandBuffer are constructed.
 *              Defaults to NO. For reference @see MPSState.
 */
@property(readwrite, nonatomic) BOOL   recurrentOutputIsTemporary;

/*! @property   storeAllIntermediateStates
 *  @abstract   If YES then calls to @ref encodeSequenceToCommandBuffer return every recurrent state
 *              in the array: recurrentOutputStates.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL   storeAllIntermediateStates;


/*! @property   bidirectionalCombineMode
 *  @abstract   Defines how to combine the output-results, when encoding bidirectional layers using
 *              @ref encodeBidirectionalSequenceToCommandBuffer.
 *              Defaults to @ref MPSRNNBidirectionalCombineModeNone.
 */
@property(readwrite, nonatomic) MPSRNNBidirectionalCombineMode   bidirectionalCombineMode;


/*!
 *  @abstract   Initializes a linear (fully connected) RNN kernel
 *  @param      device                          The MTLDevice on which this MPSRNNMatrixLayer filter will be used
 *  @param      rnnDescriptor                   The descriptor that defines the RNN layer
 *  @return     A valid MPSRNNMatrixInferenceLayer object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         rnnDescriptor: (nonnull const MPSRNNDescriptor*) rnnDescriptor
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Initializes a kernel that implements a stack of linear (fully connected) RNN layers
 *  @param      device                          The MTLDevice on which this MPSRNNMatrixLayer filter will be used
 *  @param      rnnDescriptors                  An array of RNN descriptors that defines a stack of RNN layers, starting at index zero.
 *                                                  The number of layers in stack is the number of entries in the array.
 *                                                  All entries in the array must be valid MPSRNNDescriptors.
 *  @return     A valid MPSRNNMatrixInferenceLayer object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        rnnDescriptors: (NSArray<const MPSRNNDescriptor*>  * __nonnull) rnnDescriptors
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*
 * Use initWithDevice:rnnDescriptor instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract   Encode an MPSRNNMatrixInferenceLayer kernel (stack) for a sequence of inputs into a command buffer.
 *                  Note that when encoding using this function the @see layerSequenceDirection is ignored and the layer stack operates as
 *                  if all layers were forward feeding layers. In order to run bidirectional sequences
 *                  use @ref encodeBidirectionalSequenceToCommandBuffer:sourceSequence: or alternatively run two layer stacks and combine
 *                  results at the end using utility functions.
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceMatrices                  An array of valid MPSMatrix objects containing the sequence of source matrices.
 *  @param      sourceOffsets                   An array of byte-offsets into the sourceMatrices, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in sourceMatrices.
 *  @param      destinationMatrices             An array valid MPSMatrices to be overwritten by result matrix sequence.
 *                                                  destinationMatrices may not alias sourceMatrices.
 *  @param      destinationOffsets              An array of byte-offsets into the destinationMatrices, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in destinationMatrices.
 *  @param      recurrentInputState             An optional state containing the output matrices and memory cells (for LSTMs)
 *                                                  of the layer obtained from the previous input matrices in a sequence of inputs.
 *                                                  Has to be the output of a previous call to this function or nil (assumed zero).
 *                                                  Note: can be one of the states returned in @ref intermediateRecurrentStates.
 *  @param      recurrentOutputStates            An optional array that will contain the recurrent output states. If nil then
 *                                                  the recurrent output state is discarded.
 *                                                  If @ref storeAllIntermediateStates is YES, then all intermediate states of the sequence
 *                                                  are returned in the array, the first one corresponding to the first input in the sequence,
 *                                                  otherwise only the last recurrent output state is returned.
 *                                                  If recurrentOutputIsTemporary is YES and then all returned recurrent states
 *                                                  will be temporary. @see MPSState:isTemporary.
 *                                                  Example: In order to get a new state one can do the following:
 *                                                  @code
 *                                                      MPSRNNRecurrentMatrixState* recurrent0 = nil;
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                       sourceMatrix: source0
 *                                                                  destinationMatrix: destination0
 *                                                                recurrentInputState: nil
 *                                                               recurrentOutputState: &recurrent0];
 *                                                  @endcode
 *                                                  Then use it for the next input in sequence:
 *                                                  @code
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                       sourceMatrix: source1
 *                                                                  destinationMatrix: destination1
 *                                                                recurrentInputState: recurrent0
 *                                                               recurrentOutputState: &recurrent0];
 *                                                  @endcode
 *                                                  And discard recurrent output of the third input:
 *                                                  @code
 *                                                      [filter encodeToCommandBuffer: cmdBuf
 *                                                                       sourceMatrix: source2
 *                                                                  destinationMatrix: destination2
 *                                                                recurrentInputState: recurrent0
 *                                                               recurrentOutputState: nil];
 *                                                  @endcode
 */

-( void )  encodeSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                          sourceMatrices:(NSArray<MPSMatrix*> * __nonnull)sourceMatrices
                           sourceOffsets:(NSUInteger * __nullable) sourceOffsets
                     destinationMatrices:(NSArray<MPSMatrix*> * __nonnull)destinationMatrices
                      destinationOffsets:(NSUInteger * __nullable) destinationOffsets
                     recurrentInputState:(MPSRNNRecurrentMatrixState * __nullable)recurrentInputState
                   recurrentOutputStates:(NSMutableArray<MPSRNNRecurrentMatrixState*>  * __nullable)recurrentOutputStates
MPS_SWIFT_NAME( encodeSequence(commandBuffer:sourceMatrices:sourceOffsets:destinationMatrices:destinationOffsets:recurrentInputState:recurrentOutputStates:))
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));


-( void )  encodeSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                          sourceMatrices:(NSArray<MPSMatrix*> * __nonnull)sourceMatrices
                     destinationMatrices:(NSArray<MPSMatrix*> * __nonnull)destinationMatrices
                     recurrentInputState:(MPSRNNRecurrentMatrixState * __nullable)recurrentInputState
                   recurrentOutputStates:(NSMutableArray<MPSRNNRecurrentMatrixState*>  * __nullable)recurrentOutputStates

MPS_SWIFT_NAME( encodeSequence(commandBuffer:sourceMatrices:destinationMatrices:recurrentInputState:recurrentOutputStates:));


/*!
 *  @abstract   Encode an MPSRNNMatrixInferenceLayer kernel stack for an input matrix sequences into a command buffer bidirectionally.
 *                  The operation proceeds as follows: The first source matrix x0 is passed through all forward traversing layers in the stack,
 *                  ie. those that were initialized with MPSRNNSequenceDirectionForward, recurrent input is assumed zero.
 *                  This produces forward output yf0 and recurrent states hf00, hf01, hf02, ... hf0n, one for each forward layer in the stack.
 *                  Then x1 is passed to forward layers together with recurrent state hf00, hf01, ..., hf0n, which produces yf1, and hf10,...
 *                  This procedure is iterated until the last matrix in the input sequence x_(N-1), which produces forward output yf(N-1).
 *                  The backwards layers iterate the same sequence backwards, starting from input x_(N-1) (recurrent state zero),
 *                  that produces yb(N-1) and recurrent output hb(N-1)0, hf(N-1)1, ... hb(N-1)m, one for each backwards traversing layer.
 *                  Then the backwards layers handle input x_(N-2) using recurrent state hb(N-1)0, ..., et cetera, until the
 *                  first matrix of the sequence is computed, producing output yb0. The result of the operation is either pair of sequences
 *                  ({yf0, yf1, ... , yf(N-1)},  {yb0, yb1, ... , yb(N-1)}) or a combined sequence, {(yf0 + yb0), ... , (yf(N-1) + yb(N-1)) },
 *                  where '+' stands either for sum, or concatenation along feature channels, as specified by @ref bidirectionalCombineMode.
 *
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceSequence                  An array of valid MPSMatrix objects containing the source matrix sequence (x0, x1, ... x_n-1).
 *  @param      destinationForwardMatrices      An array of valid MPSMatrices to be overwritten by result from forward input matrices. If bidirectionalCombineMode
 *                                                  is either MPSRNNBidirectionalCombineModeAdd or MPSRNNBidirectionalCombineModeConcatenate, then will
 *                                                  contain the combined results. destinationForwardMatrix may not alias with any of the source matrices.
 *  @param      destinationBackwardMatrices     If bidirectionalCombineMode is MPSRNNBidirectionalCombineModeNone, then must be an array of valid MPSMatrices
 *                                                  that will be overwritten by result from backward input matrices. Otherwise this parameter is ignored
 *                                                  and can be nil. destinationBackwardMatrices may not alias to any of the source matrices.
 */

-(void) encodeBidirectionalSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                                    sourceSequence:(NSArray<MPSMatrix*> * __nonnull)sourceSequence
                        destinationForwardMatrices:(NSArray<MPSMatrix*> * __nonnull)destinationForwardMatrices
                       destinationBackwardMatrices:(NSArray<MPSMatrix*> * __nullable)destinationBackwardMatrices
MPS_SWIFT_NAME( encodeBidirectionalSequence(commandBuffer:sourceSequence:destinationForwardMatrices:destinationBackwardMatrices:));



/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSRNNMatrixInferenceLayer
 *  @param      device      The MTLDevice on which to make the MPSRNNMatrixInferenceLayer
 *  @return     A new MPSRNNMatrixInferenceLayer object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end    /* MPSRNNMatrixInferenceLayer */




/*!
 *  @class      MPSRNNMatrixTrainingState
 *  @dependency This depends on Metal.framework
 *  @discussion This class holds the data that is passed from the forward pass needed in the backward pass.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSRNNMatrixTrainingState : MPSState
@end

/*! @enum       MPSRNNMatrixId
 *  @abstract   Defines which matrix is to be copied in or out of the trainable RNN layer -
 *              @see MPSRNNMatrixTrainingLayer:encodeCopyWeightsToCommandBuffer. That is, this identifies a
 *              matrix within the set of trainable weight parameters in @ref initWithDevice, @ref createWeightGradientMatrices
 *              @ref encodeForwardSequenceToCommandBuffer, @ref encodeGradientSequenceToCommandBuffer etc.
 */
#if defined(DOXYGEN)
#    define MPS_SWIFT_NAME(_a)
#    define MPS_ENUM_AVAILABLE_STARTING(...)
typedef enum MPSRNNMatrixId
#else
typedef NS_ENUM(NSUInteger, MPSRNNMatrixId)
#endif
{
    MPSRNNMatrixIdSingleGateInputWeights            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)) MPS_SWIFT_NAME(SingleGateInputWeights)  = 0,
    MPSRNNMatrixIdSingleGateRecurrentWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdSingleGateBiasTerms               MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),


    MPSRNNMatrixIdLSTMInputGateInputWeights         MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMInputGateRecurrentWeights     MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMInputGateMemoryWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMInputGateBiasTerms            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixIdLSTMForgetGateInputWeights         MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMForgetGateRecurrentWeights     MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMForgetGateMemoryWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMForgetGateBiasTerms            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixIdLSTMMemoryGateInputWeights         MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMMemoryGateRecurrentWeights     MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMMemoryGateMemoryWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMMemoryGateBiasTerms            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixIdLSTMOutputGateInputWeights         MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMOutputGateRecurrentWeights     MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMOutputGateMemoryWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdLSTMOutputGateBiasTerms            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),


    MPSRNNMatrixIdGRUInputGateInputWeights            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRUInputGateRecurrentWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRUInputGateBiasTerms               MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixIdGRURecurrentGateInputWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRURecurrentGateRecurrentWeights    MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRURecurrentGateBiasTerms           MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixIdGRUOutputGateInputWeights            MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRUOutputGateRecurrentWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRUOutputGateInputGateWeights        MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),
    MPSRNNMatrixIdGRUOutputGateBiasTerms               MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)),

    MPSRNNMatrixId_count // Do not use - auxiliary enum value that gives number of ids.

}
#if defined(DOXYGEN)
MPSRNNMatrixId
#endif
;




#pragma mark -
#pragma mark MPSRNNMatrixTrainingLayer

/*!
 *  @class      MPSRNNMatrixTrainingLayer
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSRNNMatrixTrainingLayer specifies a recurrent neural network layer for training on MPSMatrices.
 *
 *              A MPSRNNMatrixTrainingLayer is initialized using a @ref MPSRNNLayerDescriptor, which further specifies the
 *              recurrent network layer.
 *              The input and output vectors in encode calls are stored as rows of the input and output matrices and
 *              MPSRNNMatrixTrainingLayer supports matrices with decreasing number of rows: The row-indices identify the different
 *              sequences that may be of different lengths - for example if we have three sequences:
 *                  ( x1, x2, x3 ), ( y1, y2, y3, y4 ) and ( z1, z2 )
 *              of vectors xi, yi and zi, then these can be inserted together as a batch to the sequence encoding kernel by
 *              using the matrices:
 *                  @code
 *                           ( y1 )        ( y2 )        ( y3 )        ( y4 )
 *                      m1 = ( x1 ),  m2 = ( x2 ),  m3 = ( x3 ),  m4 =
 *                           ( z1 )        ( z2 )
 *                  @endcode
 *              The gradient computation pass is then achieved by passing the corresponding gradient sequence from the
 *              previous layer ( dx1, dx2, dx3 ), ( dy1, dy2, dy3, dy4 ) and ( dz1, dz2 ) as matrices
 *                  @code
 *                            ( dy1 )         ( dy2 )         ( dy3 )         ( dy4 )
 *                      dm1 = ( dx1 ),  dm2 = ( dx2 ),  dm3 = ( dx3 ),  dm4 =
 *                            ( dz1 )         ( dz2 )
 *                  @endcode
 *
 *              The mathematical operation described in the linear transformations of @ref MPSRNNSingleGateDescriptor
 *              @ref MPSLSTMDescriptor and @ref MPSGRUDescriptor are y^T = W x^T  <=> y = x W^T, where x is the matrix containing
 *              the input vectors as rows, y is the matrix containing the output vectors as rows and W is the weight matrix.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSRNNMatrixTrainingLayer : MPSKernel


/*! @property   inputFeatureChannels
 *  @abstract   The number of feature channels input vector/matrix.
 */
@property(readonly, nonatomic) NSUInteger       inputFeatureChannels;

/*! @property   outputFeatureChannels
 *  @abstract   The number of feature channels in the output vector/matrix.
 */
@property(readonly, nonatomic) NSUInteger       outputFeatureChannels;

/*! @property   storeAllIntermediateStates
 *  @abstract   If YES then calls to functions @ref encodeForwardSequenceToCommandBuffer and
 *              @ref encodeGradientSequenceToCommandBuffer return every recurrent state
 *              in the array: recurrentOutputStates.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL            storeAllIntermediateStates;


/*! @property   recurrentOutputIsTemporary
 *  @abstract   How recurrent output states from @ref encodeForwardSequenceToCommandBuffer
 *              and encodeGradientSequenceToCommandBuffer are constructed.
 *              Defaults to NO. For reference @see MPSState.
 */
@property(readwrite, nonatomic) BOOL            recurrentOutputIsTemporary;


/*! @property   trainingStateIsTemporary
 *  @abstract   How training output states from @ref encodeForwardSequenceToCommandBuffer are constructed.
 *              Defaults to NO. For reference @see MPSState.
 */
@property(readwrite, nonatomic) BOOL            trainingStateIsTemporary;

/*! @property   accumulateWeightGradients
 *  @abstract   If yes then the computed weight gradients are accumulated on top of existing values in
 *              calls to the gradient computation functions: encodeGradientSequenceToCommandBuffer.
 *              Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL            accumulateWeightGradients;


/*!
 *  @abstract   Initializes a linear (fully connected) RNN kernel for training
 *  @param      device                      The MTLDevice on which this MPSRNNMatrixLayer filter will be used
 *  @param      rnnDescriptor               The descriptor that defines the RNN layer
 *  @param      trainableWeights            An array where to store the weights of the layer as MPSMatrices.
 *                                          NOTE: The exact layout and number of matrices may vary between
 *                                          platforms and therefore you should not save out these weights directly,
 *                                          but instead use the function encodeCopyWeightsToCommandBuffer to identify
 *                                          the weights and biases for serialization.
 *                                          Typically you should pass here an initialized but empty NSMutableArray and
 *                                          when this function returns the array will have been populated with the
 *                                          weight matrices needed in the encode-calls, by using initial values from
 *                                          the datasources in rnnDescriptor.
 *  @return     A valid MPSRNNMatrixTrainingLayer object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         rnnDescriptor: (nonnull const MPSRNNDescriptor*) rnnDescriptor
                      trainableWeights: (NSMutableArray<MPSMatrix*>  * __nonnull)trainableWeights
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));


/*!
 *  @abstract   Initializes a set of matrices that can be used in training for weight and bias gradient outputs in
 *              @see encodeBackwardSequenceToCommandBuffer. Can be also used to easily create auxiliary matrices for example
 *              for ADAM and other advanced optimization schemes. The layout and number of matrices is the same as for the outputs of
 *              @see initWithDevice, but the data type may differ. NOTE: These matrices cannot be used as weight matrices in the
 *              forward and backward encode calls, but matrices from initWithDevice() or createWeightMatrices() should be used instead.
 *  @param      matricesOut                 An array where the newly created matrices will be stored, will be initialized to zero.
 *  @param      dataType                    Datatype for the entries - currently MPSDataTypeFloat32 and MPSDataTypeFloat16 are supported.
 */
-(void) createWeightGradientMatrices: (NSMutableArray<MPSMatrix*>  * __nonnull) matricesOut
                            dataType: (MPSDataType) dataType
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*!
 *  @abstract   As @ref createWeightGradientMatrices, but the matrices will be temporary with readCount = 1, which means that they
 *              become invalid after the first encode call that reads them. Note also that as the matrices are temporary, their
 *              storage mode will be private which means that you can only access the data using a kernel on the GPU.
 *  @param      matricesOut                 An array where the newly created matrices will be stored, will be initialized to zero.
 *  @param      dataType                    Datatype for the entries - currently MPSDataTypeFloat32 and MPSDataTypeFloat16 are supported.
 *  @param      commandBuffer               The command buffer that the temporary matrices will live on.
 */
-(void) createTemporaryWeightGradientMatrices: (NSMutableArray<MPSMatrix*>  * __nonnull) matricesOut
                                     dataType: (MPSDataType) dataType
                                commandBuffer: (nonnull id<MTLCommandBuffer>) commandBuffer
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));


/*!
 *  @abstract   Initializes a set of matrices that can be used in training for weight and bias matrices in
 *              the forward and backward passes. The layout, datatype and number of matrices is the same as for the outputs of
 *              @see initWithDevice.
 *  @param      matricesOut                 An array where the newly created matrices will be stored, will be initialized to zero.
 */
-(void) createWeightMatrices: (NSMutableArray<MPSMatrix*>  * __nonnull) matricesOut
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));


/*
 * Use initWithDevice:rnnDescriptor instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*!
 *  @abstract   Encode a copy kernel that copies one matrix from the trainable weight set to a matrix with standard layout,
 *              where the column index is the input feature channel index (in forward direction) and row index is the output
 *              feature channel index.
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      weights                         An array weights from @see initWithDevice or @see createWeightMatrices.
 *  @param      matrixId                        Which matrix to copy - has to be a valid Id based on inputs defined in
 *                                              the rnnDescriptor of @see initWithDevice.
 *  @param      matrix                          The destination or source matrix that is used in the copy.
 *  @param      copyFromWeightsToMatrix         If YES then the copy direction is from the set of trainable 'weights' to 'matrix',
 *                                              otherwise the copy is done from 'matrix' to 'weights'.
 *  @param      matrixOffset                    A (valid) offset into matrix to be applied to the copy operation.
 */

-( void )  encodeCopyWeightsToCommandBuffer: (nonnull id<MTLCommandBuffer>)commandBuffer
                                    weights: (NSArray<MPSMatrix*>  * __nonnull)weights
                                   matrixId: (MPSRNNMatrixId) matrixId
                                     matrix: (MPSMatrix * __nonnull) matrix
                    copyFromWeightsToMatrix: (BOOL) copyFromWeightsToMatrix
                               matrixOffset: (MTLOrigin) matrixOffset

MPS_SWIFT_NAME( encodeCopyWeights(commandBuffer:weights:matrixId:matrix:copyFromWeightsToMatrix:matrixOffset:));


/*!
 *  @abstract   Encode an MPSRNNMatrixTrainingLayer forward pass kernel for a sequence of inputs into a command buffer.
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceMatrices                  An array of valid MPSMatrix objects containing the sequence of source matrices.
 *  @param      sourceOffsets                   An array of byte-offsets into the sourceMatrices, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in sourceMatrices.
 *  @param      destinationMatrices             An array valid MPSMatrices to be overwritten by result matrix sequence.
 *                                                  destinationMatrices may not alias sourceMatrices.
 *  @param      destinationOffsets              An array of byte-offsets into the destinationMatrices, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in destinationMatrices.
 *  @param      trainingStates                   An array containing the training states to be passed to the gradient computation
 *                                                  encode function.
 *  @param      recurrentInputState             An optional state containing the output matrices and memory cells (for LSTMs)
 *                                                  of the layer obtained from the previous input matrices in a sequence of inputs.
 *                                                  Has to be the output of a previous call to this function or nil (assumed zero).
 *  @param      recurrentOutputStates           An array that will be appended with the recurrent output states. May not be nil.
 *                                                  If recurrentOutputIsTemporary is YES and then all returned recurrent states
 *                                                  will be temporary. @see MPSState:isTemporary.
 *  @param      weights                         An array of valid MPSMatrix objects containing the weights, should be the array
 *                                                  that was produced either by @see initWithDevice or @see createWeightMatrices.
 */

-( void )  encodeForwardSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                                 sourceMatrices:(NSArray<MPSMatrix*> * __nonnull)sourceMatrices
                                  sourceOffsets:(NSUInteger * __nullable)sourceOffsets
                            destinationMatrices:(NSArray<MPSMatrix*> * __nonnull)destinationMatrices
                             destinationOffsets:(NSUInteger * __nullable)destinationOffsets
                                 trainingStates:(NSMutableArray<MPSRNNMatrixTrainingState*> * __nonnull) trainingStates
                            recurrentInputState:(MPSRNNRecurrentMatrixState * __nullable)recurrentInputState
                          recurrentOutputStates:(NSMutableArray<MPSRNNRecurrentMatrixState*>  * __nullable)recurrentOutputStates
                                        weights:(NSArray<MPSMatrix*>  * __nonnull)weights

MPS_SWIFT_NAME( encodeForwardSequence(commandBuffer:sourceMatrices:sourceOffsets:destinationMatrices:destinationOffsets:trainingStates:recurrentInputState:recurrentOutputStates:weights:));

/*!
 *  @abstract   Encode an MPSRNNMatrixTrainingLayer forward pass kernel for a sequence of inputs into a command buffer.
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceMatrices                  An array of valid MPSMatrix objects containing the sequence of source matrices.
 *  @param      destinationMatrices             An array valid MPSMatrices to be overwritten by result matrix sequence.
 *                                                  destinationMatrices may not alias sourceMatrices.
 *  @param      trainingStates                   An array containing the training states to be passed to the gradient computation
 *                                                  encode function.
 *  @param      weights                         An array of valid MPSMatrix objects containing the weights, should be the array
 *                                                  that was produced either by @see initWithDevice or @see createWeightMatrices.
 */

-( void )  encodeForwardSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>)commandBuffer
                                 sourceMatrices:(NSArray<MPSMatrix*> * __nonnull)sourceMatrices
                            destinationMatrices:(NSArray<MPSMatrix*> * __nonnull)destinationMatrices
                                 trainingStates:(NSMutableArray<MPSRNNMatrixTrainingState*> * __nonnull) trainingStates
                                        weights:(NSArray<MPSMatrix*>  * __nonnull)weights

MPS_SWIFT_NAME( encodeForwardSequence(commandBuffer:sourceMatrices:destinationMatrices:trainingStates:weights:));



/*!
 *  @abstract   Encode an MPSRNNMatrixTrainingLayer gradient pass kernel for a sequence of input gradients into a command buffer.
 *              NOTE: The time sequence indexing follows the array indexing in the inputs: sourceGradients[0] has to contain the
 *              gradients corresponding to the first matrix in the forward pass corresponding to the current subsequence, which is
 *              typically sourceMatrices[0].
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      forwardSources                  An array of MPSMatrix objects containing the sequence of source matrices of the forward pass.
 *  @param      forwardSourceOffsets            An array of byte-offsets into the forwardSources, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in forwardSources.
 *  @param      sourceGradients                 An array of valid MPSMatrix objects containing the sequence of source gradient matrices.
 *  @param      sourceGradientOffsets           An array of byte-offsets into the sourceGradients, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in sourceGradients.
 *  @param      destinationGradients            An array valid MPSMatrix objects that will receive the backpropagated gradients, may be
 *                                                  nil if not needed (for example first layer in graph).
 *  @param      destinationOffsets              An array of byte-offsets into the destinationGradients, if nil zeros are assumed and
 *                                                  if not nil must contain offset for every matrix in destinationGradients.
 *  @param      weightGradients                 An array of valid MPSMatrix objects that will receive the gradient wrt. weights and
 *                                                  biases of the layer - should be the array that was produced either
 *                                                  by @see initWithDevice or @see createWeightMatrices. May be nil in which case
 *                                                  the gradients for the weights are not computed.
 *  @param      trainingStates                  An array containing the training states from the forward pass - the array must contain
 *                                                  the states corresponding to the input gradients is sourceGradients.
 *  @param      recurrentInputState             An optional state containing the output matrices and memory cells (for LSTMs)
 *                                                  of the layer obtained from the previous input gradients in a sequence of inputs.
 *                                                  Has to be the output of a previous call to this function or nil (assumed zero).
 *  @param      recurrentOutputStates           An array that will be appended with the recurrent output states. Can be nil.
 *                                                  If recurrentOutputIsTemporary is YES and then all returned recurrent states
 *                                                  will be temporary. @see MPSState:isTemporary.
 *  @param      weights                         An array of valid MPSMatrix objects containing the weights, should be the array
 *                                                  that was produced either by @see initWithDevice or @see createWeightMatrices.
 */

-( void )  encodeGradientSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>) commandBuffer
                                  forwardSources:(NSArray<MPSMatrix*> * __nonnull) forwardSources
                            forwardSourceOffsets:(NSUInteger * __nullable)forwardSourceOffsets
                                 sourceGradients:(NSArray<MPSMatrix*> * __nonnull) sourceGradients
                           sourceGradientOffsets:(NSUInteger * __nullable)sourceGradientOffsets
                            destinationGradients:(NSArray<MPSMatrix*> * __nullable) destinationGradients
                              destinationOffsets:(NSUInteger * __nullable)destinationOffsets
                                 weightGradients:(NSArray<MPSMatrix*> * __nullable) weightGradients
                                  trainingStates:(NSArray<MPSRNNMatrixTrainingState*> * __nonnull) trainingStates
                             recurrentInputState:(MPSRNNRecurrentMatrixState * __nullable) recurrentInputState
                           recurrentOutputStates:(NSMutableArray<MPSRNNRecurrentMatrixState*>  * __nullable) recurrentOutputStates
                                         weights:(NSArray<MPSMatrix*>  * __nonnull)weights

MPS_SWIFT_NAME( encodeGradientSequence(commandBuffer:forwardSources:forwardSourceOffsets:sourceGradients:sourceOffsets:destinationGradients:destinationOffsets:weightGradients:trainingStates:recurrentInputState:recurrentOutputStates:weights:));


/*!
 *  @abstract   Encode an MPSRNNMatrixTrainingLayer gradient pass kernel for a sequence of input gradients into a command buffer.
 *              NOTE: The time sequence indexing follows the array indexing in the inputs: sourceGradients[0] has to contain the
 *              gradients corresponding to the first matrix in the forward pass corresponding to the current subsequence, which is
 *              typically sourceMatrices[0].
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded filter
 *  @param      forwardSources                  An array of MPSMatrix objects containing the sequence of source matrices of the forward pass.
 *  @param      sourceGradients                 An array of MPSMatrix objects containing the sequence of source gradient matrices.
 *  @param      destinationGradients            An array valid MPSMatrix objects that will receive the backpropagated gradients, may be
 *                                                  nil if not needed (for example first layer in graph).
 *  @param      weightGradients                 An array valid MPSMatrix objects that will receive the gradient wrt. weights and
 *                                                  biases of the layer - should be the array that was produced either
 *                                                  by @see initWithDevice or @see createWeightMatrices. May be nil in which case
 *                                                  the gradients for the weights are not computed.
 *                                                  NOTE: The weight gradients are accumulated on top of existing values so
 *
 *  @param      trainingStates                  An array containing the training states from the forward pass - the array must contain
 *                                                  the states corresponding to the input gradients is sourceGradients.
 *  @param      weights                         An array of valid MPSMatrix objects containing the weights, should be the array
 *                                                  that was produced either by @see initWithDevice or @see createWeightMatrices.
 */

-( void )  encodeGradientSequenceToCommandBuffer:(nonnull id<MTLCommandBuffer>) commandBuffer
                                  forwardSources:(NSArray<MPSMatrix*> * __nonnull) forwardSources
                                 sourceGradients:(NSArray<MPSMatrix*> * __nonnull) sourceGradients
                            destinationGradients:(NSArray<MPSMatrix*> * __nullable) destinationGradients
                                 weightGradients:(NSArray<MPSMatrix*> * __nullable) weightGradients
                                  trainingStates:(NSArray<MPSRNNMatrixTrainingState*> * __nonnull) trainingStates
                                         weights:(NSArray<MPSMatrix*>  * __nonnull)weights

MPS_SWIFT_NAME( encodeGradientSequence(commandBuffer:forwardSources:sourceGradients:destinationGradients:weightGradients:trainingStates:weights:));



/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSRNNMatrixTrainingLayer
 *  @param      device      The MTLDevice on which to make the MPSRNNMatrixTrainingLayer
 *  @return     A new MPSRNNMatrixTrainingLayer object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end    /* MPSRNNMatrixTrainingLayer */



#ifdef __cplusplus
}
#endif



#endif /* MPSRNNLayer_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNReduce.h
//
//  MPSNNReduce.h
//  MPSNeuralNetwork
//
//  Created by Dhruv Saksena on 10/31/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSNNReduce_h
#define MPSNNReduce_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif


/*!
 *  @class      MPSNNReduceUnary
 *  @discussion The MPSNNReduce performs a reduction operation
 *              The reduction operations supported are:
 *                   - Reduce row min
 *                   - Reduce column min
 *                   - Reduce feature channels min
 *                   - Reduce row max
 *                   - Reduce column max
 *                   - Reduce feature channels max
 *                   - Reduce row mean
 *                   - Reduce column mean
 *                   - Reduce feature channels mean
 *                   - Reduce row sum
 *                   - Reduce column sum
 *                   - Reduce feature channels sum
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceUnary : MPSCNNKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSCNNKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*
 * You must use one of the sub-classes of MPSNNReduceUnary.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;

@end  /* MPSNNReduceUnary */


/*!
 *  @class      MPSNNReduceRowMin
 *  @discussion The MPSNNReduceRowMin performs a reduction operation returning the mininmum value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceRowMin : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceRowMin object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceRowMin */


/*!
 *  @class      MPSNNReduceColumnMin
 *  @discussion The MPSNNReduceColumnMin performs a reduction operation returning the mininmum value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceColumnMin : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceColumnMin object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceColumnMin */

/*!
 *  @class      MPSNNReduceFeatureChannelsMin
 *  @discussion The MPSNNReduceFeatureChannelsMin performs a reduction operation returning the mininmum value for feature channels of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsMin : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceFeatureChannelsMin object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsMin */

/*!
 *  @class      MPSNNReduceFeatureChannelsArgumentMin
 *  @discussion The MPSNNReduceFeatureChannelsArgumentMin returns the argument index that is the
 *              location of the minimum value for feature channels of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNReduceFeatureChannelsArgumentMin : MPSNNReduceUnary

/*!
*  @abstract Specifies information to apply the reduction operation on an image.
*  @param    device            The device the filter will run on
*  @return     A valid MPSNNReduceFeatureChannelsArgumentMin object or nil, if failure.
*/

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsArgumentMin */


/*!
 *  @class      MPSNNReduceRowMax
 *  @discussion The MPSNNReduceRowMax performs a reduction operation returning the maximum value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceRowMax : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceRowMax object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReducenRowMax */

/*!
 *  @class      MPSNNReduceColumnMax
 *  @discussion The MPSNNReduceColumnMax performs a reduction operation returning the maximum value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceColumnMax : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceColumnMax object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceColumnMax */

/*!
 *  @class      MPSNNReduceFeatureChannelsMax
 *  @discussion The MPSNNReduceFeatureChannelsMax performs a reduction operation returning the maximum value for feature channels of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsMax : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceFeatureChannelsMax object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsMax */

/*!
 *  @class      MPSNNReduceFeatureChannelsArgumentMax
 *  @discussion The MPSNNReduceFeatureChannelsArgumentMax performs returns the argument index that is the
 *              location of the maximum value for feature channels of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNReduceFeatureChannelsArgumentMax : MPSNNReduceUnary

/*!
*  @abstract Specifies information to apply the reduction operation on an image.
*  @param    device            The device the filter will run on
*  @return     A valid MPSNNReduceFeatureChannelsArgumentMax object or nil, if failure.
*/

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsArgumentMax */
    

/*!
 *  @class      MPSNNReduceRowMean
 *  @discussion The MPSNNReduceRowMean performs a reduction operation returning the mean value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceRowMean : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceRowMean object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceRowMean */

/*!
 *  @class      MPSNNReduceColumnMean
 *  @discussion The MPSNNReduceColumnMean performs a reduction operation returning the mean value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceColumnMean : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceColumnMean object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceColumnMean */

/*!
 *  @class      MPSNNReduceFeatureChannelsMean
 *  @discussion The MPSNNReduceFeatureChannelsMean performs a reduction operation returning the mean value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsMean : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceFeatureChannelsMean object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsMean */

/*!
 *  @class      MPSNNReduceRowSum
 *  @discussion The MPSNNReduceRowSum performs a reduction operation returning the sum for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceRowSum : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceRowSum object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceRowSum */

/*!
 *  @class      MPSNNReduceColumnSum
 *  @discussion The MPSNNReduceColumnSum performs a reduction operation returning the sum for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceColumnSum : MPSNNReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceColumnSum object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceColumnSum */


/*!
 *  @class      MPSNNReduceFeatureChannelsSum
 *  @discussion The MPSNNReduceFeatureChannelsSum performs a reduction operation returning the sum for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsSum : MPSNNReduceUnary

/*! @property   weight
 *  @abstract   The scale factor to apply to each feature channel value
 *  @discussion Each feature channel is multiplied by the weight value to compute a weighted sum or mean across feature channels
 *              The default value is 1.0.
 */
@property (readwrite, nonatomic) float weight;

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSNNReduceFeatureChannelsSum object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsSum */


#pragma mark -
#pragma mark MPSNNReduceBinary

/*!
 *  @class      MPSNNReduceBinary
 *  @discussion The MPSNNReduce performs a reduction operation
 *              The reduction operations supported are:
 *                   - Reduce feature channels mean
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceBinary : MPSCNNBinaryKernel

/*!
 *  @abstract   The source rectangle to use when reading data from primary source
 *  @discussion A MTLRegion that indicates which part of the primary source to read. If the clipRectPrimarySource does not lie
 *              completely within the primary source image, the intersection of the image bounds and clipRectPrimarySource will
 *              be used. The primarySourceClipRect replaces the MPSBinaryImageKernel primaryOffset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 *              The clipRect specified in MPSBinaryImageKernel is used to control the origin in the destination texture
 *              where the min, max values are written.  The clipRect.width must be >=2.  The clipRect.height must be >= 1.
 *
 */
@property (readwrite, nonatomic) MTLRegion primarySourceClipRect;

/*!
 *  @abstract   The source rectangle to use when reading data from secondary source
 *  @discussion A MTLRegion that indicates which part of the secondary source to read. If the clipRectSecondarySource does not lie
 *              completely within the secondary source image, the intersection of the image bounds and clipRectSecondarySource will
 *              be used. The secondarySourceClipRect replaces the MPSBinaryImageKernel secondaryOffset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 *              The clipRect specified in MPSBinaryImageKernel is used to control the origin in the destination texture
 *              where the min, max values are written.  The clipRect.width must be >=2.  The clipRect.height must be >= 1.
 *
 */
@property (readwrite, nonatomic) MTLRegion secondarySourceClipRect;

/*
 * You must use one of the sub-classes of MPSNNReduceBinary.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;

@end  /* MPSNNReduceBinary */


MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsAndWeightsMean : MPSNNReduceBinary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return   A valid MPSNNReduceFeatureChannelsAndWeightsMean object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsAndWeightsMean */

MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSNNReduceFeatureChannelsAndWeightsSum : MPSNNReduceBinary

/*!
 *  @abstract   A boolean to indicate whether the reduction should perform a weighted sum of feature channels with non-zero weights
 *  @discussion If false, computes a dot product of the feature channels and weights.
 *              If true, computes a dot product of the feature channels and weights divided by the number of non-zero weights
 */
@property (readonly, nonatomic) bool doWeightedSumByNonZeroWeights;

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return   A valid MPSNNReduceFeatureChannelsAndWeightsMean object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device ;

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device                         The device the filter will run on
 *  @param    doWeightedSumByNonZeroWeights  A boolean to indicate whether to compute a weighted sum or
 *                                           weighted sum divided by the number of non-zero weights
 *  @return   A valid MPSNNReduceFeatureChannelsAndWeightsSum object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                  doWeightedSumByNonZeroWeights: (bool)doWeightedSumByNonZeroWeights NS_DESIGNATED_INITIALIZER;

@end  /* MPSNNReduceFeatureChannelsAndWeightsSum */

#ifdef __cplusplus
}
#endif

#endif /* MPSNNReduce_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSMatrixNeuron.h
/*!
 *  @header MPSMatrixNeuron.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *  @abstract MPSMatrices fused with neurons
 */


#ifndef MPSMatrixNeuron_h
#define MPSMatrixNeuron_h

#include <MPSNeuralNetwork/MPSCNNNeuronType.h>
#include <MPSMatrix/MPSMatrix.h>

#ifdef __cplusplus
extern "C" {
#endif

/*!
 *  @class      MPSMatrixNeuron
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A neuron activation kernel that operates on matrices.
 *
 *  @discussion A MPSMatrixNeuron object computes:
 *
 *                  y = neuron(alpha * x + bias)
 *
 *              y is the output matrix, x is the input matrix corresponding
 *              to a collection of input vectors and bias is a vector which is broadcast
 *              and accumulated to each row of the intermediate result.
 *              alpha is a scale factor applied to the input.
 *
 *              neuron() defines the pointwise function that is applied to the intermediate result.
 *
 *              Note: This function computes the same result as MPSMatrixFullyConnected that has
 *                      unit weight matrix.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixNeuron : MPSMatrixUnaryKernel

/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.  This
 *              is equivalent to the number of rows to consider from the primary
 *              source matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available number of inputs is
 *              used.  The value of NSUIntegerMax thus indicates that all available input
 *              rows (beginning at sourceMatrixOrigin.x) should be considered.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The input size to to use in the operation.  This is equivalent to the
 *              number of columns in the primary (input array) source matrix to consider
 *              and the number of channels to produce for the output matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available input size is used.
 *              The value of NSUIntegerMax thus indicates that all available columns in
 *              the input array (beginning at sourceMatrixOrigin.y) should be considered.
 *              Defines also the number of output feature channels.
 *              Note: The value used in the operation will be
 *              MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels)
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;


/*! @property   alpha
 *
 *  @discussion The scale factor to apply to the input.  Specified in double
 *              precision.  Will be converted to the appropriate precision in the
 *              implementation subject to rounding and/or clamping as necessary.
 *              Defaults to 1.0 at initialization time.
 */
@property (readwrite, nonatomic) double alpha;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. For those kind of neuron activation functions,
 *              use appropriate setter functions.  An MPSMatrixNeuron kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterA;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterB;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterC;

/*!
 *  @abstract   Add per output value neuron parameters A for PReLu neuron activation functions.
 *
 *  @discussion This method sets the neuron to PReLU, zeros parameters A and B and sets the per output value
 *              neuron parameters A to an array containing a unique value of A for each output value.
 *
 *              If the neuron function is f(v,a,b), it will apply
 *
 *                     resultMatrix(i, j) = f( input(i, j), A[j], B[j] )
 *                  where j in [0, sourceInputFeatureChannels]
 *
 *              See https://arxiv.org/pdf/1502.01852.pdf for details.
 *
 *              All other neuron types, where parameter A
 *              and parameter B are shared across output values must be set using
 *              -setNeuronType:parameterA:parameterB:
 *
 *  @param      A       An array containing float values for neuron parameter A.
 *                      Number of entries must be equal to MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels)
 */
-(void) setNeuronToPReLUWithParametersA: (NSData* __nonnull) A;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixNeuron object to a command buffer.
 *
 *  @param      commandBuffer   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix     A valid MPSMatrix object which specifies the input array.
 *
 *  @param      biasVector      A valid MPSVector object which specifies the bias values, or
 *                              a null object to indicate that no bias is to be applied.
 *
 *  @param      resultMatrix    A valid MPSMatrix object which specifies the output array.
 *
 *  @discussion Encodes the operation to the specified command buffer.  resultMatrix
 *              must be large enough to hold a
 *                  MIN(sourceNumberOfFeatureVectors, inputMatrix.rows - sourceMatrixOrigin.x)
 *                  x
 *                  MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels) array.
 *
 *              The bias vector must contain at least
 *                  MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels) elements.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                   biasVector: (MPSVector * __nullable) biasVector
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:biasVector:resultMatrix:));

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixNeuron
 *  @param      device      The MTLDevice on which to make the MPSMatrixNeuron object.
 *  @return     A new MPSMatrixNeuron object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;


@end // MPSMatrixNeuron
    

/*!
 *  @class      MPSMatrixNeuronGradient
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A neuron gradient activation kernel that operates on matrices.
 *
 *  @discussion A MPSMatrixNeuronGradient object computes the results of backpropagating
 *              the gradients of a loss function with respect to the outputs of an
 *              MPSMatrixNeuron object.  The corresponding properties and data used by
 *              the MPSMatrixNeuronGradient object should correspond to those used by
 *              the forward MPSMatrixNeuron object for which the gradient is being computed.
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixNeuronGradient : MPSMatrixBinaryKernel

/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The number of feature channels in the input vectors.
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;

/*! @property   alpha
 *
 *  @discussion The scale factor to apply to the input.
 */
@property (readwrite, nonatomic) double alpha;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. For those kind of neuron activation functions,
 *              use appropriate setter functions.  An MPSMatrixNeuron kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterA;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterB;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterC;

/*!
 *  @abstract   Add per output value neuron parameters A for PReLu neuron activation functions.
 *
 *  @discussion This method sets the neuron to PReLU, zeros parameters A and B and sets the per output value
 *              neuron parameters A to an array containing a unique value of A for each output value.
 *
 *              If the neuron function is f(v,a,b), it will apply
 *
 *                     resultMatrix(i, j) = f( input(i, j), A[j], B[j] )
 *                  where j in [0, sourceInputFeatureChannels]
 *
 *              See https://arxiv.org/pdf/1502.01852.pdf for details.
 *
 *              All other neuron types, where parameter A
 *              and parameter B are shared across output values must be set using
 *              -setNeuronType:parameterA:parameterB:
 *
 *  @param      A       An array containing float values for neuron parameter A.
 *                      Number of entries must be equal to MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels)
 */
-(void) setNeuronToPReLUWithParametersA: (NSData* __nonnull) A;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixNeuronGradient object to a command buffer and compute
 *              its gradient with respect to its input data.
 *
 *  @param      commandBuffer               The commandBuffer on which to encode the operation.
 *
 *  @param      gradientMatrix              A matrix whose values represent the gradient of a
 *                                          loss function with respect to the results of a forward
 *                                          MPSMatrixNeuron operation.
 *
 *  @param      inputMatrix                 A matrix containing the inputs to a forward MPSMatrixNeuron
 *                                          operation for which the gradient values are to be computed.
 *
 *  @param      biasVector                  A vector containing the bias terms.
 *
 *  @param      resultGradientForDataMatrix The matrix containing the resulting gradient values.
 *
 *  @param      resultGradientForBiasVector If non-NULL the vector containing gradients for the bias
 *                                          terms.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               gradientMatrix: (MPSMatrix * __nonnull) gradientMatrix
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                   biasVector: (MPSVector * __nullable) biasVector
  resultGradientForDataMatrix: (MPSMatrix * __nonnull) resultGradientForDataMatrix
  resultGradientForBiasVector: (MPSVector * __nullable) resultGradientForBiasVector;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixNeuronGradient
 *  @param      device      The MTLDevice on which to make the MPSMatrixNeuronGradient object.
 *  @return     A new MPSMatrixNeuronGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixNeuronGradient

    
#ifdef __cplusplus
}
#endif
#endif /* MPSMatrixNeuron_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNNeuron.h
//
//  MPSCNNNeuron.h
//  MPS
//
//  Created by Anna Tikhonova on 10/27/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNNeuron_h
#define MPSCNNNeuron_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>
#import <MPSNeuralNetwork/MPSCNNNeuronType.h>

#ifdef __cplusplus
extern "C" {
#endif

    
#pragma mark -
#pragma mark MPSNNNeuronDescriptor
    
/*!
 *  @class      MPSNNNeuronDescriptor
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSNNNeuronDescriptor specifies a neuron descriptor.
 *              Supported neuron types:
 *
 *              Neuron type "none": f(x) = x
 *              Parameters: none
 *
 *              ReLU neuron filter: f(x) = x >= 0 ? x : a * x
 *              This is called Leaky ReLU in literature. Some literature defines
 *              classical ReLU as max(0, x). If you want this behavior, simply pass a = 0.
 *              Parameters: a
 *              For default behavior, set the value of a to 0.0f.
 *
 *              Linear neuron filter: f(x) = a * x + b
 *              Parameters: a, b
 *              For default behavior, set the value of a to 1.0f and the value of b to 0.0f.
 *
 *              Sigmoid neuron filter: f(x) = 1 / (1 + e^-x)
 *              Parameters: none
 *
 *              Hard Sigmoid filter: f(x) = clamp((x * a) + b, 0, 1)
 *              Parameters: a, b
 *              For default behavior, set the value of a to 0.2f and the value of b to 0.5f.
 *
 *              Hyperbolic tangent (TanH) neuron filter: f(x) = a * tanh(b * x)
 *              Parameters: a, b
 *              For default behavior, set the value of a to 1.0f and the value of b to 1.0f.
 *
 *              Absolute neuron filter: f(x) = fabs(x)
 *              Parameters: none
 *
 *              Parametric Soft Plus neuron filter: f(x) = a * log(1 + e^(b * x))
 *              Parameters: a, b
 *              For default behavior, set the value of a to 1.0f and the value of b to 1.0f.
 *
 *              Parametric Soft Sign neuron filter: f(x) = x / (1 + abs(x))
 *              Parameters: none
 *
 *              Parametric ELU neuron filter: f(x) = x >= 0 ? x : a * (exp(x) - 1)
 *              Parameters: a
 *              For default behavior, set the value of a to 1.0f.
 *
 *              Parametric ReLU (PReLU) neuron filter: Same as ReLU, except parameter
 *              aArray is per channel.
 *              For each pixel, applies the following function: f(x_i) = x_i, if x_i >= 0
 *                                                                     = a_i * x_i if x_i < 0
 *              i in [0...channels-1]
 *              i.e. parameters a_i are learned and applied to each channel separately. Compare
 *              this to ReLu where parameter a is shared across all channels.
 *              See https://arxiv.org/pdf/1502.01852.pdf for details.
 *              Parameters: aArray - Array of floats containing per channel value of PReLu parameter
 *                          count - Number of float values in array aArray.
 *
 *              ReLUN neuron filter: f(x) = min((x >= 0 ? x : a * x), b)
 *              Parameters: a, b
 *              As an example, the TensorFlow Relu6 activation layer can be implemented
 *              by setting the parameter b to 6.0f:
 *              https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/relu6.
 *              For default behavior, set the value of a to 1.0f and the value of b to 6.0f.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSNNNeuronDescriptor : NSObject <NSCopying>

@property (readwrite, nonatomic) MPSCNNNeuronType neuronType;
@property (readwrite, nonatomic) float a;
@property (readwrite, nonatomic) float b;
@property (readwrite, nonatomic) float c;
@property (readwrite, nonatomic, nullable, retain) NSData* data; /* Note: data is retained, not copied */

/*!
 * You must use one of the interfaces below instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract  Make a descriptor for a MPSCNNNeuron object.
 *  @param     neuronType           The type of a neuron filter.
 *  @return    A valid MPSNNNeuronDescriptor object or nil, if failure.
 */
+(nonnull MPSNNNeuronDescriptor*) cnnNeuronDescriptorWithType: (MPSCNNNeuronType)neuronType;

/*!
 *  @abstract  Make a descriptor for a MPSCNNNeuron object.
 *  @param     neuronType           The type of a neuron filter.
 *  @param     a                    Parameter "a".
 *  @return    A valid MPSNNNeuronDescriptor object or nil, if failure.
 */
+(nonnull MPSNNNeuronDescriptor*) cnnNeuronDescriptorWithType: (MPSCNNNeuronType)neuronType
                                                            a: (float) a;

/*!
 *  @abstract  Initialize the neuron descriptor.
 *  @param     neuronType           The type of a neuron filter.
 *  @param     a                    Parameter "a".
 *  @param     b                    Parameter "b".
 *  @return    A valid MPSNNNeuronDescriptor object or nil, if failure.
 */
+(nonnull MPSNNNeuronDescriptor*) cnnNeuronDescriptorWithType: (MPSCNNNeuronType)neuronType
                                                            a: (float) a
                                                            b: (float) b;

/*!
 *  @abstract  Make a descriptor for a MPSCNNNeuron object.
 *  @param     neuronType           The type of a neuron filter.
 *  @param     a                    Parameter "a".
 *  @param     b                    Parameter "b".
 *  @param     c                    Parameter "c".
 *  @return    A valid MPSNNNeuronDescriptor object or nil, if failure.
 */
+(nonnull MPSNNNeuronDescriptor*) cnnNeuronDescriptorWithType: (MPSCNNNeuronType)neuronType
                                                            a: (float) a
                                                            b: (float) b
                                                            c: (float) c;

/*! @abstract   Make a descriptor for a neuron of type MPSCNNNeuronTypePReLU.
 *  @discussion The PReLU neuron is the same as a ReLU neuron, except parameter "a" is per feature channel.
 *  @param      data                A NSData containing a float array with the per feature channel value
 *                                  of PReLu parameter. The number of float values in this array usually
 *                                  corresponds to number of output channels in a convolution layer.
 *                                  The descriptor retains the NSData object.
 *  @param      noCopy              An optimization flag that tells us whether the NSData allocation is
 *                                  suitable for use directly with no copying of the data into internal
 *                                  storage. This allocation has to match the same restrictions as listed
 *                                  for the newBufferWithBytesNoCopy:length:options:deallocator: method of
 *                                  MTLBuffer.
 *  @return     A valid MPSNNNeuronDescriptor object for a neuron of type MPSCNNNeuronTypePReLU or nil, if failure */
+(nonnull MPSNNNeuronDescriptor*) cnnNeuronPReLUDescriptorWithData: (NSData* _Nonnull) data
                                                            noCopy: (bool) noCopy;

@end /* MPSNNNeuronDescriptor */

    
#pragma mark -
#pragma mark MPSCNNNeuron
    
/*!
 *  @class      MPSCNNNeuron
 *  @dependency This depends on Metal.framework
 *  @discussion This filter applies a neuron activation function.
 *              You must use one of the sub-classes of MPSCNNNeuron.
 *
 *  The following filter types are supported:
 *  MPSCNNNeuronTypeNone            ///< f(x) = x
 *  MPSCNNNeuronTypeLinear          ///< f(x) = a * x + b
 *  MPSCNNNeuronTypeReLU            ///< f(x) = x >= 0 ? x : a * x
 *  MPSCNNNeuronTypeSigmoid         ///< f(x) = 1 / (1 + e^-x)
 *  MPSCNNNeuronTypeHardSigmoid     ///< f(x) = clamp((x * a) + b, 0, 1)
 *  MPSCNNNeuronTypeTanH            ///< f(x) = a * tanh(b * x)
 *  MPSCNNNeuronTypeAbsolute        ///< f(x) = fabs(x)
 *  MPSCNNNeuronTypeSoftPlus        ///< f(x) = a * log(1 + e^(b * x))
 *  MPSCNNNeuronTypeSoftSign        ///< f(x) = x / (1 + abs(x))
 *  MPSCNNNeuronTypeELU             ///< f(x) = x >= 0 ? x : a * (exp(x) - 1)
 *  MPSCNNNeuronTypePReLU           ///< Same as ReLU except parameter a is per channel
 *  MPSCNNNeuronTypeReLUN           ///< f(x) = min((x >= 0 ? x : a * x), b)
 *  MPSCNNNeuronTypePower           ///< f(x) = (a * x + b) ^ c
 *  MPSCNNNeuronTypeExponential     ///< f(x) = c ^ (a * x + b)
 *  MPSCNNNeuronTypeLogarithm       ///< f(x) = log_c(a * x + b)
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuron : MPSCNNKernel

@property (readonly, nonatomic) MPSCNNNeuronType neuronType MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
@property (readonly, nonatomic) float a;
@property (readonly, nonatomic) float b;
@property (readonly, nonatomic) float c;
@property (readonly, nonatomic, retain, nullable) NSData* data MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*
 * You must use initWithDevice:neuronDescriptor or use one of the sub-classes of MPSCNNNeuron instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract  Initialize the neuron filter with a neuron descriptor.
 *  @param     device                   The device the filter will run on.
 *  @param     neuronDescriptor         The neuron descriptor.
 *                                      For the neuron of type MPSCNNNeuronTypePReLU, the neuron
 *                                      descriptor references an NSData object containing a float array
 *                                      with the per feature channel value of PReLu parameter and, in this
 *                                      case, the MPSCNNNeuron retains the NSData object.
 *  @return    A valid MPSCNNNeuron object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                      neuronDescriptor: (MPSNNNeuronDescriptor* _Nonnull) neuronDescriptor NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull) aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNNeuron */
    
    
#pragma mark -
#pragma mark MPSCNNNeuronGradient

/*!
 *  @class      MPSCNNNeuronGradient
 *  @dependency This depends on Metal.framework
 *  @discussion This filter is a backward filter for the neuron activation function filter.
 *
 *  The following filter types are supported:
 *  MPSCNNNeuronTypeNone            ///< df/dx = 1
 *  MPSCNNNeuronTypeLinear          ///< df/dx = a
 *  MPSCNNNeuronTypeReLU            ///< df/dx = [ 1, if x >= 0
 *                                               [ a, if x <  0
 *  MPSCNNNeuronTypeSigmoid         ///< df/dx = e^x / (e^x + 1)^2
 *  MPSCNNNeuronTypeHardSigmoid     ///< df/dx = [ a, if (x >= 0) and (x <= 1)
 *                                               [ 0, otherwise
 *  MPSCNNNeuronTypeTanH            ///< df/dx = a * b * (1 - tanh^2(b * x))
 *  MPSCNNNeuronTypeAbsolute        ///< df/dx = sign(x)
 *  MPSCNNNeuronTypeSoftPlus        ///< df/dx = (a * b * exp(b * x)) / (exp(b * x) + 1)
 *  MPSCNNNeuronTypeSoftSign        ///< df/dx = 1 / (|x| + 1)^2
 *  MPSCNNNeuronTypeELU             ///< df/dx = [ a * exp(x), x <  0
 *                                               [          1, x >= 0
 *  MPSCNNNeuronTypePReLU           ///< df/dx = [  1, if x >= 0
 *                                               [ aV, if x <  0
 *  MPSCNNNeuronTypeReLUN           ///< df/dx = [ 1, if x >= 0
 *                                               [ a, if x <  0
 *                                               [ b, if x >= b
 *  MPSCNNNeuronTypePower           ///< df/dx = a * c * (a * x + b)^(c - 1)
 *  MPSCNNNeuronTypeExponential     ///< df/dx = [         a * exp(a * x + b), if c == -1
 *                                               [ a * log(c) * c^(a * x + b), if c != -1
 *  MPSCNNNeuronTypeLogarithm       ///< df/dx = [            a / (a * in + b), if c == -1
 *                                               [ a / (log(c) * (a * in + b)), if c != -1
 *
 * The result of the above operation is multiplied with the gradient, computed
 * by the preceeding filter (going backwards).
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNNeuronGradient : MPSCNNGradientKernel

@property (readonly, nonatomic) MPSCNNNeuronType neuronType;
@property (readonly, nonatomic) float a;
@property (readonly, nonatomic) float b;
@property (readonly, nonatomic) float c;
@property (readonly, nonatomic, retain, nullable) NSData* data;

/*
 * You must use initWithDevice:neuronDescriptor or use one of the sub-classes of MPSCNNNeuronGradient instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract  Initialize the neuron gradient filter with a neuron descriptor.
 *  @param     device                   The device the filter will run on.
 *  @param     neuronDescriptor         The neuron descriptor.
 *                                      For the neuron of type MPSCNNNeuronTypePReLU, the neuron
 *                                      descriptor references an NSData object containing a float array
 *                                      with the per feature channel value of PReLu parameter and, in this
 *                                      case, the MPSCNNNeuronGradient retains the NSData object.
 *  @return    A valid MPSCNNNeuronGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                      neuronDescriptor: (MPSNNNeuronDescriptor* _Nonnull) neuronDescriptor NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull) aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end /* MPSCNNNeuronGradient */


#pragma mark -
#pragma mark Specific MPSCNNNeuron filters

/*!
 *  @class      MPSCNNNeuronLinear
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the linear neuron filter. For each pixel, applies the following function: f(x) = a * x + b
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuronLinear : MPSCNNNeuron

/*!
 *  @abstract  Initialize the linear neuron filter
 *  @param     device   The device the filter will run on
 *  @param     a        Filter property "a". See class discussion.
 *  @param     b        Filter property "b". See class discussion.
 *  @return    A valid MPSCNNNeuronLinear object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * You must use initWithDevice:a:b instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronLinear */
    
    
/*!
 *  @class MPSCNNNeuronReLU
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the ReLU neuron filter.
 *              For each pixel, applies the following function: f(x) = x, if x >= 0
 *                                                                   = a * x if x < 0
 *              This is called Leaky ReLU in literature. Some literature defines
 *              classical ReLU as max(0, x). If you want this behavior, simply pass a = 0
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuronReLU : MPSCNNNeuron

/*!
 *  @abstract  Initialize the ReLU neuron filter
 *  @param     device           The device the filter will run on
 *  @param     a                Filter property "a". See class discussion.
 *  @return    A valid MPSCNNNeuronReLU object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a NS_DESIGNATED_INITIALIZER
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));


/*
 * Use initWithDevice:a: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronReLU */
    
/*!
 *  @class MPSCNNNeuronPReLU
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the parametric ReLU neuron filter.
 *              For each pixel, applies the following function: f(x_i) = x_i, if x_i >= 0
 *                                                                     = a_i * x_i if x_i < 0
 *              i in [0...channels-1]
 *              i.e. parameters a_i are learned and applied to each channel separately. Compare
 *              this to ReLu where parameter a is shared across all channels.
 *              See https://arxiv.org/pdf/1502.01852.pdf for details.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronPReLU : MPSCNNNeuron

/*!
 *  @abstract  Initialize the PReLU neuron filter
 *  @param     device           The device the filter will run on
 *  @param     a                Array of floats containing per channel value of PReLu parameter
 *  @param     count            Number of float values in array a.
 *                              This usually corresponds to number of output channels in convolution layer
 *  @return    A valid MPSCNNNeuronPReLU object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (const float* _Nonnull) a
                                 count: (NSUInteger) count NS_DESIGNATED_INITIALIZER
            MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));


/*
 * Use initWithDevice:a: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronPReLU */
    
    
/*!
 *  @class MPSCNNNeuronSigmoid
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the sigmoid neuron filter.  For each pixel, applies the following function: f(x) = 1 / (1 + e^-x)
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuronSigmoid : MPSCNNNeuron

/*!
 *  @abstract  Initialize a neuron filter
 *  @param      device          The device the filter will run on
 *  @return     A valid MPSCNNNeuronSigmoid object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));


@end    /* MPSCNNNeuronSigmoid */
    
    
/*!
 *  @class MPSCNNNeuronHardSigmoid
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the hard sigmoid neuron filter.  For each pixel, applies the following function: f(x) = clamp((a * x) + b, 0, 1)
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronHardSigmoid : MPSCNNNeuron

/*!
 *  @abstract  Initialize a neuron filter
 *  @param     device           The device the filter will run on
 *  @param     a                Filter property "a". See class discussion.
 *  @param     b                Filter property "b". See class discussion.
 *  @return    A valid MPSCNNNeuronHardSigmoid object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronHardSigmoid */
    
    
/*!
 *  @class MPSCNNNeuronTanH
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the hyperbolic tangent neuron filter.
 *              For each pixel, applies the following function: f(x) = a * tanh(b * x)
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuronTanH : MPSCNNNeuron

/*!
 *  @abstract  Initialize the hyperbolic tangent neuron filter
 *  @param     device           The device the filter will run on
 *  @param     a                Filter property "a". See class discussion.
 *  @param     b                Filter property "b". See class discussion.
 *  @return    A valid MPSCNNNeuronTanH object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b NS_DESIGNATED_INITIALIZER
            MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronTanH */


/*!
 *  @class MPSCNNNeuronAbsolute
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the absolute neuron filter.  For each pixel, applies the following function: f(x) = | x |
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNNeuronAbsolute : MPSCNNNeuron

/*!
 *  @abstract  Initialize a neuron filter
 *  @param      device          The device the filter will run on
 *  @return     A valid MPSCNNNeuronAbsolute object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
            MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));


@end    /* MPSCNNNeuronAbsolute */
    
    
/*!
 *  @class MPSCNNNeuronSoftPlus
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the parametric softplus neuron filter.
 *              For each pixel, applies the following function: f(x) = a * log(1 + e^(b * x))
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronSoftPlus : MPSCNNNeuron

/*!
 *  @abstract   Initialize a parametric softplus neuron filter
 *  @param      device          The device the filter will run on
 *  @param      a               Filter property "a". See class discussion.
 *  @param      b               Filter property "b". See class discussion.
 *  @return     A valid MPSCNNNeuronSoftPlus object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b NS_DESIGNATED_INITIALIZER
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronSoftPlus */
    
    
/*!
 *  @class MPSCNNNeuronSoftSign
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the softsign neuron filter.
 *              For each pixel, applies the following function: f(x) = x / (1 + abs(x))
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronSoftSign : MPSCNNNeuron

/*!
 *  @abstract   Initialize a softsign neuron filter
 *  @param      device          The device the filter will run on
 *  @return     A valid MPSCNNNeuronSoftSign object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));


@end    /* MPSCNNNeuronSoftSign */
    
    
/*!
 *  @class MPSCNNNeuronELU
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the parametric ELU neuron filter.
 *              For each pixel, applies the following function: f(x) = [ a * (exp(x) - 1), x <  0
 *                                                                     [ x               , x >= 0
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronELU : MPSCNNNeuron

/*!
 *  @abstract   Initialize a parametric ELU neuron filter
 *  @param      device          The device the filter will run on
 *  @param      a               Filter property "a". See class discussion.
 *  @return     A valid MPSCNNNeuronELU object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a NS_DESIGNATED_INITIALIZER
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronELU */
    
    
/*!
 *  @class MPSCNNNeuronReLUN
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the ReLUN neuron filter.
 *              For each pixel, applies the following function: f(x) = [ x    , x >= 0
 *                                                                     [ a * x, x <  0
 *                                                                     [ b    , x >= b
 *              As an example, the TensorFlow Relu6 activation layer can be implemented
 *              by setting the parameter b to 6.0f:
 *              https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/relu6.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNNeuronReLUN : MPSCNNNeuron

/*!
 *  @abstract   Initialize a ReLUN neuron filter
 *  @param      device          The device the filter will run on
 *  @param      a               Filter property "a". See class discussion.
 *  @param      b               Filter property "b". See class discussion.
 *  @return     A valid MPSCNNNeuronReLUN object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b NS_DESIGNATED_INITIALIZER
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronReLUN */


/*!
 *  @class MPSCNNNeuronPower
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the Power neuron filter.
 *              For each pixel, applies the following function: f(x) = (a * x + b) ^ c.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNNeuronPower : MPSCNNNeuron

/*!
 *  @abstract   Initialize a Power neuron filter.
 *  @param      device          The device the filter will run on.
 *  @param      a               Filter property "a". See class discussion.
 *  @param      b               Filter property "b". See class discussion.
 *  @param      c               Filter property "c". See class discussion.
 *  @return     A valid MPSCNNNeuronPower object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c NS_DESIGNATED_INITIALIZER
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b:c instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronPower */


/*!
 *  @class MPSCNNNeuronExponential
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the Exponential neuron filter.
 *              For each pixel, applies the following function: f(x) = c ^ (a * x + b).
 *
 *              If the value of c is -1.0f, the base (c) is set to e.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNNeuronExponential : MPSCNNNeuron

/*!
 *  @abstract   Initialize a Exponential neuron filter.
 *  @param      device          The device the filter will run on.
 *  @param      a               Filter property "a". See class discussion.
 *  @param      b               Filter property "b". See class discussion.
 *  @param      c               Filter property "c". See class discussion.
 *  @return     A valid MPSCNNNeuronExponential object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c NS_DESIGNATED_INITIALIZER
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b:c instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronExponential */


/*!
 *  @class MPSCNNNeuronLogarithm
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the Logarithm neuron filter.
 *              For each pixel, applies the following function: f(x) = log_c(a * x + b).
 *
 *              If the value of c is -1.0f, the base (c) is set to e.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNNeuronLogarithm : MPSCNNNeuron

/*!
 *  @abstract   Initialize a Logarithm neuron filter.
 *  @param      device          The device the filter will run on.
 *  @param      a               Filter property "a". See class discussion.
 *  @param      b               Filter property "b". See class discussion.
 *  @param      c               Filter property "c". See class discussion.
 *  @return     A valid MPSCNNNeuronLogarithm object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                     a: (float) a
                                     b: (float) b
                                     c: (float) c NS_DESIGNATED_INITIALIZER
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Please use MPSCNNNeuron initWithDevice:neuronDescriptor.",
                                      macos(10.13, 10.14), ios(10.0, 12.0), tvos(10.0, 12.0));

/*
 * Use initWithDevice:a:b:c instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNNeuronLogarithm */


#ifdef __cplusplus
}
#endif

#endif /* MPSCNNNeuron_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNNOptimizers.h
//
//  MPSNNOptimizers.h
//  MPSNeuralNetwork
//
//  Created by Dhruv Saksena on 2/15/18.
//  Copyright © 2018 Apple. All rights reserved.
//


#ifndef MPSNNOptimizers_h
#define MPSNNOptimizers_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>
#import <MPSNeuralNetwork/MPSCNNConvolution.h>
#import <MPSNeuralNetwork/MPSCNNBatchNormalization.h>
#import <MPSNeuralNetwork/MPSCNNInstanceNormalization.h>

#ifdef __cplusplus
extern "C" {
#endif

/*!
 *  Regularization is a technique usually used to avoid overfitting to avoid updating the weights once they become to big.
 *  To achieve this the user adds the regularization function R(weight) to the loss of the training forward pass, and since the objective is to
 *  minimize this loss we effectively keep the weight from getting too big.
 *
 *  It can be applied to all network parameters like bias, weights, gamma and beta.
 *
 *  Link to more info on regularization:
 *  https://en.wikipedia.org/wiki/Regularization_(mathematics)
 *
 */
#if defined(DOXYGEN)
    typedef enum MPSNNRegularizationType
#else
    typedef NS_ENUM(NSUInteger, MPSNNRegularizationType)
#endif
    {
        MPSNNRegularizationTypeNone MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)) MPS_SWIFT_NAME(None) = 0,
        /*!
         *  Apply L1 regularization. L1 norm of weights, will be considered to be added to the loss to be minimized.
         *  the gradient of the regularization loss turns to be 1 scaled with regularizationScale,
         *  so we add that to the incoming gradient of value.
         */
        MPSNNRegularizationTypeL1 MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)) = 1,
        /*!
         *  Apply L2 regularization. L2 norm of weights, will be considered to be added to the loss to be minimized.
         *  the gradient of the regularization loss turns to be the original value scaled with regularizationScale,
         *  so we add that to the incoming gradient of value.
         */
        MPSNNRegularizationTypeL2 MPS_ENUM_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0)) = 2,
    } NS_ENUM_AVAILABLE(10_14, 12_0)
#if defined(DOXYGEN)
    MPSNNRegularizationType
#endif
    ;


/*!
 *  @class      MPSNNOptimizerDescriptor
 *  @discussion The MPSNNOptimizerDescriptor base class. Optimizers are generally used to update trainable neural network parameters.
 *              Users are usually expected to call these MPSKernels from the update methods on their Convolution or BatchNormalization data sources.
 *
 *              Before the gradient is used to update the original value, some preprocessing occurs on each gradient where it is scaled or clipped
 *              If regularization is chosen the appropriate regularization loss gradient is added to the value gradient.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNOptimizerDescriptor : NSObject

/*! @property   learningRate
 *  @abstract   The learningRate at which we update values
 *  @discussion The default value is 0.001f
 */
@property (readwrite, nonatomic) float learningRate;

/*! @property   gradientRescale
 *  @abstract   The gradientRescale at which we apply to incoming gradient values
 *  @discussion The default value is 1.0
 */
@property (readwrite, nonatomic) float gradientRescale;

/*! @property   applyGradientClipping
 *  @abstract   A bool which decides if gradient will be clipped
 *  @discussion The default value is NO
 */
@property (readwrite, nonatomic) BOOL applyGradientClipping;

/*! @property   gradientClipMax
 *  @abstract   The maximum value at which incoming gradient will be clipped before rescaling, applyGradientClipping must be true
 */
@property (readwrite, nonatomic) float gradientClipMax;

/*! @property   gradientClipMin
 *  @abstract   The minimum value at which incoming gradient will be clipped before rescaling, applyGradientClipping must be true
 */
@property (readwrite, nonatomic) float gradientClipMin;

/*! @property   regularizationScale
 *  @abstract   The regularizationScale at which we apply L1 or L2 regularization, it gets ignored if regularization is None
 *  @discussion The default value is 0.0
 */
@property (readwrite, nonatomic) float regularizationScale;

/*! @property   regularizationType
 *  @abstract   The regularizationType which we apply.
 *  @discussion The default value is MPSRegularizationTypeNone
 */
@property (readwrite, nonatomic) MPSNNRegularizationType regularizationType;

/*
 *  @abstract   Initialization for the MPSNNOptimizerDescriptor object, no gradient clipping would be applied
 *
 *  @param      learningRate               The learningRate which will be applied
 *  @param      gradientRescale            The gradientRescale which will be applied
 *  @param      regularizationType         The regularizationType which will be applied
 *  @param      regularizationScale        The regularizationScale which will be applied
 *
 *  @return     A valid MPSNNOptimizerDescriptor object or nil, if failure.
 */
-(nonnull instancetype) initWithLearningRate: (float) learningRate
                             gradientRescale: (float) gradientRescale
                          regularizationType: (MPSNNRegularizationType) regularizationType
                         regularizationScale: (float) regularizationScale;


/*
 *  @abstract   Initialization for the MPSNNOptimizerDescriptor object
 *
 *  @param      learningRate               The learningRate which will be applied
 *  @param      gradientRescale            The gradientRescale which will be applied
 *  @param      applyGradientClipping      The BOOL which sets if gradientClipping would be applied to the gradient
 *  @param      gradientClipMax            The gradientClipMax which will be applied
 *  @param      gradientClipMin            The gradientClipMin which will be applied
 *  @param      regularizationType         The regularizationType which will be applied
 *  @param      regularizationScale        The regularizationScale which will be applied
 *
 *  @return     A valid MPSNNOptimizerDescriptor object or nil, if failure.
 */
-(nonnull instancetype) initWithLearningRate: (float) learningRate
                             gradientRescale: (float) gradientRescale
                       applyGradientClipping: (BOOL) applyGradientClipping
                             gradientClipMax: (float) gradientClipMax
                             gradientClipMin: (float) gradientClipMin
                          regularizationType: (MPSNNRegularizationType) regularizationType
                         regularizationScale: (float) regularizationScale;


/*
 *  @abstract   Creates a descriptor on autoreleaspool for the MPSNNOptimizerDescriptor object, no gradient clipping would be applied
 *
 *  @param      learningRate               The learningRate which will be applied
 *  @param      gradientRescale            The gradientRescale which will be applied
 *  @param      regularizationType         The regularizationType which will be applied
 *  @param      regularizationScale        The regularizationScale which will be applied
 *
 *  @return     A valid MPSNNOptimizerDescriptor object or nil, if failure.
 */
+(nonnull instancetype) optimizerDescriptorWithLearningRate: (float) learningRate
                                            gradientRescale: (float) gradientRescale
                                         regularizationType: (MPSNNRegularizationType) regularizationType
                                        regularizationScale: (float) regularizationScale;

/*
 *  @abstract   Creates a descriptor on autoreleaspool for the MPSNNOptimizerDescriptor object
 *
 *  @param      learningRate               The learningRate which will be applied
 *  @param      gradientRescale            The gradientRescale which will be applied
 *  @param      applyGradientClipping      The BOOL which sets if gradientClipping would be applied to the gradient
 *  @param      gradientClipMax            The gradientClipMax which will be applied
 *  @param      gradientClipMin            The gradientClipMin which will be applied
 *  @param      regularizationType         The regularizationType which will be applied
 *  @param      regularizationScale        The regularizationScale which will be applied
 *
 *  @return     A valid MPSNNOptimizerDescriptor object or nil, if failure.
 */
+(nonnull instancetype) optimizerDescriptorWithLearningRate: (float) learningRate
                                            gradientRescale: (float) gradientRescale
                                      applyGradientClipping: (BOOL) applyGradientClipping
                                            gradientClipMax: (float) gradientClipMax
                                            gradientClipMin: (float) gradientClipMin
                                         regularizationType: (MPSNNRegularizationType) regularizationType
                                        regularizationScale: (float) regularizationScale;

@end  /* MPSNNOptimizerDescriptor */





/*!
 *  @class      MPSNNOptimizer
 *  @discussion The MPSNNOptimizer base class, use one of the child classes, not to be directly used. Optimizers are generally used to update trainable neural network parameters.
 *              Users are usually expected to call these MPSKernels from the update methods on their Convolution or BatchNormalization data sources.
 *
 *              Before the gradient is used to update the original value, some preprocessing occurs on each gradient where it is scaled or clipped
 *              If regularization is chosen the appropriate regularization loss gradient is added to the value gradient.
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNOptimizer : MPSKernel

/*! @property   learningRate
 *  @abstract   The learningRate at which we update values
 *  @discussion The default value is 1e-3
 */
@property (readonly, nonatomic) float learningRate;

/*! @property   gradientRescale
 *  @abstract   The gradientRescale at which we apply to incoming gradient values
 *  @discussion The default value is 1.0
 */
@property (readonly, nonatomic) float gradientRescale;

/*! @property   applyGradientClipping
 *  @abstract   A bool which decides if gradient will be clipped
 *  @discussion The default value is NO
 */
@property (readwrite, nonatomic) BOOL applyGradientClipping;

/*! @property   gradientClipMax
 *  @abstract   The maximum value at which incoming gradient will be clipped before rescaling, applyGradientClipping must be true
 */
@property (readonly, nonatomic) float gradientClipMax;

/*! @property   gradientClipMin
 *  @abstract   The minimum value at which incoming gradient will be clipped before rescaling, applyGradientClipping must be true
 */
@property (readonly, nonatomic) float gradientClipMin;

/*! @property   regularizationScale
 *  @abstract   The regularizationScale at which we apply L1 or L2 regularization, it gets ignored if regularization is None
 *  @discussion The default value is 0.0
 */
@property (readonly, nonatomic) float regularizationScale;

/*! @property   regularizationType
 *  @abstract   The regularizationType which we apply.
 *  @discussion The default value is MPSRegularizationTypeNone
 */
@property (readonly, nonatomic) MPSNNRegularizationType regularizationType;


/*
 * You must use one of the sub-classes of MPSNNOptimizer.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;

-(void) setLearningRate: (float) newLearningRate;

@end  /* MPSNNOptimizer */








/*!
 *  @class      MPSNNOptimizerStochasticGradientDescent
 *  @discussion The MPSNNOptimizerStochasticGradientDescent performs a gradient descent with an optional momentum Update
 *              RMSProp is also known as root mean square propagation.
 *
 *              useNestrov == NO:
 *                  m[t]     = momentumScale * m[t-1] + learningRate * g
 *                  variable = variable - m[t]
 *
 *              useNestrov == YES:
 *                  m[t]     = momentumScale * m[t-1] + g
 *                  variable = variable - (learningRate * (g + m[t] * momentumScale))
 *
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                m[t] is momentum of gradients it is a state we keep updating every update iteration
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNOptimizerStochasticGradientDescent : MPSNNOptimizer


/*! @property   momentumScale
 *  @abstract   The momentumScale at which we update momentum for values array
 *  @discussion Default value is 0.0
 *
 */
@property (readonly, nonatomic) float momentumScale;

/*! @property   useNestrovMomentum
 *  @abstract   Nestrov momentum is considered an improvement on the usual momentum update
 *  @discussion Default value is NO
 *
 */
@property (readonly, nonatomic) BOOL useNestrovMomentum;

/*
 * Use one of the other init methods.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;


/*!
 *  @abstract   Convenience initialization for the momentum update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      learningRate               The learningRate which will be applied
 *
 *  @return     A valid MPSNNOptimizerStochasticGradientDescent object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                          learningRate: (float) learningRate;

/*!
 *  @abstract   Full initialization for the momentum update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      momentumScale              The momentumScale to update momentum for values array
 *  @param      useNestrovMomentum         Use the Nestrov style momentum update
 *  @param      optimizerDescriptor        The optimizerDescriptor which will have a bunch of properties to be applied
 *
 *
 *  @return     A valid MPSNNOptimizerMomentum object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         momentumScale: (float) momentumScale
                    useNestrovMomentum: (BOOL) useNestrovMomentum
                   optimizerDescriptor: (nonnull MPSNNOptimizerDescriptor *) optimizerDescriptor;

/*!
 *  @abstract   Encode an MPSNNOptimizerStochasticGradientDescent object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      inputGradientVector        A valid MPSVector object which specifies the input vector of gradients for this update.
 *  @param      inputValuesVector          A valid MPSVector object which specifies the input vector of values to be updated.
 *  @param      inputMomentumVector        A valid MPSVector object which specifies the gradient momentum vector which will
 *                                         be updated and overwritten.
 *  @param      resultValuesVector         A valid MPSVector object which specifies the resultValues vector which will
 *                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              useNestrov == NO:
 *                  m[t]     = momentumScale * m[t-1] + learningRate * g
 *                  variable = variable - m[t]
 *
 *              useNestrov == YES:
 *                  m[t]     = momentumScale * m[t-1] + g
 *                  variable = variable - (learningRate * (g + m[t] * momentumScale))
 *
 *              inputMomentumVector == nil
 *                  variable = variable - (learningRate * g)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                m[t] is momentum of gradients it is a state we keep updating every update iteration
 *
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)  commandBuffer
          inputGradientVector: (nonnull MPSVector *) inputGradientVector
            inputValuesVector: (nonnull MPSVector *) inputValuesVector
          inputMomentumVector: (nullable MPSVector *) inputMomentumVector
           resultValuesVector: (nonnull MPSVector *) resultValuesVector
MPS_SWIFT_NAME(encode(commandBuffer:inputGradientVector:inputValuesVector:inputMomentumVector:resultValuesVector:));

/*!
 *  @abstract   Encode an MPSNNOptimizerStochasticGradientDescent object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      convolutionGradientState   A valid MPSCNNConvolutionGradientState object which specifies the input state with gradients for this update.
 *  @param      convolutionSourceState     A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the input state with values to be updated.
 *  @param      inputMomentumVectors       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                         be updated and overwritten. The index 0 corresponds to weights, index 1 corresponds to biases, array can be of
 *                                         size 1 in which case biases won't be updated
 *  @param      resultState                A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the resultValues state which will
 *                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              useNestrov == NO:
 *                  m[t]     = momentumScale * m[t-1] + learningRate * g
 *                  variable = variable - m[t]
 *
 *              useNestrov == YES:
 *                  m[t]     = momentumScale * m[t-1] + g
 *                  variable = variable - (learningRate * (g + m[t] * momentumScale))
 *
 *              inputMomentumVector == nil
 *                  variable = variable - (learningRate * g)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                m[t] is momentum of gradients it is a state we keep updating every update iteration
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
     convolutionGradientState: (MPSCNNConvolutionGradientState* __nonnull) convolutionGradientState
       convolutionSourceState: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) convolutionSourceState
         inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
                  resultState: (nonnull MPSCNNConvolutionWeightsAndBiasesState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:convolutionGradientState:convolutionSourceState:inputMomentumVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerStochasticGradientDescent object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationState                    A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients and original gamma/beta for this update.
 *  @param      inputMomentumVectors                       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              useNestrov == NO:
 *                  m[t]     = momentumScale * m[t-1] + learningRate * g
 *                  variable = variable - m[t]
 *
 *              useNestrov == YES:
 *                  m[t]     = momentumScale * m[t-1] + g
 *                  variable = variable - (learningRate * (g + m[t] * momentumScale))
 *
 *              inputMomentumVector == nil
 *                  variable = variable - (learningRate * g)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                m[t] is momentum of gradients it is a state we keep updating every update iteration
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
      batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
         inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
                  resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationState:inputMomentumVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerStochasticGradientDescent object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationGradientState            A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients for this update.
 *  @param      batchNormalizationSourceState              A valid MPSCNNBatchNormalizationState object which specifies the input state with original gamma/beta for this update.
 *  @param      inputMomentumVectors                       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              useNestrov == NO:
 *                  m[t]     = momentumScale * m[t-1] + learningRate * g
 *                  variable = variable - m[t]
 *
 *              useNestrov == YES:
 *                  m[t]     = momentumScale * m[t-1] + g
 *                  variable = variable - (learningRate * (g + m[t] * momentumScale))
 *
 *              inputMomentumVector == nil
 *                  variable = variable - (learningRate * g)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                m[t] is momentum of gradients it is a state we keep updating every update iteration
 *
 *
 */
-(void)            encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
         batchNormalizationGradientState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationGradientState
           batchNormalizationSourceState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationSourceState
                    inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
                             resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationGradientState:batchNormalizationSourceState:inputMomentumVectors:resultState:));




@end  /* MPSNNOptimizerStochasticGradientDescent */









/*!
 *  @class      MPSNNOptimizerRMSProp
 *  @discussion The MPSNNOptimizerRMSProp performs an RMSProp Update
 *              RMSProp is also known as root mean square propagation.
 *
 *              s[t]     = decay * s[t-1] + (1 - decay) * (g ^ 2)
 *              variable = variable - learningRate * g / (sqrt(s[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                s[t] is weighted sum of squares of gradients
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNOptimizerRMSProp : MPSNNOptimizer


/*! @property   decay
 *  @abstract   The decay at which we update sumOfSquares
 *  @discussion Default value is 0.9
 *
 */
@property (readonly, nonatomic) double decay;

/*! @property   epsilon
 *  @abstract   The epsilon at which we update values
 *  @discussion This value is usually used to ensure to avoid divide by 0, default value is 1e-8
 *
 */
@property (readonly, nonatomic) float epsilon;

/*
 * Use one of the other init methods.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;


/*!
 *  @abstract   Convenience initialization for the RMSProp update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      learningRate               The learningRate which will be applied
 *
 *  @return     A valid MPSNNOptimizerRMSProp object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                          learningRate: (float) learningRate;

/*!
 *  @abstract   Full initialization for the rmsProp update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      decay                      The decay to update sumOfSquares
 *  @param      epsilon                    The epsilon which will be applied
 *  @param      optimizerDescriptor        The optimizerDescriptor which will have a bunch of properties to be applied
 *
 *
 *  @return     A valid MPSNNOptimizerRMSProp object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                 decay: (double) decay
                               epsilon: (float) epsilon
                   optimizerDescriptor: (nonnull MPSNNOptimizerDescriptor *) optimizerDescriptor;

/*!
 *  @abstract   Encode an MPSNNOptimizerRMSProp object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      inputGradientVector        A valid MPSVector object which specifies the input vector of gradients for this update.
 *  @param      inputValuesVector          A valid MPSVector object which specifies the input vector of values to be updated.
 *  @param      inputSumOfSquaresVector    A valid MPSVector object which specifies the gradient velocity vector which will
 *                                         be updated and overwritten.
 *  @param      resultValuesVector         A valid MPSVector object which specifies the resultValues vector which will
 *                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              s[t]     = decay * s[t-1] + (1 - decay) * (g ^ 2)
 *              variable = variable - learningRate * g / (sqrt(s[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                s[t] is weighted sum of squares of gradients
 *
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)  commandBuffer
          inputGradientVector: (nonnull MPSVector *) inputGradientVector
            inputValuesVector: (nonnull MPSVector *) inputValuesVector
      inputSumOfSquaresVector: (nonnull MPSVector *) inputSumOfSquaresVector
           resultValuesVector: (nonnull MPSVector *) resultValuesVector
MPS_SWIFT_NAME(encode(commandBuffer:inputGradientVector:inputValuesVector:inputSumOfSquaresVector:resultValuesVector:));


/*!
 *  @abstract   Encode an MPSNNOptimizerRMSProp object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      convolutionGradientState   A valid MPSCNNConvolutionGradientState object which specifies the input state with gradients for this update.
 *  @param      convolutionSourceState     A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the input state with values to be updated.
 *  @param      inputSumOfSquaresVectors   An array MPSVector object which specifies the gradient sumOfSquares vectors which will
 *                                         be updated and overwritten. The index 0 corresponds to weights, index 1 corresponds to biases, array can be of
 *                                         size 1 in which case biases won't be updated
 *  @param      resultState                A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the resultValues state which will
 *                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              s[t]     = decay * s[t-1] + (1 - decay) * (g ^ 2)
 *              variable = variable - learningRate * g / (sqrt(s[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                s[t] is weighted sum of squares of gradients
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
     convolutionGradientState: (MPSCNNConvolutionGradientState* __nonnull) convolutionGradientState
       convolutionSourceState: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) convolutionSourceState
     inputSumOfSquaresVectors: (nullable NSArray<MPSVector *>*) inputSumOfSquaresVectors
                  resultState: (nonnull MPSCNNConvolutionWeightsAndBiasesState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:convolutionGradientState:convolutionSourceState:inputSumOfSquaresVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerRMSProp object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationState                    A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients and original gamma/beta for this update.
 *  @param      inputSumOfSquaresVectors                   An array MPSVector object which specifies the gradient sumOfSquares vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              s[t]     = decay * s[t-1] + (1 - decay) * (g ^ 2)
 *              variable = variable - learningRate * g / (sqrt(s[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                s[t] is weighted sum of squares of gradients
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
      batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
     inputSumOfSquaresVectors: (nullable NSArray<MPSVector *>*) inputSumOfSquaresVectors
                  resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationState:inputSumOfSquaresVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerRMSProp object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationGradientState            A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients for this update.
 *  @param      batchNormalizationSourceState              A valid MPSCNNBatchNormalizationState object which specifies the input state with original gamma/beta for this update.
 *  @param      inputSumOfSquaresVectors                   An array MPSVector object which specifies the gradient sumOfSquares vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *
 *              s[t]     = decay * s[t-1] + (1 - decay) * (g ^ 2)
 *              variable = variable - learningRate * g / (sqrt(s[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                s[t] is weighted sum of squares of gradients
 *
 *
 */
-(void)            encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
         batchNormalizationGradientState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationGradientState
           batchNormalizationSourceState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationSourceState
                inputSumOfSquaresVectors: (nullable NSArray<MPSVector *>*) inputSumOfSquaresVectors
                             resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationGradientState:batchNormalizationSourceState:inputSumOfSquaresVectors:resultState:));

@end  /* MPSNNOptimizerRMSProp */









/*!
 *  @class      MPSNNOptimizerAdam
 *  @discussion The MPSNNOptimizerAdam performs an Adam Update
 *
 *              Initialization time
 *              m[0] = 0 (Initialize initial 1st moment vector aka momentum, user is responsible for this)
 *              v[0] = 0 (Initialize initial 2nd moment vector aka velocity, user is responsible for this)
 *              t    = 0 (Initialize timestep)
 *
 *              https://arxiv.org/abs/1412.6980
 *
 *              At update time:
 *              t = t + 1
 *              lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
 *
 *              m[t]     = beta1 * m[t-1] + (1 - beta1) * g
 *              v[t]     = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
 *              variable = variable - lr[t] * m[t] / (sqrt(v[t]) + epsilon)
 *
 *              where,
 *                g    is gradient of error wrt variable
 *                v[t] is velocity
 *                m[t] is momentum
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSNNOptimizerAdam : MPSNNOptimizer

/*! @property   beta1
 *  @abstract   The beta1 at which we update values
 *  @discussion Default value is 0.9
 *
 */
@property (readonly, nonatomic) double beta1;

/*! @property   beta2
 *  @abstract   The beta2 at which we update values
 *  @discussion Default value is 0.999
 *
 */
@property (readonly, nonatomic) double beta2;

/*! @property   epsilon
 *  @abstract   The epsilon at which we update values
 *  @discussion This value is usually used to ensure to avoid divide by 0, default value is 1e-8
 *
 */
@property (readonly, nonatomic) float epsilon;

/*! @property   timeStep
 *  @abstract   Current timeStep for the update, number of times update has occurred
 *
 */
@property (readwrite, nonatomic) NSUInteger timeStep;

/*
 * Use one of the other init methods.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;


/*!
 *  @abstract   Convenience initialization for the adam update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      learningRate               The learningRate at which we will update values
 *
 *  @return     A valid MPSNNOptimizerAdam object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                          learningRate: (float) learningRate;

/*!
 *  @abstract   Full initialization for the adam update
 *
 *  @param      device                     The device on which the kernel will execute.
 *  @param      beta1                      The beta1 to update values
 *  @param      beta2                      The beta2 to update values
 *  @param      epsilon                    The epsilon at which we update values
 *  @param      timeStep                   The timeStep at which values will start updating
 *  @param      optimizerDescriptor        The optimizerDescriptor which will have a bunch of properties to be applied
 *
 *
 *  @return     A valid MPSNNOptimizerAdam object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                 beta1: (double) beta1
                                 beta2: (double) beta2
                               epsilon: (float) epsilon
                              timeStep: (NSUInteger) timeStep
                   optimizerDescriptor: (nonnull MPSNNOptimizerDescriptor *) optimizerDescriptor;


/*!
 *  @abstract   Encode an MPSNNOptimizerAdam object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer          A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      inputGradientVector    A valid MPSVector object which specifies the input vector of gradients for this update.
 *  @param      inputValuesVector      A valid MPSVector object which specifies the input vector of values to be updated.
 *  @param      inputMomentumVector    A valid MPSVector object which specifies the gradient momentum vector which will
 *                                     be updated and overwritten.
 *  @param      inputVelocityVector    A valid MPSVector object which specifies the gradient velocity vector which will
 *                                     be updated and overwritten.
 *  @param      resultValuesVector     A valid MPSVector object which specifies the resultValues vector which will
 *                                     be updated and overwritten.
 *
 *  @discussion The following operations would be applied
 *
 *              t = t + 1
 *              lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
 *
 *              m[t]     = beta1 * m[t-1] + (1 - beta1) * g
 *              v[t]     = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
 *              variable = variable - lr[t] * m[t] / (sqrt(v[t]) + epsilon)
 *
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)  commandBuffer
          inputGradientVector: (nonnull MPSVector *) inputGradientVector
            inputValuesVector: (nonnull MPSVector *) inputValuesVector
          inputMomentumVector: (nonnull MPSVector *) inputMomentumVector
          inputVelocityVector: (nonnull MPSVector *) inputVelocityVector
           resultValuesVector: (nonnull MPSVector *) resultValuesVector
MPS_SWIFT_NAME(encode(commandBuffer:inputGradientVector:inputValuesVector:inputMomentumVector:inputVelocityVector:resultValuesVector:));

/*!
 *  @abstract   Encode an MPSNNOptimizerAdam object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      convolutionGradientState   A valid MPSCNNConvolutionGradientState object which specifies the input state with gradients for this update.
 *  @param      convolutionSourceState     A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the input state with values to be updated.
 *  @param      inputMomentumVectors       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                         be updated and overwritten. The index 0 corresponds to weights, index 1 corresponds to biases, array can be of
 *                                         size 1 in which case biases won't be updated
 *  @param      inputVelocityVectors       An array MPSVector object which specifies the gradient velocity vectors which will
 *                                         be updated and overwritten. The index 0 corresponds to weights, index 1 corresponds to biases, array can be of
 *                                         size 1 in which case biases won't be updated
 *  @param      resultState                A valid MPSCNNConvolutionWeightsAndBiasesState object which specifies the resultValues state which will
 *                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *              t = t + 1
 *              lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
 *
 *              m[t]     = beta1 * m[t-1] + (1 - beta1) * g
 *              v[t]     = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
 *              variable = variable - lr[t] * m[t] / (sqrt(v[t]) + epsilon)
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
     convolutionGradientState: (MPSCNNConvolutionGradientState* __nonnull) convolutionGradientState
       convolutionSourceState: (MPSCNNConvolutionWeightsAndBiasesState* __nonnull) convolutionSourceState
         inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
         inputVelocityVectors: (nullable NSArray<MPSVector *>*) inputVelocityVectors
                  resultState: (nonnull MPSCNNConvolutionWeightsAndBiasesState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:convolutionGradientState:convolutionSourceState:inputMomentumVectors:inputVelocityVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerAdam object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationState                    A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients and original gamma/beta for this update.
 *  @param      inputMomentumVectors                       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      inputVelocityVectors                       An array MPSVector object which specifies the gradient velocity vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *              t = t + 1
 *              lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
 *
 *              m[t]     = beta1 * m[t-1] + (1 - beta1) * g
 *              v[t]     = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
 *              variable = variable - lr[t] * m[t] / (sqrt(v[t]) + epsilon)
 *
 *
 */
-(void) encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
      batchNormalizationState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationState
         inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
         inputVelocityVectors: (nullable NSArray<MPSVector *>*) inputVelocityVectors
                  resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationState:inputMomentumVectors:inputVelocityVectors:resultState:));

/*!
 *  @abstract   Encode an MPSNNOptimizerAdam object to a command buffer to perform out of place update
 *
 *  @param      commandBuffer                              A valid MTLCommandBuffer to receive the encoded kernel.
 *  @param      batchNormalizationGradientState            A valid MPSCNNBatchNormalizationState object which specifies the input state with gradients for this update.
 *  @param      batchNormalizationSourceState              A valid MPSCNNBatchNormalizationState object which specifies the input state with original gamma/beta for this update.
 *  @param      inputMomentumVectors                       An array MPSVector object which specifies the gradient momentum vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      inputVelocityVectors                       An array MPSVector object which specifies the gradient velocity vectors which will
 *                                                         be updated and overwritten. The index 0 corresponds to gamma, index 1 corresponds to beta, array can be of
 *                                                         size 1 in which case beta won't be updated
 *  @param      resultState                                A valid MPSCNNNormalizationGammaAndBetaState object which specifies the resultValues state which will
 *                                                         be updated and overwritten.
 *
 *
 *  @discussion The following operations would be applied
 *
 *              t = t + 1
 *              lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
 *
 *              m[t]     = beta1 * m[t-1] + (1 - beta1) * g
 *              v[t]     = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
 *              variable = variable - lr[t] * m[t] / (sqrt(v[t]) + epsilon)
 *
 *
 */
-(void)             encodeToCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
          batchNormalizationGradientState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationGradientState
            batchNormalizationSourceState: (MPSCNNBatchNormalizationState* __nonnull) batchNormalizationSourceState
                     inputMomentumVectors: (nullable NSArray<MPSVector *>*) inputMomentumVectors
                     inputVelocityVectors: (nullable NSArray<MPSVector *>*) inputVelocityVectors
                              resultState: (nonnull MPSCNNNormalizationGammaAndBetaState *) resultState
MPS_SWIFT_NAME(encode(commandBuffer:batchNormalizationGradientState:batchNormalizationSourceState:inputMomentumVectors:inputVelocityVectors:resultState:));

@end  /* MPSNNOptimizerAdam */





#ifdef __cplusplus
}
#endif

#endif /* MPSNNOptimizers_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNPooling.h
//
//  MPSCNNPooling.h
//  MPS
//
//  Created by Ian Ollmann on 8/21/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNPooling_h
#define MPSCNNPooling_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>
#import <MPSCore/MPSCore.h>

#ifdef __cplusplus
extern "C" {
#endif


/*!
 *  @class      MPSCNNPooling
 *  @dependency This depends on Metal.framework
 *  @discussion Pooling is a form of non-linear sub-sampling. Pooling partitions the input image into a set of
 *              rectangles (overlapping or non-overlapping) and, for each such sub-region, outputs a value.
 *              The pooling operation is used in computer vision to reduce the dimensionality of intermediate representations.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNPooling : MPSCNNKernel


/*!
 *  @abstract  Initialize a pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @return     A valid MPSCNNPooling object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight;

/*!
 *  @abstract  Initialize a pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPooling object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPooling
 *  @param      device      The MTLDevice on which to make the MPSCNNPooling
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));



/*
 * Use initWithDevice:kernelWidth:kernelHeight:strideInPixelsX:strideInPixelsY: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*!
 *          Pooling window notes
 *          ====================
 *          The encodeToCommandBuffer API in MPSCNNKernel can be used to encode a MPSCNNPooling kernel
 *          to a MTLCommandBuffer. The exact location of the pooling window for each output value is determined
 *          as follows: The pooling window center for the first (top left) output pixel of the clipping
 *          rectangle is at spatial coordinates (offset.x, offset.y) in the input image. From this
 *          the top left corner of the pooling window is at
 *              (offset.x - floor(kernelWidth/2), offset.y - floor(kernelHeight/2)) and
 *          extends of course (kernelWidth, kernelHeight) pixels to the right and down direction, which means
 *          that the last pixel to be included into the pooling window is at:
 *              (offset.x + floor((kernelWidth-1)/2), offset.y + floor((kernelHeight-1)/2)),
 *          so that for even kernel sizes the pooling window is extends one pixel more into the left and up
 *          directions.
 *          The following pooling windows can be then easily deduced from the first one by simple shifting
 *          the source coordinates according to values of @ref strideInPixelsX and @ref strideInPixelsY.
 *          For example the pooling window center w(x,y) for the output value at coordinate (x,y) of the
 *          destination clip rectangle ((x,y) computed wrt. clipping rectangle origin) is at:
 *              w(x,y) = ( offset.x + strideInPixelsX * x , offset.y + strideInPixelsY * y ).
 *
 *          Quite often it is desirable to distribute the pooling windows as evenly as possible in the
 *          input image. As explained above, if offset is zero, then the center of the first pooling
 *          window is at the top left corner of the input image, which means that the left and top stripes
 *          of the pooling window are read from outside the input image boundaries (when filter size is
 *          larger than unity). Also it may mean that some values from the bottom and right stripes are
 *          not included at all in the pooling resulting in loss of valuable information. The different
 *          padding styles and padding sizes, or alternatively @ref offset and @edgeMode, can be used
 *          to control pooling window placement and boundary handling.
 *
 *
 *          For @ref MPSCNNPoolingMax the way the input image borders are handled (see @ref edgeMode)
 *          can become important: if there are negative values in the source image near the borders of the
 *          image and the pooling window crosses the borders, then using @ref MPSImageEdgeModeZero may
 *          cause the maximum pooling operation to override the negative input data values with zeros
 *          coming from outside the source image borders, resulting in large boundary effects. A simple
 *          way to avoid this is to use @ref MPSImageEdgeModeClamp for @ref edgeMode, which for
 *          @ref MPSCNNPoolingMax effectively causes all pooling windows to remain within the source image.
 *
 *
 *          Below are a couple of examples that can be used to visualize the effects of different
 *          offset values for pooling. The illustrations show the pooling window placements inside a
 *          single feature channel of a source image. In the first examples we use strides that are
 *          larger than the pooling window sizes in order to clarify the placement of each
 *          individual pooling window.
 *@code
 *              Source image: width = 8, height = 5
 *              Destination cliprect: width = 3, height = 2
 *              o - source pixel center, one for each destination cliprect pixel
 *              x - filter taps in the pooling windows
 *@endcode
 *          1) Filter size = 2x2, stride = 3x3, offset = (0,0)
 *@code
 *              x  x     x  x     x  x
 *                |-----------------------|
 *              x |xo|  |x |xo|  |x |xo|  |
 *                |-----------------------|
 *                |  |  |  |  |  |  |  |  |
 *                |-----------------------|
 *              x |x |  |x |x |  |x |x |  |
 *                |-----------------------|
 *              x |xo|  |x |xo|  |x |xo|  |
 *                |-----------------------|
 *                |  |  |  |  |  |  |  |  |
 *                |-----------------------|
 *@endcode
 *          One can use @ref offset to move the pooling windows within the source image:
 *          Using the formula offset.xy += (int)floor( ((L.xy - 1) % s.xy) / 2 ) + 1 from above
 *          for even filter sizes gives:
 *@code
 *              offset.x = floor( (7 % 3) / 2) + 1 = 0 + 1 = 1 and
 *              offset.y = floor( (4 % 3) / 2) + 1 = 0 + 1 = 1.
 *@endcode
 *          2) Filter size = 2x2, stride = 3x3, offset = (1,1)
 *@code
 *                |-----------------------|
 *                |x |x |  |x |x |  |x |x |
 *                |-----------------------|
 *                |x |xo|  |x |xo|  |x |xo|
 *                |-----------------------|
 *                |  |  |  |  |  |  |  |  |
 *                |-----------------------|
 *                |x |x |  |x |x |  |x |x |
 *                |-----------------------|
 *                |x |xo|  |x |xo|  |x |xo|
 *                |-----------------------|
 *@endcode
 *
 *          Our third example shows the placement of additional taps when we increase
 *          the size of the pooling window to 3x3.
 *          In this case the recommended formula for offsets with odd filter sizes gives:
 *@code
 *              offset.x = ceil( (7 % 3) / 2) = 1 and
 *              offset.y = ceil( (4 % 3) / 2) = 1.
 *@endcode
 *          3) Filter size = 3x3, stride = 3x3, offset = (1,1)
 *@code
 *                |-----------------------|
 *                |x |x |x |x |x |x |x |x |x
 *                |-----------------------|
 *                |x |xo|x |x |xo|x |x |xo|x
 *                |-----------------------|
 *                |x |x |x |x |x |x |x |x |x
 *                |-----------------------|
 *                |x |x |x |x |x |x |x |x |x
 *                |-----------------------|
 *                |x |xo|x |x |xo|x |x |xo|x
 *                |-----------------------|
 *                 x  x  x  x  x  x  x  x  x
 *@endcode
 *          In order to avoid large boundary effects with max pooling in examples 1) and 3) the user can use
 *          @ref MPSImageEdgeModeClamp for @ref edgeMode, which has the same effect as constraining the pooling
 *          windows to be confined completely within the source image.
 *
 */

@end    /* MPSCNNPooling */


/*!
 *  @class MPSCNNPoolingMax
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the max pooling filter.  For each pixel, returns the maximum value of pixels
 *              in the kernelWidth x kernelHeight filter region.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNPoolingMax : MPSCNNPooling

/*!
 *  @abstract   Initialize a MPSCNNPoolingMax pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPooling object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPooling
 *  @param      device      The MTLDevice on which to make the MPSCNNPooling
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end    /* MPSCNNPoolingMax */


/*!
 *  @class MPSCNNPoolingAverage
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the average pooling filter.  For each pixel, returns the mean value of pixels
 *              in the kernelWidth x kernelHeight filter region.
 *              When @ref edgeMode is @ref MPSImageEdgeModeClamp the filtering window is shrunk to remain
 #              within the source image borders. What this means is that close to image borders the filtering window
 *              will be smaller in order to fit inside the source image and less values will be used to compute the
 *              average. In case the filtering window is entirely outside the source image border the
 *              outputted value will be zero.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSCNNPoolingAverage : MPSCNNPooling



/*! @property   zeroPadSizeX
 *  @abstract   How much zero padding to apply to both left and right borders of the input image for average pooling,
 *              when using @see edgeMode MPSImageEdgeModeClamp. For @see edgeMode MPSImageEdgeModeZero this property is
 *              ignored and the area outside the image is interpreted to contain zeros.
 *              The zero padding size is used to shrink the pooling window to fit inside the area bound by the source image
 *              and its padding region, but the effect is that the normalization factor of the average computation is computed
 *              also for the zeros in the padding region.
 */
@property(readwrite, nonatomic) NSUInteger zeroPadSizeX;

/*! @property   zeroPadSizeY
 *  @abstract   How much zero padding to apply to both top and bottom borders of the input image for average pooling,
 *              when using @see edgeMode MPSImageEdgeModeClamp. For @see edgeMode MPSImageEdgeModeZero this property is
 *              ignored and the area outside the image is interpreted to contain zeros.
 *              The zero padding size is used to shrink the pooling window to fit inside the area bound by the source image
 *              and its padding region, but the effect is that the normalization factor of the average computation is computed
 *              also for the zeros in the padding region.
 */
@property(readwrite, nonatomic) NSUInteger zeroPadSizeY;


/*!
 *  @abstract   Initialize a MPSCNNPoolingAverage pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPooling object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPooling
 *  @param      device      The MTLDevice on which to make the MPSCNNPooling
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end    /* MPSCNNPoolingAverage */


/*!
 *  @class MPSCNNPoolingL2Norm
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the L2-norm pooling filter.  For each pixel, returns L2-Norm of pixels
 *              in the kernelWidth x kernelHeight filter region.
 *                  out[c,x,y] = sqrt ( sum_{dx,dy} in[c,x+dx,y+dy] * in[c,x+dx,y+dy] ).
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSCNNPoolingL2Norm : MPSCNNPooling

/*!
 *  @abstract   Initialize a MPSCNNPoolingL2Norm pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPooling object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPooling
 *  @param      device      The MTLDevice on which to make the MPSCNNPooling
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder: (NSCoder * __nonnull)aDecoder
                                device: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNPoolingL2Norm */

/*!
 *  @class MPSCNNDilatedPoolingMax
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the dilated max pooling filter.  For each pixel, returns the maximum value of pixels
 *              in the kernelWidth x kernelHeight filter region by step size dilationRateX x dilationRateY.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0) )
@interface  MPSCNNDilatedPoolingMax : MPSCNNPooling

/*! @property   dilationRateX
 *  @abstract   dilationRateX for accessing the image passed in as source
 */
@property(readonly, nonatomic) NSUInteger dilationRateX;

/*! @property   dilationRateY
 *  @abstract   dilationRateY for accessing the image passed in as source
 */
@property(readonly, nonatomic) NSUInteger dilationRateY;




/*!
 *  @abstract   Initialize a MPSCNNDilatedPoolingMax pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      dilationRateX       The dilation rate in the x dimension.
 *  @param      dilationRateY       The dilation rate in the y dimension.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNDilatedPoolingMax object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                         dilationRateX: (NSUInteger) dilationRateX
                         dilationRateY: (NSUInteger) dilationRateY
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel.h initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNDilatedPoolingMax
 *  @param      device      The MTLDevice on which to make the MPSCNNDilatedPoolingMax
 *  @return     A new MPSCNNDilatedPoolingMax object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNDilatedPoolingMax */















#pragma mark --
#pragma mark Pooling Gradient Kernels

/*!
 *  @class MPSCNNPoolingGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the base class for computing the gradient of the pooling filters.
 *              The operation backpropagates a gradient vector using the chain rule.
 *
 *              Given the input gradient vector dL(x) = dL/d out(x), which is the derivative of the
 *              loss-function wrt. (original) pooling filter output the output gradient at position y
 *              (dL/d in(y)) is computed as follows:
 *
 *                  dL/d in(y) = sum_x (dL/d out(x)) * (d out(x)/d in(y)), where
 *
 *              the sum runs over the input gradient pixels starting from primaryOffset
 *              extending to primaryOffset + sourceSize. Note here that we need a separate
 *              variable 'sourceSize' to specify which input gradients are included in the output
 *              gradient computation as this information cannot be deduced directly from the cliprect
 *              size due to fractional striding or simply because the user wants to examine a subset
 *              of the contributions to the gradients. In normal operation the sourceSize is specified
 *              as the cliprect.size of the forward pooling filter in order to compute the gradients for
 *              all outputs the forward direction produced and the primaryOffset is set to
 *              cliprect.origin of the original forward pooling operation for the same reason.
 *
 *              The cliprect property of the filter allows the user to send the gradients to a new location,
 *              which may not match the original forward pooling filter window locations:
 *              The index 'y' in the formula above refers to the pixel location in the secondary source,
 *              which is the source image of the original forward pooling filter and secondaryOffset specifies
 *              the center of the first pooling window as specified in MPSCNNPooling filter specification.
 *              The first (top leftmost) pixel written into the cliprect computes the derivative of the first pixel
 *              within the first pooling window that is contained within the secondary source image and
 *              subsequent values are defined by normal striding rules from secondary source to primary source.
 *              This means that typically the cliprect is set to fill the effective source area of the original forward
 *              operation, clamped to edges of the original source image, which in the normal case is the same size
 *              as the size of the gradient destination image.
 *
 *              If there are any values in the destination cliprect that do not contribute to the forward
 *              pooling result in the area specified by primaryOffset and sourceSize,
 *              due to large strides or dilation factors or simply because all forward pass induced values would be
 *              outside the source area, then those result values are set to zero.
 *
 *              The actual value of d out(x) / d in(y) depends on the pooling operation and these are defined in the
 *              subclasses of MPSCNNPoolingGradient.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNPoolingGradient : MPSCNNGradientKernel

/*! @property   sourceSize
 *  @abstract   An optional source size which defines together with primaryOffset, the set of input gradient
 *              pixels to take into account in the gradient computations.
 *  @discussion A MTLSize that together with primaryOffset indicates which part of the source gradient to consider.
 *              If the area does not lie completely within the primary source image, the intersection between
 *              source area rectangle and primary source bounds is used.
 *              Default: A size where every component is NSUIntegerMax indicating the entire rest of the image,
 *              starting from an offset (see primaryOffset).
 */
@property (readwrite, nonatomic) MTLSize               sourceSize;

/*!
 *  @abstract  Initialize a gradient pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @return     A valid MPSCNNPoolingGradient object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight;

/*!
 *  @abstract  Initialize a gradient pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The input stride (upsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The input stride (upsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPoolingGradient object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*
 * Use initWithDevice:kernelWidth:kernelHeight:strideInPixelsX:strideInPixelsY: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPoolingGradient
 *  @param      device      The MTLDevice on which to make the MPSCNNPoolingGradient
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNPoolingGradient */


/*!
 *  @class MPSCNNPoolingAverageGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the filter for computing the gradient of the average pooling filter.
 *              The operation backpropagates a gradient vector using chain rule.
 *
 *              Average pooling forward pass is defined as:
 *
 *                  out(x) = sum_{dx \in Window(x)} in(s*x+dx) / N(x), where
 *
 *              the pooling window definition 'Window(x)' follows MPSCNNPooling specification,
 *              'N(x)' is effective pooling window size in pixels as specified in MPSCNNPoolingAverage,
 *              's' is the pixel stride and in() is the source input image.
 *
 *              Hence the partial derivative of the output value wrt. to the input value needed in the
 *              gradient backpropagation in MPSCNNPoolingGradient is:
 *
 *                  d out(x)/d in(y) = sum_{dx \in Window(x)} delta_{s*x+dx, y} / N(x), where
 *
 *              delta_{x,y} is the Kronecker delta symbol for which
 *
 *                  delta_{x,y} =  {  1, when x == y
 *                                 {  0, otherwise.
 *
 *              In practice this means that the gradient value for the destination image at pixel 'x' is
 *              the sum over these contributions coming from all pooling windows that contribute
 *              to the average pooling computation in the forward pass, multiplied by the input
 *              gradient value in the source area of the corresponding pooling window.
 *
 *              Note: As average pooling is a linear operation of its inputs, the gradient does not
 *              depend at all on the original input values, but the original input image size is needed
 *              so that we know the limits where the input values seize to exist to inhibit accumulation
 *              of gradient values for those pixels. Therefore, as secondary input, any correctly sized
 *              image will produce correct results for the gradient backpropagation and hence it is
 *              recommended to use a temporary image of correct size (see MPSTemporaryImage) for the
 *              secondary source image parameter.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNPoolingAverageGradient : MPSCNNPoolingGradient


/*! @property   zeroPadSizeX
 *  @abstract   How much zero padding to apply to both left and right borders of the input image for average pooling,
 *              when using @see edgeMode MPSImageEdgeModeClamp. For @see edgeMode MPSImageEdgeModeZero this property is
 *              ignored and the area outside the image is interpreted to contain zeros.
 *              The zero padding size is used to shrink the pooling window to fit inside the area bound by the source image
 *              and its padding region, but the effect is that the normalization factor of the average computation is computed
 *              also for the zeros in the padding region.
 */
@property(readwrite, nonatomic) NSUInteger zeroPadSizeX;

/*! @property   zeroPadSizeY
 *  @abstract   How much zero padding to apply to both top and bottom borders of the input image for average pooling,
 *              when using @see edgeMode MPSImageEdgeModeClamp. For @see edgeMode MPSImageEdgeModeZero this property is
 *              ignored and the area outside the image is interpreted to contain zeros.
 *              The zero padding size is used to shrink the pooling window to fit inside the area bound by the source image
 *              and its padding region, but the effect is that the normalization factor of the average computation is computed
 *              also for the zeros in the padding region.
 */
@property(readwrite, nonatomic) NSUInteger zeroPadSizeY;


/*!
 *  @abstract  Initialize a gradient average pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The input stride (upsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The input stride (upsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPoolingGradient object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPoolingAverageGradient
 *  @param      device      The MTLDevice on which to make the MPSCNNPoolingAverageGradient
 *  @return     A new MPSCNNPoolingAverageGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNPoolingAverageGradient */





/*!
 *  @class MPSCNNPoolingMaxGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the filter for computing the gradient of the max pooling filter.
 *              The operation backpropagates a gradient vector using chain rule.
 *
 *              Dilated Max pooling forward pass is defined as:
 *
 *                  out(x) = max_{dx \in Window(x)} in(s*x+D*dx), where
 *
 *              the pooling window definition 'Window(x)' follows MPSCNNPooling specification,
 *              's' is the pixel stride and in() is the source input image and D is the dilation factor.
 *              For MPSCNNPoolingMaxGradient the dilationRate 'D' is one. NOTE: For even-sized pooling
 *              windows with dilation rate greater than one the effective pooling window is centered
 *              around s*x with non-even windows leaning towards top-left corner. For example if
 *              kernel width = 2, dilation rate = 3, then the pooling considers positions '-2' and '+1'
 *              relative to the pooling window center 's*x'.
 *
 *              Hence the partial derivative of the output value wrt. to the input value needed in the
 *              gradient backpropagation in MPSCNNPoolingGradient is:
 *
 *                  d out(x)/d in(y) = delta_{x_m, y}, where
 *
 *              delta_{x,y} is the Kronecker delta symbol (see MPSCNNPoolingAverageGradient) and x_m
 *              is the index of the maximum value in the corresponding pooling window.
 *
 *              In practice this means that the gradient value for the destination image at pixel 'x' is
 *              the sum over these contributions coming from all pooling windows that contribute
 *              to the max pooling computation in the forward pass, multiplied by the input
 *              gradient value in the source area of the corresponding pooling window. If there are
 *              multiple maximal values within a single pooling window one of them is picked for the
 *              gradient and this decision is implementation specific, which means that it can vary
 *              between different architectures and even between different filter parameters.
 *
 *              Note: The gradient max pooling needs the secondary input image in order to compute
 *              the indices of maximal values for each pooling window, but this means redundant computations.
 *              Later we may add encode calls to MPSCNNPoolingMax that produce a state that contains the
 *              coordinates of the maximal values to be consumed by the gradient filters.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNPoolingMaxGradient : MPSCNNPoolingGradient


/*!
 *  @abstract  Initialize a gradient max pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The input stride (upsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The input stride (upsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPoolingGradient object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPoolingMaxGradient
 *  @param      device      The MTLDevice on which to make the MPSCNNPoolingMaxGradient
 *  @return     A new MPSCNNPoolingMaxGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNPoolingMaxGradient */


/*!
 *  @class MPSCNNPoolingL2NormGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the filter for computing the gradient of the L2-Norm pooling filter.
 *              The operation backpropagates a gradient vector using chain rule.
 *
 *              L2-Norm pooling forward pass is defined as:
 *
 *                  out(x) = sqrt( sum_{dx \in Window(x)} in(s*x+dx) * in(s*x+dx) ), where
 *
 *              the pooling window definition 'Window(x)' follows MPSCNNPooling specification and
 *              's' is the pixel stride and in() is the source input image.
 *
 *              Hence the partial derivative of the output value wrt. to the input value needed in the
 *              gradient backpropagation in MPSCNNPoolingGradient is:
 *
 *                  d out(x)/d in(y) = sum_{dx \in Window(x)} delta_{s*x+dx, y} in(s*x+dx) / out(x), where
 *
 *              delta_{x,y} is the Kronecker delta symbol for which
 *
 *                  delta_{x,y} =  {  1, when x == y
 *                                 {  0, otherwise,
 *              and out(x) is the L2-norm pooling value at point 'x' defined above.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNPoolingL2NormGradient : MPSCNNPoolingGradient



/*!
 *  @abstract  Initialize a gradient L2-norm pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      strideInPixelsX     The input stride (upsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The input stride (upsampling factor) in the y dimension.
 *  @return     A valid MPSCNNPoolingL2NormGradient object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPoolingL2NormGradient
 *  @param      device      The MTLDevice on which to make the MPSCNNPoolingL2NormGradient
 *  @return     A new MPSCNNPoolingL2NormGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNPoolingL2NormGradient */



/*!
 *  @class MPSCNNDilatedPoolingMaxGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the filter for computing the gradient of the dilated max pooling filter.
 *              For details see comments on MPSCNNPoolingMaxGradient.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSCNNDilatedPoolingMaxGradient : MPSCNNPoolingGradient

/*!
 *  @abstract   Initialize a MPSCNNDilatedPoolingMaxGradient pooling filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel.  Can be an odd or even value.
 *  @param      kernelHeight        The height of the kernel.  Can be an odd or even value.
 *  @param      dilationRateX       The dilation rate in the x dimension.
 *  @param      dilationRateY       The dilation rate in the y dimension.
 *  @param      strideInPixelsX     The output stride (downsampling factor) in the x dimension.
 *  @param      strideInPixelsY     The output stride (downsampling factor) in the y dimension.
 *  @return     A valid MPSCNNDilatedPoolingMax object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                         dilationRateX: (NSUInteger) dilationRateX
                         dilationRateY: (NSUInteger) dilationRateY
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_DESIGNATED_INITIALIZER;

/*
 * Unavailable.  Use initWithDevice:kernelWidth:kernelHeight:dilationRateX:dilationRateY:strideInPixels:strideInPixelsY
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight
                       strideInPixelsX: (NSUInteger) strideInPixelsX
                       strideInPixelsY: (NSUInteger) strideInPixelsY NS_UNAVAILABLE;


/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPoolingMaxGradient
 *  @param      device      The MTLDevice on which to make the MPSCNNPoolingMaxGradient
 *  @return     A new MPSCNNPoolingMaxGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNDilatedPoolingMaxGradient */





#ifdef __cplusplus
}
#endif


#endif /* MPSCNNPooling_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNInstanceNormalization.h

/*!
 *  @header MPSCNNInstanceNormalization.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *  @abstract MPSKernels to do instance normalization and training
 */

#ifndef MPSCNNInstanceNormalization_h
#define MPSCNNInstanceNormalization_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>
#include <MPSNeuralNetwork/MPSCNNNormalizationWeights.h>

#ifdef __cplusplus
extern "C" {
#endif
   
@class MPSCNNInstanceNormalization;

/*!
 *  @class      MPSCNNInstanceNormalizationGradientState
 *  @dependency This depends on Metal.framework
 *  @discussion A state to hold information necessary to execute a gradient
 *              pass for MPSCNNInstanceNormalization.  Gradient states should
 *              be created by using the forward kernel's methods.  This will
 *              ensure that the state captures all information necessary to
 *              execute the corresponding gradient pass.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNInstanceNormalizationGradientState : MPSNNGradientState

/*! @abstract The MPSCNNInstanceNormalization object that created this state object. */
@property (readonly, nonatomic, nonnull, retain) MPSCNNInstanceNormalization * instanceNormalization;

/*!
 *  @abstract   Return an MTLBuffer object with the state's current gamma values.
 */
@property (readonly, nonatomic) __nullable id<MTLBuffer> gamma;

/*!
 *  @abstract   Return an MTLBuffer object with the state's current beta values..
 */
@property (readonly, nonatomic) __nullable id<MTLBuffer> beta;

/*!
 *  @property   The MTLBuffer containing the gradient values for gamma.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> gradientForGamma;

/*!
 *  @property   The MTLBuffer containing the gradient values for beta.
 */
@property (readonly, nonatomic) __nonnull id<MTLBuffer> gradientForBeta;

/*!
 *  Unavailable.  Use MPSCNNInstanceNormalization state creation methods.
 */
+(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                      textureDescriptor: (MTLTextureDescriptor * __nonnull) descriptor NS_UNAVAILABLE;

+(nonnull instancetype) temporaryStateWithCommandBuffer:(__nonnull id<MTLCommandBuffer>)cmdBuf NS_UNAVAILABLE;
+(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                             bufferSize: (size_t) bufferSize NS_UNAVAILABLE;

/*!
 *  Unavailable.  Use MPSCNNInstanceNormalization state creation methods.
 */
-(nonnull instancetype) initWithDevice: (__nonnull id <MTLDevice>) device
                     textureDescriptor: (MTLTextureDescriptor * __nonnull) descriptor NS_UNAVAILABLE;

/*!
 *  Unavailable.  Use MPSCNNInstanceNormalization state creation methods.
 */
-(nonnull instancetype) initWithResource: (__nullable id <MTLResource>) resource NS_UNAVAILABLE;

-(nonnull instancetype) initWithDevice: (__nonnull id <MTLDevice>) device
                            bufferSize: (size_t) bufferSize NS_UNAVAILABLE;

@end    /* MPSCNNInstanceNormalizationGradientState */
    
typedef NSArray<MPSCNNInstanceNormalizationGradientState*>  MPSCNNInstanceNormalizationGradientStateBatch
    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));
    
/*! @protocol   MPSCNNInstanceNormalizationDataSource
 *  @abstract   The MPSCNNInstanceNormalizationDataSource protocol declares the methods that an
 *              instance of MPSCNNInstanceNormalization uses to initialize the
 *              scale factors (gamma) and bias terms (beta).
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@protocol MPSCNNInstanceNormalizationDataSource <NSObject, NSCopying>

@required
    /*!
     *  @abstract   Return a pointer to an array containing the gamma terms.
     */
    -(float* __nullable) gamma;
    
    /*!
     *  @abstract   Return a pointer to an array containing the beta terms.
     */
    -(float* __nullable) beta;
    
    /*!
     *  @property   The number of feature channels over which to normalize.
     */
    @property (readonly, nonatomic) NSUInteger numberOfFeatureChannels;
    
    /*! @abstract   A label that is transferred to the instance normalization filter at init time
     *  @discussion Overridden by a MPSCNNInstanceNormalizationNode.label if it is non-nil.
     */
    -(NSString* __nullable) label;
    
@optional
    
    /*! @abstract       Compute new gamma and beta values using current values and gradients contained within a
     *                  MPSCNNInstanceNormalizationStateBatch.
     *  @discussion     This is for use in the context of training a network within a MPSNNGraph. If you are
     *                  writing your own graph using the low level interface or aren't training instance normalization
     *                  it isn't needed.
     *
     *                  In this mathod, you should perform the update on a GPU, because at the time it is called
     *                  the data isn't in the state objects yet and the CPU can't do the work. You should not attempt
     *                  to update the MPSCNNInstanceNormalization kernel directly with the results. The state object
     *                  returned from the function will be used for that.  A batch of states will be passed in.
     *                  You should accumulate the gradients and then update the weights.
     *
     *                  This operation is expected to also decrement the read count of instanceNormalizationStateBatch by 1,
     *                  if the states are temporary.
     *
     *  @param          commandBuffer                   The command buffer on which to encode the update.
     *
     *  @param          instanceNormalizationStateBatch A batch of MPSCNNInstanceNormalizationGradientState objects containing
     *                                                  current weights and gradients.
     *
     *  @return         A MPSCNNNormalizationGammaAndBetaState object containing updated gamma and beta values.  If NULL no
     *                  update was performed.
     */
    -(MPSCNNNormalizationGammaAndBetaState * __nullable) updateGammaAndBetaWithCommandBuffer: (nonnull id<MTLCommandBuffer>) commandBuffer
                                                             instanceNormalizationStateBatch: (MPSCNNInstanceNormalizationGradientStateBatch* __nonnull) instanceNormalizationStateBatch;
    /*! @abstract       Compute new gamma and beta values using current values and gradients contained within a
     *                  batch MPSCNNInstanceNormalizationState objects.  Perform the update on the CPU.
     *
     *  @param          instanceNormalizationStateBatch A batch of MPSCNNInstanceNormalizationGradientState objects containing
     *                                                  current gamma and beta values and gradients.
     *
     *  @return         A boolean value indicating if the update was performed.
     */
    -(BOOL) updateGammaAndBetaWithInstanceNormalizationStateBatch: (MPSCNNInstanceNormalizationGradientStateBatch* __nonnull) instanceNormalizationStateBatch;
    
    /*! @abstract       An optional tiny number to use to maintain numerical stability.
     *  @discussion     output_image = (input_image - mean[c]) * gamma[c] / sqrt(variance[c] + epsilon) + beta[c];
     *                  Defalt value if method unavailable: FLT_MIN   */
    -(float) epsilon;
    
    /*!
     *  Optional NSSecureCoding compatibility.
     */
    - (void)encodeWithCoder:(NSCoder * __nonnull)aCoder;
    
    - (nullable instancetype)initWithCoder:(NSCoder * __nonnull)aDecoder; // NS_DESIGNATED_INITIALIZER
    
    @property (class, readonly) BOOL supportsSecureCoding;
    
    /*!
     *  @abstract   Optional copy method to create a copy of the data source for use with a new device.
     *
     *  @param      zone    The NSZone on which to allocate.
     *  @param      device  The device where the kernel which uses this data source will be used.
     *
     *  @result     A pointer to a copy of this data source.
     */
    - (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                                   device:(nullable id <MTLDevice>) device MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));
@end    // MPSCNNInstanceNormalizationDataSource
    
/*!
 *  @class      MPSCNNInstanceNormalization
 *  @dependency This depends on Metal.framework
 *  @discussion This kernel normalizes each image, on a per-channel basis, to
 *              have zero mean and unit variance:
 *
 *              for each image:
 *                  for each channel:
 *                      y = (x - mean) * gamma / sqrt(variance + epsilon) + beta;
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNInstanceNormalization : MPSCNNKernel
/*! @property   epsilon
 *  @abstract   The epsilon value used to bias the variance when normalizing.
 */
@property(readwrite, nonatomic) float       epsilon;

/*! @abstract   The data source that the object was initialized with */
@property (readonly, nonatomic, nonnull, retain) id <MPSCNNInstanceNormalizationDataSource> dataSource;

/*!
 *  @abstract   Initialize a MPSCNNInstanceNormalization kernel on a device.
 *  @param      dataSource  An object conforming to the MPSCNNInstanceNormalizationDataSource
 *                          protocol which
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            dataSource: (nonnull id<MPSCNNInstanceNormalizationDataSource>) dataSource NS_DESIGNATED_INITIALIZER;

/*!
 * Use initWithDevice:dataSource instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSCNNInstanceNormalization object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Reload data using a data source.
 *
 *  @param      dataSource  The data source which will provide the gamma and beta terms
 *                          to scale and bias the normalized result respectively.
 */
-(void) reloadDataSource: (__nonnull id<MPSCNNInstanceNormalizationDataSource>) dataSource
MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use -reloadGammaAndBetaFromDataSource instead.",
                                      macos(10.13.4, 10.14), ios(11.3,12.0), tvos(11.3, 12.0));

/*!
 *  @abstract   Reinitialize the filter using the data source provided at kernel initialization.
 */
-(void) reloadGammaAndBetaFromDataSource MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));

/*!
 *  @abstract   Reload data using new gamma and beta terms contained within an
 *              MPSCNNInstanceNormalizationGradientState object.
 *
 *  @param      commandBuffer               The command buffer on which to encode the reload.
 *
 *  @param      gammaAndBetaState           The state containing the updated weights which are to
 *                                          be reloaded.
 */
-(void) reloadGammaAndBetaWithCommandBuffer: (__nonnull id<MTLCommandBuffer>) commandBuffer
                          gammaAndBetaState: (MPSCNNNormalizationGammaAndBetaState* __nonnull) gammaAndBetaState;

/*!
 *  @abstract   Return a MPSCNNInstanceNormalizationGradientState object for the provided
 *              source image, source states, and destination image.
 */
-(MPSCNNInstanceNormalizationGradientState * __nullable) resultStateForSourceImage: (MPSImage *__nonnull) sourceImage
                                                                      sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                                  destinationImage: (MPSImage * __nonnull) destinationImage;

/*! @abstract       Return a temporary MPSCNNInstanceNormalizationGradientState object which may be used with
 *                  a MPSCNNInstanceNormalization filter.
 */
-(MPSCNNInstanceNormalizationGradientState * __nullable) temporaryResultStateForCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                                                                  sourceImage: (MPSImage *__nonnull) sourceImage
                                                                                 sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                                             destinationImage: (MPSImage *__nonnull) destinationImage;
@end    /* MPSCNNInstanceNormalization */

/*!
 *  @class      MPSCNNInstanceNormalizationGradient
 *  @dependency This depends on Metal.framework
 *  @discussion This kernel executes a gradient pass corresponding to MPSCNNInstanceNormalization.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNInstanceNormalizationGradient : MPSCNNGradientKernel

@end    /* MPSCNNInstanceNormalizationGradient */
#ifdef __cplusplus
}
#endif


#endif /* MPSCNNInstanceNormalization_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNNormalization.h
//
//  MPSCNNNormalization.h
//  MPS
//
//  Created by Ian Ollmann on 8/21/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNNormalization_h
#define MPSCNNNormalization_h

#include <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif


#pragma mark - SpatialNormalization

/*!
 *  @class MPSCNNSpatialNormalization
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the spatial normalization filter.
 *              The spatial normalization for a feature channel applies the filter over local regions which extend
 *              spatially, but are in separate feature channels (i.e., they have shape 1 x kernelWidth x kernelHeight).
 *              For each feature channel, the function computes the sum of squares of X inside each rectangle, N2(i,j).
 *              It then divides each element of X as follows:
 *                  Y(i,j) = X(i,j) / (delta + alpha/(kw*kh) * N2(i,j))^beta,
 *              where kw and kh are the kernelWidth and the kernelHeight.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.12), ios(10.0), tvos(10.0))
@interface MPSCNNSpatialNormalization : MPSCNNKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

/*!
 *  @abstract  Initialize a spatial normalization filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel
 *  @param      kernelHeight        The height of the kernel
 *  @return     A valid MPSCNNSpatialNormalization object or nil, if failure.
 *
 *  NOTE:  For now, kernelWidth must be equal to kernelHeight
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*
 * Use initWithDevice:kernelWidth:kernelHeight instead
 *  @abstract *** Unavailable
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNSpatialNormalization */


/*!
 *  @class MPSCNNSpatialNormalizationGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the spatial normalization gradient filter.
 *              The spatial normalization for a feature channel applies the filter over local regions which extend
 *              spatially, but are in separate feature channels (i.e., they have shape 1 x kernelWidth x kernelHeight).
 *              For each feature channel, the function computes the sum of squares of X inside each rectangle, N2(i,j).
 *              It then divides each element of X as follows:
 *                  Y(i,j) = X(i,j) / (delta + alpha/(kw*kh) * N2(i,j))^beta,
 *              where kw and kh are the kernelWidth and the kernelHeight.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined.
 *
 *              T(i,j) = (delta + alpha/(kw*kh) * N2(i,j))
 *              N      = kw * kh
 *
 *              OutputGradient:
 *                  dZ/dX(i,j) =  T(i,j)^(-beta) * ( dZ/dY(i,j) - (2*alpha*beta*X(i,j)/T(i,j)) * (sum_{l,k in L(i),K(j)} dZ/dY(l,k)*X(l,k)) )
 *              N is the kernel size. The window R(k) itself is defined as:
 *                  L(i) = [i-floor((kw-1)/2), i+floor(kw/2]
 *                  K(j) = [j-floor((kh-1)/2), j+floor(kh/2]
 *
 *              For correct gradient computation all parameters must be the same as the original normalization filter.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNSpatialNormalizationGradient : MPSCNNGradientKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

/*!
 *  @abstract  Initialize a spatial normalization filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel
 *  @param      kernelHeight        The height of the kernel
 *  @return     A valid MPSCNNSpatialNormalization object or nil, if failure.
 *
 *  NOTE:  For now, kernelWidth must be equal to kernelHeight
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNSpatialNormalizationGradient */


#pragma mark - LocalContrastNormalization


/*!
 *  @class MPSCNNLocalContrastNormalization
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the local contrast normalization filter.
 *              The local contrast normalization is quite similar to spatial normalization
 *              (see @ref MPSCNNSpatialNormalization) in that it applies the filter over local regions which extend
 *              spatially, but are in separate feature channels (i.e., they have shape 1 x kernelWidth x kernelHeight),
 *              but instead of dividing by the local "energy" of the feature, the denominator uses the local variance
 *              of the feature - effectively the mean value of the feature is subtracted from the signal.
 *              For each feature channel, the function computes the variance VAR(i,j) and
 *              mean M(i,j) of X(i,j) inside each rectangle around the spatial point (i,j).
 *
 *              Then the result is computed for each element of X as follows:
 *
 *                  Y(i,j) = pm + ps * ( X(i,j) - p0 * M(i,j)) / (delta + alpha * VAR(i,j))^beta,
 *
 *              where kw and kh are the kernelWidth and the kernelHeight and pm, ps and p0 are parameters that
 *              can be used to offset and scale the result in various ways. For example setting
 *              pm=0, ps=1, p0=1, delta=0, alpha=1.0 and beta=0.5 scales input data so that the result has
 *              unit variance and zero mean, provided that input variance is positive.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined. A good way to guard
 *              against tiny variances is to regulate the expression with a small value for delta, for example
 *              delta = 1/1024 = 0.0009765625.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNLocalContrastNormalization : MPSCNNKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 0.0
 *  @discussion The default value 0.0 is not recommended and is
 *              preserved for backwards compatibility. With alpha 0,
 *              it performs a local mean subtraction. The
 *              MPSCNNLocalContrastNormalizationNode used with
 *              the MPSNNGraph uses 1.0 as a default.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 0.5
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1/1024
 */
@property (readwrite, nonatomic) float   delta;

/*! @property   p0
 *  @abstract   The value of p0.  Default is 1.0
 */
@property (readwrite, nonatomic) float   p0;

/*! @property   pm
 *  @abstract   The value of pm.  Default is 0.0
 */
@property (readwrite, nonatomic) float   pm;

/*! @property   ps
 *  @abstract   The value of ps.  Default is 1.0
 */
@property (readwrite, nonatomic) float   ps;


/*!
 *  @abstract  Initialize a local contrast normalization filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel
 *  @param      kernelHeight        The height of the kernel
 *  @return     A valid MPSCNNLocalContrastNormalization object or nil, if failure.
 *
 *  NOTE:  For now, kernelWidth must be equal to kernelHeight
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*
 * Use initWithDevice:kernelWidth:kernelHeight instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSCNNLocalContrastNormalization */


/*!
 *  @class MPSCNNLocalContrastNormalizationGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the local contrast normalization gradient filter.
 *              The local contrast normalization is quite similar to spatial normalization
 *              (see @ref MPSCNNSpatialNormalization) in that it applies the filter over local regions which extend
 *              spatially, but are in separate feature channels (i.e., they have shape 1 x kernelWidth x kernelHeight),
 *              but instead of dividing by the local "energy" of the feature, the denominator uses the local variance
 *              of the feature - effectively the mean value of the feature is subtracted from the signal.
 *              For each feature channel, the function computes the variance VAR(i,j) and
 *              mean M(i,j) of X(i,j) inside each rectangle around the spatial point (i,j).
 *
 *              Then the result is computed for each element of X as follows:
 *
 *                  Y(i,j) = pm + ps * ( X(i,j) - p0 * M(i,j)) / (delta + alpha * VAR(i,j))^beta,
 *
 *              where kw and kh are the kernelWidth and the kernelHeight and pm, ps and p0 are parameters that
 *              can be used to offset and scale the result in various ways. For example setting
 *              pm=0, ps=1, p0=1, delta=0, alpha=1.0 and beta=0.5 scales input data so that the result has
 *              unit variance and zero mean, provided that input variance is positive.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined. A good way to guard
 *              against tiny variances is to regulate the expression with a small value for delta, for example
 *              delta = 1/1024 = 0.0009765625.
 *
 *              T(i,j) = (delta + alpha * VAR(i,j))
 *              N      = kw * kh
 *
 *              OutputGradient:
 *                  dZ/dX(i,j) =  ps * T(i,j)^(-beta) * ( dZ/dY(i,j) - (sum_{l,k in L(i),K(j)} dZ/dY(l,k) * (((p0/N) + (2*alpha*beta/N)*(X(k,l)-1)*(X(i,j)-M(i,j)*p0)/T(i,j)))) )
 *              N is the kernel size. The window L(i) and K(j) itself is defined as:
 *                  L(i) = [i-floor((kw-1)/2), i+floor(kw/2]
 *                  K(j) = [j-floor((kh-1)/2), j+floor(kh/2]
 *
 *              For correct gradient computation all parameters must be the same as the original normalization filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNLocalContrastNormalizationGradient : MPSCNNGradientKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 0.0
 *  @discussion The default value 0.0 is not recommended and is
 *              preserved for backwards compatibility. With alpha 0,
 *              it performs a local mean subtraction. The
 *              MPSCNNLocalContrastNormalizationNode used with
 *              the MPSNNGraph uses 1.0 as a default.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 0.5
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1/1024
 */
@property (readwrite, nonatomic) float   delta;

/*! @property   p0
 *  @abstract   The value of p0.  Default is 1.0
 */
@property (readwrite, nonatomic) float   p0;

/*! @property   pm
 *  @abstract   The value of pm.  Default is 0.0
 */
@property (readwrite, nonatomic) float   pm;

/*! @property   ps
 *  @abstract   The value of ps.  Default is 1.0
 */
@property (readwrite, nonatomic) float   ps;

/*!
 *  @abstract  Initialize a local contrast normalization filter
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel
 *  @param      kernelHeight        The height of the kernel
 *  @return     A valid MPSCNNLocalContrastNormalization object or nil, if failure.
 *
 *  NOTE:  For now, kernelWidth must be equal to kernelHeight
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger) kernelWidth
                          kernelHeight: (NSUInteger) kernelHeight NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


@end    /* MPSCNNLocalContrastNormalizationGradient */


#pragma mark - CrossChannelNormalization


/*!
 *  @class MPSCNNCrossChannelNormalization
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the normalization filter across feature channels.
 *               This normalization filter applies the filter to a local region across nearby feature channels,
 *              but with no spatial extent (i.e., they have shape kernelSize x 1 x 1).
 *              The normalized output is given by:
 *                  Y(i,j,k) = X(i,j,k) / L(i,j,k)^beta,
 *              where the normalizing factor is:
 *                  L(i,j,k) = delta + alpha/N * (sum_{q in Q(k)} X(i,j,q)^2, where
 *              N is the kernel size. The window Q(k) itself is defined as:
 *                  Q(k) = [max(0, k-floor(N/2)), min(D-1, k+floor((N-1)/2)], where
 *
 *              k is the feature channel index (running from 0 to D-1) and
 *              D is the number of feature channels, and alpha, beta and delta are paremeters.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSCNNCrossChannelNormalization : MPSCNNKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

/*! @property   kernelSize
 *  @abstract   The size of the square filter window.  Default is 5
 */
@property(readonly, nonatomic) NSUInteger       kernelSize;

/*!
 *  @abstract  Initialize a local response normalization filter in a channel
 *  @param      device              The device the filter will run on
 *  @param      kernelSize          The kernel filter size in each dimension.
 *  @return     A valid MPSCNNCrossChannelNormalization object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            kernelSize: (NSUInteger) kernelSize NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*
 * Use initWithDevice:kernelSize: instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


@end    /* MPSCNNCrossChannelNormalization */


/*!
 *  @class MPSCNNCrossChannelNormalizationGradient
 *  @dependency This depends on Metal.framework
 *  @discussion Specifies the normalization gradient filter across feature channels.
 *               This normalization filter applies the filter to a local region across nearby feature channels,
 *              but with no spatial extent (i.e., they have shape kernelSize x 1 x 1).
 *              The normalized output is given by:
 *                  Y(i,j,k) = X(i,j,k) / L(i,j,k)^beta,
 *              where the normalizing factor is:
 *                  L(i,j,k) = delta + alpha/N * (sum_{q in Q(k)} X(i,j,q)^2, where
 *              N is the kernel size. The window Q(k) itself is defined as:
 *                  Q(k) = [max(0, k-floor(N/2)), min(D-1, k+floor((N-1)/2)], where
 *
 *              k is the feature channel index (running from 0 to D-1) and
 *              D is the number of feature channels, and alpha, beta and delta are paremeters.
 *              It is the end-users responsibility to ensure that the combination of the
 *              parameters delta and alpha does not result in a situation where the denominator
 *              becomes zero - in such situations the resulting pixel-value is undefined.
 *
 *              OutputGradient:
 *                  dZ/dX(i,j,k) = dZ/dY(i,j,k) * (L(i,j,k)^-beta) - 2 * alpha * beta * X(i,j,k) * ( sum_{r in R(k)} dZ/dY(i,j,r) * X(i,j,r) * (L(i,j,r) ^ (-beta-1)) )
 *              N is the kernel size. The window L(i) and K(j) itself is defined as:
 *                  R(k) = [max(0, k-floor((N-1)/2)), min(D-1, k+floor(N/2)]
 *
 *              For correct gradient computation all parameters must be the same as the original normalization filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNCrossChannelNormalizationGradient : MPSCNNGradientKernel

/*! @property   alpha
 *  @abstract   The value of alpha.  Default is 1.0. Must be non-negative.
 */
@property (readwrite, nonatomic) float   alpha;

/*! @property   beta
 *  @abstract   The value of beta.  Default is 5.0
 */
@property (readwrite, nonatomic) float   beta;

/*! @property   delta
 *  @abstract   The value of delta.  Default is 1.0
 */
@property (readwrite, nonatomic) float   delta;

/*! @property   kernelSize
 *  @abstract   The size of the square filter window.  Default is 5
 */
@property(readonly, nonatomic) NSUInteger       kernelSize;

/*!
 *  @abstract  Initialize a cross channel normalization gradient filter
 *  @param      device              The device the filter will run on
 *  @param      kernelSize          The kernel filter size in each dimension.
 *  @return     A valid MPSCNNCrossChannelNormalization object or nil, if failure.
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            kernelSize: (NSUInteger) kernelSize NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSCNNCrossChannelNormalizationGradient */



#ifdef __cplusplus
}
#endif

    
#endif /* MPSCNNNormalization_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNNeuronType.h
//
//  MPSCNNNeuronType.h
//  MPS
//
//  Created by iano on 12/1/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSCNNNeuronType_h
#define MPSCNNNeuronType_h

#if defined(DOXYGEN) || defined(__METAL_VERSION__)
#    define MPS_SWIFT_NAME(_a)
#    define MPS_ENUM_AVAILABLE_STARTING(...)
typedef enum MPSCNNNeuronType 
#else
#    import <MPSNeuralNetwork/MPSNeuralNetworkTypes.h>
typedef NS_ENUM(int32_t, MPSCNNNeuronType)
#endif
{
    MPSCNNNeuronTypeNone            MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(none) = 0, ///< f(x) = x
    MPSCNNNeuronTypeReLU            MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = x >= 0 ? x : a * x;  rectified linear unit
    MPSCNNNeuronTypeLinear          MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = a * x + b
    MPSCNNNeuronTypeSigmoid         MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = 1 / (1 + e^-x)
    MPSCNNNeuronTypeHardSigmoid     MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = clamp((x * a) + b, 0, 1)
    MPSCNNNeuronTypeTanH            MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = a * tanh(b * x)
    MPSCNNNeuronTypeAbsolute        MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = fabs(x)
    MPSCNNNeuronTypeSoftPlus        MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = a * log(1 + e^(b * x))
    MPSCNNNeuronTypeSoftSign        MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = x / (1 + abs(x))
    MPSCNNNeuronTypeELU             MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = x >= 0 ? x : a * (exp(x) - 1); exponential linear unit
    MPSCNNNeuronTypePReLU           MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< Same as ReLU except parameter a is per channel; parameterized rectified linear unit
    MPSCNNNeuronTypeReLUN           MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< f(x) = min((x >= 0 ? x : a * x), b); clamped rectified liniear unit
    MPSCNNNeuronTypePower           MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)), ///< f(x) = (a * x + b) ^ c
    MPSCNNNeuronTypeExponential     MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)), ///< f(x) = c ^ (a * x + b)
    MPSCNNNeuronTypeLogarithm       MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)), ///< f(x) = log_c(a * x + b)
    
    // must be last
    MPSCNNNeuronTypeCount           MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0)), ///< holds the number of MPSCNNNeuronTypes
}
#if defined(DOXYGEN) || defined(__METAL_VERSION__)
    MPSCNNNeuronType
#endif
;

#endif /* MPSCNNNeuronType_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSMatrixFullyConnected.h
//
//  MPSMatrixFullyConnected.h
//  MPS
//
//  Created by Justin Voo on 8/30/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSMatrixFullyConnected_h
#define MPSMatrixFullyConnected_h

#include <MPSNeuralNetwork/MPSCNNNeuronType.h>
#include <MPSMatrix/MPSMatrix.h>

#ifdef __cplusplus
extern "C" {
#endif
    
/*!
 *  @class      MPSMatrixFullyConnected
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   Applies a fully connected neural network layer by performing a
 *              a matrix multiplication, adding a bias vector, scaling, and applying a
 *              neuron activation function.
 *
 *  @discussion A MPSMatrixFullyConnected object computes:
 *
 *                  y = neuron(alpha * x * W + bias)
 *
 *              y is the output matrix, x and W are input matrices corresponding
 *              to a collection of input vectors and weights respectively, and bias
 *              is a vector which is broadcast and accumulated to each row
 *              of the product.  alpha is a scale factor applied to the product.
 *
 *              neuron() is a pointwise function applied to the intermediate result.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixFullyConnected : MPSMatrixBinaryKernel
/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.  This
 *              is equivalent to the number of rows to consider from the primary
 *              source matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available number of inputs is
 *              used.  The value of NSUIntegerMax thus indicates that all available input
 *              rows (beginning at primarySourceMatrixOrigin.x) should be considered.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The input size to to use in the operation.  This is equivalent to the
 *              number of columns and the number of rows in the primary (input array) and
 *              secondary (weight array) source matrices respectively.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available input size is used.
 *              The value of NSUIntegerMax thus indicates that all available
 *              columns in the input array (beginning at primarySourceMatrixOrigin.y) and all
 *              available rows in the weight array (beginning at secondarySourceMatrixOrigin.x)
 *              should be considered.
 *              Note: The value used in the operation will be
 *              MIN(MIN(inputMatrix.columns - primarySourceMatrixOrigin.y,
 *                      weightMatrix.rows - secondarySourceMatrixOrigin.x),
 *                  sourceInputFeatureChannels)
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;

/*! @property   sourceOutputFeatureChannels
 *
 *  @discussion The output size to to use in the operation.  This is equivalent to the
 *              number of columns to consider in the weight array, or the secondary source matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available output size is used.
 *              The value of NSUIntegerMax thus indicates that all available
 *              columns in the weight array (beginning at secondarySourceMatrixOrigin.y)
 *              should be considered.
 */
@property (readwrite, nonatomic) NSUInteger sourceOutputFeatureChannels;

/*! @property   alpha
 *
 *  @discussion The scale factor to apply to the product.  Specified in double
 *              precision.  Will be converted to the appropriate precision in the
 *              implementation subject to rounding and/or clamping as necessary.
 *              Defaults to 1.0 at initialization time.
 */
@property (readwrite, nonatomic) double alpha;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. For those kind of neuron activation functions,
 *              use appropriate setter functions.  An MPSMatrixFullyConnected kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterA;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterB;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterC;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixFullyConnected object to a command buffer.
 *
 *  @param      commandBuffer   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix     A valid MPSMatrix object which specifies the input array.
 *
 *  @param      weightMatrix    A valid MPSMatrix object which specifies the weight array.
 *
 *  @param      biasVector      A valid MPSVector object which specifies the bias values, or
 *                              a null object to indicate that no bias is to be applied.
 *
 *  @param      resultMatrix    A valid MPSMatrix object which specifies the output array.
 *
 *  @discussion Encodes the operation to the specified command buffer.  resultMatrix
 *              must be large enough to hold a
 *                  MIN(sourceNumberOfInputs,
 *                      inputMatrix.rows - primarySourceMatrixOrigin.x)
 *                  x
 *                  MIN(sourceOutputFeatureChannels,
 *                      weightMatrix.columns - secondarySourceMatrixOrigin.y) array.
 *
 *              The bias vector must contain at least
 *                  MIN(sourceOutputFeatureChannels, weightMatrix.columns - secondarySourceMatrixOrigin.y) elements.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                 weightMatrix: (MPSMatrix * __nonnull) weightMatrix
                   biasVector: (MPSVector * __nullable) biasVector
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:weightMatrix:biasVector:resultMatrix:));

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixFullyConnected
 *  @param      device      The MTLDevice on which to make the MPSMatrixFullyConnected object.
 *  @return     A new MPSMatrixFullyConnected object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixFullyConnected

/*!
 *  @class      MPSMatrixFullyConnectedGradient
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   Computes the gradient of the fully connected layer with respect
 *              to either the weights and bias terms or the input feature vectors.
 *
 *  @discussion An MPSMatrixFullyConnectedGradient kernel may be used to compute
 *              the gradients corresponding to a MPSMatrixFullyConnected kernel.
 *              The properties, input, and weight data must match those values
 *              used in the forward computation.
 *              This kernel does not compute the gradient of any non-identity
 *              activation function which may have been applied in the forward
 *              kernel.  Such a kernel must be expressed using both MPSMatrixFullyConnected
 *              and MPSMatrixNeuron if a gradient is to be computed.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixFullyConnectedGradient : MPSMatrixBinaryKernel
/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.
 *              This is equivalent to the number of rows in both the input
 *              matrix and the source gradient matrix.
 *
 *              This value should be equal to the corresponding value in the
 *              forward fully connected kernel.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceOutputFeatureChannels
 *
 *  @discussion The number of feature channels in the output of the forward
 *              fully connected layer.
 *              This is equivalent to the number of columns in both the weight
 *              matrix and the source gradient matrix.
 *
 *              This value should be equal to the corresponding value in the
 *              forward fully connected kernel.
 */
@property (readwrite, nonatomic) NSUInteger sourceOutputFeatureChannels;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The number of feature channels in the input to the forward
 *              fully connected layer.
 *              This is equivalent to the number of columns in the input matrix.
 *
 *              This value should be equal to the corresponding value in the
 *              forward fully connected kernel.
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;

/*! @property   alpha
 *
 *  @discussion Scale factor to apply to the product.  This value should be equal
 *              to the corresponding value in the forward fully connected kernel.
 */
@property (readwrite, nonatomic) double alpha;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixFullyConnectedGradient object to a command buffer and
 *              produce the gradient of the loss function with respect to the input data.
 *
 *  @param      commandBuffer               A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      gradientMatrix              A valid MPSMatrix object which specifies the input gradient.
 *
 *  @param      weightMatrix                A valid MPSMatrix object which specifies the weight array.
 *
 *  @param      resultGradientForDataMatrix A valid MPSMatrix object which specifies the result gradient.
 *
 *  @discussion This operation computes the resulting gradient of the loss function with respect
 *              to the forward kernel's input data.  weightMatrix should contain the same values
 *              used to compute the result of the forward kernel.
 */
-(void) encodeGradientForDataToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                              gradientMatrix: (MPSMatrix const* __nonnull) gradientMatrix
                                weightMatrix: (MPSMatrix const* __nonnull) weightMatrix
                 resultGradientForDataMatrix: (MPSMatrix* __nonnull) resultGradientForDataMatrix;

/*!
 *  @abstract   Encode a MPSMatrixFullyConnectedGradient object to a command buffer and
 *              produce the gradient of the loss function with respect to the weight matrix
 *              and bias vector.
 *
 *  @param      commandBuffer                   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      gradientMatrix                  A valid MPSMatrix object which specifies the input gradient.
 *
 *  @param      inputMatrix                     A valid MPSMatrix object which specifies the input array.
 *
 *  @param      resultGradientForWeightMatrix   A valid MPSMatrix object which specifies the resulting gradients
 *                                              with respect to the weights.
 *
 *  @param      resultGradientForBiasVector     A valid MPSVector object which specifies the resulting gradients
 *                                              with respect to the bias terms.  If NULL these values will not be
 *                                              returned.
 *
 *  @discussion This operation computes the resulting gradient of the loss function with respect
 *              to the forward kernel's weight data.  inputMatrix should contain the same values
 *              used to compute the result of the forward kernel.
 */
-(void) encodeGradientForWeightsAndBiasToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                        gradientMatrix: (MPSMatrix const* __nonnull) gradientMatrix
                                           inputMatrix: (MPSMatrix const* __nonnull) inputMatrix
                         resultGradientForWeightMatrix: (MPSMatrix* __nonnull) resultGradientForWeightMatrix
                           resultGradientForBiasVector: (MPSVector* __nullable) resultGradientForBiasVector;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixFullyConnectedGradient
 *  @param      device      The MTLDevice on which to make the MPSMatrixFullyConnectedGradient object.
 *  @return     A new MPSMatrixFullyConnected object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end    // MPSMatrixFullyConnectedGradient

#ifdef __cplusplus
}
#endif
#endif /* MPSMatrixFullyConnected_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSNeuralNetwork.h
/*!
 *  @header MPSNeuralNetwork.h
 *  @framework MPSNeuralNetwork
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 */

#import <MPSNeuralNetwork/MPSNeuralNetworkTypes.h>
#import <MPSNeuralNetwork/MPSCNNKernel.h>
#import <MPSNeuralNetwork/MPSCNNConvolution.h>
#import <MPSNeuralNetwork/MPSCNNPooling.h>
#import <MPSNeuralNetwork/MPSCNNLoss.h>
#import <MPSNeuralNetwork/MPSCNNMath.h>
#import <MPSNeuralNetwork/MPSCNNNormalization.h>
#import <MPSNeuralNetwork/MPSCNNSoftMax.h>
#import <MPSNeuralNetwork/MPSCNNUpsampling.h>
#import <MPSNeuralNetwork/MPSCNNBatchNormalization.h>
#import <MPSNeuralNetwork/MPSCNNInstanceNormalization.h>
#import <MPSNeuralNetwork/MPSCNNDropout.h>
#import <MPSNeuralNetwork/MPSRNNLayer.h>
#import <MPSNeuralNetwork/MPSMatrixLayer.h>
#import <MPSNeuralNetwork/MPSNNOptimizers.h>
#import <MPSNeuralNetwork/MPSNNReduce.h>
#import <MPSNeuralNetwork/MPSNNReshape.h>
#import <MPSNeuralNetwork/MPSNNResize.h>
#import <MPSNeuralNetwork/MPSNNSlice.h>

#import <MPSNeuralNetwork/MPSNNGraph.h>

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNDropout.h
//
//  MPSCNNDropout.h
//  MPS
//
//  Created by Anna Tikhonova on 9/7/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNDropout_h
#define MPSCNNDropout_h

#import <MPSNeuralNetwork/MPSCNNKernel.h>

#ifdef __cplusplus
extern "C" {
#endif


#pragma mark -
#pragma mark MPSCNNDropoutGradientState

/*!
 *  @class      MPSCNNDropoutGradientState
 *  @dependency This depends on Metal.framework.
 *  @discussion The MPSCNNDropoutGradientState is used to hold the mask used by both
 *              MPSCNNDropout forward filter and MPSCNNDropoutGradient backward filter.
 *              The MPSCNNDropout forward filter populates the MPSCNNDropoutGradientState
 *              object and the MPSCNNDropoutGradient backward filter consumes the state
 *              object.
 *
 *              While the mask is stored internally, the mask data is accessible by the
 *              user for debugging purposes via an accessor method.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDropoutGradientState : MPSNNGradientState

/*
 * Use [MPSCNNDropout resultStateForSourceImage:...] or other variants instead
 */
-(nonnull instancetype) init NS_UNAVAILABLE;

/*!
 *  @abstract   Mask data accessor method.
 *  @return     An autoreleased NSData object, containing the mask data.
 *              The mask data is populated in the -encode call, thus the contents
 *              are undefined until you -encode the filter.
 *              Use for debugging purposes only.
 *
 *              In order to gaurantee that the mask data is correctly synchronized for CPU side access,
 *              it is the application's responsibility to call the [gradientState synchronizeOnCommandBuffer:]
 *              method before accessing the mask data.
 */
-(nonnull NSData*) maskData;

@end

#pragma mark -
#pragma mark MPSCNNDropoutGradientStateBatch

typedef NSArray<MPSCNNDropoutGradientState*> MPSCNNDropoutGradientStateBatch
    MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

#pragma mark -
#pragma mark MPSCNNDropout

/*!
 *  @class      MPSCNNDropout
 *  @dependency This depends on Metal.framework
 *  @discussion Dropout is a regularization technique used to prevent neural networks from
 *              overfitting during training. With probability keepProbability, this filter
 *              outputs the input element scaled by 1 / keepProbability. Otherwise, it
 *              outputs 0. Each input element is kept or dropped independently. The scaling
 *              is performed to keep the energy of the output unchanged.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDropout : MPSCNNKernel

/*! @property   keepProbability
 *  @abstract   The probability that each element in the input is kept.
 *              The valid range is (0.0f, 1.0f).
 */
@property (readonly, nonatomic) float keepProbability;

/*! @property   seed
 *  @abstract   The seed used to generate random numbers.
 */
@property (readonly, nonatomic) NSUInteger seed;

/*! @property   maskStrideInPixels
 *  @abstract   The mask stride in the x, y, and x dimensions, which
 *              allows for the broadcasting the mask data.
 *  @discussion The only valid values are 0 and 1 for each dimension.
 *              For no broadcasting, set the values for each dimension
 *              to 1. For broadcasting, set desired values to 0.
 */
@property (readonly, nonatomic) MTLSize maskStrideInPixels;

/*
 * You must use initWithDevice:keepProbability:seed:maskStrideInPixels: instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract <NSSecureCoding> support */
-(nullable instancetype) initWithCoder: (NSCoder * __nonnull) aDecoder
                                device: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract   Standard init with default properties per filter type.
 *  @param      device              The device that the filter will be used on.
 *  @param      keepProbability     The probability that each element in the input is kept.
 *                                  The valid range is (0.0f, 1.0f).
 *  @param      seed                The seed used to generate random numbers.
 *  @param      maskStrideInPixels  The mask stride in the x, y, and z dimensions, which
 *                                  allows for the broadcasting of mask data. The only valid
 *                                  values are 0 and 1 for each dimension. For no
 *                                  broadcasting, set the values for each dimension to 1.
 *                                  For broadcasting, set desired values to 0.
 *  @result     A valid MPSCNNDropout object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       keepProbability: (float) keepProbability
                                  seed: (NSUInteger) seed
                    maskStrideInPixels: (MTLSize) maskStrideInPixels NS_DESIGNATED_INITIALIZER;

-(MPSCNNDropoutGradientState * __nullable) temporaryResultStateForCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                                                                    sourceImage: (MPSImage *__nonnull) sourceImage
                                                                   sourceStates: (NSArray <MPSState *> *__nullable) sourceStates
                                                               destinationImage: (MPSImage * __nonnull) dest NS_UNAVAILABLE;

-(MPSCNNDropoutGradientStateBatch * __nullable) temporaryResultStateBatchForCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer NS_UNAVAILABLE;

@end /* MPSCNNDroput */


#pragma mark -
#pragma mark MPSCNNDropoutGradient
    
/*!
 *  @class      MPSCNNDropoutGradient
 *  @dependency This depends on Metal.framework
 *  @discussion This filter is the backward filter for the MPSCNNDropout forward filter.
 *              It requires the mask data, along with all the associated parameters used
 *              to generate the mask, from the forward pass. The mask is associated with
 *              a MPSCNNDropoutGradientState object.
 *
 *              In this kernel, use the secondaryOffset to apply an offset to the mask data.
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSCNNDropoutGradient : MPSCNNGradientKernel

/*! @property   keepProbability
 *  @abstract   The probability that each element in the input is kept.
 *              The valid range is (0.0f, 1.0f).
 */
@property (readonly, nonatomic) float keepProbability;

/*! @property   seed
 *  @abstract   The seed used to generate random numbers.
 */
@property (readonly, nonatomic) NSUInteger seed;

/*! @property   maskStrideInPixels
 *  @abstract   The mask stride in the x, y, and x dimensions, which
 *              allows for the broadcasting the mask data.
 *  @discussion The only valid values are 0 and 1 for each dimension.
 *              For no broadcasting, set the values for each dimension
 *              to 1. For broadcasting, set desired values to 0.
 */
@property (readonly, nonatomic) MTLSize maskStrideInPixels;

/*
 * You must use initWithDevice:keepProbability:seed:maskStrideInPixels: instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract <NSSecureCoding> support */
-(nullable instancetype) initWithCoder: (NSCoder * __nonnull) aDecoder
                                device: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract   Standard init with default properties per filter type.
 *  @param      device              The device that the filter will be used on.
 *  @param      keepProbability     The probability that each element in the input is kept.
 *                                  The valid range is (0.0f, 1.0f).
 *  @param      seed                The seed used to generate random numbers.
 *  @param      maskStrideInPixels  The mask stride in the x, y, and z dimensions, which
 *                                  allows for the broadcasting of mask data. The only valid
 *                                  values are 0 and 1 for each dimension. For no
 *                                  broadcasting, set the values for each dimension to 1.
 *                                  For broadcasting, set desired values to 0.
 *  @result     A valid MPSCNNDropoutGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       keepProbability: (float) keepProbability
                                  seed: (NSUInteger) seed
                    maskStrideInPixels: (MTLSize) maskStrideInPixels NS_DESIGNATED_INITIALIZER;


@end /* MPSCNNDroputGradient */


#ifdef __cplusplus
}
#endif

#endif /* MPSCNNDropout_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSCNNTypes.h
//
//  MPSCNNTypes.h
//  MPS
//
//  Created by Anna Tikhonova on 10/20/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSCNNTypes_h
#define MPSCNNTypes_h

#pragma mark -
#pragma mark MPSCNNLossType

/*!
 * Supported loss filter types (see MPSCNNLoss.h for more information):
 */
#if defined(DOXYGEN) || defined(__METAL_VERSION__)
#    define MPS_SWIFT_NAME(_a)
#    define MPS_ENUM_AVAILABLE_STARTING(...)
typedef enum MPSCNNLossType
#else
typedef NS_ENUM(uint32_t, MPSCNNLossType)
#endif
{
    MPSCNNLossTypeMeanAbsoluteError             MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                                                                                  MPS_SWIFT_NAME(meanAbsoluteError) = 0,  // Mean Absolute Error
    MPSCNNLossTypeMeanSquaredError              MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Mean Squared Error
    MPSCNNLossTypeSoftMaxCrossEntropy           MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // SoftMax Cross Entropy
    MPSCNNLossTypeSigmoidCrossEntropy           MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Sigmoid Cross Entropy
    MPSCNNLossTypeCategoricalCrossEntropy       MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Categorical Cross Entropy
    MPSCNNLossTypeHinge                         MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Hinge
    MPSCNNLossTypeHuber                         MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Huber
    MPSCNNLossTypeCosineDistance                MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Cosine Distance
    MPSCNNLossTypeLog                           MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Log
    MPSCNNLossTypeKullbackLeiblerDivergence     MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Kullback-Leibler Divergence
    
    // Must be last
    MPSCNNLossTypeCount // Holds the number of MPSCNNLossTypes
}
#if defined(DOXYGEN) || defined(__METAL_VERSION__)
MPSCNNLossType
#endif
;


#pragma mark -
#pragma mark MPSCNNReductionType

#if defined(DOXYGEN) || defined(__METAL_VERSION__)
#    define MPS_SWIFT_NAME(_a)
#    define MPS_ENUM_AVAILABLE_STARTING(...)
typedef enum MPSCNNReductionType
#else
typedef NS_ENUM(int32_t, MPSCNNReductionType)
#endif
{
    MPSCNNReductionTypeNone                     MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
                                                                                               MPS_SWIFT_NAME(none) = 0,  // No reduction
    MPSCNNReductionTypeSum                      MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Sum
    MPSCNNReductionTypeMean                     MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Mean
    MPSCNNReductionTypeSumByNonZeroWeights      MPS_ENUM_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3)),      // Sum divided by the number of non-zero weights
    
    // Must be last
    MPSCNNReductionTypeCount // Holds the number of MPSCNNReductionTypes
}
#if defined(DOXYGEN) || defined(__METAL_VERSION__)
MPSCNNReductionType
#endif
;

#endif /* MPSCNNTypes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSNeuralNetwork.framework/Headers/MPSMatrixBatchNormalization.h
/*!
 *  @header MPSMatrixBatchNormalization.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *  @abstract Batch normalization of arrays stored as MPSMatrix objects.
 */


#ifndef MPSMatrixBatchNormalization_h
#define MPSMatrixBatchNormalization_h

#include <MPSNeuralNetwork/MPSCNNNeuronType.h>
#include <MPSMatrix/MPSMatrix.h>

#ifdef __cplusplus
extern "C" {
#endif

/*!
 *  @class      MPSMatrixBatchNormalization
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   Applies a batch normalization to a matrix.
 *
 *  @discussion A MPSMatrixBatchNormalization object computes the batch normalization
 *              of a collection of feature vectors stored in an MPSMatrix.
 *
 *              Feature vectors are stored in a row of the supplied input matrix and the
 *              normalization is performed along columns:
 *
 *                  y[i,j] = gamma[j] * (x[i,j] - mean(x[:,j])) / (variance(x[:,j]) + epsilon) + beta[j]
 *
 *              where gamma and beta are supplied weight and bias factors and epsilon is a small value added
 *              to the variance.
 *
 *              Optionally a neuron activation function may be applied to the result.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixBatchNormalization : MPSMatrixUnaryKernel

/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.  This
 *              is equivalent to the number of rows to consider from the primary
 *              source matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available number of inputs is
 *              used.  The value of NSUIntegerMax thus indicates that all available input
 *              rows (beginning at sourceMatrixOrigin.x) should be considered.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The input size to to use in the operation.  This is equivalent to the
 *              number of columns in the primary (input array) source matrix to consider
 *              and the number of channels to produce for the output matrix.
 *              This property is modifiable and defaults to NSUIntegerMax.  At encode
 *              time the larger of this property or the available input size is used.
 *              The value of NSUIntegerMax thus indicates that all available columns in
 *              the input array (beginning at sourceMatrixOrigin.y) should be considered.
 *              Defines also the number of output feature channels.
 *              Note: The value used in the operation will be
 *              MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels)
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;


/*! @property   epsilon
 *
 *  @discussion A small value to add to the variance when normalizing the inputs.  Defaults
 *              to FLT_MIN upon initialization.
 */
@property (readwrite, nonatomic) float epsilon;

/*! @property   computeStatistics
 *
 *  @discussion If YES the batch statistics will be computed prior to performing the normalization.
 *              Otherwise the provided statistics will be used.  Defaults to NO at initialization
 *              time.
 */
@property (readwrite, nonatomic) BOOL computeStatistics;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values.  An MPSMatrixNeuron kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterA;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterB;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterC;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixBatchNormalization object to a command buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix         A valid MPSMatrix object which specifies the input array.
 *
 *  @param      meanVector          A valid MPSVector object containing batch mean values to be used
 *                                  to normalize the inputs if computeStatistics is NO.  If
 *                                  computeStatistics is YES the resulting batch mean values
 *                                  will be returned in this array.
 *
 *  @param      varianceVector      A valid MPSVector object containing batch variance values to be used
 *                                  to normalize the inputs if computeStatistics is NO.  If
 *                                  computeStatistics is YES the resulting batch variance values
 *                                  will be returned in this array.
 *
 *  @param      gammaVector         A valid MPSVector object which specifies the gamma terms, or
 *                                  a null object to indicate that no scaling is to be applied.
 *
 *  @param      betaVector          A valid MPSVector object which specifies the beta terms, or
 *                                  a null object to indicate that no values are to be added.
 *
 *  @param      resultMatrix        A valid MPSMatrix object which specifies the output array.
 *
 *  @discussion Encodes the operation to the specified command buffer.  resultMatrix
 *              must be large enough to hold a
 *                  MIN(sourceNumberOfFeatureVectors, inputMatrix.rows - sourceMatrixOrigin.x)
 *                  x
 *                  MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels) array.
 *
 *              Let numChannels = MIN(inputMatrix.columns - sourceMatrixOrigin.y, sourceInputFeatureChannels)
 *
 *              The gamma, beta, mean, and variance vectors must contain at least numChannels elements.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                   meanVector: (MPSVector * __nonnull) meanVector
               varianceVector: (MPSVector * __nonnull) varianceVector
                  gammaVector: (MPSVector * __nullable) gammaVector
                   betaVector: (MPSVector * __nullable) betaVector
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:meanVector:varianceVector:gammaVector:betaVector:resultMatrix:));

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixBatchNormalization object.
 *  @param      device      The MTLDevice on which to make the MPSMatrixBatchNormalization object.
 *  @return     A new MPSMatrixBatchNormalization object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixBatchNormalization
    

/*!
 *  @class      MPSMatrixBatchNormalizationGradient
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel to compute the gradient of the batch normalization operation.
 *
 *  @discussion A MPSMatrixBatchNormalizationGradient object computes the results of backpropagating
 *              the gradients of a loss function with respect to the outputs of an
 *              MPSMatrixBatchNormalization object.  The corresponding properties and data used by
 *              the MPSMatrixBatchNormalizationGradient object should correspond to those used by
 *              the forward MPSMatrixBatchNormalization object for which the gradient is being computed.
 *
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixBatchNormalizationGradient : MPSMatrixBinaryKernel

/*! @property   sourceNumberOfFeatureVectors
 *
 *  @discussion The number of input vectors which make up the input array.
 */
@property (readwrite, nonatomic) NSUInteger sourceNumberOfFeatureVectors;

/*! @property   sourceInputFeatureChannels
 *
 *  @discussion The number of feature channels in the input vectors.
 */
@property (readwrite, nonatomic) NSUInteger sourceInputFeatureChannels;

/*! @property   epsilon
 *
 *  @discussion A small term added to the variance when normalizing the input.
 */
@property (readwrite, nonatomic) float epsilon;

/*!
 *  @abstract   Specifies a neuron activation function to be used.
 *
 *  @discussion This method can be used to add a neuron activation funtion of given type with
 *              associated scalar parameters A, B, and C that are shared across all output values.
 *              Note that this method can only be used to specify neurons which are specified by three (or fewer)
 *              parameters shared across all output values (or channels, in CNN nomenclature). It is an error to call
 *              this method for neuron activation functions like MPSCNNNeuronTypePReLU,
 *              which require per-channel parameter values. An MPSMatrixBatchNormalizationGradient kernel is initialized
 *              with a default neuron function of MPSCNNNeuronTypeNone.
 *
 *  @param      neuronType      Type of neuron activation function. For full list see MPSCNNNeuronType.h
 *  @param      parameterA      parameterA of neuron activation that is shared across all output values.
 *  @param      parameterB      parameterB of neuron activation that is shared across all output values.
 *  @param      parameterC      parameterC of neuron activation that is shared across all output values.
 */
-(void) setNeuronType: (MPSCNNNeuronType) neuronType
           parameterA: (float) parameterA
           parameterB: (float) parameterB
           parameterC: (float) parameterC;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(MPSCNNNeuronType) neuronType;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterA;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterB;

/*!
 *  @abstract   Getter funtion for neuronType set using setNeuronType:parameterA:parameterB:parameterC method
 */
-(float) neuronParameterC;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixBatchNormalizationGradient object to a command buffer and compute
 *              its gradient with respect to its input data.
 *
 *  @param      commandBuffer                   The commandBuffer on which to encode the operation.
 *
 *  @param      gradientMatrix                  A matrix whose values represent the gradient of a
 *                                              loss function with respect to the results of a forward
 *                                              MPSMatrixBatchNormalization operation.
 *
 *  @param      inputMatrix                     A matrix containing the inputs to a forward MPSMatrixBatchNormalization
 *                                              operation for which the gradient values are to be computed.
 *
 *  @param      meanVector                      A vector containing the batch mean values.  Should contain either the specified
 *                                              values used to compute the forward result, or the computed values resulting from
 *                                              the forward kernel execution.
 *
 *  @param      varianceVector                  A vector containing the batch variance values.  Should contain either the specified
 *                                              values used to compute the forward result, or the computed values resulting from
 *                                              the forward kernel execution.
 *
 *  @param      gammaVector                     A vector containing the gamma terms.  Should be the same values as used
 *                                              when computing the forward result.
 *
 *  @param      betaVector                      A vector containing the beta terms.  Should be the same values as used when
 *                                              computing the forward result.
 *
 *  @param      resultGradientForDataMatrix     The matrix containing the resulting gradient values.
 *
 *  @param      resultGradientForGammaVector    If non-NULL the vector containing gradients for the gamma
 *                                              terms.
 *
 *  @param      resultGradientForBetaVector     If non-NULL the vector containing gradients for the beta
 *                                              terms.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               gradientMatrix: (MPSMatrix * __nonnull) gradientMatrix
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                   meanVector: (MPSVector * __nonnull) meanVector
               varianceVector: (MPSVector * __nonnull) varianceVector
                  gammaVector: (MPSVector * __nullable) gammaVector
                   betaVector: (MPSVector * __nullable) betaVector
  resultGradientForDataMatrix: (MPSMatrix * __nonnull) resultGradientForDataMatrix
 resultGradientForGammaVector: (MPSVector * __nullable) resultGradientForGammaVector
  resultGradientForBetaVector: (MPSVector * __nullable) resultGradientForBetaVector;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixBatchNormalizationGradient
 *  @param      device      The MTLDevice on which to make the MPSMatrixBatchNormalizationGradient object.
 *  @return     A new MPSMatrixBatchNormalizationGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     A pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixBatchNormalizationGradient

    
#ifdef __cplusplus
}
#endif
#endif /* MPSMatrixBatchNormalization_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSCore.h
/*!
 *  @header MPSCore.h
 *  @framework MPSCore
 *  @description  This header is the public entrypoint into the MPSCore subframework
 *                of MetalPerformanceShaders. It defines a few shared types used by
 *                the other MPS subframeworks, and some core infrastructure.
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 */

#ifndef MPSCore_h
#define MPSCore_h

#import <MPSCore/MPSCoreTypes.h>
#import <MPSCore/MPSImage.h>
#import <MPSCore/MPSKernel.h>
#import <MPSCore/MPSState.h>
#import <MPSCore/MPSKernelTypes.h>
#import <MPSCore/MPSKeyedUnarchiver.h>

#endif /* MPSCore_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSKernel.h
/*!
 *  @header MPSKernel.h
 *  @framework MPSCore.framework
 *  @copyright Copyright (c) 2015-2017 Apple Inc. All rights reserved.
 *
 *  @discussion  MPSKernel objects encode tuned image processing operations into a MTLCommandBuffer.
 */


#ifndef MPSKernel_h
#define MPSKernel_h

#include <MPSCore/MPSCoreTypes.h>

#ifdef __cplusplus
extern "C" {
#endif



/*!
 *  @class      MPSKernel
 *  @dependency This depends on Metal.framework
 *  @discussion The MPSKernel class is the base class for all MPS objects.  It defines a standard interface for
 *              MPS kernels.   You should not use the MPSKernel class directly. Instead, a  number of MPSKernel 
 *              subclasses are available in MetalPerformanceShaders.framework that define specific high-performance
 *              image processing operations.
 *
 *              The basic sequence for applying a MPSKernel to an image is as follows:
 *
 *              1.  Create a MPSKernel corresponding to the operation you wish to perform:
 *                  @code
 *                  MPSImageSobel *sobel = [[MPSImageSobel alloc] initWithDevice: mtlDevice];
 *                  @endcode
 *
 *              2.  Encode the filter into a command buffer:
 *                  @code
 *                  sobel.offset = ...;
 *                  sobel.clipRect = ...;
 *                  sobel.options = ...;
 *                  [sobel encodeToCommandBuffer: commandBuffer
 *                                 sourceTexture: inputImage
 *                            destinationTexture: resultImage ];
 *                  @endcode
 *                  Encoding the kernel merely encodes the operation into a MTLCommandBuffer. It does not modify any pixels, yet.
 *                  All MPSKernel state has been copied to the command buffer. MPSKernels may be reused.  If the texture was previously
 *                  operated on by another command encoder (e.g. MTLRenderCommandEncoder), you should call -endEncoding on the other
 *                  encoder before encoding the filter.
 *
 *                  Some MPS filters work in place (inputImage = resultImage) even in situations where Metal might not
 *                  normally allow in place operation on textures. If in-place operation is desired, you may attempt to call
 *                  [MPSKernel encodeKernelInPlace...]. If the operation can not be completed in place, then
 *                  NO will be returned and you will have to create a new result texture and try again. To make an in-place
 *                  image filter reliable, pass a fallback MPSCopyAllocator to the method to create a new texture to write
 *                  to in the event that a filter can not operate in place.
 *
 *                  (Repeat steps 2 for more filters, as desired.)
 *
 *                      It should be self evident that step 2 may not be thread safe. That is, you can not have
 *                      multiple threads manipulating the same properties on the same MPSKernel object at the
 *                      same time and achieve coherent output. In common usage, the MPSKernel properties don't
 *                      often need to be changed from their default values, but if you need to apply the same
 *                      filter to multiple images on multiple threads with cropping / tiling, make additional
 *                      MPSKernel objects per thread. They are cheap. You can use multiple MPSKernel objects on
 *                      multiple threads, as long as only one thread is operating on any particular MPSKernel
 *                      object at a time.
 *
 *              3.  After encoding any additional work to the command buffer using other encoders, submit the MTLCommandBuffer
 *                  to your MTLCommandQueue, using:
 *                  @code
 *                  [mtlCommandBuffer commit];
 *                  @endcode
 *
 *              A MPSKernel can be saved to disk / network using NSCoders such as NSKeyedArchiver.
 *              When decoding, the system default MTLDevice will be chosen unless the NSCoder adopts
 *              the <MPSDeviceProvider> protocol.  To accomplish this, subclass or extend your unarchiver 
 *              to add this method.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface MPSKernel  : NSObject <NSCopying, NSSecureCoding>

/****************
 *  Properties  *
 ****************/

/*! @property   options
 *  @abstract   The set of options used to run the kernel.
 *  @ref        subsubsection_options
 */
@property (readwrite, nonatomic) MPSKernelOptions                   options;

/*! @property device
 *  @abstract  The device on which the kernel will be used
 */
@property (readonly, retain, nonatomic, nonnull)  id <MTLDevice>    device;

/*!
 @property label
 @abstract A string to help identify this object.
 */
@property (copy, atomic, nullable)  NSString *                      label;

/*********************
 *  Object creation  *
 *********************/


/*!
 *  @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     a pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be 
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        NS_DESIGNATED_INITIALIZER;


/*!
 *  @abstract   Make a copy of this MPSKernel for a new device
 *  @discussion -copyWithZone: will call this API to make a copy of the
 *              MPSKernel on the same device.  This interface may also be
 *              called directly to make a copy of the MPSKernel on a new
 *              device. Typically, the same MPSKernels should not be used
 *              to encode kernels on multiple command buffers from multiple
 *              threads. Many MPSKernels have mutable properties that might 
 *              be changed by the other thread while this one is trying to 
 *              encode. If you need to use a MPSKernel from multiple threads
 *              make a copy of it for each additional thread using -copyWithZone:
 *              or -copyWithZone:device:
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

/*! @abstract   Called by NSCoder to decode MPSKernels
 *  @discussion This isn't the right interface to decode a MPSKernel, but
 *              it is the one that NSCoder uses. To enable your NSCoder
 *              (e.g. NSKeyedUnarchiver) to set which device to use
 *              extend the object to adopt the MPSDeviceProvider 
 *              protocol. Otherwise, the Metal system default device
 *              will be used. */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't 
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid 
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


@end


#ifdef __cplusplus
    }       /* extern "C" */
#endif

#endif  /* MPSKernel_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSKeyedUnarchiver.h
//
//  MPSKeyedUnarchiver.h
//  MPSCore
//
//  Created by Ian Ollmann on 1/23/18.
//  Copyright © 2018 Apple. All rights reserved.
//

#ifndef MPSKeyedUnarchiver_h
#define MPSKeyedUnarchiver_h
#ifdef __cplusplus
extern "C" {
#endif
    
#import <MPSCore/MPSCoreTypes.h>

/*! @class MPSKeyedUnarchiver
 *  @abstract A NSKeyedArchiver that supports the MPSDeviceProvider protocol for MPSKernel decoding */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSKeyedUnarchiver : NSKeyedUnarchiver <MPSDeviceProvider>

/* Common NSKeyedUnarchiver methods */
+ (nullable id)unarchivedObjectOfClasses:(NSSet<Class> * __nonnull)classes
                                fromData:(NSData * __nonnull)data
                                  device: (__nonnull id <MTLDevice>) device
                                   error:(NSError * __nullable * __nullable)error
        MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

+ (nullable id)unarchivedObjectOfClass:(__nonnull Class)cls
                              fromData:(NSData * __nonnull)data
                                device: (__nonnull id <MTLDevice>) device
                                 error:(NSError * __nullable * __nullable)error
        MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

- (nonnull instancetype)initForReadingFromData: (NSData * __nonnull)data
                                        device: (__nonnull id <MTLDevice>) device
                                         error: (NSError * __nullable * __nullable)error
        MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract   Reports which device to use for unarchiving MPSKernels */
-(__nonnull id <MTLDevice>) mpsMTLDevice;

/* Unavailable API from superclass. */
+ (nullable id)unarchivedObjectOfClasses:(NSSet<Class> * __nonnull)classes
                                fromData:(NSData * __nonnull)data
                                   error:(NSError * __nullable * __nullable)error NS_UNAVAILABLE;
+ (nullable id)unarchivedObjectOfClass:(__nonnull Class)cls
                              fromData:(NSData * __nonnull)data
                                 error:(NSError * __nullable * __nullable)error NS_UNAVAILABLE;
- (nonnull instancetype)init NS_UNAVAILABLE;
- (nonnull instancetype)initForReadingFromData:(NSData * __nonnull)data
                                         error:(NSError * __nullable * __nullable)error NS_UNAVAILABLE;


/* Deprecated API. */
+ (nullable id)unarchiveObjectWithData: (NSData * __nonnull)data NS_UNAVAILABLE;
+ (nullable id)unarchiveObjectWithData: (NSData *__nonnull)data
                                device: (__nonnull id <MTLDevice>) device
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -unarchivedObjectOfClass:fromData:device:error: instead", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3,12.0));
+ (nullable id)unarchiveTopLevelObjectWithData:(NSData *__nonnull)data
                                         error:(NSError * __nullable * __nullable)error NS_UNAVAILABLE;
+ (nullable id)unarchiveTopLevelObjectWithData:(NSData *__nonnull)data
                                        device: (__nonnull id <MTLDevice>) device
                                         error:(NSError * __nullable * __nullable)error
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -unarchivedObjectOfClass:fromData:device:error: instead", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3,12.0));

+ (nullable id)unarchiveObjectWithFile:(NSString * __nonnull)path NS_UNAVAILABLE;
- (nonnull instancetype)initForReadingWithData:(NSData * __nonnull)data NS_UNAVAILABLE;

+ (nullable id)unarchiveObjectWithFile:(NSString * __nonnull)path
                                device: (__nonnull id <MTLDevice>) device
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -initForReadingFromData:device:error: instead", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3,12.0));

- (nullable instancetype)initWithDevice: (__nonnull id <MTLDevice>) device
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -initForReadingFromData:device:error: instead", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3,12.0));

- (nonnull instancetype)initForReadingWithData:(NSData * __nonnull)data
                                        device: (__nonnull id <MTLDevice>) device
        MPS_AVAILABLE_STARTING_BUT_DEPRECATED("Please use -initForReadingFromData:device:error: instead", macos(10.13.4, 10.14), ios(11.3, 12.0), tvos(11.3,12.0));

@end
    
    
#ifdef __cplusplus
}
#endif


#endif /* MPSKeyedArchiver_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSCoreTypes.h
/*!
 *  @header     MPSTypes.h
 *  @framework  MPSCore
 *  @copyright  Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @discussion Types common to MetalPerformanceShaders.framework
 */

#ifndef MPSCoreTypes_h
#define MPSCoreTypes_h

#import <Foundation/NSObject.h>
#import <Metal/Metal.h>

#ifdef __cplusplus
extern "C" {
#endif
    
/*
 *  Identify compiler capabilities
 */
#ifndef __has_attribute          /* clang will define this. Other compilers maybe not. */
#    define __has_attribute(a)   0
#endif
#ifndef __has_feature           /* clang will define this. Other compilers maybe not. */
#    define __has_feature(f)     0
#endif
#ifndef __has_extension         /* clang will define this. Other compilers maybe not. */
#    define __has_extension(e)   0
#endif
    
#if defined(DOXYGEN)
#   define  MPS_HIDE_AVAILABILITY 1
#endif
    
/*
 *  Macros to describe MPS functionality availability by operating system revision.
 */
#ifdef MPS_HIDE_AVAILABILITY
#    define MPS_ENUM_AVAILABLE_STARTING(...)
#    define MPS_ENUM_AVAILABLE_STARTING_BUT_DEPRECATED(...)
#    define MPS_CLASS_AVAILABLE_STARTING(...)
#    define MPS_AVAILABLE_STARTING(...)
#    define MPS_AVAILABLE_STARTING_BUT_DEPRECATED(...)
#else
#    define MPS_ENUM_AVAILABLE_STARTING(...)                    __API_AVAILABLE(__VA_ARGS__)
#    define MPS_ENUM_AVAILABLE_STARTING_BUT_DEPRECATED(...)     __API_DEPRECATED_WITH_REPLACEMENT(__VA_ARGS__)
#    define MPS_CLASS_AVAILABLE_STARTING(...)                   __API_AVAILABLE(__VA_ARGS__)
#    define MPS_AVAILABLE_STARTING(...)                         __API_AVAILABLE(__VA_ARGS__)
#    define MPS_AVAILABLE_STARTING_BUT_DEPRECATED(...)          __API_DEPRECATED_WITH_REPLACEMENT(__VA_ARGS__)
#endif
    
/*
 *  Some MPS interfaces have a slightly different name in Swift to 
 *  enhance readability. MPS_SWIFT_NAME is used to rename the interface
 *  for Swift.
 */
#if DOXYGEN
#   define  MPS_SWIFT_NAME(...)
#elif __has_feature(objc_class_property)
#   define  MPS_SWIFT_NAME(_name)    CF_SWIFT_NAME(_name)
#else
#   define  MPS_SWIFT_NAME(_name)
#endif
    
/*! @enum       MPSKernelOptions
 *  @memberof   MPSKernel
 *  @abstract   Options used when creating MPSKernel objects
 */
#if defined(DOXYGEN)
    typedef enum MPSKernelOptions
#else
    typedef NS_OPTIONS(NSUInteger, MPSKernelOptions)
#endif
{
    /*! Use default options */
    MPSKernelOptionsNone                         MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0)) MPS_SWIFT_NAME(none) = 0U,
    
    /*! Most MPS functions will sanity check their arguments. This has a small but
     *  non-zero CPU cost. Setting the MPSKernelOptionsSkipAPIValidation will skip these checks.
     *  MPSKernelOptionsSkipAPIValidation does not skip checks for memory allocation failure.
     *  Caution:  turning on MPSKernelOptionsSkipAPIValidation can result in undefined behavior
     *  if the requested operation can not be completed for some reason. Most error states
     *  will be passed through to Metal which may do nothing or abort the program if Metal
     *  API validation is turned on. */
    MPSKernelOptionsSkipAPIValidation            MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))  = 1U << 0,
    
    /*! When possible, MPSKernels use a higher precision data representation internally than
     *  the destination storage format to avoid excessive accumulation of computational
     *  rounding error in the result. MPSKernelOptionsAllowReducedPrecision advises the
     *  MPSKernel that the destination storage format already has too much precision for
     *  what is ultimately required downstream, and the MPSKernel may use reduced precision
     *  internally when it feels that a less precise result would yield better performance.
     *  The expected performance win is often small, perhaps 0-20%. When enabled, the
     *  precision of the result may vary by hardware and operating system. */
    MPSKernelOptionsAllowReducedPrecision        MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))  = 1U << 1,
    
    /*! Some MPSKernels may automatically split up the work internally into multiple tiles.
     *  This improves performance on larger textures and reduces the amount of memory needed by
     *  MPS for temporary storage. However, if you are using your own tiling scheme to achieve
     *  similar results, your tile sizes and MPS's choice of tile sizes may interfere with
     *  one another causing MPS to subdivide your tiles for its own use inefficiently. Pass
     *  MPSKernelOptionsDisableInternalTiling to force MPS to process your data tile as a
     *  single chunk.   */
    MPSKernelOptionsDisableInternalTiling        MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = 1U << 2,
    
    /*! Enabling this bit will cause various -encode... methods to call MTLCommandEncoder
     *  push/popDebugGroup.  The debug string will be drawn from MPSKernel.label, if any
     *  or the name of the class otherwise. */
    MPSKernelOptionsInsertDebugGroups            MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = 1U << 3,

    /*! Some parts of MPS can provide debug commentary and tuning advice when run.
     *  Setting this bit to 1 will cause the commentary to be emitted to stderr. Otherwise,
     *  the code is silent.  This is especially useful for debugging MPSNNGraph. This option
     *  is on by default when the MPS_LOG_INFO environment variable is defined.  For
     *  even more detailed output on a MPS object, you can use the po command in llvm
     *  with MPS objects:
     *  @code
     *    llvm>  po  <MPS object pointer>
     *  @endcode */
    MPSKernelOptionsVerbose                      MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) = 1U << 4,
    
};
    
/*! @enum       MPSImageEdgeMode
 *  @memberof   MPSKernel
 *  @abstract   Options used to control edge behaviour of filter when filter reads beyond boundary of src image
 */
#if defined(DOXYGEN)
typedef enum MPSImageEdgeMode
#else
typedef NS_ENUM(NSUInteger, MPSImageEdgeMode)
#endif
{
    /*! Out of bound pixels are (0,0,0,1) for image with pixel format without alpha channel
     *  and (0,0,0,0) for image with pixel format that has an alpha channel */
    MPSImageEdgeModeZero                MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(9.0), tvos(9.0)) MPS_SWIFT_NAME(zero)  = 0,
    
    /*! Out of bound pixels are clamped to nearest edge pixel */
    MPSImageEdgeModeClamp               MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(9.0), tvos(9.0))  = 1,
}
#if defined(DOXYGEN)
    MPSImageEdgeMode
#endif
;

    
/*! @enum       MPSImageFeatureChannelFormat
 *  @memberof   MPSImage
 *  @abstract   Encodes the representation of a single channel within a MPSImage.
 *  @discussion A MPSImage pixel may have many channels in it, sometimes many more than 4, the
 *              limit of what MTLPixelFormats encode. The storage format for a single channel
 *              within a pixel can be given by the MPSImageFeatureChannelFormat. The number
 *              of channels is given by the featureChannels parameter of appropriate MPSImage
 *              APIs. The size of the pixel is size of the channel format multiplied by the
 *              number of feature channels. No padding is allowed, except to round out to a full
 *              byte.
 */
#if defined(DOXYGEN)
typedef enum MPSImageFeatureChannelFormat
#else
typedef NS_ENUM(NSUInteger, MPSImageFeatureChannelFormat)
#endif
{
    /*! No format. This can mean  according to context invalid format or any format.  In the
        latter case, it is an invitation to MPS to pick a format. */
    MPSImageFeatureChannelFormatNone        MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0)) MPS_SWIFT_NAME(none)  = 0,
    
    /*! uint8_t with value [0,255] encoding [0,1.0] */
    MPSImageFeatureChannelFormatUnorm8      MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))   = 1,
    
    /*! uint16_t with value [0,65535] encoding [0,1.0] */
    MPSImageFeatureChannelFormatUnorm16     MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))  = 2,
    
    /*! IEEE-754 16-bit floating-point value. "half precision" Representable normal range is +-[2**-14, 65504], 0, Infinity, NaN. 11 bits of precision + exponent. */
    MPSImageFeatureChannelFormatFloat16     MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))  = 3,
    
    /*! IEEE-754 32-bit floating-point value.  "single precision" (standard float type in C) 24 bits of precision + exponent */
    MPSImageFeatureChannelFormatFloat32     MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))  = 4,
    
    
    /* Always last */
    MPSImageFeatureChannelFormatCount        MPS_ENUM_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
}
#if defined(DOXYGEN)
    MPSImageFeatureChannelFormat
#endif
;

    
/*! @enum        MPSDataType
 *  @discussion A value to specify a type of data.
 *
 *  @constant   MPSDataTypeFloatBit         A common bit for all floating point data types.  Zero for integer types
 *  @constant   MPSDataTypeNormalizedBit    If set, the value of the shall be interpreted as value / UNORM_TYPE_MAX
 *                                          Normalized values have range [0, 1.0] if unsigned and [-1,1] if signed.
 *                                          SNORM_TYPE_MIN is interpreted as SNORM_TYPE_MIN+1 per standard Metal rules.
 *
 *  @constant   MSPDataTypeFloat32      32-bit floating point (single-precision).
 *  @constant   MSPDataTypeFloat16      16-bit floating point (half-precision).  (IEEE-754-2008 float16 exchange format)
 *  @constant   MPSDataTypeInt8         Signed 8-bit integer.
 *  @constant   MPSDataTypeInt16        Signed 16-bit integer.
 *  @constant   MPSDataTypeUInt8        Unsigned 8-bit integer. Not normalized
 *  @constant   MPSDataTypeUInt16       Unsigned 16-bit integer. Not normalized
 *  @constant   MPSDataTypeUInt32       Unsigned 32-bit integer. Not normalized
 *  @constant   MPSDataTypeUnorm1       Unsigned 1-bit normalized value.
 *  @constant   MPSDataTypeUnorm8       Unsigned 8-bit normalized value.
 */
#if defined(DOXYGEN)
    typedef enum MPSDataType
#else
    typedef NS_ENUM(uint32_t, MPSDataType)
#endif
{
    MPSDataTypeInvalid MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) MPS_SWIFT_NAME(invalid) = 0,
    
    MPSDataTypeFloatBit MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = 0x10000000,
    MPSDataTypeFloat32  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = MPSDataTypeFloatBit | 32,
    MPSDataTypeFloat16  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) = MPSDataTypeFloatBit | 16,
    
    // signed integers
    MPSDataTypeSignedBit MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = 0x20000000,
    MPSDataTypeIntBit DEPRECATED_ATTRIBUTE = MPSDataTypeSignedBit,
    MPSDataTypeInt8   MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))   = MPSDataTypeSignedBit | 8,
    MPSDataTypeInt16  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))   = MPSDataTypeSignedBit | 16,

    // unsigned integers. Range: [0, UTYPE_MAX]
    MPSDataTypeUInt8   MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))   = 8,
    MPSDataTypeUInt16  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))   = 16,
    MPSDataTypeUInt32  MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))   = 32,

    // unsigned normalized  (see for example Metal's unorm8 and unorm16 pixel formats). Range: [0, 1.0]
    MPSDataTypeNormalizedBit MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))   = 0x40000000,
    MPSDataTypeUnorm1   MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) = MPSDataTypeNormalizedBit | 1,
    MPSDataTypeUnorm8   MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) = MPSDataTypeNormalizedBit | 8,
}
#if defined(DOXYGEN)
    MPSDataType
#endif
    ;


/*!
 *  @struct     MPSOffset
 *  @memberof   MPSKernel
 *  @abstract   A signed coordinate with x, y and z components
 */
typedef struct
{
    NSInteger x;    /**<    The horizontal component of the offset. Units: pixels   */
    NSInteger y;    /**<    The vertical component of the offset. Units: pixels     */
    NSInteger z;    /**<    The depth component of the offset. Units: pixels        */
}MPSOffset;

/*!
 *  @struct     MPSOrigin
 *  @memberof   MPSKernel
 *  @abstract   A position in an image
 */
typedef struct MPSOrigin
{
    double  x;  /**< The x coordinate of the position       */
    double  y;  /**< The y coordinate of the position       */
    double  z;  /**< The z coordinate of the position       */
}MPSOrigin;

/*!
 *  @struct     MPSSize
 *  @memberof   MPSKernel
 *  @abstract   A size of a region in an image
 */
typedef struct MPSSize
{
    double  width;      /**< The width of the region    */
    double  height;     /**< The height of the region   */
    double  depth;      /**< The depth of the region    */
}MPSSize;

/*!
 *  @struct     MPSRegion
 *  @memberof   MPSKernel
 *  @abstract   A region of an image
 */
typedef struct MPSRegion
{
    MPSOrigin       origin;     /**< The top left corner of the region.  Units: pixels  */
    MPSSize         size;       /**< The size of the region. Units: pixels              */
}MPSRegion;
    
/*!
 *  @struct         MPSScaleTransform
 *  @abstract       Transform matrix for explict control over resampling in MPSImageLanczosScale.
 *  @discussion     The MPSScaleTransform is equivalent to:
 *       @code
 *          (CGAffineTransform) {
 *               .a = scaleX,        .b = 0,
 *               .c = 0,             .d = scaleY,
 *               .tx = translateX,   .ty = translateY
 *           }
 *       @endcode
 *
 *  @memberof       MPSImageLanczosScale
 */
typedef struct MPSScaleTransform
{
    double  scaleX;                         /**< horizontal scaling factor */
    double  scaleY;                         /**< vertical scaling factor */
    double  translateX;                     /**< horizontal translation */
    double  translateY;                     /**< vertical translation */
}MPSScaleTransform;


    
/*!
 *  @memberof   MPSKernel
 *  @constant   MPSRectNoClip
 *  @discussion This is a special constant to indicate no clipping is to be done.
 *              The entire image will be used.
 *              This is the default clipping rectangle or the input extent for MPSKernels.
 */
extern const MTLRegion  MPSRectNoClip
    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));
    
/*! @abstract   A way of extending a NSCoder to enable the setting of MTLDevice for unarchived objects
 *  @discussion When a object is initialized by a NSCoder, it calls -initWithCoder:, which is 
 *              missing the necessary MTLDevice to correctly initialize the MPSKernel, or MPSNNGraph.
 *              If the coder does not conform to MPSDeviceProvider, the system default device
 *              will be used.  If you would like to specify which device to use, subclass the 
 *              NSCoder (NSKeyedUnarchiver, etc.) to conform to MPSDeviceProvider so that 
 *              the device can be gotten from the NSCoder.
 *
 *              See MPSKeyedUnarchiver for one implementation of this protocol. It reads files
 *              prepared with the NSKeyedArchiver and allows you to set the MTLDevice that the
 *              unarchived objects use.
 */
@protocol   MPSDeviceProvider
    /*! @abstract   Return the device to use when making MPSKernel subclasses from the NSCoder */
    -(id <MTLDevice>) mpsMTLDevice;
@end

#ifdef __cplusplus
}
#endif


#endif /* MPSCoreTypes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSState.h
//
//  MPSState.h
//  MPS
//
//  Created by Ian Ollmann on 10/17/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSState_h
#define MPSState_h

#ifdef __cplusplus
extern "C" {
#endif
    
#import <MPSCore/MPSCoreTypes.h>
 
@class MPSImage;
@class MPSKernel;
@class MPSImageDescriptor;
    
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSStateResourceList : NSObject
/*! @abstract Init an empty autoreleased resource list */
+(nonnull instancetype) resourceList;

/*! @abstract Init a resource list with a nil terminated list of MTLTextureDescriptors */
+(nonnull instancetype) resourceListWithTextureDescriptors: (MTLTextureDescriptor *__nonnull) d, ...
                            NS_REQUIRES_NIL_TERMINATION;

/*! @abstract Init a resource list with a 0 terminated list of Buffer Sizes */
+(nonnull instancetype) resourceListWithBufferSizes: (NSUInteger) firstSize, ...
                            NS_REQUIRES_NIL_TERMINATION;

/*! @abstract Init an empty list */
-(nonnull instancetype) init;

/*! @abstract append a texture to the resource list */
-(void) appendTexture: (MTLTextureDescriptor * __nonnull) descriptor;

/*! @abstract append a buffer to the resource list */
-(void) appendBuffer: (NSUInteger) size;

@end
    
typedef struct MPSStateTextureInfo
{
    NSUInteger          width;          // MTLTexture.width
    NSUInteger          height;         // MTLTexture.height
    NSUInteger          depth;          // MTLTexture.depth
    NSUInteger          arrayLength;    // MTLTexture.arrayLength
    MTLPixelFormat      pixelFormat;    // MTLTexture.pixelFormat
    MTLTextureType      textureType;    // MTLTexture.textureType
    MTLTextureUsage     usage;          // MTLTexture.usage
    
    NSUInteger          _reserved[4];
}MPSStateTextureInfo;
    
#if defined(DOXYGEN)
    typedef enum MPSStateResourceType
#else
    typedef NS_ENUM(NSUInteger, MPSStateResourceType)
#endif
    {
        MPSStateResourceTypeNone            MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3)) MPS_SWIFT_NAME(none)  = 0,
        MPSStateResourceTypeBuffer          MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))  = 1,
        MPSStateResourceTypeTexture         MPS_ENUM_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))  = 2,
    }
#if defined(DOXYGEN)
    MPSStateResourceType
#endif
;


/*!
 *  @class          MPSState
 *  @dependency     This depends on Metal Framework
 *  @abstract       A semi-opaque data container for large storage in MPS CNN filters
 *  @discussion     Some MPS CNN kernels produce additional information beyond a
 *                  MPSImage. These may be pooling indices where the result came from,
 *                  convolution weights, or other information not contained in the
 *                  usual MPSImage result from a MPSCNNKernel. A MPSState object 
 *                  typically contains one or more expensive MTLResources such as 
 *                  textures or buffers to store this information.  It provides a 
 *                  base class with interfaces for managing this storage. Child 
 *                  classes may add additional functionality specific to their
 *                  contents.
 *
 *                  Some MPSState objects are temporary. Temporary state objects, 
 *                  like MPSTemporaryImages and Matrices, are for very short lived storage,
 *                  perhaps just a few lines of code within the scope of a single
 *                  MTLCommandBuffer.  They are very efficient for storage, as several
 *                  temporary objects can share the same memory over the course of a 
 *                  MTLCommandBuffer. This can improve both memory usage and time spent
 *                  in the kernel wiring down memory and such. You may find that some 
 *                  large CNN tasks can not be computed without them, as non-temporary
 *                  storage would simply take up too much memory.
 *
 *                  In exchange, the lifetime of the underlying storage in temporary 
 *                  MPSState objects needs to be carefully managed. ARC often waits 
 *                  until the end of scope to release objects. Temporary storage often 
 *                  needs to be released sooner than that. Consequently the lifetime of 
 *                  the data in the underlying MTLResources is managed by a readCount 
 *                  property. Each time a MPSCNNKernel reads a temporary MPSState object
 *                  the readCount is automatically decremented. When it reaches zero, the
 *                  underlying storage is recycled for use by other MPS temporary objects,
 *                  and the data is becomes undefined.  If you need to consume the data
 *                  multiple times, you should set the readCount to a larger number to 
 *                  prevent the data from becomming undefined.  You may set the readCount
 *                  to 0 yourself to return the storage to MPS, if for any reason, you 
 *                  realize that the MPSState object will no longer be used. 
 *
 *                  The contents of a temporary MPSState object are only valid from 
 *                  creation to the time the readCount reaches 0. The data is only valid 
 *                  for the MTLCommandBuffer on which it was created.  Non-temporary
 *                  MPSState objects are valid on any MTLCommandBuffer on the same
 *                  device until they are released.
 *
 *
 *                  Finally, temporary MPSState objects are complicated to use with blit encoders.
 *                  Your application should assume that the temporary MPSState is backed by a MTLHeap,
 *                  and consequently needs a MTLFence to ensure that compute command encoders and other
 *                  encoders do not trip over one another with heap based memory. MPS will almost never
 *                  use a blit encoder for this reason. If you do need one, then you will need to make
 *                  a new compute encoder to block on whatever previous compute encoder last used the
 *                  heap block. (MPS will not tell you who previously used the heap block. That encoder
 *                  is almost certainly long dead anyway.) If concurrent encoders are involved, then a
 *                  barrier might be needed. Within that compute encoder, you will call -updateFence.
 *                  End the compute encoder, make a blit encoder wait for the fence, do the blit, update
 *                  a new fence, then make a new compute encoder, wait for the second fence, then you
 *                  can continue. Possibly the second do-nothing compute encoder needs to be ended so
 *                  MPS can be called. Frankly, we don't bother with blit encoders and just write a compute
 *                  operation for copy / clear as needed, or better yet find a way to eliminate the
 *                  clear / copy pass so we don't have to pay for it. Your application needs to use
 *                  temporary MPSStates and MPSTemporaryImages. Memory costs skyrocket, otherwise.
 *                  It is the blit encoder that is hopefully optional. Note: the most common use of a
 *                  blit encoder, -synchronizeResource: can not encounter this problem because temporary
 *                  images and states live in GPU private memory and can not be read by the CPU.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSState : NSObject

/******************************
 *  States with one resource  *
 ******************************/

/*! @abstract Create a MPSState holding a temporary MTLBuffer
 *  @param   cmdBuf     The command buffer against which the temporary resource is allocated
 *  @param   bufferSize The size of the buffer in bytes
 */
+(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                             bufferSize: (size_t) bufferSize;

/*! @abstract Create a MPSState holding a temporary MTLTexture
 *  @param   cmdBuf     The command buffer against which the temporary resource is allocated
 *  @param   descriptor A descriptor for the new temporary texture
 */
+(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                      textureDescriptor: (MTLTextureDescriptor * __nonnull) descriptor;

/*! @abstract Create a new autoreleased temporary state object without underlying resource
    @param cmdBuf  The command buffer with which the temporary resource is associated */
+(nonnull instancetype) temporaryStateWithCommandBuffer:(__nonnull id<MTLCommandBuffer>)cmdBuf;

-(nonnull instancetype) initWithDevice: (__nonnull id <MTLDevice>) device
                            bufferSize: (size_t) bufferSize;
-(nonnull instancetype) initWithDevice: (__nonnull id <MTLDevice>) device
                     textureDescriptor: (MTLTextureDescriptor * __nonnull) descriptor;

/*! @abstract Create a MPSState with a non-temporary MTLResource
 *  @param      resource    A MTLBuffer or MTLTexture. May be nil.  */
-(nonnull instancetype) initWithResource: (__nullable id <MTLResource>) resource;
-(nullable instancetype) init NS_UNAVAILABLE;



/*! @abstract       Initialize a non-temporary state to hold a number of textures and buffers
 *  @discussion     The allocation of each resource will be deferred  until it is needed.
 *                  This occurs when -resource or -resourceAtIndex: is called.
 *  @param          resourceList The list of resources to create. */
-(nonnull instancetype) initWithDevice: (__nonnull id <MTLDevice>) device
                          resourceList: (MPSStateResourceList * __nonnull) resourceList
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Initialize a temporary state to hold a number of textures and buffers
 *  @discussion     The textures occur first in sequence*/
 +(nonnull instancetype) temporaryStateWithCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
                                            resourceList: (MPSStateResourceList * __nonnull) resourceList
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));
 
/*! @abstract Create a state object with a list of MTLResources
 *  @discussion     Because MPS prefers deferred allocation of resources
 *                  your application should use -initWithTextures:bufferSizes:bufferCount:
 *                  whenever possible. This method is useful for cases when the
 *                  MTLResources must be initialized by the CPU. */
-(nonnull instancetype) initWithResources: (NSArray<id <MTLResource>>*__nullable) resources;


/*! @abstract   Return the number of MTLResource objects held by the state */
@property (readonly, nonatomic)  NSUInteger resourceCount;

/*! @abstract   Get the MTLResource at the indicated index
 *  @discussion  By convention, except where otherwise documented, the MTLResources
 *               held by the MPSState are private to the MPSState object, owned
 *               by the MPSState subclass author. If the MPSState subclass
 *               author is MPS, then the identity (e.g. texture vs. buffer)
 *               and information contained in the resource should be considered
 *               implementation dependent. It may change by operating system
 *               version or device. If you are the author of the subclass then it
 *               is for your own use, and MPS will not look at it, except perhaps
 *               so as to pass it to a custom kernel.  Otherwise, the method is made
 *               available to facilitate debugging and to allow you to write your own
 *               state objects. Provide accessors to read this information
 *               in a defined format.
 *
 *  @param  index   The index of the MTLResource to retrieve
 *  @param  allocateMemory  It is very important to avoid allocating memory to hold
 *               MTLResources until it is absolutely necessary, especially when working
 *               with temporary MPSStates. When allocateMemory is set to NO and the
 *               resource has not yet been allocated, nil will be returned instead.
 *               If you just need information about the resource such as buffer size
 *               or MTLTexture properties, but not the resource itself, please use
 *               -bufferSizeAtIndex: or -textureInfoAtIndex: instead, as these will
 *               not force the creation of the MTLResource.
 */
-(__nullable id <MTLResource>) resourceAtIndex: (NSUInteger) index
                                allocateMemory: (BOOL) allocateMemory;


/*******************
 *   All states    *
 *******************/
#pragma mark -
#pragma mark All MPSStates

/*
 *  @abstract       The number of times temporary data may be read by a MPSCNNKernel
 *                  before its contents become undefined.
 *
 *  @discussion     MPSState must release their underlying resources for reuse
 *                  immediately after last use. So as to facilitate *prompt* convenient
 *                  memory recycling, each time a temporary MPSState is read by a
 *                  MPSCNNKernel -encode... method, its readCount is automatically
 *                  decremented. When the readCount reaches 0, the underlying resources are
 *                  automatically made available for reuse by MPS prior to return from 
 *                  the -encode.. method. Any data stored by the MPSState becomes undefined 
 *                  at this time.
 *
 *                  By default, the readCount is initialized to 1, indicating a image that
 *                  may be overwritten any number of times, but read only once. Non-temporary
 *                  MPSState objects have their readCount initialized to 0. Please use the
 *                  isTemporary method to learn whether a MPSState object is temporary.
 *
 *                  You may change the readCount as desired to allow MPSCNNKernels to read
 *                  the state object additional times. However, it is an error to change
 *                  the readCount once it is zero. It is an error to read or write to a
 *                  temporary MPSState object with a zero readCount. You may set the 
 *                  readCount to 0 yourself to cause the underlying storage to be returned
 *                  to MPS.
 *
 *                  The Metal API Validation layer will assert if a temporary MPSState is
 *                  deallocated with non-zero readCount to help identify cases when resources
 *                  are not returned promptly.
 *
 *                  ReadCount behavior for MPSStateBatches:
 *                      In some cases, the same state object is used for the entire batch.
 *                  This is common when the filter needs to accumulate state over an entire
 *                  batch, such as during weight update for convolution or statistics accumulation
 *                  in batch normalization. In such cases, the single MPSState accumulator
 *                  is represented as a MPSStateBatch with batch.count pointers to the same
 *                  MPSState object.  When MPS decrements the read count on states or images
 *                  in a batch it only does so on unique objects. Your application should follow
 *                  the same convention. MPSStateBatchIncrementReadCount() is provided to help you.
 */
@property (readwrite, nonatomic)  NSUInteger  readCount;

/*
 *  @abstract       Describes whether or not a MPSState object is temporary or not.
 *  
 *  @discussion     Temporary MPSState objects are designed to efficiently share
 *                  memory with other objects on a MTLCommandBuffer. Their valid lifetime
 *                  is limited to the lifetime of a MTLCommandBuffer. The valid lifetime
 *                  of the data that they contain stretches from creation to when their
 *                  readcount reaches 0. Non-temporary MPSState objects can in contrast
 *                  be used at any time with any command buffer, until they are released and
 *                  their reference count reaches 0. (Reference and read counts are different
 *                  things.)  Since the lifetimes of temporary and non-temporary MPSState
 *                  objects vary widely, it can be important to know what sort of state
 *                  object it is so that it can be handled correctly.  MPSStates without
 *                  a resource are not temporary.
 */
@property (readonly, nonatomic) BOOL    isTemporary;


/*!
 *  @property label
 *  @abstract A string to help identify this object.
 */
@property (nullable, copy, atomic) NSString *label;


/*! @abstract Return the buffer size of the MTLBuffer at index or 0 if it is not a MTLBuffer
 *  @discussion Does not force allocation of the MTLResource */
-(NSUInteger) bufferSizeAtIndex: (NSUInteger) index;

/*! @abstract Return the texture size {width,height,depth} or {0,0,0} if it is not a MTLTexture
 *  @discussion Does not force allocation of the MTLResource */
-(MPSStateTextureInfo) textureInfoAtIndex: (NSUInteger) index;

/*! @abstract Return YES if the resource at index is a buffer
 *  @discussion Does not force allocation of the MTLResource */
-(MPSStateResourceType) resourceTypeAtIndex:(NSUInteger) index;


/*! @abstract   Flush any copy of MTLResources held by the state from the device's caches, and invalidate any CPU caches if needed.
 *  @discussion This will call [id <MTLBlitEncoder> synchronizeResource: ] on the state's MTLResources.
 *              This is necessary for all MTLStorageModeManaged resources. For other resources, including temporary
 *              resources (these are all MTLStorageModePrivate), nothing is done.
 *  @param      commandBuffer       The commandbuffer on which to synchronize   */
-(void) synchronizeOnCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
            MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Get the number of bytes used to allocate underyling MTLResources
 *  @discussion     This is the size of the backing store of underlying MTLResources.
 *                  It does not include all storage used by the object, for example
 *                  the storage used to hold the MPSState instantiation and MTLTexture
 *                  or MTLBuffer is not included. It only measures the size of the
 *                  allocation used to hold the texels in the texture or bytes in the
 *                  buffer. This value is subject to change between different devices
 *                  and operating systems.
 *
 *                  Except when -initWithResource: is used, most MPSStates are allocated
 *                  without a backing store. The backing store is allocated lazily when
 *                  it is needed, typically when the .texture property is called.
 *                  Consequently, in most cases, it should be inexpensive to make
 *                  a MPSImage to see how much memory it will need, and release it
 *                  if it is too large.
 *
 *                  This method may fail in certain circumstances, such as when the
 *                  MPSImage is created with -initWithTexture:featureChannels:, in
 *                  which case 0 will be returned.
 */
-(NSUInteger)  resourceSize
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*! @abstract       Determine padding and sizing of result images
 *  @discussion     A MPSState has the opportunity to reconfigure the MPSImageDescriptor
 *                  used to create the filter result state and set the MPSKernel.offset
 *                  to the correct value.  By default, the MPSState does not modify the
 *                  descriptor.
 *
 *                  There is a order of operations defined for who may update the descriptor:
 *
 *                      1) Default padding code runs based on the MPSNNPaddingMethod in
 *                          the MPSCNNKernel.padding. This creates the descriptor and
 *                          picks a starting value for the MPSCNNKernel.offset.
 *                      2) MPSStates are called in order to apply this function and update
 *                          the offset.
 *                      3) The MPSNNPadding custom padding method of the same name is called.
 *                      4)
 *
 *
 *                  Some code that may prove helpful:
 *
 *                  @code
 *                  const int centeringPolicy = 0;  // When kernelSize is even: 0 pad bottom right. 1 pad top left.    Centers the kernel for even sized kernels.
 *
 *                  typedef enum Style{
 *                      StyleValidOnly = -1,
 *                      StyleSame = 0,
 *                      StyleFull = 1
 *                  }Style;
 *
 *                  // Typical destination size in one dimension for forward filters (most filters)
 *                  static int DestSize( int sourceSize, int stride, int filterWindowSize, Style style ){
 *                      sourceSize += style * (filterWindowSize - 1);       // adjust how many pixels we are allowed to read
 *                      return (sourceSize + stride - 1) / stride;          // sourceSize / stride, round up
 *                  }
 *
 *                  // Typical destination size in one dimension for reverse filters (e.g. convolution transpose)
 *                  static int DestSizeReverse( int sourceSize, int stride, int filterWindowSize, Style style ){
 *                      return (sourceSize-1) * stride +        // center tap for the last N-1 results. Take stride into account
 *                              1 +                             // center tap for the first result
 *                              style * (filterWindowSize-1);   // add or subtract (or ignore) the filter extent
 *                  }
 *
 *                  // Find the MPSOffset in one dimension
 *                  static int Offset( int sourceSize, int stride, int filterWindowSize, Style style ){
 *                      // The correction needed to adjust from position of left edge to center per MPSOffset definition
 *                      int correction = filterWindowSize / 2;
 *
 *                      // exit if all we want is to start consuming pixels at the left edge of the image.
 *                      if( 0 )
 *                          return correction;
 *
 *                      // Center the area consumed in the source image:
 *                      // Calculate the size of the destination image
 *                      int destSize = DestSize( sourceSize, stride, filterWindowSize, style ); // use DestSizeReverse here instead as appropriate
 *
 *                      // calculate extent of pixels we need to read in source to populate the destination
 *                      int readSize = (destSize-1) * stride + filterWindowSize;
 *
 *                      // calculate number of missing pixels in source
 *                      int extraSize = readSize - sourceSize;
 *
 *                      // number of missing pixels on left side
 *                      int leftExtraPixels = (extraSize + centeringPolicy) / 2;
 *
 *                      // account for the fact that the offset is based on the center pixel, not the left edge
 *                      return correction - leftExtraPixels;
 *                  }
 *                  @endcode
 *
 *  @param          sourceImages        The list of source images to be used
 *  @param          sourceStates        The list of source states to be used
 *  @param          kernel              The MPSKernel the padding method will be applied to. Set the kernel.offset
 *  @param          inDescriptor        MPS will prepare a starting guess based on the padding policy (exclusive of
 *                                      MPSNNPaddingMethodCustom) set for the object. You should adjust the offset
 *                                      and image size accordingly. It is on an autoreleasepool.
 *
 *  @return         The MPSImageDescriptor to use to make a MPSImage to capture the results from the filter.
 *                  The MPSImageDescriptor is assumed to be on an autoreleasepool. Your method must also set the
 *                  kernel.offset property.
 */
-(MPSImageDescriptor * __nonnull) destinationImageDescriptorForSourceImages: (NSArray <MPSImage *> *__nonnull) sourceImages
                                                               sourceStates: (NSArray <MPSState *> * __nullable) sourceStates
                                                                  forKernel: (MPSKernel * __nonnull) kernel
                                                        suggestedDescriptor: (MPSImageDescriptor * __nonnull) inDescriptor;




/*! @abstract Get the private MTLResource underlying the MPSState
 *  @discussion  When the state is not directly initialized with a MTLResource,
 *               the actuall MTLResource creation is deferred. Especially with
 *               temporary resources, it is important to delay this creation
 *               as late as possible to avoid increasing the memory footprint.
 *               The memory is returned for reuse when the readCount = 0. Calling
 *               the -resource method will force the resource to be allocated,
 *               so you should not use it lightly, for purposes such as finding
 *               the MTLPixelFormat of a texture in the state.
 *
 *               By convention, except where otherwise documented, the MTLResources
 *               held by the MPSState are private to the MPSState object, owned
 *               by the MPSState subclass author. If the MPSState subclass
 *               author is MPS, then the identity (e.g. texture vs. buffer)
 *               and information contained in the resource should be considered
 *               implementation dependent. It may change by operating system
 *               version or device. If you are the author of the subclass then it
 *               is for your own use, and MPS will not look at it, except perhaps
 *               so as to pass it to a custom kernel.  Otherwise, the method is made
 *               available to facilitate debugging and to allow you to write your own
 *               state objects.
 */
@property (readonly, nonatomic, retain, nullable) id <MTLResource> resource
    MPS_AVAILABLE_STARTING_BUT_DEPRECATED( "Please use -resourceAtIndex:allocateMemory: instead",
        macos(10.13, 10.13.4), ios(11.0,12.0), tvos(11.0, 12.0));


@end
   
typedef NSArray<MPSState*>  MPSStateBatch;
    
/*! @abstract raise or lower the readcount of a batch by a set amount
 *  @discussion     In some circumstances, a MPSState may appear in a MPSStateBatch
 *                  multiple times. This is particularly common when the MPSState serves
 *                  as an accumulator across the entire batch, such as when accumulating
 *                  gradients for convolution weight update or batch statistics for
 *                  batch normalization.  A naive function would then end up incrementing
 *                  the state multiple times, probably leading to an error.
 *
 *                  MPSStateBatchIncrementReadCount() will efficiently increment the
 *                  readCounts of each object in the batch only once, avoiding this problem.
 *
 *  @param  batch   The MPSStateBatch to increment
 *  @param  amount  The value to add to the read count for each unique state in the batch
 *  @return  The number of different objects in the batch
 */
NSUInteger MPSStateBatchIncrementReadCount( MPSStateBatch * __nullable batch, NSInteger amount )
     MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));
    
/*! @abstract Call [MTLBlitEncoder synchronizeResource:] on unique resources */
void MPSStateBatchSynchronize( MPSStateBatch * __nonnull batch, __nonnull id <MTLCommandBuffer> cmdBuf )
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract Call [MTLBlitEncoder resourceSize] on unique resources */
NSUInteger MPSStateBatchResourceSize( MPSStateBatch * __nullable batch )
    MPS_AVAILABLE_STARTING( macos(10.14.0), ios(12.0), tvos(12.0));

#ifdef __cplusplus
}
#endif

#endif /* MPSState_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSImage.h
/*!
 *  @header MPSImage.h
 *  @framework MPSCore.framework
 *
 *  @copyright Copyright (c) 2015-2017 Apple Inc. All rights reserved.
 *  @discussion A MPSImage is a MTLTexture abstraction that allows for more than 4 channels, and
 *              for temporary images.
 */

#ifndef MPSImage_h
#define MPSImage_h


#include <MPSCore/MPSCoreTypes.h>
#import <Metal/MTLBuffer.h>

#ifdef __cplusplus
extern "C" {
#endif


/*!
 *  @class      MPSImageDescriptor
 *  @dependency This depends on Metal.framework
 *  @abstract   A MPSImageDescriptor object describes a attributes of MPSImage and is used to
 *              create one (see MPSImage discussion below)
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSImageDescriptor : NSObject <NSCopying>
/*! @property   width
 *  @abstract   The width of the CNN image.
 *  @discussion The formal width of the CNN image in pixels.  Default = 1.
 */
@property (readwrite, nonatomic) NSUInteger width;

/*! @property   height
 *  @abstract   The height of the CNN image.
 *  @discussion The formal height of the CNN image in pixels. Default = 1.
 */
@property (readwrite, nonatomic) NSUInteger height;

/*! @property   featureChannels
 *  @abstract   The number of feature channels per pixel.  Default = 1.
 */
@property (readwrite, nonatomic) NSUInteger featureChannels;

/*! @property   numberOfImages
 *  @abstract   The number of images for batch processing.   Default = 1.
 */
@property (readwrite, nonatomic) NSUInteger numberOfImages;

/*! @property   pixelFormat
 *  @abstract   The MTLPixelFormat expected for the underlying texture.
 */
@property (readonly, nonatomic) MTLPixelFormat pixelFormat;


/*! @property   channelFormat
 *  @abstract   The storage format to use for each channel in the image.
 */
@property (readwrite, nonatomic) MPSImageFeatureChannelFormat channelFormat;

/*!
 @property cpuCacheMode
 @abstract Options to specify CPU cache mode of texture resource. Default = MTLCPUCacheModeDefaultCache
 */
@property (readwrite, nonatomic) MTLCPUCacheMode cpuCacheMode;

/*!
 @property storageMode
 @abstract To specify storage mode of texture resource.
 @discussion Storage mode options:
        @code
            Default =   MTLStorageModeShared on iOS
                        MTLStorageModeManaged on Mac OSX
            MTLStorageModeShared not supported on Mac OSX.
            See Metal headers for synchronization requirements when using StorageModeManaged
        @endcode
 */
@property (readwrite, nonatomic) MTLStorageMode storageMode;

/*!
 *  @property   usage
 *  @abstract   Description of texture usage.  Default = MTLTextureUsageShaderRead/Write
 */
@property (readwrite, nonatomic) MTLTextureUsage usage;

/*! @abstract   Create a MPSImageDescriptor for a single read/write cnn image.
 */
+(__nonnull instancetype) imageDescriptorWithChannelFormat: (MPSImageFeatureChannelFormat)channelFormat
                                                     width: (NSUInteger)width
                                                    height: (NSUInteger)height
                                           featureChannels: (NSUInteger)featureChannels;

/*! @abstract   Create a MPSImageDescriptor for a read/write cnn image with option to set usage and batch size (numberOfImages).
 */
+(__nonnull instancetype) imageDescriptorWithChannelFormat: (MPSImageFeatureChannelFormat)channelFormat
                                                     width: (NSUInteger)width
                                                    height: (NSUInteger)height
                                           featureChannels: (NSUInteger)featureChannels
                                            numberOfImages: (NSUInteger)numberOfImages
                                                     usage: (MTLTextureUsage)usage;

-(nonnull instancetype) copyWithZone: (NSZone* __nullable) zone
        MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));
@end

@class MPSKernel;
@class MPSImage;
typedef NSArray<MPSImage*>  MPSImageBatch;

/*! @abstract raise or lower the readcount of a batch by a set amount
 *  @discussion     In some circumstances, a MPSImage may appear in a MPSImageBatch
 *                  multiple times. This is particularly common when the MPSImage serves
 *                  as an accumulator across the entire batch, such as when accumulating
 *                  gradients for convolution weight update or batch statistics for
 *                  batch normalization.  A naive function would then end up incrementing
 *                  the state multiple times, probably leading to an error.
 *
 *                  MPSImageBatchIncrementReadCount() will efficiently increment the readCounts of
 *                  each object in the batch only once, avoiding this problem. Non-temporary
 *                  images and images with readCount already 0 will be ignored.
 *
 *  @param  batch   The MPSImageBatch to increment
 *  @param  amount  The value to add to the read count for each unique image in the batch
 *  @return         The number of different images in the batch
 */
NSUInteger MPSImageBatchIncrementReadCount( MPSImageBatch * __nonnull batch, NSInteger amount )
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract Call [MTLBlitEncoder synchronizeResource:] on unique resources*/
void MPSImageBatchSynchronize( MPSImageBatch * __nonnull batch, __nonnull id <MTLCommandBuffer> cmdBuf )
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract Call [MTLBlitEncoder resourceSize] on unique resources and return sum */
NSUInteger MPSImageBatchResourceSize( MPSImageBatch * __nonnull batch )
    MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract   Iterate over unique images in the batch
 *  @discussion This function looks only at image address to determine uniqueness.
 *              The same texture stored in different MPSImages would be considered not unique.
 *  @param      batch           The image batch
 *  @param      iteratorBlock   Callback block to execute once for each unique image.
 *                              Return a value greater than NSIntegerMin to terminate early.
 *                              The index gives the first position in the batch where the image appears.
 *                              Behavior is undefined if MPSImageBatchIterate is called recursively on the same images.
 *  @return     The value returned by the iterator block for the last image on which it ran */
NSInteger MPSImageBatchIterate( MPSImageBatch * __nonnull batch,
                                NSInteger (^__nonnull iteratorBlock)( MPSImage * __nonnull image, NSUInteger index ) )
    MPS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0));

/*! @abstract       A class  that allocates new MPSImage or MPSTemporaryImage
 *  @discussion     Sometimes it is prohibitively costly for MPS to figure out how
 *                  big an image should be in advance. In addition, you may want to
 *                  have some say over whether the image is a temporary image or not.
 *                  In such circumstances, the MPSImageAllocator is used to
 *                  provide the developer with an opportunity for just in time feedback
 *                  about how the image should be allocated.
 *
 *                  Two standard MPSImageAllocators are provided: MPSImageDefaultAllocator
 *                  and MPSTemporaryImageDefaultAllocator. You may of course provide
 *                  your own allocator instead.
 *
 *                  Example:
 *                  @code
 *                      // Note: MPSImageDefaultAllocator is already provided
 *                      //       by the framework under that name.  It is provided here
 *                      //       as sample code for writing your own variant.
 *                      -(MPSImage * __nonnull)  imageForCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
 *                                                     imageDescriptor: (MPSImageDescriptor * __nonnull) descriptor
 *                                                              kernel: (MPSKernel * __nonnull) kernel
 *                      {
 *                          MPSImage * result = [[MPSImage alloc] initWithDevice: cmdBuf.device
 *                                                               imageDescriptor: descriptor ];
 *
 *                          // make sure the object sticks around at least as lomg as the command buffer
 *                          [result retain];
 *                          [cmdBuf addCompletedHandler: ^(id <MTLCommandBuffer> c){[result release];}];
 *
 *                          // return autoreleased result
 *                          return [result autorelease];
 *                      };
 *
 *                      -(BOOL) supportsSecureCoding{ return YES; }
 *                      -(void)encodeWithCoder:(NSCoder * __nonnull)aCoder
 *                      {
 *                          [super encodeWithCoder: aCoder];
 *
 *                          // encode any data owned by the class at this level
 *                      }
 *
 *                      -(nullable instancetype) initWithCoder: (NSCoder*__nonnull) aDecoder
 *                      {
 *                          self =  [super initWithCoder: aDecoder ];
 *                          if( nil == self )
 *                              return self;
 *
 *                          // use coder to load any extra data kept by this object here
 *
 *                          return self;
 *                      }
 *                  @endcode
 *
 *              Please see [MPSImage defaultAllocator] and [MPSTemporaryImage defaultAllocator]
 *              for implentations of the protocol already provided by MPS.
 *
 *              When considering whether to write your own MPSImageAllocator, you should know
 *              the existing MPSImage and MPSTemporaryImage default allocators are optimized
 *              to make image batch allocation much faster than one MPSImage at a time in a loop.
 *              When possible, it can be better to use the MPS provided allocators and override
 *              the behavior in a padding policy instead, if the changes can be contained in
 *              the MPSImageDescriptor. This will help reduce CPU encode time. However, custom
 *              padding policies can inhibit optimizations in the MPSNNGraph, particularly node
 *              fusion, resulting in more work for the GPU. In cases where the custom padding method
 *              does not change filter properties but only adjusts the result image (e.g. adjust result
 *              feature channel format) then MPSNNPaddingMethodCustomWhitelistForNodeFusion may be
 *              used to signal that node fusion is acceptable. 
 */

@protocol MPSImageAllocator <NSObject, NSSecureCoding>
@required
/*! @abstract   Create a new MPSImage
 *  @discussion See class description for sample implementation
 *  @param          cmdBuf      The MTLCommandBuffer on which the image will be initialized.
 *                              cmdBuf.device encodes the MTLDevice.
 *  @param          descriptor  A MPSImageDescriptor containing the image format to use.
 *                              This format is the result of your MPSPadding policy.
 *  @param          kernel      The kernel that will overwrite the image returned by the filter.
 *                              Note that the MPS implementations of this protocol don't need
 *                              this field. It is provided for your convenience.
 *
 *  @return         A valid MPSImage or MPSTemporaryImage. It will be automatically released when the command buffer completes.
 */
-(MPSImage * __nonnull)  imageForCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                               imageDescriptor: (MPSImageDescriptor * __nonnull) descriptor
                                        kernel: (MPSKernel * __nonnull) kernel;

/* The allocator must support all three methods of NSSecureCoding as well:
 @property (class, readonly) BOOL supportsSecureCoding;
 - (void)encodeWithCoder:(NSCoder *__nonnull)aCoder;
 - (nullable instancetype)initWithCoder:(NSCoder *__nonnull)aDecoder NS_DESIGNATED_INITIALIZER;
 */
    
@optional
    /*! @abstract   Efficiently create an array of MPSImages with a common descriptor
     *  @discussion See class description for sample implementation
     *  @param          cmdBuf      The MTLCommandBuffer on which the image will be initialized.
     *                              cmdBuf.device encodes the MTLDevice.
     *  @param          descriptor  A MPSImageDescriptor containing the image format to use.
     *                              This format is the result of your MPSPadding policy.
     *  @param          kernel      The kernel that will overwrite the image returned by the filter.
     *                              Note that the MPS implementations of this protocol don't need
     *                              this field. It is provided for your convenience.
     *  @param          count       The number of images in the batch
     *
     *  @return         A valid MPSImage or MPSTemporaryImage. It will be automatically released when the command buffer completes.
     */
-(MPSImageBatch * __nonnull)  imageBatchForCommandBuffer: (__nonnull id <MTLCommandBuffer>) cmdBuf
                                         imageDescriptor: (MPSImageDescriptor * __nonnull) descriptor
                                                  kernel: (MPSKernel * __nonnull) kernel
                                                   count: (NSUInteger) count;
@end



#if defined(DOXYGEN)
    typedef enum MPSPurgeableState
#else
    typedef NS_ENUM(NSUInteger, MPSPurgeableState)
#endif
{
    MPSPurgeableStateAllocationDeferred MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) MPS_SWIFT_NAME(allocationDeferred) = 0,                // The buffer hasn't been allocated yet. Attempts to set purgeability will be ignored.
    MPSPurgeableStateKeepCurrent        MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = MTLPurgeableStateKeepCurrent,
    
    MPSPurgeableStateNonVolatile        MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = MTLPurgeableStateNonVolatile,
    MPSPurgeableStateVolatile           MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = MTLPurgeableStateVolatile,
    MPSPurgeableStateEmpty              MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0)) = MTLPurgeableStateEmpty,
} NS_ENUM_AVAILABLE(10_11, 8_0)
#if defined(DOXYGEN)
    MPSPurgeableState
#endif
;
    
#if defined(DOXYGEN)
    typedef enum MPSDataLayout
#else
    typedef NS_ENUM(NSUInteger, MPSDataLayout)
#endif
{
    // output as order [imageNum][imageHeight][imageWidth][numberOfFeatureChannels]
    MPSDataLayoutHeightxWidthxFeatureChannels MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) MPS_SWIFT_NAME(HeightxWidthxFeatureChannels) = 0,
    // output as order [imageNum][numberOfFeatureChannels][imageHeight][imageWidth]
    MPSDataLayoutFeatureChannelsxHeightxWidth MPS_ENUM_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0)) = 1,
} NS_ENUM_AVAILABLE(10_13, 11_0)
#if defined(DOXYGEN)
    MPSDataLayout
#endif
;
    
/*!
 *  @struct     MPSImageReadWriteParams
 *  @abstract   these parameters are passed in to allow user to read/write to a particular set of featureChannels in an MPSImage
 */
typedef struct
{
    NSUInteger featureChannelOffset;               /**< featureChannel offset from which to read/write featureChannels, this should be a multiple of 4 */
    NSUInteger numberOfFeatureChannelsToReadWrite; /**< is number of featureChannels, should be greater than 0 and multiple of 4 unless featureChannelOffset is 0 */
}MPSImageReadWriteParams;

/*!
 *  @class      MPSImage
 *  @dependency This depends on Metal.framework
 *  @abstract   A MPSImage object describes a MTLTexture that may have more than 4 channels.
 *  @discussion Some image types, such as those found in convolutional neural networks (CNN) 
 *              differ from a standard texture in that they may have more than 4 channels
 *              per image. While the channels could hold RGBA data, they will more commonly 
 *              hold a number of structural permutations upon a multi-channel image as the neural
 *              network progresses. It is not uncommon for each pixel to have 32 or 64 channels 
 *              in it.
 *
 *              A standard MTLTexture may have no more than 4 channels. The additional
 *              channels are stored in slices of 2d texture array (i.e. texture type is MTLTextureType2DArray) 
 *              such that 4 consecutive channels are stored in each slice of this array.
 *              If the number of feature channels is N, number of array slices needed is (N+3)/4.
 *              E.g. a CNN image with width 3 and height 2 with 9 channels will be stored as
 * @code
 *              slice 0   RGBA   RGBA  RGBA
 *                        RGBA   RGBA  RGBA
 *
 *              slice 1      RGBA   RGBA   RGBA
 *                           RGBA   RGBA   RGBA         (ASCII art /diagonal offset/ intended to show a Z dimension)
 *
 *              slice 2         R???   R???   R???
 *                              R???   R???   R???
 *@endcode
 *              The width and height of underlying 2d texture array is the same as the width and height of the MPSImage.
 *              The array length is equal to (featureChannels + 3) / 4. Channels marked with ? are just
 *              for padding and should not contain NaNs or Infs.
 *
 *              A MPSImage can be container of multiple CNN images for batch processing. In order to create a
 *              MPSImage that contains N images, create MPSImageDescriptor with numberOfImages set to N.
 *
 *              Although a MPSImage can contain numberOfImages > 1, the actual number of images among these processed by MPSCNNKernel
 *              is controlled by z-dimension of the clipRect. A MPSCNNKernel processes n=clipRect.size.depth images from this collection.
 *              The starting source image index to process is given by offset.z. The starting index of the destination image is given by 
 *              clipRect.origin.z. The MPSCNNKernel takes n=clipRect.size.depth images from tje source at indices [offset.z, offset.z+n], 
 *              processes each independently and stores the result in the destination at indices [clipRect.origin.z, clipRect.origin.z+n] 
 *              respectively. Offset.z+n should be <= [src numberOfImage] and clipRect.origin.z+n should be <= [dest numberOfImages] and 
 *              offset.z must be >= 0.
 *
 *              Example: Suppose MPSCNNConvolution takes an input image with 8 channels and outputs an image with 16 channels. The number of
 *              slices needed in the source 2d texture array is 2 and the number of slices needed in the destination 2d array is 4. Suppose 
 *              the source batch size is 5 and destination batch size is 4. (Multiple N-channel images can be processed concurrently in a 
 *              batch.) The number of source slices will be 2*5=10 and number of destination slices will be 4*4=16. If you want to process
 *              just images 2 and 3 of the source and store the result at index 1 and 2 in the destination, you may achieve this by setting
 *              offset.z=2, clipRect.origin.z=1 and clipRect.size.depth=2. MPSCNNConvolution will take, in this case, slice 4 and 5 of source and
 *              produce slices 4 to 7 of destination. Similarly, slices 6 and 7 will be used to produce slices 8 to 11 of destination.
 *
 *              All MPSCNNKernels process images within each batch independently. That is, calling a MPSCNNKernel on an
 *              batch is formally the same as calling it on each image in the batch one at a time. However, quite a lot of CPU and GPU overhead 
 *              will be avoided if batch processing is used. This is especially important for better performance on small images.
 *
 *              If the number of feature channels is <= 4 and numberOfImages = 1 i.e. only one slice is needed to represent a MPSImage, the underlying
 *              metal texture type will be MTLTextureType2D rather than MTLTextureType2DArray.
 *
 *              There are also MPSTemporaryImages, intended for use for very short-lived image data that are produced and consumed
 *              immediately in the same MTLCommandBuffer. They are a useful way to minimize CPU-side texture allocation costs and 
 *              greatly reduce the amount of memory used by your image pipeline.
 *
 *              Creation of the underlying texture may in some cases occur lazily.  You should
 *              in general avoid calling MPSImage.texture except when unavoidable to avoid
 *              materializing memory for longer than necessary. When possible, use the other MPSImage
 *              properties to get information about the MPSImage instead.
 *
 *              Most MPSImages of 4 or fewer feature channels can generate quicklooks output in
 *              Xcode for easy visualization of image data in the object. MPSTemporaryImages can not.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSImage :  NSObject

/*!     Get a well known MPSImageAllocator that makes MPSImages */
+(nonnull id <MPSImageAllocator>)  defaultAllocator;

/*! @property device
 *  @abstract  The device on which the MPSImage will be used
 */
@property (readonly, retain, nonatomic, nonnull)  id <MTLDevice>    device;

/*! @property   width
 *  @abstract   The formal width of the image in pixels.
 */
@property (readonly, nonatomic) NSUInteger width;

/*! @property   height
 *  @abstract   The formal height of the image in pixels.
 */
@property (readonly, nonatomic) NSUInteger height;

/*! @property   featureChannels
 *  @abstract   The number of feature channels per pixel.
 */
@property (readonly, nonatomic) NSUInteger featureChannels;

/*! @property   numberOfImages
 *  @abstract   numberOfImages for batch processing
 */
@property (readonly, nonatomic) NSUInteger numberOfImages;

/*! @property   textureType
 *  @abstract   The type of the underlying texture, typically MTLTextureType2D
 *              or MTLTextureType2DArray
 */
@property (readonly, nonatomic) MTLTextureType textureType;

/*! @property   pixelFormat
 *  @abstract   The MTLPixelFormat of the underlying texture
 */
@property (readonly, nonatomic) MTLPixelFormat pixelFormat;

/*! @property   precision
 *  @abstract   The number of bits of numeric precision available for each feature channel.
 *  @discussion This is precision, not size.  That is, float is 24 bits, not 32. half
 *              precision floating-point is 11 bits, not 16. SNorm formats have one less
 *              bit of precision for the sign bit, etc. For formats like MTLPixelFormatB5G6R5Unorm
 *              it is the precision of the most precise channel, in this case 6.  When this
 *              information is unavailable, typically compressed formats, 0 will be returned.
 */
@property (readonly, nonatomic) NSUInteger precision;

/*!
 *  @property   usage
 *  @abstract   Description of texture usage.
 */
@property (readonly, nonatomic) MTLTextureUsage usage;

/*!
 *  @property   pixelSize
 *  @abstract   Number of bytes from the first byte of one pixel to the first byte of the next 
 *              pixel in storage order.  (Includes padding.)
 */
@property (readonly, nonatomic) size_t  pixelSize;


/*! @property   texture
 *  @abstract   The associated MTLTexture object.
 *              This is a 2D texture if numberOfImages is 1 and number of feature channels <= 4.
 *              It is a 2D texture array otherwise.
 *  @discussion To avoid the high cost of premature allocation of the underlying texture, avoid calling this
 *              property except when strictly necessary. [MPSCNNKernel encode...] calls typically cause
 *              their arguments to become allocated. Likewise, MPSImages initialized with -initWithTexture:
 *              featureChannels: have already been allocated.
 */
@property (readonly, nonnull, nonatomic) id <MTLTexture> texture;

/*!
 @property label
 @abstract A string to help identify this object.
 */
@property (copy, atomic, nullable)  NSString *label;

/*! @abstract   The MPSImage from which this MPSImage was derived. Otherwise nil.
 *  @discussion This will point to the original image if this image was created using
 *              -batchRepresentation, -batchRepresentationWithRange: or
 *              -subImageWithRange:.  */
@property (readonly, nullable, retain, nonatomic)  MPSImage * parent;

/*!
 *  @abstract   Initialize an empty image object
 *  @param      device              The device that the image will be used. May not be NULL.
 *  @param      imageDescriptor     The MPSImageDescriptor. May not be NULL.
 *  @return     A valid MPSImage object or nil, if failure.
 *  @discussion Storage to store data needed is allocated lazily on first use of MPSImage or
 *              when application calls MPSImage.texture
 *
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       imageDescriptor: (const MPSImageDescriptor * __nonnull) imageDescriptor;


/*! @abstract   Use -batchRepresentation or -subImageWithFeatureChannelRange instead
 *  @discussion Generally, you should call -batchRepresentation or -subImageWithFeatureChannelRange
 *              instead because they are safer. This is provided so that these interfaces will work
 *              with your MPSImage subclass.
 *
 *  @param  parent  The parent image that owns the texture. It may be a sub-image.
 *  @param  sliceRange  The range of MTLTexture2dArray slices to be included in the sub-image
 *  @param  featureChannels The number of feature channels in the new image. The number of images
 *                          is inferred.
 *  @return A MPSImage that references a subregion of the texel storage in parent instead of
 *              using its own storage.  */
-(nonnull instancetype) initWithParentImage: (MPSImage * __nonnull) parent
                                 sliceRange: (NSRange) sliceRange
                            featureChannels: (NSUInteger) featureChannels NS_DESIGNATED_INITIALIZER;


/*!
 *  @abstract   Initialize an MPSImage object using Metal texture. Metal texture has been created by
 *              user for specific number of feature channels and number of images.
 *  @param      texture          The MTLTexture allocated by the user to be used as backing for MPSImage.
 *  @param      featureChannels  Number of feature channels this texture contains.
 *  @return     A valid MPSImage object or nil, if failure.
 *  @discussion Application can let MPS framework allocate texture with properties specified in imageDescriptor 
 *              using initWithDevice:MPSImageDescriptor API above. However in memory intensive application, 
 *              you can save memory (and allocation/deallocation time) by using MPSTemporaryImage where MPS 
 *              framework aggressively reuse memory underlying textures on same command buffer. See MPSTemporaryImage
 *              class for details below. However, in certain cases, application developer may want more control
 *              on allocation, placement, reusing/recycling of memory backing textures used in application using
 *              Metal Heaps API. In this case, application can create MPSImage from pre-allocated texture using 
 *              initWithTexture:featureChannels.
 *
 *              MTLTextureType of texture can be MTLTextureType2D ONLY if featureChannels <= 4 in which case
 *              MPSImage.numberOfImages is set to 1. Else it should be MTLTextureType2DArray with
 *              arrayLength == numberOfImage * ((featureChannels + 3)/4). MPSImage.numberOfImages is set to
 *              texture.arrayLength / ((featureChannels + 3)/4).
 *
 *              For MTLTextures containing typical image data which application may obtain from MetalKit or 
 *              other libraries such as that drawn from a JPEG or PNG, featureChannels should
 *              be set to number of valid color channel e.g. for RGB data, even thought MTLPixelFormat will be
 *              MTLPixelFormatRGBA, featureChannels should be set to 3.
 *
 */
-(nonnull instancetype) initWithTexture: (nonnull id <MTLTexture>) texture
                        featureChannels: (NSUInteger) featureChannels;


/*
 * Use initWithDevice:texture: or initWithDevice:imageDescriptor: instead
 */
-(nonnull instancetype) init NS_UNAVAILABLE;


/*! @abstract       Make a representation of a MPSImage (batch) as a MPSImageBatch
 *  @discussion     Before the MPSImageBatch was introduced, several images could be concatenated
 *                  into a MPSImage as multiple image slices in a MTLTexture2DArray to make
 *                  a image batch. If the image contained more than 4 feature channels, then each
 *                  image would have round_up( feature channels / 4) slices and the total number
 *                  of slices in the MPSImage would be slices * number of images.
 *
 *                  Because many devices can operate on texture arrays of no more than 2048 slices,
 *                  storage in this format is limited. For example in InceptionV3, 2048 feature
 *                  channels at its widest point, the largest batch size that can be processed in
 *                  this way is 4, well under commonly accepted practice for training. Consequently,
 *                  the older batching style is deprecated and the MPSImageBatch is introduced.
 *                  It is also easier to manage sub-batches and to concatenate sub-batches for
 *                  memory management with the MPSImageBatch, so this format is favored going forward.
 *
 *                  To facilitate forward migration, this method will prepare an array of MPSImages that
 *                  each point to the appropriate set of slices in storage for the original image.
 *                  Since they share storage, writes to the parent will alter the content of the
 *                  children, and vice versa.
 *
 *                  If the original is a temporary image, the result will be a temporary image.
 *                  It will hold 1 readCount on the original. When the readCount drops to 0, it
 *                  will decrement the appropriate counter on the parent.
 *
 *                  This is a much cheaper form of the slice operator, and should be used instead
 *                  when the slice operator does not need to operate out of place.
 *
 *  @param  subRange  The range of images in the original image from which the batch will be derived.
 *  @return A MPSImageBatch referencing a subregion of the original batch image.
 */
-(MPSImageBatch * __nonnull)  batchRepresentationWithSubRange: (NSRange) subRange;
 
 /*! @abstract    Make a MPSImageBatch that points to the individual images in the MPSImage
  * @discussion   If the original is a temporary image, the result will be a temporary image.
  *               It will hold 1 readCount on the original. When the readCount drops to 0, it
  *               will decrement the appropriate counter on the parent.
  *  @return A MPSImageBatch aliasing the texel storage in the original batch image */
-(MPSImageBatch * __nonnull)  batchRepresentation;

/* @abstract    Make a sub-image that points to a subset of feature channels in the original
 * @discussion  This makes a MPSImage that points to a feature channel subregion within
 *              the original image. It is a much cheaper form of the slice operator in both
 *              memory usage and GPU time, and should be used instead when the slice operator
 *              does not need to operate out of place and the feature channel range is a
 *              multiple of 4.
 *
 *              If the original is a temporary image, the result will be a temporary image.
 *              It will hold 1 readCount on the original. When the readCount drops to 0, it
 *              will decrement the appropriate counter on the parent.
 *
 * @param       range     A range describing the sub-range within the MPSImage
 *                        to make the subImage within. The location and length
 *                        must be multiples of 4. If the length is too big, it
 *                        will be reduced to fit in the image.
 */
-(MPSImage * __nonnull) subImageWithFeatureChannelRange: (NSRange) range;
 
/*! @abstract       Get the number of bytes used to allocate underyling MTLResources
 *  @discussion     This is the size of the backing store of underlying MTLResources.
 *                  It does not include all storage used by the object, for example
 *                  the storage used to hold the MPSImage instantiation and MTLTexture
 *                  is not included. It only measures the size of the allocation
 *                  used to hold the texels in the image. This value is subject to
 *                  change between different devices and operating systems.
 *
 *                  Except when -initWithTexture:featureChannels: is used, most
 *                  MPSImages (including MPSTemporaryImages) are allocated without
 *                  a backing store. The backing store is allocated lazily when
 *                  it is needed, typically when the .texture property is called.
 *                  Consequently, in most cases, it should be inexpensive to make
 *                  a MPSImage to see how much memory it will need, and release it
 *                  if it is too large.
 *
 *                  This method may fail in certain circumstances, such as when the
 *                  MPSImage is created with -initWithTexture:featureChannels:, in
 *                  which case 0 will be returned. 0 will also be returned if
 *                  it is a sub-image or sub-batch (.parent is not nil).
 */
-(NSUInteger)  resourceSize
    MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*! @abstract       Set (or query) the purgeability state of a MPSImage
 *  @discussion     Usage is per [MTLResource setPurgeableState:], except that the MTLTexture might be
 *                  MPSPurgeableStateAllocationDeferred, which means there is no texture to mark volatile / nonvolatile.
 *                  Attempts to set purgeability on MTLTextures that have not been allocated will be ignored.
 */
- (MPSPurgeableState)setPurgeableState:(MPSPurgeableState)state;

/*!
 *  @method         readBytes
 *  @abstract       Get the values inside MPSImage and put them in the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          bytesPerRow                  Bytes to stride to point to next row(pixel just below current one) in the user buffer.
 *  @param          featureChannelInfo           information user fills in to write to a set of feature channels in the image
 *  @param          imageIndex                   Image index in MPSImage to write to.
 *  @param          region                       region of the MPSImage to read from. A region is a structure with the origin in the Image from which to start
 *                                               reading values and a size which represents the width and height of the rectangular region to read from.
 *                                               The z direction denotes the number of images, thus for 1 image, origin.z = 0 and size.depth = 1
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor.
 */
-(void)  readBytes: (void * __nonnull)dataBytes
        dataLayout: (MPSDataLayout)dataLayout
       bytesPerRow: (NSUInteger)bytesPerRow
            region: (MTLRegion)region
featureChannelInfo: (MPSImageReadWriteParams)featureChannelInfo
        imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  @method         writeBytes
 *  @abstract       Set the values inside MPSImage with the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          bytesPerRow                  Bytes to stride to point to next row(pixel just below current one) in the user buffer.
 *  @param          region                       region of the MPSImage to write to. A region is a structure with the origin in the Image from which to start
 *                                               writing values and a size which represents the width and height of the rectangular region to write in.
 *                                               The z direction denotes the number of images, thus for 1 image, origin.z = 0 and size.depth = 1
 *  @param          featureChannelInfo           information user fills in to read from a set of feature channels in the image
 *  @param          imageIndex                   Image index in MPSImage to write to.
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor.
 */
-(void) writeBytes: (const void * __nonnull)dataBytes
        dataLayout: (MPSDataLayout)dataLayout
       bytesPerRow: (NSUInteger)bytesPerRow
            region: (MTLRegion)region
featureChannelInfo: (MPSImageReadWriteParams)featureChannelInfo
        imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @method         readBytes
 *  @abstract       Get the values inside MPSImage and put them in the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          bytesPerRow                  Bytes to stride to point to next row(pixel just below current one) in the user buffer.
 *  @param          bytesPerImage                Bytes to stride to point to next slice in the user buffer.
 *  @param          featureChannelInfo           information user fills in to write to a set of feature channels in the image
 *  @param          imageIndex                   Image index in MPSImage to write to.
 *  @param          region                       region of the MPSImage to read from. A region is a structure with the origin in the Image from which to start
 *                                               reading values and a size which represents the width and height of the rectangular region to read from.
 *                                               The z direction denotes the number of images, thus for 1 image, origin.z = 0 and size.depth = 1
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor.
 */
-(void)  readBytes: (void * __nonnull)dataBytes
        dataLayout: (MPSDataLayout)dataLayout
       bytesPerRow: (NSUInteger)bytesPerRow
     bytesPerImage: (NSUInteger)bytesPerImage
            region: (MTLRegion)region
featureChannelInfo: (MPSImageReadWriteParams)featureChannelInfo
        imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @method         writeBytes
 *  @abstract       Set the values inside MPSImage with the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          bytesPerRow                  Bytes to stride to point to next row(pixel just below current one) in the user buffer.
 *  @param          bytesPerImage                Bytes to stride to point to next slice in the user buffer.
 *  @param          region                       region of the MPSImage to write to. A region is a structure with the origin in the Image from which to start
 *                                               writing values and a size which represents the width and height of the rectangular region to write in.
 *                                               The z direction denotes the number of images, thus for 1 image, origin.z = 0 and size.depth = 1
 *  @param          featureChannelInfo           information user fills in to read from a set of feature channels in the image
 *  @param          imageIndex                   Image index in MPSImage to write to.
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor.
 */
-(void) writeBytes: (const void * __nonnull)dataBytes
        dataLayout: (MPSDataLayout)dataLayout
       bytesPerRow: (NSUInteger)bytesPerRow
     bytesPerImage: (NSUInteger)bytesPerImage
            region: (MTLRegion)region
featureChannelInfo: (MPSImageReadWriteParams)featureChannelInfo
        imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


/*!
 *  @method         readBytes
 *  @abstract       Get the values inside MPSImage and put them in the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          imageIndex                   Image index in MPSImage to read from.
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor. Region is full image, buffer width and height is same as MPSImage width and height.
 */
-(void) readBytes: (void * __nonnull)dataBytes
       dataLayout: (MPSDataLayout)dataLayout
       imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  @method         writeBytes
 *  @abstract       Set the values inside MPSImage with the Buffer passed in.
 *  @param          dataBytes                    The array allocated by the user to be used to put data from MPSImage, the length should be
 *                                               imageWidth * imageHeight * numberOfFeatureChannels and dataType should be inferred from pixelFormat
 *                                               defined in the Image Descriptor.
 *  @param          dataLayout                   The enum tells how to layout MPS data in the buffer.
 *  @param          imageIndex                   Image index in MPSImage to write to.
 *  @discussion     Use the enum to set data is coming in with what order. The data type will be determined by the pixelFormat
 *                  defined in the Image Descriptor. Region is full image, buffer width and height is same as MPSImage width and height.
 */
-(void) writeBytes: (const void * __nonnull)dataBytes
        dataLayout: (MPSDataLayout)dataLayout
        imageIndex: (NSUInteger)imageIndex
        MPS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0));

/*! @abstract   Flush the underlying MTLTexture from the device's caches, and invalidate any CPU caches if needed.
 *  @discussion This will call [id <MTLBlitEncoder> synchronizeResource: ] on the image's MTLTexture, if any.
 *              This is necessary for all MTLStorageModeManaged resources. For other resources, including temporary
 *              resources (these are all MTLStorageModePrivate), and textures that have not yet been allocated, nothing is done.
 *              It is more efficient to use this method than to attempt to do this yourself with the texture property.
 *  @param      commandBuffer       The commandbuffer on which to synchronize   */
-(void) synchronizeOnCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));


@end

/*!
 *  @class      MPSTemporaryImage
 *  @dependency MPSImage
 *  @abstract   MPSTemporaryImages are for MPSImages with short lifetimes.
 *
 *  @discussion What is temporary memory? It is memory, plain and simple. Analogy: If we
 *              use an app as an analogy for a command buffer, then "Regular memory"
 *              (such as what backs a MPSImage or the typical MTLTexture) would be memory 
 *              that you allocate at launch and never free. Temporary memory would be memory 
 *              that you free when you are done with it so it can be used for something 
 *              else as needed later in your app.  You /could/ write your app to allocate 
 *              everything you will ever need up front, but this is very inefficient and 
 *              quite frankly a pain to plan out in advance. You don't do it for your app,
 *              so why would you do it for your command buffers? 
 *
 *              Welcome to the 1970's! We have added a heap.
 *
 *              Unsurprisingly, MPSTemporaryImages can provide for profound reduction in
 *              the the amount of memory used by your application.  Like malloc, MPS 
 *              maintains a heap of memory usable in a command buffer. Over the lifetime 
 *              of a command buffer, the same piece of memory may be reused many times. 
 *              This means that each time the same memory is reused, it aliases with previous
 *              uses. If we aren't careful, we might find that needed data is overwritten
 *              by successive allocations. However, this is no different than accessing freed
 *              memory only to discover it doesn't contain what you thought it did anymore, 
 *              so you should be able to keep out of trouble by following a few simple rules,
 *              like with malloc.
 *
 *              To this end, we added some restrictions to help you out and get a bit more 
 *              performance. Some comments are appended in parentheses below to extend the
 *              analogy of command buffer = program:
 *
 *              - The textures are MTLStorageModePrivate. You can not, for example, use
 *                [MTLTexture getBytes...] or [MTLTexture replaceRegion...] with them. 
 *                MPSTemporaryImages are strictly read and written by the GPU. (There is
 *                protected memory to prevent other processes from overwriting your heap.)
 *
 *              - The temporary image may be used only on a single MTLCommandBuffer.
 *                This limits the chronology to a single linear time stream. (The heap
 *                is specific to just one command buffer. There are no mutexes to
 *                coordinate timing of simultaneous access by multiple GPUs. Nor are we
 *                likely to like them if there were. So, we disallow it.)
 *
 *              - The readCount property must be managed correctly. Please see
 *                the description of the readCount property for full details. (The readCount
 *                is a reference count for the block of memory that holds your data. The
 *                usual undefined behaviors apply to reading data that has been released.
 *                We assert when we can to prevent that from happening accidentally,
 *                just as a program might segfault. The readCount counts procedural users 
 *                of the object -- MPSKernel.encode... calls that read the MPSTemporaryImage. 
 *                As each reads from it, the readCount is automatically decremented. The 
 *                texture data will be freed in typical usage at the right time as the 
 *                readCount reaches zero, typically with little user involvement other
 *                than to set the readCount up front. We did examine using the main MPSTemporaryImage
 *                reference count for this instead so that ARC would do work for you automatically.
 *                Alas, ARC destroys things at end of scope rather than promptly, sometimes resulting
 *                in greatly increased memory usage. These allocations are large! So, we 
 *                use this method instead.)
 *
 *              Since MPSTemporaryImages can only be used with a single MTLCommandBuffer,
 *              and can not be used off the GPU, they generally should not be kept 
 *              around past the completion of the MTLCommandBuffer. The lifetime of
 *              MPSTemporaryImages is expected to be typically extremely short, perhaps 
 *              only a few lines of code. Like malloc, it is intended to be fairly cheap 
 *              to make MPSTemporaryImages and throw them away. Please do so.
 *
 *              To keep the lifetime of the underlying texture allocation as short as 
 *              possible, the underlying texture is not allocated until the first time
 *              the MPSTemporaryImage is used by a MPSCNNKernel or the .texture property
 *              is read. The readCount property serves to limit the lifetime on the
 *              other end.
 *
 *              You may use the MPSTemporaryImage.texture with MPSUnaryImageKernel -encode... methods,
 *              iff featureChannels <= 4 and the MTLTexture conforms to requirements of that MPSKernel.
 *              There is no locking mechanism provided to prevent a MTLTexture returned 
 *              from the .texture property from becoming invalid when the readCount reaches 0.
 *
 *              Finally, MPSTemporaryImages are complicated to use with blit encoders.
 *              Your application should assume that the MPSTemporaryImage is backed by a MTLHeap,
 *              and consequently needs a MTLFence to ensure that compute command encoders and other
 *              encoders do not trip over one another with heap based memory. MPS will almost never
 *              use a blit encoder for this reason. If you do need one, then you will need to make
 *              a new compute encoder to block on whatever previous compute encoder last used the
 *              heap block. (MPS will not tell you who previously used the heap block. That encoder
 *              is almost certainly long dead anyway.) If concurrent encoders are involved, then a
 *              barrier might be needed. Within that compute encoder, you will call -updateFence.
 *              End the compute encoder, make a blit encoder wait for the fence, do the blit, update
 *              a new fence, then make a new compute encoder, wait for the second fence, then you
 *              can continue. Possibly the second do-nothing compute encoder needs to be ended so
 *              MPS can be called. Frankly, we don't bother with blit encoders and just write a compute
 *              operation for copy / clear as needed, or better yet find a way to eliminate the
 *              clear / copy pass so we don't have to pay for it. Note: the most common use of a
 *              blit encoder, -synchronizeResource: can not encounter this problem because
 *              MPSTemporaryImages live in GPU private memory and can not be read by the CPU.
 *
 *              MPSTemporaryImages can otherwise be used wherever MPSImages are used.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSTemporaryImage : MPSImage

/*!     Get a well known MPSImageAllocator that makes MPSTemporaryImages */
+(nonnull id <MPSImageAllocator>)  defaultAllocator;


/*!
 *  @abstract   Initialize a MPSTemporaryImage for use on a MTLCommandBuffer
 *
 *  @param      commandBuffer   The MTLCommandBuffer on which the MPSTemporaryImage will be exclusively used
 *
 *  @param      imageDescriptor A valid imageDescriptor describing the MPSImage format to create.
 *
 *  @return     A valid MPSTemporaryImage.  The object will be released when the command buffer
 *              is committed. The underlying texture will become invalid before this time
 *              due to the action of the readCount property.
 *
 */
+(nonnull instancetype) temporaryImageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                        imageDescriptor: (const MPSImageDescriptor * __nonnull) imageDescriptor;


/*!
 *  @abstract       Low level interface for creating a MPSTemporaryImage using a MTLTextureDescriptor
 *  @discussion     This function provides access to MTLPixelFormats not typically covered by -initForCommandBuffer:imageDescriptor:
 *                  The feature channels will be inferred from the MTLPixelFormat without changing the width. 
 *                  The following restrictions apply:
 *  
 *                      MTLTextureType must be MTLTextureType2D or MTLTextureType2DArray
 *                      MTLTextureUsage must contain at least one of MTLTextureUsageShaderRead, MTLTextureUsageShaderWrite
 *                      MTLStorageMode must be MTLStorageModePrivate
 *                      depth must be 1
 *
 *  @param commandBuffer        The command buffer on which the MPSTemporaryImage may be used
 *  @param textureDescriptor    A texture descriptor describing the MPSTemporaryImage texture
 *
 *  @return     A valid MPSTemporaryImage.  The object will be released when the command buffer
 *              is committed. The underlying texture will become invalid before this time
 *              due to the action of the readCount property.
 */
+(nonnull instancetype) temporaryImageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                      textureDescriptor: (const MTLTextureDescriptor * __nonnull) textureDescriptor;

/*!
 *  @abstract       Low level interface for creating a MPSTemporaryImage using a MTLTextureDescriptor
 *  @discussion     This function provides access to MTLPixelFormats not typically covered by -initForCommandBuffer:imageDescriptor:
 *                  The number of images will be inferred from number of slices in the descriptor.arrayLength and
 *                  the number of feature channels.
 *
 *                  The following restrictions apply:
 *
 *                      MTLTextureType must be MTLTextureType2D or MTLTextureType2DArray
 *                      MTLTextureUsage must contain at least one of MTLTextureUsageShaderRead, MTLTextureUsageShaderWrite
 *                      MTLStorageMode must be MTLStorageModePrivate
 *
 *  @param commandBuffer        The command buffer on which the MPSTemporaryImage may be used
 *  @param textureDescriptor    A texture descriptor describing the MPSTemporaryImage texture
 *
 *  @return     A valid MPSTemporaryImage.  The object will be released when the command buffer
 *              is committed. The underlying texture will become invalid before this time
 *              due to the action of the readCount property.
 */
+(nonnull instancetype) temporaryImageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                      textureDescriptor: (const MTLTextureDescriptor * __nonnull) textureDescriptor
                                        featureChannels: (NSUInteger) featureChannels
                            MPS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Help MPS decide which allocations to make ahead of time
 *  @discussion     The texture cache that underlies the MPSTemporaryImage can automatically allocate new storage as
 *                  needed as you create new temporary images.  However, sometimes a more global view of what you
 *                  plan to make is useful for maximizing memory reuse to get the most efficient operation.
 *                  This class method hints to the cache what the list of images will be.
 *
 *                  It is never necessary to call this method. It is purely a performance and memory optimization.
 *
 *                  This method makes a conservative estimate of memory required and may not fully
 *                  cover temporary memory needs, resulting in additional allocations later that could
 *                  encounter pathological behavior, if they are too small. If the full extent and timing of the
 *                  workload is known in advance, it is recommended that MPSHintTemporaryMemoryHighWaterMark() be
 *                  used instead.
 *
 *  @param commandBuffer        The command buffer on which the MPSTemporaryImages will be used
 *  @param descriptorList       A NSArray of MPSImageDescriptors, indicating images that will be created
 */
+(void) prefetchStorageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                     imageDescriptorList: (NSArray <MPSImageDescriptor*> * __nonnull) descriptorList;

/*! Unavailable. Use temporaryImageForCommandBuffer:textureDescriptor: or -temporaryImageForCommandBuffer:imageDescriptor: instead. */
-(nonnull instancetype) initWithTexture: (nonnull id <MTLTexture>) texture
                        featureChannels: (NSUInteger) featureChannels NS_UNAVAILABLE;

/*! Unavailable. Use itemporaryImageForCommandBuffer:textureDescriptor: instead. */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                       imageDescriptor: (const MPSImageDescriptor * __nonnull) imageDescriptor      NS_UNAVAILABLE;

/*
 *  @abstract       The number of times a temporary image may be read by a MPSCNNKernel
 *                  before its contents become undefined.
 *
 *  @discussion     MPSTemporaryImages must release their underlying textures for reuse
 *                  immediately after last use. So as to facilitate *prompt* convenient 
 *                  memory recycling, each time a MPSTemporaryImage is read by a 
 *                  MPSCNNKernel -encode... method, its readCount is automatically 
 *                  decremented. When the readCount reaches 0, the underlying texture is 
 *                  automatically made available for reuse to MPS for its own needs and for 
 *                  other MPSTemporaryImages prior to return from the -encode.. function.  
 *                  The contents of the texture become undefined at this time. 
 *
 *                  By default, the readCount is initialized to 1, indicating a image that
 *                  may be overwritten any number of times, but read only once.
 *
 *                  You may change the readCount as desired to allow MPSCNNKernels to read
 *                  the MPSTemporaryImage additional times. However, it is an error to change
 *                  the readCount once it is zero. It is an error to read or write to a
 *                  MPSTemporaryImage with a zero readCount. You may set the readCount to 0
 *                  yourself to cause the underlying texture to be returned to MPS. Writing
 *                  to a MPSTemporaryImage does not adjust the readCount.
 *
 *                  The Metal API Validation layer will assert if a MPSTemporaryImage is
 *                  deallocated with non-zero readCount to help identify cases when resources
 *                  are not returned promptly.
 */
@property (readwrite, nonatomic)  NSUInteger  readCount;


@end


    
#ifdef __cplusplus
}   // extern "C"
#endif


#endif /* MPSImage_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSCore.framework/Headers/MPSKernelTypes.h
//
//  MPSKernelTypes.h
//  MPSCore
//
//  Created by Ian Ollmann on 9/25/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSKernelTypes_h
#define MPSKernelTypes_h    1

#define MPSFunctionConstantIndex               127
#define MPSBatchSizeIndex                      126

#if defined(__METAL_MACOS__) || MPS_TARGET_MAC || (defined (__i386__) || defined(__x86_64__))
#    define MPSMaxTextures                     128
#else
#    define MPSMaxTextures                      32
#endif

typedef enum
{
    MPSCustomKernelIndexDestIndex = 0,
    MPSCustomKernelIndexSrc1Index = 1,
    MPSCustomKernelIndexSrc2Index = 2,
    MPSCustomKernelIndexSrc3Index = 3,   // caution: may overlap with MPSCustomKernelIndexUserDataIndex
    // ...
    MPSCustomKernelIndexUserDataIndex = 30
}MPSCustomKernelIndex;

/*! @abstract the [[function_constant(index)]] index where the kMPSConstant is passed to the kernel */


#if defined(__METAL_VERSION__)
#   include <metal_stdlib>
    using namespace metal;
#else
#   include <simd/simd.h>
#endif

typedef struct
{
    uint16_t divisor;
    
    // numerator/divisor as  (numerator * recip + uint(addend)) >> shift.
    // Valid for all uint16_t numerator/divisor
    uint16_t recip;         // fixed point reciprocal with value [32768, 65535]
    uint16_t addend;
    uint16_t shift;
}MPSIntegerDivisionParams;

typedef struct
{
    vector_short2   kernelOrigin;   // MPS estimate of where the top left corner of the kernel will fall. May be negative !
    vector_ushort2  kernelPhase;    // for gradient filters, when stride > 1, stride kernel taps may apply to each source input. The phase gives which one corresponds to non-zero input.
    vector_ushort2  kernelSize;     // MPSCNNCustomKernel.kernelSize
    vector_short2   offset;         // MPSCNNCustomKernel.offsetAtIndex:   may be negative!
    vector_ushort2  stride;         // MPSCNNCustomKernel.strideInPixelsAtIndex:
    vector_ushort2  dilationRate;   // MPSCNNCustomKernel.dilationRateAtIndex:
    uint16_t        featureChannelOffset;   // offset into image for the first feature channel to read
    uint16_t        featureChannels; // number of actual feature channels. May be smaller than slices * 4
    uint16_t        imageArrayOffset;       // offset into batch for first image to read
    uint16_t        imageArraySize; // number of images in a MPSTextureArray, or 1 if texture2d/texture2d_array
}MPSCustomKernelSourceInfo;

typedef struct
{
    vector_ushort4  clipOrigin;                    // {x, y, slices, batch}
    vector_ushort4  clipSize;                      // {x, y, slices, batch}
    uint16_t        destinationFeatureChannels;     // raw feature channels
    uint16_t        destImageArraySize;
    uint16_t        sourceImageCount;   // number of non-auxilliary source textures. Some may be NULL.
    uint16_t        threadgroupSize;
    uint16_t        subbatchIndex;      // Batches may be split into sub-batches. The first sub-batch has index 0. They increment up from there.
    uint16_t        subbatchStride;     // The number of images handled in each sub-batch.
    // This is the number of entries in the MPSCustomKernelSourceInfo array passed as [[buffer(1)]]
    // It is not the number of images in the batch. It is 1 for unary, 2 for binary, etc.

    MPSIntegerDivisionParams    idiv;   // Used to decompose the Z grid component into feature channel and batchID
                                        // globalID.z = batchID * feature_channel_slice_count + feature_channel_slice
}MPSCustomKernelInfo;

typedef enum : uint32_t {
    MPSImageType2d,                              // texture2d                Standard Metal type
    MPSImageType2d_array,                        // texture2d_array          Standard Metal type
    MPSImageTypeArray2d,                         // texture2d[]              MPSTextureArray<texture2d>
    MPSImageTypeArray2d_array,                   // texture2d_array[]        MPSTextureArray<texture2d_array>
    MPSImageType_ArrayMask = 1,                  // type & MPSImageType_ArrayMask == true means its array (not 2d texture)
    MPSImageType_BatchMask = 2,
    MPSImageType_typeMask = 3,
    MPSImageType_noAlpha = 4,
    MPSImageType2d_noAlpha = MPSImageType2d | MPSImageType_noAlpha,
    MPSImageType2d_array_noAlpha = MPSImageType2d_array | MPSImageType_noAlpha,
    MPSImageTypeArray2d_noAlpha = MPSImageType2d | MPSImageType_BatchMask| MPSImageType_noAlpha,
    MPSImageTypeArray2d_array_noAlpha = MPSImageType2d_array | MPSImageType_BatchMask | MPSImageType_noAlpha,
}MPSImageType;

#ifdef __cplusplus
    inline MPSImageType operator|(MPSImageType a, MPSImageType b){ return MPSImageType(uint32_t(a)|uint32_t(b));}
    inline MPSImageType operator&(MPSImageType a, MPSImageType b){ return MPSImageType(uint32_t(a)&uint32_t(b));}
    inline MPSImageType operator^(MPSImageType a, MPSImageType b){ return MPSImageType(uint32_t(a)^uint32_t(b));}
    inline MPSImageType operator~(MPSImageType a){ return MPSImageType(~uint32_t(a));}
#endif /* __cplusplus */

#ifdef __METAL_VERSION__
// Metal shading language
#include <metal_stdlib>
using namespace metal;
typedef uint32_t MPSFunctionConstant;
constant MPSFunctionConstant  kMPSConstant [[function_constant(MPSFunctionConstantIndex)]];
constant MPSFunctionConstant  MPSMaxBatchSize[[function_constant(MPSBatchSizeIndex)]];
constant MPSImageType  kMPSDestTextureType = MPSImageType(kMPSConstant & 7);
constant MPSImageType  kMPSSrc1TextureType = MPSImageType((kMPSConstant >> 3) & 7);
constant MPSImageType  kMPSSrc2TextureType = MPSImageType((kMPSConstant >> 6) & 7);
constant MPSImageType  kMPSSrc3TextureType = MPSImageType((kMPSConstant >> 9) & 7);
constant MPSImageType  kMPSSrc4TextureType = MPSImageType((kMPSConstant >>12) & 7);
constant uint16_t      kMPSUserConstant = (kMPSConstant >> 16) & 0xffffU;


// template <typename T> using MPSTextureArray = array<T, MPSMaxBatchSize>;

typedef struct
{
    ushort2         globalID;        //{x, y}
    ushort2         threadgroupID;   //{x, y}
    ushort2         localID;         //{x, y}
    ushort2         gridSize;        //{x, y}
    uniform<ushort> imageID;
    uniform<ushort> imageCount;
    uniform<ushort> threadgroupStorageSize;
}ThreadgroupInfo;

static inline ThreadgroupInfo MPSInitThreadgroupInfo( ushort3 globalID,
                                                      ushort3 threadgroupID,
                                                      ushort3 localID,
                                                      constant MPSCustomKernelInfo &info )
{
    return (ThreadgroupInfo){
        .globalID = globalID.xy,
        .threadgroupID = threadgroupID.xy,
        .localID = localID.xy,
        .gridSize = info.clipSize.xy,
        .imageID = make_uniform(globalID.z),            // Caution: Assumes threadgroup is 2D: {X, Y, 1}. MPSNNSimpleCustomKernel does this automatically.
        .imageCount = make_uniform(info.clipSize.z),
        .threadgroupStorageSize = make_uniform(info.threadgroupSize)
    };
}

// Decompose a ushort3 globalID on a grid {width, height, feature_channel_slice_count * batch_image_count}
// into a ushort3 {x, y, feature_channel} and a uniform<ushort> batch_image_id.
//
// The MPSIntegerDivisionParams are initialized on the CPU as MPSFindIntegerDivisionParams(feature_channel_slice_count)
// Caution: for the uniform<> guarantee to be valid, the threadgroup size must be 1 or featureChannelSliceCount, so
// that all threads in the threadgroup are in the same image.  Behavior is undefined otherwise.
static inline uniform<ushort>  MPSDecomposeGlobalID( constant MPSIntegerDivisionParams &  params, thread ushort3 * globalID )
{
    ushort3 gid = *globalID;
    
    // batch image = globalID.z / feature_channel_slice_count
    ushort image = (gid.z * params.recip + (uint) params.addend) >> params.shift;  //imad(16,16,32), shift
    
    // feature channel = globalID.z % feature_channel_slice_count;
    ushort featureChannel = gid.z - image * params.divisor;                        //imad16
    
    // reduce globalID to {X, Y, feature channel}
    gid.z = featureChannel;
    
    // write out results
    *globalID = gid;
    return make_uniform(image);
}

constant bool kMPSDestIs2d = (kMPSDestTextureType & MPSImageType_typeMask) == MPSImageType2d;
constant bool kMPSDestIs2dArray = (kMPSDestTextureType & MPSImageType_typeMask) == MPSImageType2d_array;
constant bool kMPSDestIsArray2d = (kMPSDestTextureType & MPSImageType_typeMask) == MPSImageTypeArray2d;
constant bool kMPSDestIsArray2dArray = (kMPSDestTextureType & MPSImageType_typeMask) == MPSImageTypeArray2d_array;

constant bool kMPSSrc1Is2d = (kMPSSrc1TextureType & MPSImageType_typeMask) == MPSImageType2d;
constant bool kMPSSrc1Is2dArray = (kMPSSrc1TextureType & MPSImageType_typeMask) == MPSImageType2d_array;
constant bool kMPSSrc1IsArray2d = (kMPSSrc1TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d;
constant bool kMPSSrc1IsArray2dArray = (kMPSSrc1TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d_array;

constant bool kMPSSrc2Is2d = (kMPSSrc2TextureType & MPSImageType_typeMask) == MPSImageType2d;
constant bool kMPSSrc2Is2dArray = (kMPSSrc2TextureType & MPSImageType_typeMask) == MPSImageType2d_array;
constant bool kMPSSrc2IsArray2d = (kMPSSrc2TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d;
constant bool kMPSSrc2IsArray2dArray = (kMPSSrc2TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d_array;

constant bool kMPSSrc3Is2d = (kMPSSrc3TextureType & MPSImageType_typeMask) == MPSImageType2d;
constant bool kMPSSrc3Is2dArray = (kMPSSrc3TextureType & MPSImageType_typeMask) == MPSImageType2d_array;
constant bool kMPSSrc3IsArray2d = (kMPSSrc3TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d;
constant bool kMPSSrc3IsArray2dArray = (kMPSSrc3TextureType & MPSImageType_typeMask) == MPSImageTypeArray2d_array;


template <class _type, access _access, class _type4>
class _MPSSrcImage
{
    private:
    thread texture2d<_type, _access> &                          _img;
    thread texture2d_array<_type, _access> &                    _imgA;
    array_ref<texture2d<_type, _access>>                        _Aimg;
    array_ref<texture2d_array<_type, _access>>                  _AimgA;
    const int                                                   _texType;
    constant MPSCustomKernelSourceInfo &                        _info;
    
    public:
    _MPSSrcImage( thread texture2d<_type, _access> & img,
                  thread texture2d_array<_type, _access> & imgA,
                  array_ref<texture2d<_type, _access>> Aimg,
                  array_ref<texture2d_array<_type, _access>> AimgA,
                  const int texType,
                  constant MPSCustomKernelSourceInfo &info ) : _img(img), _imgA(imgA), _Aimg(Aimg), _AimgA(AimgA), _texType(texType), _info(info) { }
    
    // image metadata
    //  The imageIndex parameter for width, height and slices, can be ignored.
    //  It may save some registers / memory access if you use the same where.w as your read/sample/gather call
    ushort  get_width(uint imageIndex = 0) const;
    ushort  get_height(uint imageIndex = 0) const;
    ushort  get_slices(uint imageIndex = 0) const;
    ushort  get_image_count(void) const;
    ushort4 get_size(uint imageIndex = 0) const { return ushort4(get_width(imageIndex), get_height(imageIndex), get_slices(imageIndex), get_image_count()); }
    ushort  get_feature_channels(void) const {return _info.featureChannels;}
    
    short2  get_offset(void) const { return _info.offset; }
    ushort  get_slice_offset(void) const { return _info.featureChannelOffset; }
    uniform<ushort> get_image_offset(void) const { return make_uniform(_info.imageArrayOffset); }
    short4  get_all_offsets(void) const { return short4(get_offset(), short(get_slice_offset()), short(get_image_offset())); }
    ushort2 get_stride(void) const { return _info.stride; }
    ushort2 get_dilation_rate(void) const { return _info.dilationRate; }
    ushort2 get_kernel_size(void) const { return _info.kernelSize; }
    ushort2 get_kernel_phase(void) const { return _info.kernelPhase; }
    float2  get_kernel_origin(ushort2 globalID ) const { return float2(_info.kernelOrigin) + float2(get_stride() * globalID); }
    
    // Some platforms require that all threads access the same image in the
    // array of textures, or else performance is dreadful. We have broken out
    // the 4 dimensional coordinate into two 2D coordinates to contain
    // where = {X,Y} and which = {feature channel slice, batch image}.
    // It wasn't possible to declare just the w component of a float4 to be uniform<>
    // See small uniform tutorial below
    _type4  sample( sampler s, float2 where, ushort slice, uniform<ushort> image, int2 offset = int2(0) ) const;
    _type4  gather( sampler s, float2 where, ushort slice, uniform<ushort> image, int2 offset = int2(0), component c = component::x ) const;
    _type4  read( ushort2 where, ushort slice, uniform<ushort> image ) const;
    _type4  read( uint2 where, uint slice, uniform<uint> image ) const;
};

//
//  Small Uniform tutorial:
//      A uniform<Type> has the same value across all threads in the threadgroup.
//      In certain cases, it allows the compiler to significantly optimize code,
//      and is essential to good performance here. (Factor of 3 slowdowns otherwise.)
//
//      Currently it is not possible to do memberwise access of uniform vector types.
//          Cast or assign to a non-uniform type and do memberwise access to that
//
//      If you need to cast a non-uniform type to a uniform type, use  uniform<T> make_uniform(T)
//      It is up to you to guarantee that the value is actually uniform.
//
//      If your threadgroup is two dimensional  {X, Y, 1}, then your globalID.z
//      component is in practice uniform. You can pull it out of the vector
//      and declare it to be such. The MPSNNSimpleCustomKernel does this.
//


#define __MPS_TEX_TYPE_SELECT( _2d, _2da, _a2d, _a2da, _default )       \
    switch(_texType & MPSImageType_typeMask){ /* _texType is known at compile time, so */   \
                      /* this switch should be optimized away. */       \
        case MPSImageType2d: return (_2d);                              \
        case MPSImageType2d_array: return (_2da);                       \
        case MPSImageTypeArray2d: return (_a2d);                        \
        case MPSImageTypeArray2d_array: return (_a2da);                 \
        default: return (_default);                                     \
    }

// 1- and 2-channel image formats always set alpha = 1, which is the wrong
// thing to do for feature channel based images. These need to be zero.
#define __MPS_TEX_TYPE_SELECT_TYPE4( _2d, _2da, _a2d, _a2da, _default )     \
     { _type4 _r;                                                           \
        switch(_texType & MPSImageType_typeMask){                           \
        /* _texType is known at compile time, so */                         \
        /* this switch should be optimized away. */                         \
            case MPSImageType2d: _r = (_2d); break;                         \
            case MPSImageType2d_array: _r = (_2da); break;                  \
            case MPSImageTypeArray2d: _r = (_a2d); break;                   \
            case MPSImageTypeArray2d_array: _r = (_a2da); break;            \
            default: _r = (_default); break;                                \
        }                                                                   \
        if( _texType & MPSImageType_noAlpha)                                \
            _r.w = 0;                                                       \
        return _r;  }

//ushort  get_width(uint imageIndex = 0) const;
template<class _type, access _access, class _type4> ushort _MPSSrcImage<_type, _access, _type4>::get_width(uint index) const {
    __MPS_TEX_TYPE_SELECT( _img.get_width(), _imgA.get_width(), _Aimg[index].get_width(), _AimgA[index].get_width(), 0);
}

//ushort  get_height(uint imageIndex = 0) const;
template<class _type, access _access, class _type4> ushort  _MPSSrcImage<_type, _access, _type4>::get_height(uint index) const {
    __MPS_TEX_TYPE_SELECT( _img.get_height(), _imgA.get_height(), _Aimg[index].get_height(), _AimgA[index].get_height(), 0);
}

//ushort  get_slices(uint imageIndex = 0) const;
template<class _type, access _access, class _type4> ushort _MPSSrcImage<_type, _access, _type4>::get_slices(uint index) const {
    __MPS_TEX_TYPE_SELECT( (uint16_t) 1, _imgA.get_array_size(), (uint16_t) 1, _AimgA[index].get_array_size(), 0);
}

//ushort  get_image_count(void) const;
template<class _type, access _access, class _type4> ushort _MPSSrcImage<_type, _access, _type4>::get_image_count(void) const {
    __MPS_TEX_TYPE_SELECT( (uint16_t) 1, (uint16_t) 1, _info.imageArraySize, _info.imageArraySize, 0);
}

//_type4   sample( sampler s, float2 where, ushort slice, uniform<ushort> image, int2 offset = int2(0) );
template<class _type, access _access, class _type4> _type4 _MPSSrcImage<_type, _access, _type4>::sample( sampler s, float2 where, ushort slice, uniform<ushort> image, int2 offset ) const {
    __MPS_TEX_TYPE_SELECT_TYPE4( _img.sample(s, where, offset),
                                 _imgA.sample(s, where, slice, offset),
                                 _Aimg[image].sample(s, where, offset),
                                 _AimgA[image].sample(s, where, slice, offset),
                                0);
}


//_type4   gather( sampler s, float2 where, ushort2 which, int2 offset = int2(0), component c = component::x );
template<class _type, access _access, class _type4> _type4 _MPSSrcImage<_type, _access, _type4>::gather( sampler s, float2 where, ushort slice, uniform<ushort> image, int2 offset, component c ) const {
    switch(c)
    { // this switch should be optimized away, since c is known at compile time.
        case component::x:
        __MPS_TEX_TYPE_SELECT_TYPE4( _img.gather(s, where, offset, component::x),
                                     _imgA.gather(s, where, slice, offset, component::x),
                                     _Aimg[image].gather(s, where, offset, component::x),
                                     _AimgA[image].gather(s, where, slice, offset, component::x),
                                     0);
        break;
        case component::y:
        __MPS_TEX_TYPE_SELECT_TYPE4( _img.gather(s, where, offset, component::y),
                                     _imgA.gather(s, where, slice, offset, component::y),
                                     _Aimg[image].gather(s, where, offset, component::y),
                                     _AimgA[image].gather(s, where, slice, offset, component::y),
                                     0);
        break;
        case component::z:
        __MPS_TEX_TYPE_SELECT_TYPE4( _img.gather(s, where, offset, component::z),
                                     _imgA.gather(s, where, slice, offset, component::z),
                                     _Aimg[image].gather(s, where, offset, component::z),
                                     _AimgA[image].gather(s, where, slice, offset, component::z),
                                    0);
        break;
        case component::w:
        __MPS_TEX_TYPE_SELECT_TYPE4( _img.gather(s, where, offset, component::w),
                                     _imgA.gather(s, where, slice, offset, component::w),
                                     _Aimg[image].gather(s, where, offset, component::w),
                                     _AimgA[image].gather(s, where, slice, offset, component::w),
                                     0);
        break;
    }
}

//_type4   read( ushort4 where );
template<class _type, access _access, class _type4> _type4 _MPSSrcImage<_type, _access, _type4>::read( ushort2 where, ushort slice, uniform<ushort> image ) const {
    __MPS_TEX_TYPE_SELECT_TYPE4( _img.read(where),
                                _imgA.read(where, slice),
                                _Aimg[image].read( where),
                                _AimgA[image].read( where, slice),
                                0);
}

//_type4   read( uint4 where );
template<class _type, access _access, class _type4> _type4 _MPSSrcImage<_type, _access, _type4>::read( uint2 where, uint slice, uniform<uint> image ) const {
    __MPS_TEX_TYPE_SELECT_TYPE4( _img.read(where),
                                _imgA.read(where, slice),
                                _Aimg[image].read( where),
                                _AimgA[image].read( where, slice),
                                0);
}





template <class _type, class _type4>
class _MPSDestImage
{
    private:
    thread texture2d<_type, access::write> &                            _img;
    thread texture2d_array<_type, access::write> &                      _imgA;
#if defined(__METAL_MACOS__)    // writable arrays of texture are not an iOS feature
    array_ref<texture2d<_type, access::write>>                          _Aimg;
    array_ref<texture2d_array<_type, access::write>>                    _AimgA;
#endif
    const int                                                           _texType;
    constant MPSCustomKernelInfo &                                      _info;
    
    public:
    _MPSDestImage(thread texture2d<_type, access::write> & img,
                  thread texture2d_array<_type, access::write> & imgA,
#if defined(__METAL_MACOS__)    // writable arrays of texture are not an iOS feature
                  array_ref<texture2d<_type, access::write>> Aimg,
                  array_ref<texture2d_array<_type, access::write>> AimgA,
#endif
                  const int texType,
                  constant MPSCustomKernelInfo &info ) : _img(img), _imgA(imgA),
#if defined(__METAL_MACOS__)    // writable arrays of texture are not an iOS feature
                                                         _Aimg(Aimg), _AimgA(AimgA),
#endif
                                                         _texType(texType), _info(info) { }
    
    // image metadata
    // The imageIndex parameter for width, height and slices, can be ignored.
    // It may save some registers / memory access if you use the same where.w as your write call
    ushort  get_width(uint imageIndex = 0) const;
    ushort  get_height(uint imageIndex = 0) const;
    ushort  get_slices(uint imageIndex = 0) const;
    ushort  get_image_count(void) const;
    ushort4 get_size(uint imageIndex = 0){ return ushort4(get_width(imageIndex), get_height(imageIndex), get_slices(imageIndex), get_image_count()); }
    ushort  get_feature_channels(void) const {return _info.destinationFeatureChannels;}
    
    uniform<ushort> get_image_offset(void)const { return make_uniform(_info.clipOrigin.w);}
    ushort  get_slice_offset(void)const { return _info.clipOrigin.z;}
    ushort4 get_clip_origin(void){ return _info.clipOrigin;}   // {x,y, destinationFeatureChannelOffset}
    ushort4 get_clip_size(void){ return _info.clipSize;}
    
    void    write( _type4 v, ushort2 where, ushort slice, uniform<ushort> image );
    void    write( _type4 v, uint2 where, uint slice, uniform<uint> image );
};

#if defined(__METAL_MACOS__)    // writable arrays of texture are not an iOS feature
#   define __MPS_DEST_TEX_TYPE_SELECT( _2d, _2da, _a2d, _a2da )     \
            switch(_texType & MPSImageType_typeMask){               \
                case MPSImageType2d: return (_2d);                  \
                case MPSImageType2d_array: return (_2da);           \
                case MPSImageTypeArray2d: return (_a2d);            \
                case MPSImageTypeArray2d_array: return (_a2da);     \
    }
#else
#   define __MPS_DEST_TEX_TYPE_SELECT( _2d, _2da, _a2d, _a2da )                         \
    {                                                                                   \
        return ((_texType & MPSImageType_typeMask) == MPSImageType2d) ? (_2d) : (_2da); \
    }
#endif


//ushort  get_width(uint imageIndex = 0) const;
template<class _type, class _type4> ushort _MPSDestImage<_type, _type4>::get_width(uint index) const {
    __MPS_DEST_TEX_TYPE_SELECT( _img.get_width(), _imgA.get_width(), _Aimg[index].get_width(), _AimgA[index].get_width());
}

//ushort  get_height(uint imageIndex = 0) const;
template<class _type, class _type4> ushort _MPSDestImage<_type, _type4>::get_height(uint index) const {
    __MPS_DEST_TEX_TYPE_SELECT( _img.get_height(), _imgA.get_height(), _Aimg[index].get_height(), _AimgA[index].get_height());
}

//ushort  get_slices(uint imageIndex = 0) const;
template<class _type, class _type4> ushort _MPSDestImage<_type, _type4>::get_slices(uint index) const {
    __MPS_DEST_TEX_TYPE_SELECT( ushort(1), _imgA.get_array_size(), ushort(1), _AimgA[index].get_array_size());
}

//ushort  get_image_count(void) const;
template<class _type, class _type4> ushort _MPSDestImage<_type, _type4>::get_image_count(void) const {
    __MPS_DEST_TEX_TYPE_SELECT( (uint16_t) 1, (uint16_t) 1, _info.destImageArraySize, _info.destImageArraySize);
}

//void    write( _vec4 v, ushort2 where, ushort slice, uniform<ushort> image );
template<class _type, class _type4> void _MPSDestImage<_type, _type4>::write( _type4 v, ushort2 where, ushort slice, uniform<ushort> image ) {
    __MPS_DEST_TEX_TYPE_SELECT( _img.write(v,where),
                                _imgA.write(v, where, slice),
                                _Aimg[image].write(v, where),
                                _AimgA[image].write(v, where, slice));
}

//void    write( _vec4 v, uint2 where, uint slice, uniform<uint> image );
template<class _type, class _type4> void _MPSDestImage<_type, _type4>::write( _type4 v, uint2 where, uint slice, uniform<uint> image ) {
    __MPS_DEST_TEX_TYPE_SELECT( _img.write(v,where),
                                _imgA.write(v, where, slice),
                                _Aimg[image].write(v, where),
                                _AimgA[image].write(v, where, slice));
}


#define MPS_CUSTOM_FUNCTION  template <access _srcAccess, class MPSType, class MPSType2, class MPSType3, class MPSType4>         \
                                void __attribute__((__always_inline__))

#define MPSSrcImage     _MPSSrcImage<MPSType, _srcAccess, MPSType4>
#define MPSDestImage    _MPSDestImage<MPSType, MPSType4>

#   define __MPS_SRC_IMAGE_ARG( _name, _access, _type, _index  )                                                                                                                \
        texture2d<_type, access::_access>  _name  [[texture(MPSCustomKernelIndexSrc##_index##Index * MPSMaxBatchSize), function_constant(kMPSSrc##_index##Is2d)]],                                       \
        texture2d_array<_type, access::_access>  _name##A  [[texture(MPSCustomKernelIndexSrc##_index##Index * MPSMaxBatchSize), function_constant(kMPSSrc##_index##Is2dArray)]],                         \
        array_ref<texture2d<_type, access::_access>>  A##_name [[texture(MPSCustomKernelIndexSrc##_index##Index * MPSMaxBatchSize), function_constant(kMPSSrc##_index##IsArray2d)]] [[array_ref_size(MPSMaxBatchSize)]],                \
        array_ref<texture2d_array<_type, access::_access>> A##_name##A [[texture(MPSCustomKernelIndexSrc##_index##Index * MPSMaxBatchSize), function_constant(kMPSSrc##_index##IsArray2dArray)]] [[array_ref_size(MPSMaxBatchSize)]],  \
        constant MPSCustomKernelSourceInfo & _name##Info [[buffer(MPSCustomKernelIndexSrc##_index##Index)]]

#   define __MPS_SRC_IMAGE_PARAMS(_name, _index)     _name, _name##A, A##_name, A##_name##A, kMPSSrc##_index##TextureType, _name##Info

#if defined(__METAL_MACOS__)
//  macOS:  use texture arrays
#   define __MPS_DEST_IMAGE_ARG( _name, _type  )                                                                                                                \
        texture2d<_type, access::write>  _name  [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIs2d)]],                                         \
        texture2d_array<_type, access::write>  _name ## A  [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIs2dArray)]],                         \
        array_ref<texture2d<_type, access::write>> A##_name [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIsArray2d)]] [[array_ref_size(MPSMaxBatchSize)]],                  \
        array_ref<texture2d_array<_type, access::write>> A##_name##A [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIsArray2dArray)]] [[array_ref_size(MPSMaxBatchSize)]],    \
        constant MPSCustomKernelInfo & _name ## Info [[buffer(MPSCustomKernelIndexDestIndex)]]

#   define __MPS_DEST_IMAGE_PARAMS(_name)     _name, _name##A, A##_name, A##_name##A, kMPSDestTextureType, _name ## Info
#else
//  not macOS: writable texture arrays are not a feature
#   define __MPS_DEST_IMAGE_ARG( _name, _type  )                                                                                            \
        texture2d<_type, access::write>  _name  [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIs2d)]],                     \
        texture2d_array<_type, access::write>  _name ## A  [[texture(MPSCustomKernelIndexDestIndex * MPSMaxBatchSize), function_constant(kMPSDestIs2dArray)]],     \
        constant MPSCustomKernelInfo & _name ## Info [[buffer(MPSCustomKernelIndexDestIndex)]]

#   define __MPS_DEST_IMAGE_PARAMS(_name)     _name, _name##A, kMPSDestTextureType, _name ## Info
#endif


//  _func           function name to inline
//  _access         read or sample
//  _type           float or half
#   define __MPS_MAKE_CUSTOM_KERNEL( _func, _access, _type)                                                                                                 \
    kernel void _func ## _MPSCustomV1_ ## _ ##_access ## _ ##  _type (                                                                   \
        __MPS_DEST_IMAGE_ARG( dest, _type),                                                                                                                 \
        __MPS_SRC_IMAGE_ARG( src, _access, _type, 1 ),                                                                                                      \
        device void * userData [[buffer(MPSCustomKernelIndexUserDataIndex)]],                                                                                    \
        threadgroup void * threadgroupData [[threadgroup(MPSCustomKernelIndexUserDataIndex)]],                                                                   \
        ushort3 globalID [[thread_position_in_grid]],                                                                                                       \
        ushort3 threadgroupID [[threadgroup_position_in_grid]],                                                                                             \
        ushort3 localID [[thread_position_in_threadgroup]] )                                                                                                \
    {                                                                                                                                                       \
        if( any(globalID >= destInfo.gridSize.xyz) )  return;                                                                                               \
        ThreadgroupInfo threadgroupInfo = MPSInitThreadgroupInfo( globalID, threadgroupID, localID, destInfo );                                             \
        _MPSSrcImage<_type, access::_access, _type ## 4> srcImage( __MPS_SRC_IMAGE_PARAMS(src, 1));                                                         \
        _MPSDestImage<_type, _type ## 4> destImage( __MPS_DEST_IMAGE_PARAMS(dest));                                                                         \
        _func<access::_access, _type, _type ## 2, _type ## 3, _type ## 4>                                                                                   \
            (srcImage, destImage, userData, threadgroupData, threadgroupInfo);                                                                              \
    }


#   define MPS_MAKE_CUSTOM_KERNELS(_funcName)                                       \
    __MPS_MAKE_CUSTOM_KERNEL( _funcName, sample, half)                              \
    __MPS_MAKE_CUSTOM_KERNEL( _funcName, sample, float)



#else
// Not metal shading language
#   include <stdint.h>
typedef int64_t  MPSFunctionConstant;
typedef uint32_t MPSFunctionConstantInMetal;
static const MPSFunctionConstant    MPSFunctionConstantNone = -1LL;


#   ifdef __cplusplus
static inline MPSFunctionConstant MPSMakeFunctionConstant( uint16_t    userValue,
                                                           MPSImageType destType,
                                                           MPSImageType src1Type,
                                                           MPSImageType src2Type = MPSImageType2d,
                                                           MPSImageType src3Type = MPSImageType2d,
                                                           MPSImageType src4Type = MPSImageType2d )
{
    MPSFunctionConstant result = uint32_t(destType) & 7;
    result |= (uint32_t(src1Type) & 7) << 3;
    result |= (uint32_t(src2Type) & 7) << 6;
    result |= (uint32_t(src3Type) & 7) << 9;
    result |= (uint32_t(src4Type) & 7) << 12;
    result |= uint32_t(userValue) << 16;
    return result;
}

#   endif /* __cplusplus */

#ifdef __cplusplus
extern "C" {
#endif

// a/b = (a * recip + addend) >> shift      imad(a, recip, addend) >> shift
// valid for all uint16_t a and b
// div/0 is returned as div/1
static inline MPSIntegerDivisionParams   MPSFindIntegerDivisionParams( uint16_t divisor )
{
    if( divisor < 2 )
        return (MPSIntegerDivisionParams){ .divisor = 1, .recip = 1, .addend = 0, .shift = 0};
    
    // calculate high precision fixed point reciprocal
    uint32_t reciprocal = 0x80000000U / divisor;
    
    // reduce to 16 bits
    uint32_t zeros = (uint32_t) __builtin_clz(reciprocal);  // count leading zero bits
    reciprocal >>= 16 - zeros;
    
    MPSIntegerDivisionParams result;
    result.shift = (uint16_t) (31 - (16-zeros));
    
    // Find addend
    // b/a = (b * r + c) >> n
    // (b << n)/a = b * r + c
    // (b << n - b * r * a)/a = c           // rounding direction unimportant here.
    // We choose 65536 as our largest possible b. Really USHRT_MAX is, but 65536 is a bit nicer to work with.
    // The b << n terms always overflows so we omit it and rely on modulo behavior in subtraction to save us.
    // The 65536 * reciprocal * divisor also overflows. The modulo remainder is what we want.
    // tested against all combinations of a & b.
    uint32_t addend = (-65536U * reciprocal * divisor /*overflows*/ ) / divisor;
    
    // make sure they fit in uint16_t
    if( addend > reciprocal )
    {
        addend -= reciprocal;
        reciprocal++;
    }
    
    //return results
    result.addend = (uint16_t) addend;
    result.recip = (uint16_t) reciprocal;
    result.divisor = divisor;
    return result;
}

typedef struct MPSCustomKernelArgumentCount
{
    unsigned long   destinationTextureCount;        ///< must be 1. Can't have 2 or more. If you have 0, this is 1.
    unsigned long   sourceTextureCount;             ///< number of source textures. These textures will be scaled by batchCount.
    unsigned long   broadcastTextureCount;          ///< number of source textures shared across all parts of a batch
}MPSCustomKernelArgumentCount;

/*! @abstract  maximum allowed batch size   */
static inline unsigned long MPSGetCustomKernelMaxBatchSize( MPSCustomKernelArgumentCount  c ){ return (MPSMaxTextures - c.broadcastTextureCount) / (1 + c.sourceTextureCount);}

/*! @abstract  The index of the first destination texture argument   */
static inline unsigned long MPSGetCustomKernelBatchedDestinationIndex( MPSCustomKernelArgumentCount c ){ return 0;}

/*! @abstract  The index of the ith batched source texture argument  */
static inline unsigned long MPSGetCustomKernelBatchedSourceIndex( MPSCustomKernelArgumentCount c, unsigned long sourceIndex )
{
    unsigned long maxBatchSize = MPSGetCustomKernelMaxBatchSize(c);
    return (sourceIndex+1) * maxBatchSize;
}

/*! @abstract The index of the ith non-batched source texture argument.
 *  @discussion  A non-batched source is one that is shared for all items in a batch   */
static inline unsigned long MPSGetCustomKernelBroadcastSourceIndex( MPSCustomKernelArgumentCount c, unsigned long sourceIndex )
{
    unsigned long maxBatchSize = MPSGetCustomKernelMaxBatchSize(c);
    return (c.sourceTextureCount+1) * maxBatchSize + sourceIndex;
}

#   ifdef __cplusplus
}
#   endif

#endif /* __METAL_VERSION__ */


#endif /* MPSKernelTypes_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageStatistics.h
/*!
 *  @header MPSImageStatistics.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2016 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders image statistics filters
 */

#ifndef MPS_MPSImageStatistics_h
#define MPS_MPSImageStatistics_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>


/*!
 *  @class      MPSImageStatisticsMinAndMax
 *  @discussion The MPSImageStatisticsMinAndMax computes the minimum and maximum pixel values for a given region of an image.
 *              The min and max values are written to the destination image at the following pixel locations:
 *                  - min value is written at pixel location (0, 0)
 *                  - max value is written at pixel location (1, 0)
 *              
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageStatisticsMinAndMax : MPSUnaryImageKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 * 
 *              The clipRect specified in MPSUnaryImageKernel is used to control the origin in the destination texture
 *              where the min, max values are written.  The clipRect.width must be >=2.  The clipRect.height must be >= 1.
 *
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*!
 *  @abstract Specifies information to apply the statistics min-max operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageStatisticsMinAndMax object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageStatisticsMinAndMax */


/*!
 *  @class      MPSImageStatisticsMeanAndVariance
 *  @discussion The MPSImageStatisticsMeanAndVariance computes the mean and variance for a given region of an image.
 *              The mean and variance values are written to the destination image at the following pixel locations:
 *                  - mean value is written at pixel location (0, 0)
 *                  - variance value is written at pixel location (1, 0)
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageStatisticsMeanAndVariance : MPSUnaryImageKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 *              The clipRect specified in MPSUnaryImageKernel is used to control the origin in the destination texture
 *              where the mean value is written.
 *
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*!
 *  @abstract Specifies information to apply the statistics mean operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageStatisticsMeanAndVariance object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageStatisticsMeanAndVariance */


/*!
 *  @class      MPSImageStatisticsMean
 *  @discussion The MPSImageStatisticsMean computes the mean for a given region of an image.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageStatisticsMean : MPSUnaryImageKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 *              The clipRect specified in MPSUnaryImageKernel is used to control the origin in the destination texture
 *              where the mean value is written.
 *
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*!
 *  @abstract Specifies information to apply the statistics mean operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageStatisticsMean object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageStatisticsMean */


#endif  /* MPS_MSImageStatistics_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageCopy.h
/*!
 *  @header MPSImageCopy.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders histogram filters
 */

#ifndef MPS_MPSImageCopy_h
#define MPS_MPSImageCopy_h

#include <MPSCore/MPSImage.h>
#include <MPSImage/MPSImageKernel.h>
#include <MPSMatrix/MPSMatrix.h>
#include <simd/simd.h>

/*!
 *  @class      MPSImageCopyToMatrix
 *  @discussion The MPSImageCopyToMatrix copies image data to a MPSMatrix.
 *              The image data is stored in a row of a matrix.  The dataLayout
 *              specifies the order in which the feature channels in the MPSImage
 *              get stored in the matrix.  If MPSImage stores a batch of images,
 *              the images are copied into multiple rows, one row per image.
 *
 *              The number of elements in a row in the matrix must be >= image width * 
 *              image height * number of featureChannels in the image.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageCopyToMatrix : MPSKernel

/*! @property   destinationMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the destination matrix, at which to
 *              start writing results.  This property is modifiable and defaults
 *              to [0, 0] at initialization time.  If a different origin is desired
 *              then this should be modified prior to encoding the kernel.  The z
 *              value must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin destinationMatrixOrigin;

/*! @property   destinationMatrixBatchIndex
 *
 *  @discussion The index of the destination matrix in the batch.  This property is
 *              modifiable and defaults to 0 at initialization time.  
 */
@property (readwrite, nonatomic) NSUInteger destinationMatrixBatchIndex;

/*! @property   dataLayout
 *  @abstract   The data layout to use
 *  @discussion Returns the data layout.  When copying from a MPSImage to a MPSMatrix, this
 *              describes the order in which the image values are stored in the buffer associated
 *              with the MPSMatrix.
 *              Default: MPSDataLayoutFeatureChannelsxHeightxWidth
 */
@property (readonly, nonatomic)  MPSDataLayout dataLayout;

/*!
 *  @abstract Initialize a MPSMatrixCopy object on a device
 *  @param    device        The device the kernel will run on
 *  @param    dataLayout    The data layout
 *  @return   A valid MPSMatrixCopy object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            dataLayout: (MPSDataLayout) dataLayout NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract Encode a kernel that copies a MPSImage to a MPSMatrix into a command buffer
 *            using a MTLComputeCommandEncoder.
 *  @discussion The kernel copies feature channels from sourceImage to the buffer
 *              associated with destinationMatrix.  The kernel will not begin to execute until
 *              after the command buffer has been enqueued and committed.
 *
 *              NOTE: The destinationMatrix.dataType must match the feature channel data type in sourceImage.
 *
 *  @param  commandBuffer       A valid MTLCommandBuffer.
 *  @param  sourceImage         A valid MPSImage describing the image to copy from.
 *  @param  destinationMatrix   A valid MPSMatrix or MPSTemporaryMatrix object describing the matrix to copy to.
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (nonnull MPSImage *) sourceImage
            destinationMatrix: (nonnull MPSMatrix *) destinationMatrix
        MPS_SWIFT_NAME(encode(commandBuffer:sourceImage:destinationMatrix:));

/*!
 *  @abstract Encode a kernel that copies a MPSImageBatch to a MPSMatrix into a command buffer
 *            using a MTLComputeCommandEncoder.
 *  @discussion The kernel copies feature channels from sourceImage to the buffer
 *              associated with destinationMatrix.  The kernel will not begin to execute until
 *              after the command buffer has been enqueued and committed.
 *              Each image will be copied to its own row in the matrix, starting with row
 *              destinationMatrixOrigin.x.
 *
 *              NOTE: The destinationMatrix.dataType must match the feature channel data type in sourceImage.
 *              NOTE: All the images in the source batch should be of the same size and have numberOfImages = 1.
 *
 *
 *  @param  commandBuffer       A valid MTLCommandBuffer.
 *  @param  sourceImages        A valid MPSImageBatch describing the images to copy from.
 *  @param  destinationMatrix   A valid MPSMatrix or MPSTemporaryMatrix object describing the matrix to copy to.
 *
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceImages: (nonnull MPSImageBatch *) sourceImages
                 destinationMatrix: (nonnull MPSMatrix *) destinationMatrix
    MPS_SWIFT_NAME(encodeBatch(commandBuffer:sourceImages:destinationMatrix:))
    MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));




@end  /* MPSImageCopy */

/*!
 *  @class      MPSMatrixCopyToImage
 *  @discussion The MPSMatrixCopyToImage copies matrix data to a MPSImage.
 *              The operation is the reverse of MPSImageCopyToMatrix.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0))
@interface  MPSMatrixCopyToImage : MPSKernel

/*! @property   sourceMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the source matrix.
 *              This property is modifiable and defaults
 *              to [0, 0] at initialization time.  If a different origin is desired
 *              then this should be modified prior to encoding the kernel.  The z
 *              value must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin sourceMatrixOrigin;

/*! @property   sourceMatrixBatchIndex
 *
 *  @discussion The index of the source matrix in the batch.  This property is
 *              modifiable and defaults to 0 at initialization time.
 */
@property (readwrite, nonatomic) NSUInteger sourceMatrixBatchIndex;

/*! @property   dataLayout
 *  @abstract   The data layout to use
 *  @discussion Returns the data layout.  When copying from a MPSMatrix to a MPSImage, this
 *              describes the order in which the image values are to be stored in the buffer associated
 *              with the MPSMatrix.
 *              Default: MPSDataLayoutFeatureChannelsxHeightxWidth
 */
@property (readonly, nonatomic)  MPSDataLayout dataLayout;

/*!
 *  @abstract Initialize a MPSMatrixCopyToImage object on a device
 *  @param    device        The device the kernel will run on
 *  @param    dataLayout    The data layout
 *  @return   A valid MPSMatrixCopyToImage object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            dataLayout: (MPSDataLayout) dataLayout NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract Encode a kernel that copies a MPSMatrix to a MPSImage into a command buffer
 *            using a MTLComputeCommandEncoder.
 *  @discussion The kernel copies feature channels from sourceMatrix to the destinationImage.
 *              The kernel will not begin to execute until
 *              after the command buffer has been enqueued and committed.
 *
 *              NOTE: The sourceMatrix.dataType must match the feature channel data type in destinationImage.
 *
 *  @param  commandBuffer       A valid MTLCommandBuffer.
 *  @param  sourceMatrix        A valid MPSMatrix or MPSTemporaryMatrix object describing the source matrix.
 *  @param  destinationImage    A valid MPSImage describing the image to copy to.
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (nonnull MPSMatrix *) sourceMatrix
             destinationImage: (nonnull MPSImage *) destinationImage
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:destinationImage:));


/*!
 *  @abstract Encode a kernel that copies a MPSMatrix to a MPSImageBatch into a command buffer
 *            using a MTLComputeCommandEncoder.
 *  @discussion The kernel copies feature channels from sourceImage to the buffer
 *              associated with destinationMatrix.  The kernel will not begin to execute until
 *              after the command buffer has been enqueued and committed.
 *              Each image will be copied to its own row in the matrix, starting with row
 *              destinationMatrixOrigin.x.
 *
 *              NOTE: The destinationMatrix.dataType must match the feature channel data type in sourceImage.
 *              NOTE: All the images in the source batch should be of the same size and have numberOfImages = 1.
 *
 *
 *  @param  commandBuffer       A valid MTLCommandBuffer.
 *  @param  sourceMatrix        A valid MPSMatrix or MPSTemporaryMatrix object describing the source matrix.
 *  @param  destinationImages   A valid MPSImageBatch describing the images to copy to.
 *
 */
-(void) encodeBatchToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                      sourceMatrix: (nonnull MPSMatrix *) sourceMatrix
                 destinationImages: (nonnull MPSImageBatch *) destinationImages
MPS_SWIFT_NAME(encodeBatch(commandBuffer:sourceMatrix:destinationImages:))
MPS_AVAILABLE_STARTING(macos(10.14), ios(12.0), tvos(12.0));




@end  /* MPSMatrixCopyToImage */



#endif  /* MPSImageCopy.h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageThreshold.h
/*!
 *  @header MPSImageThreshold.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders thresholding filters
 */

#ifndef MPS_MPSImageThreshold_h
#define MPS_MPSImageThreshold_h

#include <MPSImage/MPSImageKernel.h>

/*!
 *  @class      MPSImageThresholdBinary
 *  @discussion The MPSThreshold filter applies a fixed-level threshold to each pixel in the image.
 *              The threshold functions convert a single channel image to a binary image.
 *              If the input image is not a single channel image, convert the inputimage to a single channel
 *              luminance image using the linearGrayColorTransform and then apply the threshold.
 *              The ThresholdBinary function is:
 *                  destinationPixelValue = sourcePixelValue > thresholdValue ? maximumValue : 0
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageThresholdBinary : MPSUnaryImageKernel

/*!
 *  @abstract   initialize a MPSImageThresholdBinary filter
 *  @param      device          The device the filter will run on
 *  @param      thresholdValue  The threshold value to use
 *  @param      maximumValue    The maximum value to use
 *  @param      transform       This matrix is an array of 3 floats.
 *                              The default if no transform is specifed is BT.601/JPEG: {0.299f, 0.587f, 0.114f};
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        thresholdValue: (float)thresholdValue
                          maximumValue: (float)maximumValue
              linearGrayColorTransform: (const float * __nullable)transform       NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:thresholdValue:maximumValue:linearGrayColorTransform: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @property thresholdValue
 *  @discussion The threshold value used to init the threshold filter
 */
@property (readonly, nonatomic) float   thresholdValue;

/*! @property maximumValue
 *  @discussion The maximum value used to init the threshold filter
 */
@property (readonly, nonatomic) float   maximumValue;

/*! @property transform
 *  @discussion The color transform used to init the threshold filter
 */
@property (readonly, nonatomic, nonnull) const float *transform;


@end  /* MPSImageThresholdBinary */

/*!
 *  @class      MPSImageThresholdBinaryInverse
 *  @discussion The MPSImageThresholdBinaryInverse filter applies a fixed-level threshold to each pixel in the image.
 *              The threshold functions convert a single channel image to a binary image.
 *              If the input image is not a single channel image, convert the inputimage to a single channel
 *              luminance image using the linearGrayColorTransform and then apply the threshold.
 *              The ThresholdBinaryInverse function is:
 *                  destinationPixelValue = sourcePixelValue > thresholdValue ? 0 : maximumValue
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageThresholdBinaryInverse : MPSUnaryImageKernel

/*!
 *  @abstract   initialize a MPSImageThresholdBinaryInverse filter
 *  @param      device          The device the filter will run on
 *  @param      thresholdValue  The threshold value to use
 *  @param      maximumValue    The maximum value to use
 *  @param      transform       This matrix is an array of 3 floats.
 *                              The default if no transform is specifed is BT.601/JPEG: {0.299f, 0.587f, 0.114f};
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        thresholdValue: (float)thresholdValue
                          maximumValue: (float)maximumValue
              linearGrayColorTransform: (const float * __nullable)transform       NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:thresholdValue:maximumValue:linearGrayColorTransform: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @property thresholdValue
 *  @discussion The threshold value used to init the threshold filter
 */
@property (readonly, nonatomic) float   thresholdValue;

/*! @property maximumValue
 *  @discussion The maximum value used to init the threshold filter
 */
@property (readonly, nonatomic) float   maximumValue;

/*! @property transform
 *  @discussion The color transform used to init the threshold filter
 */
@property (readonly, nonatomic, nonnull) const float *transform;

@end  /* MPSImageThresholdBinaryInverse */

/*!
 *  @class      MPSImageThresholdTruncate
 *  @discussion The MPSImageThresholdTruncate filter applies a fixed-level threshold to each pixel in the image:
 *              The threshold functions convert a single channel image to a binary image.
 *              If the input image is not a single channel image, convert the inputimage to a single channel
 *              luminance image using the linearGrayColorTransform and then apply the threshold.
 *              The ThresholdTruncate function is:
 *                  destinationPixelValue = sourcePixelValue > thresholdValue ? thresholdValue : sourcePixelValue
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageThresholdTruncate : MPSUnaryImageKernel

/*! 
 *  @abstract   initialize a MPSImageThresholdTruncate filter
 *  @param      device          The device the filter will run on
 *  @param      thresholdValue The threshold value to use
 *  @param      transform       This matrix is an array of 3 floats.
 *                              The default if no transform is specifed is BT.601/JPEG: {0.299f, 0.587f, 0.114f};
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        thresholdValue: (float)thresholdValue
              linearGrayColorTransform: (const float * __nullable)transform       NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:thresholdValue:linearGrayColorTransform: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @property thresholdValue
 *  @discussion The threshold value used to init the threshold filter
 */
@property (readonly, nonatomic) float   thresholdValue;

/*! @property transform
 *  @discussion The color transform used to init the threshold filter
 */
@property (readonly, nonatomic, nonnull) const float *transform;

@end  /* MPSImageThresholdTruncate */


/*!
 *  @class      MPSImageThresholdToZero
 *  @discussion The MPSImageThresholdToZero filter applies a fixed-level threshold to each pixel in the image.
 *              The threshold functions convert a single channel image to a binary image.
 *              If the input image is not a single channel image, convert the inputimage to a single channel
 *              luminance image using the linearGrayColorTransform and then apply the threshold.
 *              The ThresholdToZero function is:
 *                  destinationPixelValue = sourcePixelValue > thresholdValue ? sourcePixelValue : 0
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageThresholdToZero : MPSUnaryImageKernel

/*!
 *  @abstract   initialize a MPSImageThresholdToZero filter
 *  @param      device          The device the filter will run on
 *  @param      thresholdValue  The threshold value to use
 *  @param      transform       This matrix is an array of 3 floats.
 *                              The default if no transform is specifed is BT.601/JPEG: {0.299f, 0.587f, 0.114f};
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        thresholdValue: (float)thresholdValue
              linearGrayColorTransform: (const float * __nullable)transform       NS_DESIGNATED_INITIALIZER;

/* You must use initWithDevice:thresholdValue:linearGrayColorTransform: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*! @property thresholdValue
 *  @discussion The threshold value used to init the threshold filter
 */
@property (readonly, nonatomic) float   thresholdValue;

/*! @property transform
 *  @discussion The color transform used to init the threshold filter
 */
@property (readonly, nonatomic, nonnull) const float *transform;

@end  /* MPSImageThresholdToZero */

/*!
 *  @class      MPSImageThresholdToZeroInverse
 *  @discussion The MPSImageThresholdToZeroInverse filter applies a fixed-level threshold to each pixel in the image.
 *              The threshold functions convert a single channel image to a binary image.
 *              If the input image is not a single channel image, convert the inputimage to a single channel
 *              luminance image using the linearGrayColorTransform and then apply the threshold.
 *              The ThresholdToZeroINverse function is:
 *                  destinationPixelValue = sourcePixelValue > thresholdValue ? 0 : sourcePixelValue
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageThresholdToZeroInverse : MPSUnaryImageKernel

/*!
 *  @abstract  initialize a MPSImageThresholdToZeroInverse filter
 *  @param      device          The device the filter will run on
 *  @param      thresholdValue The threshold value to use
 *  @param      transform       This matrix is an array of 3 floats.
 *                              The default if no transform is specifed is BT.601/JPEG: {0.299f, 0.587f, 0.114f};
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        thresholdValue: (float)thresholdValue
              linearGrayColorTransform: (const float * __nullable)transform       NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:thresholdValue:linearGrayColorTransform: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @property thresholdValue
 *  @discussion The threshold value used to init the threshold filter
 */
@property (readonly, nonatomic) float   thresholdValue;

/*! @property transform
 *  @discussion The color transform used to init the threshold filter
 */
@property (readonly, nonatomic, nonnull) const float *transform;

@end  /* MPSImageThresholdToZeroInverse */

#endif  /* MPS_MSImageThreshold_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageResampling.h
/*!
 *  @header MPSImageResampling.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract  Resampling filters for MetalPerformanceShaders
 */

#ifndef MPS_MPSImageResampling_h
#define MPS_MPSImageResampling_h

#include <MPSImage/MPSImageKernel.h>


/*!
 *  @class      MPSImageScale
 *  @abstract   Resize an image and / or change its aspect ratio
 *  @discussion The MPSImageScale filter can be used to resample an existing image
 *              using a different sampling frequency in each dimension. This can be
 *              used to enlarge or reduce the size of an image, or change the aspect
 *              ratio of an image.  
 *
 *              The resample methods supported are:
 *                    Bilinear
 *                    Bicubcic
 *                    Lanczos
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageScale : MPSUnaryImageKernel

/*
 * You must use one of the sub-classes of MPSImageScale
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/*! @property   scaleTransform
 *  @abstract   An optional transform that describes how to scale and translate the source image
 *  @discussion If the scaleTransform is NULL, then the MPSImageLanczosScale filter will
 *              rescale the image so that the source image fits exactly into the destination
 *              texture.  If the transform is not NULL, then the transform is used for determining
 *              how to map the source image to the destination. Default: NULL
 *
 *              When the scaleTransform is set to non-NULL, the values pointed to by the new
 *              scaleTransform are copied to object storage, and the pointer is updated to point
 *              to internal storage. Do not attempt to free it.  You may free your copy of
 *              the MPSScaleTransform as soon as the property set operation is complete.
 *
 *              When calculating a scaleTransform, use the limits of the bounding box for the intended
 *              source region of interest and the destination clipRect. Adjustments for pixel center
 *              coordinates are handled internally to the function.  For example,
 *              the scale transform to convert the entire source image to the entire destination image
 *              size (clipRect = MPSRectNoClip) would be:
 *
 *              @code
 *                  scaleTransform.scaleX = (double) dest.width / source.width;
 *                  scaleTransform.scaleY = (double) dest.height / source.height;
 *                  scaleTransform.translateX = scaleTransform.translateY = 0.0;
 *              @endcode
 *
 *              The translation parameters allow you to adjust the region of the source image used
 *              to create the destination image. They are in destination coordinates. To place the
 *              top left corner of the destination clipRect to represent the position {x,y} in source
 *              coordinates, we solve for the translation based on the standard scale matrix operation
 *              for each axis:
 *
 *              @code
 *                  dest_position = source_position * scale + translation;
 *                  translation = dest_position - source_position * scale;
 *              @endcode
 *
 *              For the top left corner of the clipRect, the dest_position is considered to be {0,0}.
 *              This gives us a translation of:
 *
 *              @code
 *                  scaleTransform.translateX = -source_origin.x * scaleTransform.scaleX;
 *                  scaleTransform.translateY = -source_origin.y * scaleTransform.scaleY;
 *              @endcode
 *
 *              One would typically use non-zero translations to do tiling, or provide a resized
 *              view into a internal segment of an image.
 *
 *              Changing the Lanczos scale factor may trigger recalculation of signficant state internal
 *              to the object when the filter is encoded to the command buffer. The scale factor is
 *              scaleTransform->scaleX,Y, or the ratio of source and destination image sizes if
 *              scaleTransform is NULL. Reuse a MPSImageLanczosScale object for frequently used scalings
 *              to avoid redundantly recreating expensive resampling state.
 */
@property (readwrite, nonatomic, nullable) const MPSScaleTransform *scaleTransform;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end


/*!
 *  @class      MPSImageLanczosScale
 *  @abstract   Resize an image and / or change its aspect ratio
 *  @discussion The MPSImageLanczosScale filter can be used to resample an existing image
 *              using a different sampling frequency in each dimension. This can be
 *              used to enlarge or reduce the size of an image, or change the aspect
 *              ratio of an image.  The filter uses a Lanczos resampling algorithm
 *              which typically produces better quality for photographs, but is slower
 *              than linear sampling using the GPU texture units. Lanczos downsampling 
 *              does not require a low pass filter to be applied before it is used. 
 *              Because the resampling function has negative lobes, Lanczos can result 
 *              in ringing near sharp edges, making it less suitable for vector art.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageLanczosScale : MPSImageScale

-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end


/*!
 *  @class      MPSImageBilinearScale
 *  @abstract   Resize an image and / or change its aspect ratio
 *  @discussion The MPSImageBilinearScale filter can be used to resample an existing image
 *              using a bilinear filter. This is typically used to reduce the size of an image.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageBilinearScale : MPSImageScale

-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>)device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end


#endif /* MPS_MSImageResampling_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageKernel.h
/*!
 *  @header MPSImageKernel.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders filter base classes
 */

#ifndef _MPS_MPSImageKernel_
#define _MPS_MPSImageKernel_

#import <MPSCore/MPSKernel.h>
#import <MPSImage/MPSImageTypes.h>

/*!
 *  @typedef    MPSCopyAllocator
 *  @memberof   MPSKernel
 *  @abstract   A block to make a copy of sourceTexture for MPSKernels that can only execute out of place.
 *  @discussion Some MPSKernel objects may not be able to operate in place. When that occurs, and in-place
 *              operation is requested, MPS will call back to this block to get a new texture
 *              to return instead. To avoid spending long periods of time allocating pages to back the
 *              MTLTexture, the block should attempt to reuse textures. The texture returned from the
 *              MPSCopyAllocator will be returned instead of the sourceTexture from the MPSKernel method
 *              on return.
 *              @code
 *              // A MPSCopyAllocator to handle cases where in-place operation fails.
 *              MPSCopyAllocator myAllocator = ^id <MTLTexture>( MPSKernel * __nonnull filter,
 *                                                              __nonnull id <MTLCommandBuffer> cmdBuf,
 *                                                              __nonnull id <MTLTexture> sourceTexture)
 *              {
 *                  MTLPixelFormat format = sourceTexture.pixelFormat;  // FIXME: is this format writable?
 *                  MTLTextureDescriptor *d = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat: format
 *                                               width: sourceTexture.width
 *                                              height: sourceTexture.height
 *                                           mipmapped: NO];
 *                  d.usage = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;
 *
 *                  //FIXME: Allocating a new texture each time is slow. They take up to 1 ms each.
 *                  //       There are not too many milliseconds in a video frame! You can recycle
 *                  //       old textures (or MTLBuffers and make textures from them) and reuse
 *                  //       the memory here.
 *                  id <MTLTexture> result = [cmdBuf.device newTextureWithDescriptor: d];
 *
 *                  // FIXME: If there is any metadata associated with sourceTexture such as colorspace
 *                  //        information, MTLResource.label, MTLResource.cpuCacheMode mode,
 *                  //        MTLResource.MTLPurgeableState, etc., it may need to be similarly associated
 *                  //        with the new texture to avoid losing your metadata.
 *
 *                  // FIXME: If filter.clipRect doesn't cover the entire image, you may need to copy
 *                  //        pixels from sourceTexture to the new texture or regions of the new texture
 *                  //        will be uninitialized. You can make a MTLCommandEncoder to encode work on
 *                  //        the MTLCommandBuffer here to do that work, if necessary. It will be
 *                  //        scheduled to run immediately before the MPSKernel work. Do not call
 *                  //        [MTLCommandBuffer enqueue/commit/waitUntilCompleted/waitUntilScheduled]
 *                  //        in the MPSCopyAllocator block. Make sure to call -endEncoding on the
 *                  //        MTLCommandEncoder so that the MTLCommandBuffer has no active encoder
 *                  //        before returning.
 *
 *                  // CAUTION: The next command placed on the MTLCommandBuffer after the MPSCopyAllocator
 *                  //          returns is almost assuredly going to be encoded with a MTLComputeCommandEncoder.
 *                  //          Creating any other type of encoder in the MPSCopyAllocator will probably cost
 *                  //          an additional 0.5 ms of both CPU _AND_ GPU time (or more!) due to a double
 *                  //          mode switch penalty.
 *
 *                  // CAUTION: If other objects (in addition to the caller of -encodeToCommandBuffer:inPlaceTexture:...)
 *                  //          own a reference to sourceTexture, they may need to be notified that
 *                  //          sourceTexture has been replaced so that they can release that resource
 *                  //          and adopt the new texture.
 *
 *                  //          The reference to sourceTexture owned by the caller of
 *                  //          -encodeToCommandBuffer:inPlaceTexture... will be released by
 *                  //          -encodeToCommandBuffer:inPlaceTexture:... after the kernel is encoded if
 *                  //          and only if the MPSCopyAllocator is called, and the operation is successfully
 *                  //          encoded out of place.
 *
 *                  return result;
 *                  // d is autoreleased
 *              };
 *              @endcode
 *              If nil is returned by the allocator, NO will be returned by the calling function.
 *
 *              When the MPSCopyAllocator is called, no MTLCommandEncoder is active on the commandBuffer.
 *              You may create a MTLCommandEncoder in the block to initialize the texture. Make sure
 *              to call -endEncoding on it before returning, if you do.
 *
 *  @param      filter          A valid pointer to the MPSKernel that is calling the MPSCopyAllocator. From
 *                              it you can get the clipRect of the intended operation.
 *  @param      commandBuffer   A valid MTLCommandBuffer. It can be used to obtain the device against
 *                              which to allocate the new texture. You may also enqueue operations on
 *                              the commandBuffer to initialize the texture on a encoder allocated in the
 *                              block. You may not submit, enqueue or wait for scheduling/completion of
 *                              the command buffer.
 *  @param      sourceTexture   The texture that is providing the source image for the filter. You may
 *                              wish to use its size and MTLPixelFormat for the new texture, but it is
 *                              not requred.
 *
 *  @return     A new valid MTLTexture to use as the destination for the MPSKernel. If the calling function
 *              succeeds, its texture parameter will be overwritten with a pointer to this texture. If the
 *              calling function fails (highly unlikely, except for user error) then the texture
 *              will be released before the calling function returns.
 */



#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wignored-attributes"
/* Squelch broken clan warning here for ns_returns_retained rdar://problem/20130079 */

typedef id <MTLTexture> __nonnull NS_RETURNS_RETAINED (^MPSCopyAllocator)(
      MPSKernel * __nonnull filter,
      id <MTLCommandBuffer> __nonnull commandBuffer,
      id <MTLTexture> __nonnull sourceTexture);


#pragma clang diagnostic pop




@class MPSImage;

/*!
 *  @class      MPSUnaryImageKernel
 *  @dependency This depends on Metal.framework
 *  @discussion A MPSUnaryImageKernel consumes one MTLTexture and produces one MTLTexture.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface MPSUnaryImageKernel : MPSKernel


/*! @property   offset
 *  @abstract   The position of the destination clip rectangle origin relative to the source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and source image align.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                offset;

/*! @property   clipRect
 *  @abstract   An optional clip rectangle to use when writing data. Only the pixels in the rectangle will be overwritten.
 *  @discussion A MTLRegion that indicates which part of the destination to overwrite. If the clipRect does not lie
 *              completely within the destination image, the intersection between clip rectangle and destination bounds is
 *              used.   Default: MPSRectNoClip (MPSKernel::MPSRectNoClip) indicating the entire image.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_clipRect
 */
@property (readwrite, nonatomic) MTLRegion               clipRect;


/*! @property   edgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of an image
 *  @discussion Most MPSKernel objects can read off the edge of the source image. This can happen because of a
 *              negative offset property, because the offset + clipRect.size is larger than the
 *              source image or because the filter looks at neighboring pixels, such as a Convolution
 *              or morphology filter.   Default: usually MPSImageEdgeModeZero. (Some MPSKernel types default
 *              to MPSImageEdgeModeClamp, because MPSImageEdgeModeZero is either not supported or
 *              would produce unexpected results.)
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_edgemode
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        edgeMode;

/*!
 *  @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     a pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  This method attempts to apply the MPSKernel in place on a texture.
 *
 *          In-place operation means that the same texture is used both to hold the input
 *          image and the results. Operating in-place can be an excellent way to reduce
 *          resource utilization, and save time and energy. While simple Metal kernels can
 *          not operate in place because textures can not be readable and writable at the
 *          same time, some MPSKernels can operate in place because they use
 *          multi-pass algorithms. Whether a MPSKernel can operate in-place can
 *          depend on current hardware, operating system revision and the parameters
 *          and properties passed to it. You should never assume that a MPSKernel will
 *          continue to work in place, even if you have observed it doing so before.
 *
 *  If the operation succeeds in-place, YES is returned.  If the in-place operation
 *  fails and no copyAllocator is provided, then NO is returned. Without a fallback
 *  MPSCopyAllocator, in neither case is the pointer held at *texture modified.
 *
 *  Failure during in-place operation is very common and will occur inconsistently across
 *  different hardware platforms and OS releases. Without a fallback MPSCopyAllocator,
 *  operating in place may require significant error handling code to accompany each
 *  call to -encodeToCommandBuffer:..., complicating your code.
 *
 *  You may find it simplifies your code to provide a fallback MPSCopyAllocator so
 *  that the operation can proceed reliably even when it can not proceed in-place.
 *  When an in-place filter fails, the MPSCopyAllocator (if any) will be
 *  invoked to create a new texture in which to write the results, allowing the
 *  filter to proceed reliably out-of-place. The original texture will be released,
 *  replaced with a pointer to the new texture and YES will be returned. If the
 *  allocator returns an invalid texture, it is released, *texture remains unmodified
 *  and NO is returned.  Please see the MPSCopyAllocator definition for a sample allocator
 *  implementation.
 *
 *  Sample usage with a copy allocator:
 *  @code
 *  id <MTLTexture> inPlaceTex = ...;
 *  MPSImageSobel *sobelFiler = [[MPSImageSobel alloc] initWithDevice: my_device];
 *
 *  // With a fallback MPSCopyAllocator, failure should only occur in exceptional
 *  // conditions such as MTLTexture allocation failure or programmer error.
 *  // That is, the operation is roughly as robust as the MPSCopyAllocator.
 *  // Depending on the quality of that, we might decide we are justified here
 *  // in not checking the return value.
 *  [sobelFilter encodeToCommandBuffer: my_command_buffer
 *                      inPlaceTexture: &inPlaceTex  // may be replaced!
 *               fallbackCopyAllocator: myAllocator];
 *
 *  // If myAllocator was not called:
 *  //
 *  //      inPlaceTex holds the original texture with the result pixels in it
 *  //
 *  // else,
 *  //
 *  //      1) myAllocator creates a new texture.
 *  //      2) The new texture pixel data is overwritten by MPSUnaryImageKernel.
 *  //      3) The old texture passed in *inPlaceTex is released once.
 *  //      4) *inPlaceTex = the new texture
 *  //
 *  // In either case, the caller should now hold one reference to the texture now held in
 *  // inPlaceTex, whether it was replaced or not. Most of the time that means that nothing
 *  // further needs to be done here, and you can proceed to the next image encoding operation.
 *  // However, if other agents held references to the original texture, they still hold them
 *  // and may need to be alerted that the texture has been replaced so that they can retain
 *  // the new texture and release the old one.
 *
 *  [sobelFilter release];  // if not ARC, clean up the MPSImageSobel object
 *  @endcode
 *
 *  Note: Image filters that look at neighboring pixel values may actually consume more
 *        memory when operating in place than out of place. Many such operations are
 *        tiled internally to save intermediate texture storage, but can not tile when
 *        operating in place. The memory savings for tiling is however very short term,
 *        typically the lifetime of the MTLCommandBuffer.
 *
 *  @abstract   Attempt to apply a MPSKernel to a texture in place.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      texture             A pointer to a valid MTLTexture containing source image.
 *                                  On success, the image contents and possibly texture itself
 *                                  will be replaced with the result image.
 *  @param      copyAllocator       An optional block to allocate a new texture to hold the
 *                                  results, in case in-place operation is not possible. The
 *                                  allocator may use a different MTLPixelFormat or size than
 *                                  the original texture. You may enqueue operations on the
 *                                  provided MTLCommandBuffer using the provided
 *                                  MTLComputeCommandEncoder to initialize the texture contents.
 *  @return     On success, YES is returned. The texture may have been replaced with a new
 *              texture if a copyAllocator was provided.  On failure, NO is returned. The
 *              texture is unmodified.
 */
-(BOOL)    encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)commandBuffer
                  inPlaceTexture: (__nonnull id <MTLTexture> __strong * __nonnull) texture
           fallbackCopyAllocator: (nullable MPSCopyAllocator) copyAllocator
                MPS_SWIFT_NAME(encode(commandBuffer:inPlaceTexture:fallbackCopyAllocator:));


/*!
 *  @abstract   Encode a MPSKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceTexture       A valid MTLTexture containing the source image.
 *  @param      destinationTexture  A valid MTLTexture to be overwritten by result image. DestinationTexture may not alias sourceTexture.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                sourceTexture: (nonnull id <MTLTexture>) sourceTexture
           destinationTexture: (nonnull id <MTLTexture>) destinationTexture
            MPS_SWIFT_NAME(encode(commandBuffer:sourceTexture:destinationTexture:));


/*!
 *  @abstract   Encode a MPSKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      sourceImage         A valid MPSImage containing the source image.
 *  @param      destinationImage    A valid MPSImage to be overwritten by result image. DestinationImage may not alias sourceImage.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  sourceImage: (nonnull MPSImage *) sourceImage
             destinationImage: (nonnull MPSImage *) destinationImage
            MPS_SWIFT_NAME(encode(commandBuffer:sourceImage:destinationImage:))
            // FIXME: availability info missing
            ;

/*!
 *  sourceRegionForDestinationSize: is used to determine which region of the
 *  sourceTexture will be read by encodeToCommandBuffer:sourceTexture:destinationTexture
 *  (and similar) when the filter runs. This information may be needed if the
 *  source image is broken into multiple textures.  The size of the full
 *  (untiled) destination image is provided. The region of the full (untiled)
 *  source image that will be read is returned. You can then piece together an
 *  appropriate texture containing that information for use in your tiled context.
 *
 *  The function will consult the MPSUnaryImageKernel offset and clipRect parameters, 
 *  to determine the full region read by the function. Other parameters such as
 *  sourceClipRect, kernelHeight and kernelWidth will be consulted as necessary.
 *  All properties should be set to intended values prior to calling 
 *  sourceRegionForDestinationSize:.
 *
 *      Caution: This function operates using global image coordinates, but
 *      -encodeToCommandBuffer:... uses coordinates local to the source and
 *      destination image textures. Consequently, the offset and clipRect 
 *      attached to this object will need to be updated using a global to 
 *      local coordinate transform before -encodeToCommandBuffer:... is 
 *      called.
 *
 *  @abstract   Determine the region of the source texture that will be read for a encode operation 
 *  @param      destinationSize The size of the full virtual destination image.
 *  @return     The area in the virtual source image that will be read.
 */
-(MPSRegion) sourceRegionForDestinationSize: (MTLSize) destinationSize
            MPS_SWIFT_NAME(sourceRegion(destinationSize:));

@end



/*!
 *  @class      MPSBinaryImageKernel
 *  @dependency This depends on Metal.framework
 *  @discussion A MPSBinaryImageKernel consumes two MTLTextures and produces one MTLTexture.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface MPSBinaryImageKernel : MPSKernel

/*! @property   primaryOffset
 *  @abstract   The position of the destination clip rectangle origin relative to the primary source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and primary source image align.
 *
 *              See Also: @ref MetalPerformanceShaders.h  subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                primaryOffset;

/*! @property   secondaryOffset
 *  @abstract   The position of the destination clip rectangle origin relative to the secondary source buffer.
 *  @discussion The offset is defined to be the position of clipRect.origin in source coordinates.
 *              Default: {0,0,0}, indicating that the top left corners of the clipRect and secondary source image align.
 *
 *              See Also: @ref MetalPerformanceShaders.h  subsubsection_mpsoffset
 */
@property (readwrite, nonatomic) MPSOffset                secondaryOffset;


/*! @property   primaryEdgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of the primary source image
 *  @discussion Most MPSKernel objects can read off the edge of a source image. This can happen because of a
 *              negative offset property, because the offset + clipRect.size is larger than the
 *              source image or because the filter looks at neighboring pixels, such as a Convolution
 *              or morphology filter.   Default: usually MPSImageEdgeModeZero. (Some MPSKernel types default
 *              to MPSImageEdgeModeClamp, because MPSImageEdgeModeZero is either not supported or
 *              would produce unexpected results.)
 *
 *              See Also: @ref MetalPerformanceShaders.h  subsubsection_edgemode
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        primaryEdgeMode;

/*! @property   secondaryEdgeMode
 *  @abstract   The MPSImageEdgeMode to use when texture reads stray off the edge of the secondary source image
 *  @discussion Most MPSKernel objects can read off the edge of a source image. This can happen because of a
 *              negative offset property, because the offset + clipRect.size is larger than the
 *              source image or because the filter looks at neighboring pixels, such as a Convolution
 *              or morphology filter.   Default: usually MPSImageEdgeModeZero. (Some MPSKernel types default
 *              to MPSImageEdgeModeClamp, because MPSImageEdgeModeZero is either not supported or
 *              would produce unexpected results.)
 *
 *              See Also: @ref MetalPerformanceShaders.h  subsubsection_edgemode
 */
@property (readwrite, nonatomic) MPSImageEdgeMode        secondaryEdgeMode;

/*! @property   clipRect
 *  @abstract   An optional clip rectangle to use when writing data. Only the pixels in the rectangle will be overwritten.
 *  @discussion A MTLRegion that indicates which part of the destination to overwrite. If the clipRect does not lie
 *              completely within the destination image, the intersection between clip rectangle and destination bounds is
 *              used.   Default: MPSRectNoClip (MPSKernel::MPSRectNoClip) indicating the entire image.
 *
 *              See Also: @ref MetalPerformanceShaders.h subsubsection_clipRect
 */
@property (readwrite, nonatomic) MTLRegion               clipRect;

/*!
 *  @abstract   Standard init with default properties per filter type
 *  @param      device      The device that the filter will be used on. May not be NULL.
 *  @result     a pointer to the newly initialized object. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*!
 *  This method attempts to apply the MPSKernel in place on a texture.
 *
 *          In-place operation means that the same texture is used both to hold the input
 *          image and the results. Operating in-place can be an excellent way to reduce
 *          resource utilization, and save time and energy. While simple Metal kernels can
 *          not operate in place because textures can not be readable and writable at the
 *          same time, some MPSKernels can operate in place because they use
 *          multi-pass algorithms. Whether a MPSKernel can operate in-place can
 *          depend on current hardware, operating system revision and the parameters
 *          and properties passed to it. You should never assume that a MPSKernel will
 *          continue to work in place, even if you have observed it doing so before.
 *
 *  If the operation succeeds in-place, YES is returned.  If the in-place operation
 *  fails and no copyAllocator is provided, then NO is returned. In neither
 *  case is the pointer held at *texture modified.
 *
 *  Failure during in-place operation is common. You may find it simplifies your
 *  code to provide a copyAllocator. When an in-place filter fails, your
 *  copyAllocator will be invoked to create a new texture in which to write
 *  the results, allowing the filter to proceed reliably out-of-place. The
 *  original texture will be released, replaced with a pointer to the new texture
 *  and YES will be returned. If the allocator returns an invalid texture, it is
 *  released, *texture remains unmodified and NO is returned.  Please see the
 *  MPSCopyAllocator definition for a sample allocator implementation.
 *
 *  Note: Image filters that look at neighboring pixel values may actually consume more
 *        memory when operating in place than out of place. Many such operations are
 *        tiled internally to save intermediate texture storage, but can not tile when
 *        operating in place. The memory savings for tiling is however very short term,
 *        typically the lifetime of the MTLCommandBuffer.
 *
 *  @abstract   Attempt to apply a MPSKernel to a texture in place.
 *  @param      commandBuffer           A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryTexture          A pointer to a valid MTLTexture containing the
 *                                      primary source image. It will not be overwritten.
 *  @param      inPlaceSecondaryTexture A pointer to a valid MTLTexture containing secondary image.
 *                                      On success, the image contents and possibly texture itself
 *                                      will be replaced with the result image.
 *  @param      copyAllocator           An optional block to allocate a new texture to hold the
 *                                      results, in case in-place operation is not possible. The
 *                                      allocator may use a different MTLPixelFormat or size than
 *                                      the original texture. You may enqueue operations on the
 *                                      provided MTLCommandBuffer using the provided
 *                                      MTLComputeCommandEncoder to initialize the texture contents.
 *  @return     On success, YES is returned. The texture may have been replaced with a new
 *              texture if a copyAllocator was provided.  On failure, NO is returned. The
 *              texture is unmodified.
 */
-(BOOL)    encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)commandBuffer
                  primaryTexture: (nonnull id <MTLTexture>) primaryTexture
         inPlaceSecondaryTexture: (__nonnull id <MTLTexture> __strong * __nonnull) inPlaceSecondaryTexture
           fallbackCopyAllocator: (nullable MPSCopyAllocator) copyAllocator
    MPS_SWIFT_NAME(encode(commandBuffer:primaryTexture:inPlaceSecondaryTexture:fallbackCopyAllocator:));

/*!
 *  @abstract   Attempt to apply a MPSKernel to a texture in place.
 *  @discussion This method attempts to apply the MPSKernel in place on a texture.
 *  @code
 *          In-place operation means that the same texture is used both to hold the input
 *          image and the results. Operating in-place can be an excellent way to reduce
 *          resource utilization, and save time and energy. While simple Metal kernels can
 *          not operate in place because textures can not be readable and writable at the
 *          same time, some MPSKernels can operate in place because they use
 *          multi-pass algorithms. Whether a MPSKernel can operate in-place can
 *          depend on current hardware, operating system revision and the parameters
 *          and properties passed to it. You should never assume that a MPSKernel will
 *          continue to work in place, even if you have observed it doing so before.
 *  @endcode
 *  If the operation succeeds in-place, YES is returned.  If the in-place operation
 *  fails and no copyAllocator is provided, then NO is returned. In neither
 *  case is the pointer held at *texture modified.
 *
 *  Failure during in-place operation is common. You may find it simplifies your
 *  code to provide a copyAllocator. When an in-place filter fails, your
 *  copyAllocator will be invoked to create a new texture in which to write
 *  the results, allowing the filter to proceed reliably out-of-place. The
 *  original texture will be released, replaced with a pointer to the new texture
 *  and YES will be returned. If the allocator returns an invalid texture, it is
 *  released, *texture remains unmodified and NO is returned.  Please see the
 *  MPSCopyAllocator definition for a sample allocator implementation.
 *
 *  Note: Image filters that look at neighboring pixel values may actually consume more
 *        memory when operating in place than out of place. Many such operations are
 *        tiled internally to save intermediate texture storage, but can not tile when
 *        operating in place. The memory savings for tiling is however very short term,
 *        typically the lifetime of the MTLCommandBuffer.
 *
 *  @param      commandBuffer           A valid MTLCommandBuffer to receive the encoded filter
 *  @param      inPlacePrimaryTexture   A pointer to a valid MTLTexture containing secondary image.
 *                                      On success, the image contents and possibly texture itself
 *                                      will be replaced with the result image.
 *  @param      secondaryTexture        A pointer to a valid MTLTexture containing the
 *                                      primary source image. It will not be overwritten.
 *  @param      copyAllocator           An optional block to allocate a new texture to hold the
 *                                      results, in case in-place operation is not possible. The
 *                                      allocator may use a different MTLPixelFormat or size than
 *                                      the original texture. You may enqueue operations on the
 *                                      provided MTLCommandBuffer using the provided
 *                                      MTLComputeCommandEncoder to initialize the texture contents.
 *  @return     On success, YES is returned. The texture may have been replaced with a new
 *              texture if a copyAllocator was provided.  On failure, NO is returned. The
 *              texture is unmodified.
 */
-(BOOL)    encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)commandBuffer
           inPlacePrimaryTexture: (__nonnull id <MTLTexture> __strong * __nonnull) inPlacePrimaryTexture
                secondaryTexture: (nonnull id <MTLTexture>) secondaryTexture
           fallbackCopyAllocator: (nullable MPSCopyAllocator) copyAllocator
        MPS_SWIFT_NAME(encode(commandBuffer:inPlacePrimaryTexture:secondaryTexture:fallbackCopyAllocator:));



/*!
 *  @abstract   Encode a MPSKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryTexture      A valid MTLTexture containing the primary source image.
 *  @param      secondaryTexture    A valid MTLTexture containing the secondary source image.
 *  @param      destinationTexture  A valid MTLTexture to be overwritten by result image. destinationTexture may not alias the source textures.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               primaryTexture: (nonnull id <MTLTexture>) primaryTexture
             secondaryTexture: (nonnull id <MTLTexture>) secondaryTexture
           destinationTexture: (nonnull id <MTLTexture>) destinationTexture
    MPS_SWIFT_NAME(encode(commandBuffer:primaryTexture:secondaryTexture:destinationTexture:));

/*! @abstract   Encode a MPSKernel into a command Buffer.  The operation shall proceed out-of-place.
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *  @param      primaryImage        A valid MPSImage containing the primary source image.
 *  @param      secondaryImage      A valid MPSImage containing the secondary source image.
 *  @param      destinationImage    A valid MPSImage to be overwritten by result image. destinationImage may not alias the source images.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 primaryImage: (MPSImage * __nonnull) primaryImage
               secondaryImage: (MPSImage * __nonnull) secondaryImage
             destinationImage: (MPSImage * __nonnull) destinationImage
    MPS_SWIFT_NAME(encode(commandBuffer:primaryImage:secondaryImage:destinationImage:))
    // FIXME: availability info missing
    ;

/*!
 *  primarySourceRegionForDestinationSize: is used to determine which region of the
 *  primaryTexture will be read by encodeToCommandBuffer:primaryTexture:secondaryTexture:destinationTexture
 *  (and in-place variants) when the filter runs. This information may be needed if the
 *  primary source image is broken into multiple textures.  The size of the full
 *  (untiled) destination image is provided. The region of the full (untiled)
 *  source image that will be read is returned. You can then piece together an
 *  appropriate texture containing that information for use in your tiled context.
 *
 *  The function will consult the MPSBinaryImageKernel primaryOffset and clipRect parameters,
 *  to determine the full region read by the function. Other parameters such as
 *  kernelHeight and kernelWidth will be consulted as necessary. All properties
 *  should be set to intended values prior to calling primarySourceRegionForDestinationSize:.
 *
 *      Caution: This function operates using global image coordinates, but
 *      -encodeToCommandBuffer:... uses coordinates local to the source and
 *      destination image textures. Consequently, the primaryOffset and clipRect
 *      attached to this object will need to be updated using a global to
 *      local coordinate transform before -encodeToCommandBuffer:... is
 *      called.
 *
 *  @abstract   Determine the region of the source texture that will be read for a encode operation
 *  @param      destinationSize     The size of the full virtual destination image.
 *  @return     The area in the virtual source image that will be read.
 */
-(MPSRegion) primarySourceRegionForDestinationSize: (MTLSize) destinationSize;

/*!
 *  secondarySourceRegionForDestinationSize: is used to determine which region of the
 *  sourceTexture will be read by encodeToCommandBuffer:primaryTexture:secondaryTexture:destinationTexture
 *  (and in-place variants) when the filter runs. This information may be needed if the
 *  secondary source image is broken into multiple textures.  The size of the full
 *  (untiled) destination image is provided. The region of the full (untiled)
 *  secondary source image that will be read is returned. You can then piece together an
 *  appropriate texture containing that information for use in your tiled context.
 *
 *  The function will consult the MPSBinaryImageKernel secondaryOffset and clipRect
 *  parameters, to determine the full region read by the function. Other parameters
 *  such as kernelHeight and kernelWidth will be consulted as necessary.  All properties
 *  should be set to intended values prior to calling secondarySourceRegionForDestinationSize:.
 *
 *      Caution: This function operates using global image coordinates, but
 *      -encodeToCommandBuffer:... uses coordinates local to the source and
 *      destination image textures. Consequently, the secondaryOffset and clipRect
 *      attached to this object will need to be updated using a global to
 *      local coordinate transform before -encodeToCommandBuffer:... is
 *      called.
 *
 *  @abstract   Determine the region of the source texture that will be read for a encode operation
 *  @param      destinationSize     The size of the full virtual destination image.
 *  @return     The area in the virtual source image that will be read.
 */
-(MPSRegion) secondarySourceRegionForDestinationSize: (MTLSize) destinationSize;



@end

#endif /* defined(_MetalPerformanceShaders_MSImageKernel_) */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageMath.h
/*!
 *  @header MPSImageMath.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2016 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders math filters
 */

#ifndef MPS_MPSImageMath_h
#define MPS_MPSImageMath_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>


/*!
 *  @class      MPSImageArithmetic
 *  @dependency This depends on Metal.framework.
 *  @discussion This filter takes two source images, a primary source image and a secondary source image,
 *              and outputs a single destination image. It applies an element-wise arithmetic operator to
 *              each pixel in a primary source image and a corresponding pixel in a secondary source image
 *              over a specified region.
 *              
 *              The supported arithmetic operators are the following:
 *              - Addition
 *              - Subtraction
 *              - Multiplication
 *              - Division
 *
 *              This filter takes additional parameters: primaryScale, secondaryScale, and bias. The default
 *              value for primaryScale and secondaryScale is 1.0f. The default value for bias is 0.0f. This
 *              filter applies primaryScale, secondaryScale, and bias to the primary source pixel (x) and
 *              secondary source pixel (y) in the following way:
 *              - Addition:         result = ((primaryScale * x) + (secondaryScale * y)) + bias
 *              - Subtraction:      result = ((primaryScale * x) - (secondaryScale * y)) + bias
 *              - Multiplicaton:    result = ((primaryScale * x) * (secondaryScale * y)) + bias
 *              - Division:         result = ((primaryScale * x) / (secondaryScale * y)) + bias
 *
 *              To clamp the result of an arithmetic operation, where
 *              result = clamp(result, minimumValue, maximumValue),
 *              set the minimumValue and maximumValue appropriately. The default value of minimumValue
 *              is -FLT_MAX. The default value of maximumValue is FLT_MAX.
 *
 *              This filter also takes the following additional parameters:
 *              - primaryStrideInPixels
 *              - secondaryStrideInPixels
 *              These parameters can be used to control broadcasting for the data stored in the primary and
 *              secondary source images. For example, setting all strides for the primary source image to 0
 *              will result in the primarySource image being treated as a scalar value. The only supported
 *              values are 0 or 1. The default value of these parameters is 1.
 *
 *              This filter accepts uint and int data in addition to unorm and floating-point data.
 *              
 *              You must use one of the sub-classes of MPSImageArithmetic.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageArithmetic : MPSBinaryImageKernel

@property (readwrite, nonatomic) float primaryScale;
@property (readwrite, nonatomic) float secondaryScale;
@property (readwrite, nonatomic) float bias;

/*! @property   primaryStrideInPixels
 *  @abstract   The secondarySource stride in the x, y, and z dimensions. The only supported values are 0 or 1.
 *              The default value for each dimension is 1.
 */
@property(readwrite, nonatomic) MTLSize         primaryStrideInPixels;

/*! @property   secondaryStrideInPixels
 *  @abstract   The secondarySource stride in the x, y, and z dimensions. The only supported values are 0 or 1.
 *              The default value for each dimension is 1.
 */
@property(readwrite, nonatomic) MTLSize         secondaryStrideInPixels;

/*! @property   minimumValue
 *  @abstract   minimumValue is to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of minimumValue is -FLT_MAX.
 */
@property (readwrite, nonatomic) float          minimumValue    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @property   maximumValue
 *  @abstract   maximumValue is used to clamp the result of an arithmetic operation:
 *              result = clamp(result, minimumValue, maximumValue).
 *              The default value of maximumValue is FLT_MAX.
 */
@property (readwrite, nonatomic) float          maximumValue    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*
 * You must use one of the sub-classes of MPSImageArithmetic.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

@end    /* MPSImageArithmetic */


/*!
 *  @class      MPSImageAdd
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the addition operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) + (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageAdd : MPSImageArithmetic

/*!
 *  @abstract  Initialize the addition operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSImageAdd object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSImageAdd */


/*!
 *  @class      MPSImageSubtract
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the subtraction operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) - (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageSubtract : MPSImageArithmetic

/*!
 *  @abstract  Initialize the subtraction operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSImageSubtract object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSImageSubtract */


/*!
 *  @class      MPSImageMultiply
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the multiplication operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) * (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageMultiply : MPSImageArithmetic

/*!
 *  @abstract  Initialize the multiplication operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSImageMultiply object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSImageMultiply */


/*!
 *  @class      MPSImageDivide
 *  @dependency This depends on Metal.framework.
 *  @discussion Specifies the division operator.
 *              For each pixel in the primary source image (x) and each pixel in a secondary source image (y),
 *              it applies the following function: result = ((primaryScale * x) / (secondaryScale * y)) + bias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageDivide : MPSImageArithmetic

/*!
 *  @abstract  Initialize the division operator
 *  @param     device           The device the filter will run on.
 *  @return    A valid MPSImageDivide object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end    /* MPSImageDivide */


#endif  /* MPSImageMath_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageIntegral.h
/*!
 *  @header MPSImageIntegral.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders integral filters
 */

#ifndef MPS_MPSImageIntegral_h
#define MPS_MPSImageIntegral_h

#include <MPSImage/MPSImageKernel.h>

/*!
 *  @class      MPSImageIntegral
 *  @discussion The MPSImageIntegral calculates the sum of pixels over a specified region in the image.
 *              The value at each position is the sum of all pixels in a source image rectangle, sumRect:
 *
 *                  sumRect.origin = MPSUnaryImageKernel.offset
 *                  sumRect.size = dest_position - MPSUnaryImageKernel.clipRect.origin
 *
 *              If the channels in the source image are normalized, half-float or floating values,
 *              the destination image is recommended to be a 32-bit floating-point image.
 *              If the channels in the source image are integer values, it is recommended that
 *              an appropriate 32-bit integer image destination format is used.
 *
 *              This kernel accepts uint and int textures in addition to unorm and floating-point textures.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageIntegral : MPSUnaryImageKernel

@end    /* MPSImageIntegral */


/*!
 *  @class      MPSImageIntegralOfSquares
 *  @discussion The MPSImageIntegralOfSquares calculates the sum of squared pixels over a specified region in the image.
 *              The value at each position is the sum of all squared pixels in a source image rectangle, sumRect:
 *
 *                  sumRect.origin = MPSUnaryImageKernel.offset
 *                  sumRect.size = dest_position - MPSUnaryImageKernel.clipRect.origin
 *
 *              If the channels in the source image are normalized, half-float or floating values,
 *              the destination image is recommended to be a 32-bit floating-point image.
 *              If the channels in the source image are integer values, it is recommended that
 *              an appropriate 32-bit integer image destination format is used.
 *
 *              This kernel accepts uint and int textures in addition to unorm and floating-point textures.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageIntegralOfSquares : MPSUnaryImageKernel

@end    /* MPSImageIntegralOfSquares */

#endif  /* MPS_MSImageIntegral_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageReduce.h
/*!
 *  @header MPSImageReduce.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @abstract  Reduction filters for MetalPerformanceShaders
 */

#ifndef MPS_MPSImageReduce_h
#define MPS_MPSImageReduce_h

#include <MPSImage/MPSImageKernel.h>


/*!
 *  @class      MPSImageReduceUnary
 *  @discussion The MPSImageReduce performs a reduction operation
 *              The reduction operations supported are:
 *                   - Reduce row min
 *                   - Reduce column min
 *                   - Reduce row max
 *                   - Reduce column max
 *                   - Reduce row mean
 *                   - Reduce column mean
 *                   - Reduce row sum
 *                   - Reduce column sum
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceUnary : MPSUnaryImageKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 *
 *              The clipRect specified in MPSUnaryImageKernel is used to control the origin in the destination texture
 *              where the min, max values are written.  The clipRect.width must be >=2.  The clipRect.height must be >= 1.
 *
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*
 * You must use one of the sub-classes of MPSImageReduceUnary.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_UNAVAILABLE;

@end  /* MPSImageReduceUnary */

/*!
 *  @class      MPSImageReduceRowMin
 *  @discussion The MPSImageReduceRowMin performs a reduction operation returning the mininmum value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceRowMin : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceRowMin */


/*!
 *  @class      MPSImageReduceColumnMin
 *  @discussion The MPSImageReduceColumnMin performs a reduction operation returning the mininmum value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceColumnMin : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceColumnMin */

/*!
 *  @class      MPSImageReduceRowMax
 *  @discussion The MPSImageReduceRowMax performs a reduction operation returning the maximum value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceRowMax : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReducenRowMax */

/*!
 *  @class      MPSImageReduceColumnMax
 *  @discussion The MPSImageReduceColumnMax performs a reduction operation returning the maximum value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceColumnMax : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceColumnMax */

/*!
 *  @class      MPSImageReduceRowMean
 *  @discussion The MPSImageReduceRowMean performs a reduction operation returning the mean value for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceRowMean : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceRowMean */

/*!
 *  @class      MPSImageReduceColumnMean
 *  @discussion The MPSImageReduceColumnMean performs a reduction operation returning the mean value for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceColumnMean : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceColumnMean */

/*!
 *  @class      MPSImageReduceRowSum
 *  @discussion The MPSImageReduceRowSum performs a reduction operation returning the sum for each row of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceRowSum : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceRowSum */

/*!
 *  @class      MPSImageReduceColumnSum
 *  @discussion The MPSImageReduceColumnSum performs a reduction operation returning the sum for each column of an image
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageReduceColumnSum : MPSImageReduceUnary

/*!
 *  @abstract Specifies information to apply the reduction operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageReduce object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device     NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageReduceColumnSum */

#endif  /* MPS_MSImageReduce_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageMedian.h
/*!
 *  @header MPSImageMedian.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders median filters
 */

#ifndef MPS_MPSImageMedian_h
#define MPS_MPSImageMedian_h

#include <MPSImage/MPSImageKernel.h>


/*!
 *  @class      MPSImageMedian
 *  @discussion The MPSImageMedian applies a median filter to an image.  A median filter finds the 
 *              median color value for each channel within a kernelDiameter x kernelDiameter 
 *              window surrounding the pixel of interest.  It is a common means of noise reduction
 *              and also as a smoothing filter with edge preserving qualities.
 *
 *              NOTE: The MPSImageMedian filter currently only supports images with <= 8 bits/channel.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageMedian : MPSUnaryImageKernel

/*! @property   kernelDiameter
 *  @abstract   The diameter in pixels of the filter window.
 *  @discussion The median filter is applied to a kernelDiameter x kernelDiameter window
 *              of pixels centered on the corresponding source pixel for each destination
 *              pixel.  The kernel diameter must be an odd number.
 */
@property (readonly, nonatomic) NSUInteger kernelDiameter;


/*! @abstract   Initialize a filter for a particular kernel size and device
 *  @param      device          The device the filter will run on
 *  @param      kernelDiameter  Diameter of the median filter. Must be an odd number.
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        kernelDiameter: (NSUInteger)kernelDiameter   NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/* You must use initWithDevice:kernelDiameter: instead. */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device    NS_UNAVAILABLE;


/*! @abstract   The maximum diameter in pixels of the filter window supported by the median filter.
 */
+(NSUInteger) maxKernelDiameter;

/*! @abstract   The minimum diameter in pixels of the filter window supported by the median filter.
 */
+(NSUInteger) minKernelDiameter;

@end  /* MPSImageMedian */

#endif  /* MPS_MSImageMedian_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageMorphology.h
/*!
 *  @header MPSImageMorphology.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders morphological operators
 */

#ifndef MPS_MPSImageMorphology_h
#define MPS_MPSImageMorphology_h

#include <MPSImage/MPSImageKernel.h>


/*!
 *  @class      MPSImageAreaMax
 *  @discussion The MPSImageAreaMax kernel finds the maximum pixel value in a rectangular region centered around each pixel
 *              in the source image. If there are multiple channels in the source image, each channel is processed independently.
 *              The edgeMode property is assumed to always be MPSImageEdgeModeClamp for this filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageAreaMax : MPSUnaryImageKernel

/*! @property kernelHeight
 *  @abstract  The height of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelHeight;

/*! @property kernelWidth
 *  @abstract  The width of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelWidth;

/*!
 *  @abstract Set the kernel height and width
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel. Must be an odd number.
 *  @param      kernelHeight        The height of the kernel. Must be an odd number.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger)kernelWidth
                          kernelHeight: (NSUInteger)kernelHeight            NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                    MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:kernelWidth:kernelHeight: instead. */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device        NS_UNAVAILABLE;

@end  /* MPSImageAreaMax */

/*!
 *  @class      MPSImageAreaMin
 *  @discussion The MPSImageAreaMin finds the minimum pixel value in a rectangular region centered around each pixel in the
 *               source image. If there are multiple channels in the source image, each channel is processed independently.
 *               It has the same methods as MPSImageAreaMax
 *               The edgeMode property is assumed to always be MPSImageEdgeModeClamp for this filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageAreaMin : MPSImageAreaMax

@end  /* MPSImageAreaMin */

/*!
 *  @class      MPSImageDilate
 *  @discussion The MPSImageDilate finds the maximum pixel value in a rectangular region centered around each pixel in the
 *              source image. It is like the MPSImageAreaMax, except that the intensity at each position is calculated relative
 *              to a different value before determining which is the maximum pixel value, allowing for shaped, non-rectangular
 *              morphological probes.
 *  @code
 *          for each pixel in the filter window:
 *              value =  pixel[filterY][filterX] - filter[filterY*filter_width+filterX]
 *              if( value > bestValue ){
 *                   result = value
 *                   bestValue = value;
 *              }
 *  @endcode
 *              A filter that contains all zeros and is identical to a MPSImageAreaMax filter.  The center filter element
 *              is assumed to be 0 to avoid causing a general darkening of the image.
 *
 *              The edgeMode property is assumed to always be MPSImageEdgeModeClamp for this filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageDilate : MPSUnaryImageKernel
/*! @property kernelHeight
 *  @abstract  The height of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelHeight;

/*! @property kernelWidth
 *  @abstract  The width of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelWidth;


/*!
 *  @abstract   Init a object with kernel height, width and weight values.
 *  @discussion Each dilate shape probe defines a 3D surface of values.
 *              These are arranged in order left to right, then top to bottom
 *              in a 1D array. (values[kernelWidth*y+x] = probe[y][x])
 *              Values should be generally be in the range [0,1] with the center 
 *              pixel tending towards 0 and edges towards 1. However, any numerical
 *              value is allowed. Calculations are subject to the usual floating-point
 *              rounding error.
 *
 *  @param      device              The device the filter will run on
 *  @param      kernelWidth         The width of the kernel. Must be an odd number.
 *  @param      kernelHeight        The height of the kernel. Must be an odd number.
 *  @param      values              The set of values to use as the dilate probe.
 *                                  The values are copied into the filter. To avoid 
 *                                  image ligthening or darkening, the center value should
 *                                  be 0.0f.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger)kernelWidth
                          kernelHeight: (NSUInteger)kernelHeight
                                values: (const float* __nonnull) values       NS_DESIGNATED_INITIALIZER;

/* You must use initWithDevice:kernelWidth:kernelHeight:values: instead. */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device        NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


@end  /* MPSImageDilate */


/*!
 *  @class      MPSImageErode
 *  @discussion The MPSImageErode filter finds the minimum pixel value in a rectangular region centered around each pixel in the
 *              source image. It is like the MPSImageAreaMin, except that the intensity at each position is calculated relative
 *              to a different value before determining which is the maximum pixel value, allowing for shaped, non-rectangular
 *              morphological probes.
 *  @code
 *          for each pixel in the filter window:
 *              value =  pixel[filterY][filterX] + filter[filterY*filter_width+filterX]
 *              if( value < bestValue ){
 *                   result = value
 *                   bestValue = value;
 *              }
 *  @endcode
 *              A filter that contains all zeros is identical to a MPSImageAreaMin filter. The center filter element
 *              is assumed to be 0, to avoid causing a general lightening of the image.
 *
 *              The definition of the filter for MPSImageErode is different from vImage. (MPSErode_filter_value = 1.0f-vImageErode_filter_value.)
 *              This allows MPSImageDilate and MPSImageErode to use the same filter, making open and close operators easier to write.
 *              The edgeMode property is assumed to always be MPSImageEdgeModeClamp for this filter.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageErode : MPSImageDilate
@end

#endif  /* MPS_MSImageMorphology_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImage.h
/*!
 *  @header MPSImage.h
 *  @framework MPSImage
 *
 *  @discussion MPSImage filters support a variety of classical image
 *              processing primitives. 
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 */

#import <MPSCore/MPSKernel.h>
#import <MPSImage/MPSImageTypes.h>
#import <MPSImage/MPSImageConversion.h>
#import <MPSImage/MPSImageConvolution.h>
#import <MPSImage/MPSImageCopy.h>
#import <MPSImage/MPSImageDistanceTransform.h>
#import <MPSImage/MPSImageGuidedFilter.h>
#import <MPSImage/MPSImageKeypoint.h>
#import <MPSImage/MPSImageHistogram.h>
#import <MPSImage/MPSImageIntegral.h>
#import <MPSImage/MPSImageMath.h>
#import <MPSImage/MPSImageMedian.h>
#import <MPSImage/MPSImageMorphology.h>
#import <MPSImage/MPSImageReduce.h>
#import <MPSImage/MPSImageResampling.h>
#import <MPSImage/MPSImageStatistics.h>
#import <MPSImage/MPSImageThreshold.h>
#import <MPSImage/MPSImageTranspose.h>

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageTranspose.h
/*!
 *  @header MPSImageTranspose.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders transpose filters
 */

#ifndef MPS_MPSImageTranspose_h
#define MPS_MPSImageTranspose_h

#include <MPSImage/MPSImageKernel.h>

/*!
 *  @class      MPSImageTranspose
 *  @discussion The MPSImageTranspose transposes an image
 *
 *              This kernel accepts uint and int textures in addition to unorm and floating-point textures.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageTranspose : MPSUnaryImageKernel

@end    /* MPSImageTranspose */


#endif  /* MPS_MSImageTranspose_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageDistanceTransform.h
/*!
 *  @header MPSImageDistanceTransform.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders distance transform filters
 */

#ifndef MPS_MPSImageDistanceTransform_h
#define MPS_MPSImageDistanceTransform_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>

/*!
 *  @class      MPSImageEuclideanDistanceTransform
 *  @discussion Perform a Euclidean Distance Transform
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageEuclideanDistanceTransform : MPSUnaryImageKernel

/*!
 *  @abstract Specifies information to apply the statistics min-max operation on an image.
 *  @param    device            The device the filter will run on
 *  @return     A valid MPSImageEuclideanDistanceTransform object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

@end  /* MPSImageEuclideanDistanceTransform */

#endif  /* MPS_MPSImageDistanceTransform_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageConversion.h
/*!
 *  @header MPSImageConversions.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders conversion filters
 *  @ignorefuncmacro MPS_CLASS_AVAILABLE_STARTING
 */

#ifndef MPS_Conversions_h
#define MPS_Conversions_h

#include <MPSImage/MPSImageKernel.h>
#include <CoreGraphics/CGColorConversionInfo.h>


/*!
 *  @class      MPSImageConversion
 *  @discussion The MPSImageConversion filter performs a conversion from source to destination
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageConversion : MPSUnaryImageKernel

/*! @property   sourceAlpha
 *  @abstract   Premultiplication description for the source texture
 *  @discussion Most colorspace conversion operations can not work directly on premultiplied data.
 *              Use this property to tag premultiplied data so that the source texture can
 *              be unpremultiplied prior to application of these transforms. 
 *              Default: MPSPixelAlpha_AlphaIsOne
 */
@property (readonly, nonatomic) MPSAlphaType sourceAlpha;

/*! @property   destinationAlpha
 *  @abstract   Premultiplication description for the destinationAlpha texture
 *  @discussion Colorspace conversion operations produce non-premultiplied data.
 *              Use this property to tag cases where premultiplied results are required.
 *              If MPSPixelAlpha_AlphaIsOne is used, the alpha channel will be set to 1. 
 *              Default: MPSPixelAlpha_AlphaIsOne
 */
@property (readonly, nonatomic) MPSAlphaType destinationAlpha;



/*!
 *  @abstract   Create a converter that can convert texture colorspace, alpha and texture format
 *  @discussion Create a converter that can convert texture colorspace, alpha and MTLPixelFormat. 
 *              Optimized cases exist for NULL color space converter and no alpha conversion.
 *  @param      device              The device the filter will run on
 *  @param      srcAlpha            The alpha encoding for the source texture
 *  @param      destAlpha           The alpha encoding for the destination texture
 *  @param      backgroundColor     An array of CGFloats giving the background color to use when flattening an image.
 *                                  The color is in the source colorspace.  The length of the array is the number 
 *                                  of color channels in the src colorspace. If NULL, use {0}.
 *  @param      conversionInfo      The colorspace conversion to use. May be NULL, indicating no
 *                                  color space conversions need to be done.
 *
 *  @result     An initialized MPSImageConversion object.
 */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device
                              srcAlpha:(MPSAlphaType) srcAlpha
                             destAlpha:(MPSAlphaType) destAlpha
                       backgroundColor:(nullable CGFloat*) backgroundColor
                        conversionInfo:(nullable CGColorConversionInfoRef) conversionInfo;


@end  /* MPSImageConversion */

#endif
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageConvolution.h
/*!
 *  @header MPSImageConvolution.h
 *  @framework MetalPerformanceShaders
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *
 *  @abstract MetalPerformanceShaders Convolution Filters
 */

#ifndef MPS_MSImageConvolution_h
#define MPS_MSImageConvolution_h

#include <MPSImage/MPSImageKernel.h>

/*!
 *  @class      MPSImageConvolution
 *  @discussion The MPSImageConvolution convolves an image with given filter of odd width and height.
 *              The center of the kernel aligns with the MPSImageConvolution.offset. That is, the position 
 *              of the top left corner of the area covered by the kernel is given by 
 *              MPSImageConvolution.offset - {kernel_width>>1, kernel_height>>1, 0}
 *
 *              Optimized cases include 3x3,5x5,7x7,9x9,11x11, 1xN and Nx1. If a convolution kernel 
 *              does not fall into one of these cases but is a rank-1 matrix (a.k.a. separable)
 *              then it will fall on an optimzied separable path. Other convolutions will execute with
 *              full MxN complexity.
 *
 *              If there are multiple channels in the source image, each channel is processed independently.
 *  
 *  @performance Separable convolution filters may perform better when done in two passes. A convolution filter
 *              is separable if the ratio of filter values between all rows is constant over the whole row. For
 *              example, this edge detection filter:
 *                  @code
 *                      -1      0       1
 *                      -2      0       2
 *                      -1      0       1
 *                  @endcode
 *              can be separated into the product of two vectors:
 *                  @code
 *                      1
 *                      2      x    [-1  0   1]
 *                      1
 *                  @endcode
 *              and consequently can be done as two, one-dimensional convolution passes back to back on the same image. 
 *              In this way, the number of multiplies (ignoring the fact that we could skip zeros here) is reduced from
 *              3*3=9 to 3+3 = 6. There are similar savings for addition. For large filters, the savings can be profound.
 *
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageConvolution : MPSUnaryImageKernel

/*! @property kernelHeight
 *  @abstract  The height of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelHeight;

/*! @property kernelWidth
 *  @abstract  The width of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelWidth;


/*! @property    bias
 *  @discussion  The bias is a value to be added to convolved pixel before it is converted back to the storage format.
 *               It can be used to convert negative values into a representable range for a unsigned MTLPixelFormat.
 *               For example, many edge detection filters produce results in the range [-k,k]. By scaling the filter
 *               weights by 0.5/k and adding 0.5, the results will be in range [0,1] suitable for use with unorm formats. 
 *               It can be used in combination with renormalization of the filter weights to do video ranging as part 
 *               of the convolution effect. It can also just be used to increase the brightness of the image.
 *
 *               Default value is 0.0f.
 */
@property (readwrite, nonatomic) float bias;

/*!
 *  @abstract  Initialize a convolution filter
 *  @param      device          The device the filter will run on
 *  @param      kernelWidth     the width of the kernel
 *  @param      kernelHeight    the height of the kernel
 *  @param      kernelWeights   A pointer to an array of kernelWidth * kernelHeight values to be used as the kernel.
 *                              These are in row major order.
 *  @return     A valid MPSImageConvolution object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger)kernelWidth
                          kernelHeight: (NSUInteger)kernelHeight
                               weights: (const float*__nonnull)kernelWeights     NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end


/*!
 *  @class      MPSImageLaplacian
 *  @discussion The MPSImageLaplacian is an optimized variant of the MPSImageConvolution filter provided primarily for ease of use.
 *              This filter uses an optimized convolution filter with a 3 x 3 kernel with the following weights:
 *                  [ 0  1  0
 *                    1 -4  1
 *                    0  1  0 ]
 *
 *              The optimized convolution filter used by MPSImageLaplacian can also be used by creating a MPSImageConvolution
 *              object with kernelWidth = 3, kernelHeight = 3 and weights as specified above.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageLaplacian : MPSUnaryImageKernel

/*! @property    bias
 *  @discussion  The bias is a value to be added to convolved pixel before it is converted back to the storage format.
 *               It can be used to convert negative values into a representable range for a unsigned MTLPixelFormat.
 *               For example, many edge detection filters produce results in the range [-k,k]. By scaling the filter
 *               weights by 0.5/k and adding 0.5, the results will be in range [0,1] suitable for use with unorm formats.
 *               It can be used in combination with renormalization of the filter weights to do video ranging as part
 *               of the convolution effect. It can also just be used to increase the brightness of the image.
 *
 *               Default value is 0.0f.
 */
@property (readwrite, nonatomic) float bias;

@end


/*!
 *  @class      MPSImageBox
 *  @discussion The MPSImageBox convolves an image with given filter of odd width and height. The kernel elements
 *              all have equal weight, achieving a blur effect. (Each result is the unweighted average of the
 *              surrounding pixels.) This allows for much faster algorithms, espcially for larger blur radii.
 *              The box height and width must be odd numbers. The box blur is a separable filter. The implementation 
 *              is aware of this and will act accordingly to give best performance for multi-dimensional blurs.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageBox : MPSUnaryImageKernel


/*! @property kernelHeight
 *  @abstract  The height of the filter window.
 */
@property (readonly, nonatomic)   NSUInteger  kernelHeight;

/*! @property kernelWidth
 *  @abstract  The width of the filter window.
 */
@property (readonly, nonatomic)   NSUInteger  kernelWidth;

/*! @abstract   Initialize a filter for a particular kernel size and device
 *  @param      device  The device the filter will run on
 *  @param      kernelWidth  the width of the kernel.  Must be an odd number.
 *  @param      kernelHeight the height of the kernel. Must be an odd number.
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger)kernelWidth
                          kernelHeight: (NSUInteger)kernelHeight        NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/* You must use initWithDevice:kernelWidth:kernelHeight: instead. */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device    NS_UNAVAILABLE;
@end

/*!
 *  @class      MPSImageTent
 *  @discussion The box filter, while fast, may yield square-ish looking blur effects. However, multiple
 *              passes of the box filter tend to smooth out with each additional pass. For example, two 3-wide
 *              box blurs produces the same effective convolution as a 5-wide tent blur:
 *              @code
 *                      1   1   1
 *                          1   1   1
 *                      +       1   1   1
 *                      =================
 *                      1   2   3   2   1
 *              @endcode
 *              Addition passes tend to approximate a gaussian line shape.
 *
 *              The MPSImageTent convolves an image with a tent filter. These form a tent shape with incrementally
 *              increasing sides, for example:
 *
 *                  1   2   3   2   1
 *
 *
 *                  1   2   1
 *                  2   4   2
 *                  1   2   1
 *
 *              Like the box filter, this arrangement allows for much faster algorithms, espcially for for larger blur
 *              radii but with a more pleasing appearance.
 *
 *              The tent blur is a separable filter. The implementation is aware of this and will act accordingly
 *              to give best performance for multi-dimensional blurs.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface MPSImageTent : MPSImageBox

@end

/*!
 *  @class      MPSImageGaussianBlur
 *  @discussion The MPSImageGaussianBlur convolves an image with gaussian of given sigma in both x and y direction.
 *
 *                  The MPSImageGaussianBlur utilizes a very fast algorith that typically runs at approximately
 *                  1/2 of copy speeds. Notably, it is faster than either the tent or box blur except perhaps
 *                  for very large filter windows. Mathematically, it is an approximate gaussian. Some
 *                  non-gaussian behavior may be detectable with advanced analytical methods such as FFT.  
 *                  If a analytically clean gaussian filter is required, please use the MPSImageConvolution 
 *                  filter instead with an appropriate set of weights. The MPSImageGaussianBlur is intended
 *                  to be suitable for all common image processing needs demanding ~10 bits of precision or
 *                  less.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageGaussianBlur : MPSUnaryImageKernel

/*! @abstract   Initialize a gaussian blur filter for a particular sigma and device
 *  @param      device  The device the filter will run on
 *  @param      sigma   The standard deviation of gaussian blur filter. 
 *                      Gaussian weight, centered at 0, at integer grid i is given as 
 *                            w(i) = 1/sqrt(2*pi*sigma) * exp(-i^2/2*sigma^2)
 *                      If we take cut off at 1% of w(0) (max weight) beyond which weights
 *                      are considered 0, we have 
 *                              ceil (sqrt(-log(0.01)*2)*sigma) ~ ceil(3.7*sigma) 
 *                      as rough estimate of filter width
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                 sigma: (float)sigma                   NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/* You must use initWithDevice:sigma: instead. */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device    NS_UNAVAILABLE;

/*! @property sigma
 *  @abstract Read-only sigma value with which filter was created
 */
@property (readonly, nonatomic) float sigma;

@end

/*!
 *  @class      MPSImageSobel
 *  @discussion The MPSImageSobel implements the Sobel filter.
 *              When the color model (e.g. RGB, two-channel, grayscale, etc.) of source 
 *              and destination textures match, the filter is applied to each channel 
 *              separately. If the destination is monochrome (single channel) but source 
 *              multichannel, the pixel values are converted to grayscale before applying Sobel
 *              operator using the linear gray color transform vector (v).
 *
 *                  Luminance = v[0] * pixel.x + v[1] * pixel.y + v[2] * pixel.z;
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageSobel : MPSUnaryImageKernel

/*! @abstract   Initialize a Sobel filter on a given device using the default color 
 *              transform. Default: BT.601/JPEG {0.299f, 0.587f, 0.114f}
 *
 *              For non-default conversion matrices, use -initWithDevice:linearGrayColorTransform:
 *
 *  @param      device  The device the filter will run on
 *  @return     A valid object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device;

/*! @abstract   Initialize a Sobel filter on a given device with a non-default color transform
 *  @param      device          The device the filter will run on
 *  @param      transform       Array of three floats describing the rgb to gray scale color transform.
 *                @code
 *                          Luminance = transform[0] * pixel.x +
 *                                      transform[1] * pixel.y +
 *                                      transform[2] * pixel.z;
 *                @endcode
 *  @return     A valid object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
              linearGrayColorTransform: (const float * __nonnull) transform      NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));


/*! @property    colorTransform
 *  @discussion  Returns a pointer to the array of three floats used to convert RGBA, RGB or RG images
 *               to the destination format when the destination is monochrome.
 */
@property (readonly, nonatomic, nonnull) const float* colorTransform;

@end  /* MPSImageSobel */



/*!
 *  @class      MPSImagePyramid
 *  @discussion The MPSImagePyramid is a base class for creating different kinds of pyramid images
 *
 *              Currently supported pyramid-types are:
 *              @ref MPSImageGaussianPyramid
 *
 *              The Gaussian image pyramid kernel is enqueued as a in-place operation using
 *              @ref MPSUnaryImageKernel::encodeToCommandBuffer:inPlaceTexture:fallbackCopyAllocator:
 *              and all mipmap levels after level=1, present in the provided image are filled using
 *              the provided filtering kernel. The fallbackCopyAllocator parameter is not used.
 *
 *              The Gaussian image pyramid filter ignores @ref clipRect and @ref offset and fills
 *              the entire mipmap levels.
 *
 *  @note       Make sure your texture type is compatible with mipmapping and supports texture views
 *                  (see @ref MTLTextureUsagePixelFormatView).
 *  @note       Recall the size of the nth mipmap level:
 *              @code
 *                  w_n = max(1, floor(w_0 / 2^n))
 *                  h_n = max(1, floor(h_0 / 2^n)),
 *              @endcode
 *              where w_0, h_0 are the zeroth level width and height. ie the image dimensions themselves.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImagePyramid : MPSUnaryImageKernel

/*! @abstract   Initialize a downwards 5-tap image pyramid with the default filter kernel and device
 *  @param      device  The device the filter will run on
 *
 *  @discussion The filter kernel is the outer product of w = [ 1/16,  1/4,  3/8,  1/4,  1/16 ]^T, with itself
 *
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device;


/*! @abstract   Initialize a downwards 5-tap image pyramid with a central weight parameter and device
 *  @param      device  The device the filter will run on
 *  @param      centerWeight Defines form of the filter-kernel  through the outer product ww^T, where
 *              w = [ (1/4 - a/2),  1/4,  a,  1/4,  (1/4 - a/2) ]^T and 'a' is centerWeight.
 *
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                          centerWeight: (float) centerWeight;


/*! @abstract   Initialize a downwards n-tap pyramid with a custom filter kernel and device
 *  @param      device  The device the filter will run on
 *  @param      kernelWidth The width of the filtering kernel. See @ref MPSImageConvolution.
 *  @param      kernelHeight    The height of the filtering kernel. See @ref MPSImageConvolution.
 *  @param      kernelWeights   A pointer to an array of kernelWidth * kernelHeight values to be
 *                              used as the kernel.
 *                              These are in row major order. See @ref MPSImageConvolution.
 *
 *  @return     A valid object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                           kernelWidth: (NSUInteger)kernelWidth
                          kernelHeight: (NSUInteger)kernelHeight
                               weights: (const float*__nonnull)kernelWeights NS_DESIGNATED_INITIALIZER;


/*! @property kernelHeight
 *  @abstract  The height of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelHeight;

/*! @property kernelWidth
 *  @abstract  The width of the filter window. Must be an odd number.
 */
@property (readonly, nonatomic)   NSUInteger  kernelWidth;

/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSCNNPooling
 *  @param      device      The MTLDevice on which to make the MPSCNNPooling
 *  @return     A new MPSCNNPooling object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end

/*!
 *  @class      MPSImageGaussianPyramid
 *  @discussion The Gaussian image pyramid is constructed as follows:
 *              First the zeroth level mipmap of the input image is filtered with the specified
 *              convolution kernel.
 *              The default the convolution filter kernel is
 *              @code
 *                  k = w w^T, where w = [ 1/16,  1/4,  3/8,  1/4,  1/16 ]^T,
 *              @endcode
 *              but the user may also tweak this kernel with a @ref centerWeight parameter: 'a':
 *              @code
 *                  k = w w^T, where w = [ (1/4 - a/2),  1/4,  a,  1/4,  (1/4 - a/2) ]^T
 *              @endcode
 *              or the user can provide a completely custom kernel. After this the image is downsampled by
 *              removing all odd rows and columns, which defines the next level in the Gaussian image pyramid.
 *              This procedure is continued until every mipmap level present in the image texture are
 *              filled with the pyramid levels.
 *
 *              In case of the Gaussian pyramid the user must run the operation in-place using:
 *              @ref MPSUnaryImageKernel::encodeToCommandBuffer:inPlaceTexture:fallbackCopyAllocator:,
 *              where the fallback allocator is ignored.
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageGaussianPyramid : MPSImagePyramid
@end



/*!
 *  @class      MPSImageLaplacianPyramid
 *  @discussion Laplacian pyramid levels are constructed as difference between the current source level and 2x interpolated version of the
 *              half-resolution source level immediately above it.
 *
 *                  LaplacianMipLevel[l] := GaussianMipLevel[l] – Interpolate(GaussianMipLevel[l + 1])
 *
 *                  The Interpolate function is the classical 2x signal interpolation procedure applied
 *                  to all color channels of the source mip-level in both dimensions.
 *                  It is logically equivalent to the following two-step process :
 *                      1) Zero-stuffing (sometimes called "upsampling").
 *                         It is the process of interleaving source pixel values with zero values:
 *                         dst.at(x, y) := src.at(x, y) if even(x) and even(y) else 0
 *                      2) Filtering (sometimes called "interpolation").
 *                         It is the same procedure as implemented by the MPSImageConvolution class,
 *                         using filter weights provided by the initializer methods inherited from MPSImagePyramid.
 *
 *              The source for Laplacian pyramid construction is typically produced
 *              by the Gaussian pyramid algorithm -- a closely related image processing technique,
 *              but the Laplacian pyramid construction itself makes no assumptions neither about 
 *              the data stored in the source texture nor about the interpolation filter weights,
 *              so Gaussian pyramid is just a conventional name for the source texture.
 *
 *              Please refer to the classical "The Laplacian Pyramid as a Compact Image Code" whitepaper 
 *              by Burt & Anderson, originally published in 532 IEEE TRANSACTIONS ON COMMUNICATIONS, VOL. COM-3l, NO. 4, APRIL 1983
 *              for more detailed discussion.
 *
 *              Since the subtraction operation extends the value range of LaplacianMipLevelRaw
 *              relative to the value range of GaussianMipLevel (even for the case of
 *              normalized interpolation filter), in order to avoid unwanted range clamping
 *              when working with normalized texture types, laplacianBias and laplacianScale class properties
 *              specify point-wise linear mapping of the LaplacianMipLevelRaw result data
 *              into the value range of the destination texture :
 *                  LaplacianRangeScale(pixel, laplacianBias, laplacianScale) := laplacianBias + pixel * laplacianScale,
 *                  LaplacianMipLevelStored[j]                                := LaplacianRangeScale(LaplacianMipLevel[j], laplacianBias, laplacianScale),
 *                  with the default values being laplacianBias = 0.0, laplacianScale = 1.0
 *
 *              Limitations of the current software revision :
 *                 1) In-place operation is not supported, e.g. source and destination textures need 
 *                    to have separate storage and can't be aliased.
 *                 2) The number of channels, bit depth and resolution of the source and destination textures need to match.
 *                 3) Values of the offset and clipRect properties are fixed to the defaults provided by MPSUnaryImageKernel 
 *                    (from which they are inherited), corresponding to no offset applied to the source and unbounded region of interest
 *                    in every destination mip-level; all updates to these properties are ignored.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageLaplacianPyramid : MPSImagePyramid
    @property(readwrite, assign, nonatomic, setter = setLaplacianBias:,  getter = getLaplacianBias  ) float laplacianBias;
    @property(readwrite, assign, nonatomic, setter = setLaplacianScale:, getter = getLaplacianScale ) float laplacianScale;
@end



/*
 *  @class      MPSImageLaplacianPyramidSubtract
 *  @discussion For each mip-level of the destination, MPSImageLaplacianPyramidSubtract constructs Laplacian pyramid
 *              according to the procedure specified in the discussion section for MPSImageLaplacianPyramid
 *
 *              There needs to be at least as many mip-levels in the destination texture
 *              as in the source texture less one, which is the exact number of destination mip-levels
 *              that will be overwritten by MPSImageLaplacianPyramidSubtract, starting from the bottom level. 
 *              Note that the top mip-level of the source texture still contains data required 
 *              for reconstruction of the original Gaussian pyramid data, and it is user's responsibility 
 *              to propagate it around, i.e. via the use of MTLBlitCommandEncoder.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageLaplacianPyramidSubtract : MPSImageLaplacianPyramid
@end



/*
 * @class      MPSImageLaplacianPyramidAdd
 * @discussion The MPSImageLaplacianPyramidAdd class is responsible for reconstruction of Gaussian pyramid
 *             from the Laplacian pyramid supplied in the source texture. Mathematically it is the inverse 
 *             of the process specified in the discussion section for MPSImageLaplacianPyramid.
 *
 *             It is an iterative process, starting from the top mip-level and on each iteration feeding off both 
 *             the LaplacianMipLevel[l] data coming directly from the source and GaussianMipLevel[l + 1] 
 *             just written to the destination on the previous iteration :
 *             
 *                 GaussianMipLevel[l] := LaplacianRangeScale^-1(LaplacianMipLevelStored[l], laplacianBias, laplacianScale) + Interpolate(GaussianMipLevel[l + 1])
 *
 *             As initial state for the first iteration only, the data for GaussianMipLevel[l + 1] in the formula above 
 *             is provided in the LaplacianMipLevel[#top] level. This corresponds to the special handling of the top mip-level of Gaussian pyramid 
 *             discussed for MPSImageLaplacianPyramidSubtract. Just like for MPSImageLaplacianPyramidSubtract, if the destination texture needs 
 *             to contain all mip-level of the Gaussian pyramid including the top level, it can be just from the source texture.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface  MPSImageLaplacianPyramidAdd : MPSImageLaplacianPyramid
@end





#endif    /* MPS_MSImageConvolution_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageGuidedFilter.h
/*!
 *  @header MPSImageGuidedFilter.h
 *  @framework MetalPerformanceShaders
 *
 *  @copyright Copyright (c) 2018 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders guided filter
 */

#ifndef MPS_MPSImageGuidedFilter_h
#define MPS_MPSImageGuidedFilter_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>


/*!
 *  @class      MPSImageGuidedFilter
 *  @discussion Perform Guided Filter to produce a coefficients image
 *              The filter is broken into two stages:
 *                  - Regression
 *                  - Reconstruction
 *
 *              The regression stage learns a 4-channel "coefficient" texture (typically at a very low resolution),
 *              and represents the per-pixel linear regression of the source texture to the guidance texture.
 *
 *              The reconstruction stage upsamples the coefficeints to the same size as the final output and
 *              then at each pixel computes the inner product to produce the output.
 *
 *              The filter is broken into two stages to allow coefficients to be filtered (such as for example - temporally filtering for video to prevent flicker).
 *
 *              There is also support for an optional weight texture that can be used to discard values in the source data.
 *
 *              Guided Filter is described at https://arxiv.org/pdf/1505.00996.pdf.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13.4), ios(11.3), tvos(11.3))
@interface  MPSImageGuidedFilter : MPSKernel

/*! @property   kernelDiameter
 *  @abstract   The local window size
 *  @discussion The local window size.
 */
@property (readonly, nonatomic) NSUInteger kernelDiameter;

/*! @property   epsilon
 *  @abstract   The regularization parameter
 *  @discussion The parameter used when computing the linear coefficients a and b.
 */
@property (readwrite, nonatomic) float epsilon;

/*! @property   reconstructScale
 *  @abstract   The scale parameter
 *  @discussion The parameter used to scale the result of the reconstruction operation.
 *              The default value is 1.0f.
 */
@property (readwrite, nonatomic) float reconstructScale;

/*! @property   reconstructOffset
 *  @abstract   The offset parameter
 *  @discussion The offset parameter added to the result of the scaled reconstructed value.
 *              The default value is 0.0f.
 */
@property (readwrite, nonatomic) float reconstructOffset;

/*!
 *  @abstract Specifies information to apply the guided filter regression.
 *  @param    device            The device the filter will run on
 *  @param    kernelDiameter    The local window size
 *  @return     A valid MPSImageGuidedFilterRegression object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                        kernelDiameter: (NSUInteger) kernelDiameter NS_DESIGNATED_INITIALIZER;

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder: (NSCoder * __nonnull)aDecoder
                                device: (nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 * @abstract Perform Guided Filter Regression (correlation) to produce a coefficients texture
 * @discussion The filter will not begin to execute until after the command buffer has been enqueued and committed.
 *
 * @param  commandBuffer           A valid MTLCommandBuffer.
 * @param  sourceTexture           Input source texture to be filtered (typically a mask).  This should be a single channel image.
 * @param  guidanceTexture         Input guidance texture.  This should be a color (RGB) image.
 * @param  weightsTexture          Optional input confidence texture.  This should also a single channel image.
 * @param  destinationCoefficientsTexture  Output texture with four coefficients that minimize the mean squared error between
 *                                         the source and an affine function of guidance R, G, B.
 * Note: The destinationCoefficientsTexture computes the linear cofficients "a" and "b".  The "a" coefficient is
 *       stored in the RGB channels of destinationCoefficientsTexture and the "b" coefficient in the alpha chnanel.
 *
 *       Set the MPSKernelOptionsAllowReducedPrecision in the "options" property for this kernel to peform the
 *       computations using half-precision arithmetic.  This can potentially improve performance and/or power usage.
 */
- (void)encodeRegressionToCommandBuffer: (nonnull id <MTLCommandBuffer>)commandBuffer
                          sourceTexture: (nonnull id <MTLTexture>)sourceTexture
                        guidanceTexture: (nonnull id <MTLTexture>)guidanceTexture
                         weightsTexture: (nullable id <MTLTexture>)weightsTexture
         destinationCoefficientsTexture: (nonnull id <MTLTexture>)destinationCoefficientsTexture;

/*!
 * @abstract Perform Guided Filter Reconstruction (inference) to produce the filtered output
 * @discussion The filter will not begin to execute until after the command buffer has been enqueued and committed.
 *
 * @pparam sourceGuidanceTexture Input guidance pixel buffer.  This should be a color (RGB) image.
 * @pparam coefficientsTexture   Input coefficients texture generated generated by a previous encodeRegressionToCommandBuffer
 * @param destinationTexture     Output texture
 *
 * Note: The coefficients are upsampled at the reconstruction of the filtered data.
 *       Reconstruct(guidance RGB) = a.r * R + a.g * G + a.b * B + b, where a and b
 *       are the coefficients learnt using encodeRegressionToCommandBuffer.
 *
 *       Final reconstructed value = value * reconstructScale + reconstructOffset
 */
- (void)encodeReconstructionToCommandBuffer: (nonnull id <MTLCommandBuffer>)commandBuffer
                            guidanceTexture: (nonnull id <MTLTexture>)guidanceTexture
                        coefficientsTexture: (nonnull id <MTLTexture>)coefficientsTexture
                         destinationTexture: (nonnull id <MTLTexture>)destinationTexture;

@end  /* MPSImageGuidedFilter */

#endif  /* MPS_MPSImageGuidedFilter_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageKeypoint.h
/*!
 *  @header MPSImageKeypoint.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders Keypoint filters
 */

#ifndef MPS_MPSImageKeypoint_h
#define MPS_MPSImageKeypoint_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>

/*!
 *  @brief      Specifies information to find the keypoints in an image.
 */
typedef struct
{
    NSUInteger maximumKeypoints;    /**< maximum number of keypoints */
    float minimumThresholdValue;    /**< minimum threshold value -  value between 0.0 and 1.0f */
} MPSImageKeypointRangeInfo;

/*!
 *  @brief      Specifies keypoint information.
 */
typedef struct
{
    vector_ushort2 keypointCoordinate;  /**< keypoint (x, y) coordinate */
    float keypointColorValue;           /**< keypoint color value */
} MPSImageKeypointData;


/*!
 *  @class      MPSImageFindKeypoints
 *  @discussion The MPSImageFindKeypoints kernel is used to find a list of keypoints whose values are >= minimumPixelThresholdValue
 *              in MPSImageKeypointRangeInfo. The keypoints are generated for a specified region in the image.  
 *              The pixel format of the source image must be MTLPixelFormatR8Unorm.
 *
 */
MPS_CLASS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSImageFindKeypoints : MPSKernel

/*! @property   keypointRangeInfo
 *  @abstract   Return a structure describing the keypoint range info
 *  @discussion Returns a MPSImageKeypointRangeInfo structure 
 */
@property (readonly, nonatomic)  MPSImageKeypointRangeInfo keypointRangeInfo;

/*!
 *  @abstract Specifies information to find keypoints in an image.
 *  @param    device    The device the filter will run on
 *  @param    info      Pointer to the MPSImageKeypointRangeInfo struct
 *  @return   A valid MPSImageFindKeypoints object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                                  info: (const MPSImageKeypointRangeInfo * __nonnull) info NS_DESIGNATED_INITIALIZER;

/* You must use initWithDevice:keypointRangeInfo: instead */
-(nonnull instancetype) initWithDevice:(nonnull id<MTLDevice>)device            NS_UNAVAILABLE;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract Encode the filter to a command buffer using a MTLComputeCommandEncoder.
 *  @discussion The filter will not begin to execute until after the command
 *  buffer has been enqueued and committed.
 *
 *  @param  commandBuffer               A valid MTLCommandBuffer.
 *  @param  source                      A valid MTLTexture containing the source image for the filter.
 *  @param  regions                     An array of rectangles that describe regions in the image.
 *                                      The list of keypoints is generated for each individual rectangle specifed.
 *  @param  keypointCountBuffer         The list of keypoints for each specified region
 *  @param  keypointCountBufferOffset   Byte offset into keypointCountBufferOffset buffer at which to write the keypoint results.
 *                                      Must be a multiple of 32 bytes.
 *  @param  keypointDataBuffer          A valid MTLBuffer to receive the keypoint data results for each rectangle.
 *                                      The keypoint data for keypoints in each rectangle are stored consecutively.
 *                                      The keypoint data for each rectangle starts at the following offset:
 *                                          MPSImageKeypointRangeInfo.maximumKeyPoints * rectangle index
 *  @param  keypointDataBufferOffset    Byte offset into keypointData buffer at which to write the keypoint results.
 *                                      Must be a multiple of 32 bytes.
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                sourceTexture: (nonnull id <MTLTexture>) source
                      regions: (const MTLRegion * __nonnull) regions
              numberOfRegions: (NSUInteger) numberOfRegions
          keypointCountBuffer: (nonnull id <MTLBuffer>) keypointCountBuffer
    keypointCountBufferOffset: (NSUInteger) keypointCountBufferOffset
           keypointDataBuffer: (nonnull id <MTLBuffer>) keypointDataBuffer
     keypointDataBufferOffset: (NSUInteger) keypointDataBufferOffset;

@end  /* MPSImageFindKeypoints */

#endif  /* MPSImageKeypoint.h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageHistogram.h
/*!
 *  @header MPSImageHistogram.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2015 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders histogram filters
 */

#ifndef MPS_MPSImageHistogram_h
#define MPS_MPSImageHistogram_h

#include <MPSImage/MPSImageKernel.h>
#include <simd/simd.h>

/*!
 *  @brief      Specifies information to compute the histogram for channels of an image.
 */
typedef struct
{
    NSUInteger      numberOfHistogramEntries;   /**<  Specifies the number of histogram entries, or "bins" for each channel.  For example, if you want 256 histogram bins then numberOfHistogramEntries must be set to 256.  The value stored in each histogram bin is a 32-bit unsigned integer.  The size of the histogram buffer in which these bins will be stored should be >= numberOfHistogramEntries * sizeof(uint32_t) * number of channels in the image. numberOfHistogramEntries must be a power of 2.   */
    BOOL            histogramForAlpha;          /**<  Specifies whether the histogram for the alpha channel should be computed or not. */
    vector_float4   minPixelValue;              /**<  Specifies the minimum pixel value.  Any pixel value less than this will be clipped to this value (for the purposes of histogram calculation), and assigned to the first histogram entry. This minimum value is applied to each of the four channels separately. */
    vector_float4   maxPixelValue;              /**<  Specifies the maximum pixel value.  Any pixel value greater than this will be clipped to this value (for the purposes of histogram calculation), and assigned to the last histogram entry. This maximum value is applied to each of the four channels separately. */
} MPSImageHistogramInfo;


/*!
 *  @class      MPSImageHistogram
 *  @discussion The MPSImageHistogram computes the histogram of an image.
 *              
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageHistogram : MPSKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*! @property   zeroHistogram
 *  @abstract   Zero-initalize the histogram results
 *  @discussion Indicates that the memory region in which the histogram results are to be written in the
 *              histogram buffer are to be zero-initialized or not. Default: YES.
 */
@property (readwrite, nonatomic) BOOL zeroHistogram;

/*! @property   minPixelThresholdValue
 *  @abstract   The minimum pixel threshold value
 *  @discussion The histogram entries will be incremented only if pixel value is >= minPixelThresholdValue.
 *              The minPixelThresholdValue is a floating-point value.  For unsigned normalized textures, the 
 *              minPixelThresholdValue should be a value between 0.0f and 1.0f (for eg. MTLPixelFormatRGBA8Unorm).
 *              For signed normalized textures, the minPixelThresholdValue should be a value between -1.0f and 1.0f
 *              (for eg. MTLPixelFormatRGBA8Snorm).  Default: vector_float4(0.0f).
 */
@property (readwrite, nonatomic) vector_float4 minPixelThresholdValue;

/*! @property   histogramInfo
 *  @abstract   Return a structure describing the histogram content
 *  @discussion Returns a MPSImageHistogramInfo structure describing the format of the
 *              histogram.
 */
@property (readonly, nonatomic)  MPSImageHistogramInfo histogramInfo;

/*!
 *  @abstract Specifies information to compute the histogram for channels of an image.
 *  @param    device            The device the filter will run on
 *  @param    histogramInfo     Pointer to the MPSHistogramInfo struct
 *  @return     A valid MPSImageHistogram object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         histogramInfo: (const MPSImageHistogramInfo * __nonnull) histogramInfo     NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                        MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract Encode the filter to a command buffer using a MTLComputeCommandEncoder.
 *  @discussion The filter will not begin to execute until after the command
 *  buffer has been enqueued and committed.
 *
 *
 *  @param  commandBuffer           A valid MTLCommandBuffer.
 *  @param  source                  A valid MTLTexture containing the source image for the filter
 *  @param  histogram               A valid MTLBuffer to receive the histogram results.
 *  @param  histogramOffset         Byte offset into histogram buffer at which to write the histogram results. Must be a multiple of 32 bytes.
 *                                  The histogram results / channel are stored together.  The number of channels for which
 *                                  histogram results are stored is determined by the number of channels in the image.
 *                                  If histogramInfo.histogramForAlpha is false and the source image is RGBA then only histogram
 *                                  results for RGB channels are stored.
 *
 *                                  The histogram results are stored in the histogram buffer as follows:
 *                                      - histogram results for the R channel for all bins followed by
 *                                      - histogram results for the G channel for all bins followed by
 *                                      - histogram results for the B channel for all bins followed by
 *                                      - histogram results for the A channel for all bins
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                sourceTexture: (nonnull id <MTLTexture>) source
                    histogram: (nonnull id <MTLBuffer>) histogram
              histogramOffset: (NSUInteger) histogramOffset;


/*!
 *  @abstract   The amount of space in the output MTLBuffer the histogram will take up.
 *  @discussion This convenience function calculates the minimum amount of space
 *              needed in the output histogram for the results.  The MTLBuffer should
 *              be at least this length, longer if histogramOffset is non-zero.
 *  @param      sourceFormat      The MTLPixelFormat of the source image. This is
 *                                the source parameter of -encodeToCommandBuffer:
 *                                sourceTexture:histogram:histogramOffset
 *  @return     The number of bytes needed to store the result histograms.
 */
-(size_t) histogramSizeForSourceFormat: (MTLPixelFormat) sourceFormat;

@end  /* MPSImageHistogram */


/*!
 *  @class      MPSImageNormalizedHistogram
 *  @discussion The MPSImageNormalizedHistogram computes the normalized histogram of an image.
 *              The minimum and maximum pixel values for a given region of an image are first computed.
 *              The max(computed minimum pixel value, MPSImageHistogramInfo.minPixelValue) and the
 *              min(computed maximum pixel value, MPSImageHistogramInfo.maxPixelValue) are used to
 *              compute the normalized histogram.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageNormalizedHistogram : MPSKernel

/*! @property   clipRectSource
 *  @abstract   The source rectangle to use when reading data.
 *  @discussion A MTLRegion that indicates which part of the source to read. If the clipRectSource does not lie
 *              completely within the source image, the intersection of the image bounds and clipRectSource will
 *              be used. The clipRectSource replaces the MPSUnaryImageKernel offset parameter for this filter.
 *              The latter is ignored.   Default: MPSRectNoClip, use the entire source texture.
 */
@property (readwrite, nonatomic) MTLRegion clipRectSource;

/*! @property   zeroHistogram
 *  @abstract   Zero-initalize the histogram results
 *  @discussion Indicates that the memory region in which the histogram results are to be written in the
 *              histogram buffer are to be zero-initialized or not. Default: YES.
 */
@property (readwrite, nonatomic) BOOL zeroHistogram;

/*! @property   histogramInfo
 *  @abstract   Return a structure describing the histogram content
 *  @discussion Returns a MPSImageHistogramInfo structure describing the format of the
 *              histogram.
 */
@property (readonly, nonatomic)  MPSImageHistogramInfo histogramInfo;

/*!
 *  @abstract Specifies information to compute the histogram for channels of an image.
 *  @param    device            The device the filter will run on
 *  @param    histogramInfo     Pointer to the MPSImageHistogramInfo struct
 *  @return     A valid MPSImageNormalizedHistogram object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         histogramInfo: (const MPSImageHistogramInfo * __nonnull) histogramInfo     NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract Encode the filter to a command buffer using a MTLComputeCommandEncoder.
 *  @discussion The filter will not begin to execute until after the command
 *  buffer has been enqueued and committed.
 *
 *
 *  @param  commandBuffer           A valid MTLCommandBuffer.
 *  @param  source                  A valid MTLTexture containing the source image for the filter
 *  @param  minmaxTexture           A valid MTLTexture in which the min/max pixel values from source will be returned
 *  @param  histogram               A valid MTLBuffer to receive the histogram results.
 *  @param  histogramOffset         Byte offset into histogram buffer at which to write the histogram results. Must be a multiple of 32 bytes.
 *                                  The histogram results / channel are stored together.  The number of channels for which
 *                                  histogram results are stored is determined by the number of channels in the image.
 *                                  If histogramInfo.histogramForAlpha is false and the source image is RGBA then only histogram
 *                                  results for RGB channels are stored.
 *
 *                                  The histogram results are stored in the histogram buffer as follows:
 *                                      - histogram results for the R channel for all bins followed by
 *                                      - histogram results for the G channel for all bins followed by
 *                                      - histogram results for the B channel for all bins followed by
 *                                      - histogram results for the A channel for all bins
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                sourceTexture: (nonnull id <MTLTexture>) source
                minmaxTexture: (nonnull id <MTLTexture>) minmaxTexture
                    histogram: (nonnull id <MTLBuffer>) histogram
              histogramOffset: (NSUInteger) histogramOffset;


/*!
 *  @abstract   The amount of space in the output MTLBuffer the histogram will take up.
 *  @discussion This convenience function calculates the minimum amount of space
 *              needed in the output histogram for the results.  The MTLBuffer should
 *              be at least this length, longer if histogramOffset is non-zero.
 *  @param      sourceFormat      The MTLPixelFormat of the source image. This is
 *                                the source parameter of -encodeToCommandBuffer:
 *                                sourceTexture:histogram:histogramOffset
 *  @return     The number of bytes needed to store the result histograms.
 */
-(size_t) histogramSizeForSourceFormat: (MTLPixelFormat) sourceFormat;

@end  /* MPSImageNormalizedHistogram */


/*!
 *  @class      MPSImageHistogramEqualization
 *  @discussion The MPSImageHistogramEqualization performs equalizes the histogram of an image.
 *              The process is divided into three steps. 
 *
 *              -# Call -initWithDevice:histogramInfo:   This creates a MPSImageHistogramEqualization
 *              object.   It is done when the method returns.
 *  
 *              -# Call -encodeTransform:sourceTexture:histogram:histogramOffset:  This creates a privately held
 *              image transform (i.e. a cumulative distribution function of the histogram) which will be used to 
 *              equalize the distribution of the histogram of the source image. This process runs on a MTLCommandBuffer
 *              when it is committed to a MTLCommandQueue. It must complete before the next step can be run.
 *              It may be performed on the same MTLCommandBuffer.  The histogram argument specifies the histogram
 *              buffer which contains the histogram values for sourceTexture.  The sourceTexture argument is used by
 *              encodeTransform to determine the number of channels and therefore which histogram data in histogram 
 *              buffer to use. The histogram for sourceTexture must have been computed either on the CPU or using 
 *              the MPSImageHistogram kernel
 *
 *              -# Call -encodeToCommandBuffer:sourceTexture:destinationTexture: to read data from
 *              sourceTexture, apply the equalization transform to it and write to destination texture.
 *              This step is also done on the GPU on a MTLCommandQueue.
 *
 *              You can reuse the same equalization transform on other images to perform the
 *              same transform on those images. (Since their distribution is probably different,
 *              they will probably not be equalized by it.) This filter usually will not be able 
 *              to work in place.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageHistogramEqualization : MPSUnaryImageKernel

/*! @property   histogramInfo
 *  @abstract   Return a structure describing the histogram content
 *  @discussion Returns a MPSImageHistogramInfo structure describing the format of the
 *              histogram.
 */
@property (readonly, nonatomic)  MPSImageHistogramInfo histogramInfo;

/*!
 *  @abstract Specifies information about the histogram for the channels of an image.
 *  @param    device            The device the filter will run on
 *  @param    histogramInfo     Pointer to the MPSHistogramInfo struct
 *  @return     A valid MPSImageHistogramEqualization object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         histogramInfo: (const MPSImageHistogramInfo * __nonnull) histogramInfo     NS_DESIGNATED_INITIALIZER;

/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract Encode the transform function to a command buffer using a MTLComputeCommandEncoder.
 *            The transform function computes the equalization lookup table.
 *  @discussion The transform function will not begin to execute until after the command
 *              buffer has been enqueued and committed.  This step will need to be repeated
 *              with the new MPSKernel if -copyWithZone:device or -copyWithZone: is called.
 *              The transform is stored as internal state to the object. You still need to 
 *              call -encodeToCommandBuffer:sourceTexture:destinationTexture: afterward
 *              to apply the transform to produce a result texture.
 *
 *  @param  commandBuffer   A valid MTLCommandBuffer.
 *  @param  source          A valid MTLTexture containing the source image for the filter.
 *  @param  histogram       A valid MTLBuffer containing the histogram results for an image.  This filter
 *                          will use these histogram results to generate the cumulative histogram for equalizing
 *                          the image.  The histogram results / channel are stored together.  The number of channels
 *                          for which histogram results are stored is determined by the number of channels in the image.
 *                          If histogramInfo.histogramForAlpha is false and the source image is RGBA then only histogram
 *                          results for RGB channels are stored.
 *  @param  histogramOffset A byte offset into the histogram MTLBuffer where the histogram starts. Must conform to
 *                          alignment requirements for [MTLComputeCommandEncoder setBuffer:offset:atIndex:] offset
 *                          parameter.
 */
 -(void) encodeTransformToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                          sourceTexture: (nonnull id <MTLTexture>) source
                              histogram: (nonnull id <MTLBuffer>) histogram
                        histogramOffset: (NSUInteger) histogramOffset;



@end  /* MPSImageHistogramEqualization */

/*!
 *  @class      MPSImageHistogramSpecification
 *  @discussion The MPSImageHistogramSpecification performs a histogram specification operation on an image.
 *              It is a generalized version of histogram equalization operation.  The histogram specificaiton filter
 *              converts the image so that its histogram matches the desired histogram.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(9.0), tvos(9.0))
@interface  MPSImageHistogramSpecification : MPSUnaryImageKernel

/*! @property   histogramInfo
 *  @abstract   Return a structure describing the histogram content
 *  @discussion Returns a MPSImageHistogramInfo structure describing the format of the
 *              histogram.
 */
@property (readonly, nonatomic)  MPSImageHistogramInfo histogramInfo;

/*!
 *  @abstract Specifies information about the histogram for the channels of an image.
 *  @discussion The MPSImageHistogramSpecification applies a transfor to convert the histogram 
 *              to a specified histogram. The process is divided into three steps:
 *
 *              -# Call -initWithDevice:histogramInfo:   This creates a MPSImageHistogramSpecification
 *              object.  It is done when the method returns.
 *
 *              -# Call -encodeTransform:sourceTexture:sourceHistogram:sourceHistogramOffset:desiredHistogram:
 *              desiredHistogramOffset: This creates a privately held image transform which will convert the
 *              the distribution of the source histogram to the desired histogram. This process runs on a 
 *              MTLCommandBuffer when it is committed to a MTLCommandQueue. It must complete before the next 
 *              step can be run. It may be performed on the same MTLCommandBuffer.  The sourceTexture argument 
 *              is used by encodeTransform to determine the number of channels and therefore which histogram data 
 *              in sourceHistogram buffer to use. The sourceHistogram and desiredHistogram must have been computed 
 *              either on the CPU or using the MPSImageHistogram kernel
 *
 *              -# Call -encodeToCommandBuffer:sourceTexture:destinationTexture: to read data from
 *              sourceTexture, apply the transform to it and write to destination texture.
 *              This step is also done on the GPU on a MTLCommandQueue.
 *
 *              You can reuse the same specification transform on other images to perform the
 *              same transform on those images. (Since their starting distribution is probably
 *              different, they will probably not arrive at the same distribution as the desired
 *              histogram.) This filter usually will not be able to work in place.
 *
 *  @param    device            The device the filter will run on
 *  @param    histogramInfo     Pointer to the MPSHistogramInfo struct
 *  @return     A valid MPSImageHistogramSpecification object or nil, if failure.
 */

-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                         histogramInfo: (const MPSImageHistogramInfo * __nonnull) histogramInfo     NS_DESIGNATED_INITIALIZER;


/*! @abstract NSSecureCoding compatability
 *  @discussion While the standard NSSecureCoding/NSCoding method
 *              -initWithCoder: should work, since the file can't
 *              know which device your data is allocated on, we
 *              have to guess and may guess incorrectly.  To avoid
 *              that problem, use initWithCoder:device instead.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSKernel
 *  @param      device      The MTLDevice on which to make the MPSKernel
 *  @return     A new MPSKernel object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
                            MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract Encode the transform function to a command buffer using a MTLComputeCommandEncoder.
 *            The transform function computes the specification lookup table.
 *  @discussion The transform function will not begin to execute until after the command
 *              buffer has been enqueued and committed. This step will need to be repeated
 *              with the new MPSKernel if -copyWithZone:device or -copyWithZone: is called.
 *
 *  @param  commandBuffer   A valid MTLCommandBuffer.
 *  @param  source          A valid MTLTexture containing the source image for the filter.
 *  @param  sourceHistogram A valid MTLBuffer containing the histogram results for the source image.  This filter
 *                          will use these histogram results to generate the cumulative histogram for equalizing
 *                          the image.  The histogram results / channel are stored together.  The number of channels
 *                          for which histogram results are stored is determined by the number of channels in the image.
 *                          If histogramInfo.histogramForAlpha is false and the source image is RGBA then only histogram
 *                          results for RGB channels are stored.
 *  @param  sourceHistogramOffset   A byte offset into the sourceHistogram MTLBuffer where the histogram starts. Must conform to
 *                                  alignment requirements for [MTLComputeCommandEncoder setBuffer:offset:atIndex:] offset
 *                                  parameter.
 *  @param  desiredHistogram    A valid MTLBuffer containing the desired histogram results for the source image.
 *                          The histogram results / channel are stored together.  The number of channels
 *                          for which histogram results are stored is determined by the number of channels in the image.
 *                          If histogramInfo.histogramForAlpha is false and the source image is RGBA then only histogram
 *                          results for RGB channels are stored.
 *  @param  desiredHistogramOffset  A byte offset into the desiredHistogram MTLBuffer where the histogram starts. Must conform to
 *                                  alignment requirements for [MTLComputeCommandEncoder setBuffer:offset:atIndex:] offset
 *                                  parameter.
 */
-(void) encodeTransformToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                         sourceTexture: (nonnull id <MTLTexture>) source
                       sourceHistogram: (nonnull id <MTLBuffer>) sourceHistogram
                 sourceHistogramOffset: (NSUInteger) sourceHistogramOffset
                      desiredHistogram: (nonnull id <MTLBuffer>) desiredHistogram
                desiredHistogramOffset: (NSUInteger) desiredHistogramOffset;



@end  /* MPSImageHistogramSpecification */

#endif  /* MPS_MSImageHistogram_h */

// ==========  MetalPerformanceShaders.framework/Frameworks/MPSImage.framework/Headers/MPSImageTypes.h
//
//  MPSImageTypes.h
//  MPS
//
//  Created by Ian Ollmann on 8/20/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSImageTypes_h
#define MPSImageTypes_h

#import <MPSCore/MPSCoreTypes.h>


#ifdef __cplusplus
extern "C" {
#endif

    

/*! @typedef MPSAlphaType
 *  @abstract Premultiplication description for the color channels of a texture
 *  @discussion Some image data is premultiplied. That is to say that the color channels
 *              are stored instead as color * alpha. This is an optimization for image compositing
 *              (alpha blending), but it can get in the way of most other image filters,
 *              especially those that apply non-linear affects like the MPSImageParametricCurveTransform
 *              multidimensional lookup tables, and functions like convolution or resampling filters
 *              that look at adjacent pixels, where the alpha may not be the same.
 *  @code
 *              Some basic conversion cases:
 *                  source                              destination                         operation
 *                  ------                              -----------                         ---------
 *                  MPSAlphaTypeNonPremultiplied        MPSAlphaTypeNonPremultiplied        <none>
 *                  MPSAlphaTypeNonPremultiplied        MPSAlphaTypeAlphaIsOne              composite with opaque background color
 *                  MPSAlphaTypeNonPremultiplied        MPSAlphaTypePremultiplied           multiply color channels by alpha
 *                  MPSAlphaTypeAlphaIsOne              MPSAlphaTypeNonPremultiplied        set alpha to 1
 *                  MPSAlphaTypeAlphaIsOne              MPSAlphaTypeAlphaIsOne              set alpha to 1
 *                  MPSAlphaTypeAlphaIsOne              MPSAlphaTypePremultiplied           set alpha to 1
 *                  MPSAlphaTypePremultiplied           MPSAlphaTypeNonPremultiplied        divide color channels by alpha
 *                  MPSAlphaTypePremultiplied           MPSAlphaTypeAlphaIsOne              composite with opaque background color
 *                  MPSAlphaTypePremultiplied           MPSAlphaTypePremultiplied           <none>
 *  @endcode
 *
 *              Color space conversion operations require the format to be either MPSPixelAlpha_NonPremultiplied or
 *              MPSPixelAlpha_AlphaIsOne to work correctly. A number of MPSKernels have similar requirements. If
 *              premultiplied data is provided or requested, extra operations will be added to the conversion to
 *              ensure correct operation. Fully opaque images should use MPSAlphaTypeAlphaIsOne.
 *
 *  @constant   MPSAlphaTypeNonPremultiplied   Image is not premultiplied by alpha. Alpha is not guaranteed to be 1. (kCGImageAlphaFirst/Last)
 *  @constant   MPSAlphaTypeAlphaIsOne         Alpha is guaranteed to be 1, even if it is not encoded as 1 or not encoded at all. (kCGImageAlphaNoneSkipFirst/Last, kCGImageAlphaNone)
 *  @constant   MPSAlphaTypePremultiplied      Image is premultiplied by alpha. Alpha is not guaranteed to be 1. (kCGImageAlphaPremultipliedFirst/Last)
 */

#if defined(DOXYGEN)
typedef enum MPSAlphaType
#else
typedef NS_ENUM( NSUInteger, MPSAlphaType )
#endif
{
    MPSAlphaTypeNonPremultiplied   MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0)) MPS_SWIFT_NAME(nonPremultiplied)  = 0,
    MPSAlphaTypeAlphaIsOne         MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))  = 1,
    MPSAlphaTypePremultiplied      MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(10.0), tvos(10.0))  = 2
}
#if defined(DOXYGEN)
    MPSAlphaType
#endif
;
    
    

#ifdef __cplusplus
}
#endif

#endif /* MPSImageTypes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixFindTopK.h
//
//  MPSMatrixFindTopK.h
//  MPSMatrix
//
//  Created by Teemu Rantalaiho on 8/11/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSMatrixFindTopK_h
#define MPSMatrixFindTopK_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>


/*!
 *  @class      MPSMatrixFindTopK
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel that find top-K values and their corresponding indices withing a row of a matrix
 *
 *  @discussion A MPSMatrixFindTopK object computes finds the 'k' largest values within
 *              a row of a matrix and returns the value found and the index of the entry
 *              in the source matrix. This operation is performed independently on the
 *              rows and matrices in batch of the source matrix.
 *
 */

MPS_CLASS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3))
@interface MPSMatrixFindTopK : MPSMatrixUnaryKernel

/*! @property   sourceRows
 *
 *  @discussion The number of rows to consider from the source in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrix available starting from sourceMatrixOrigin.x,
 *              indicating that by default the whole source matrix is used.
 *              If a different size is desired then this should be modified prior to
 *              encoding the kernel.
 *              It is the user's responsibility to ensure that the resultIndexMatrix and resultValueMatrix
 *              parameters in encodeToCommandBuffer are large enough to accommodate the results of this
 *              operation, otherwise the results of the encode call are undefined.
 *              NOTE: sourceMatrixOrigin and resultMatrixOrigin from MPSMatrixUnaryKernel
 *              can be used to control the starting points in the source and destination
 *              at kernel encode time (see encodeToCommandBuffer).
 */
@property (readwrite, nonatomic) NSUInteger sourceRows;

/*! @property   sourceColumns
 *
 *  @discussion The number of columns to consider from the source in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrix available starting from sourceMatrixOrigin.y,
 *              indicating that by default the whole source matrix is used.
 *              If a different size is desired then this should be modified prior to
 *              encoding the kernel.
 *              It is the user's responsibility to ensure that the resultIndexMatrix and resultValueMatrix
 *              parameters in encodeToCommandBuffer are large enough to accommodate the results of this
 *              operation, otherwise the results of the encode call are undefined.
 *              NOTE: sourceMatrixOrigin and resultMatrixOrigin from MPSMatrixUnaryKernel
 *              can be used to control the starting points in the source and destination
 *              at kernel encode time (see encodeToCommandBuffer).
 */
@property (readwrite, nonatomic) NSUInteger sourceColumns;

/*! @property   indexOffset
 *
 *  @discussion Specifies a number that will be added to all the indices written to
 *              resultIndexMatrix in encodeToCommandBuffer. This value can be used
 *              to offset later computations for example by adding the value for
 *              the source matrix column offset sourceMatrixOrigin.y.
 *              Example: Let numberOfTopKValues be 3, let the source be the following:
 *
 *                  source = [ 6.0, 3.0, 8.0, 1.0, 9.0, 4.0, 5.0 ]
 *
 *              and let the sourceMatrixOrigin.y = 2.
 *
 *              Then if indexOffset = 2 then the result value and result index matrices will be:
 *
 *                  result values  = [ 9.0, 8.0, 5.0 ]
 *                  result indices = [  4 ,  2 ,  6  ],
 *
 *              which gives the user indices into the original source matrix.
 *
 *              On the other hand if the indexOffset = 0 then the results  are as follows:
 *
 *                  result values  = [ 9.0, 8.0, 5.0 ]
 *                  result indices = [  2 ,  0 ,  4  ],
 *
 *              which on the other hand gives the user indices into the submatrix starting
 *              from sourceMatrixOrigin.y == 2.
 *
 *              This property is modifiable and defaults to 0. If a different behavior
 *              is desired then this should be modified prior to encoding the kernel.
 */
@property (readwrite, nonatomic) NSUInteger indexOffset;

/*! @property   numberOfTopKValues
 *
 *  @discussion The number of highest values (and their indices) to be found in each row
 *              by the kernel. This property is initialized in the kernel initialization call
 *              initWithDevice, but can be modified before encoding the kernel.
 *              Must be less or equal to 16 and requesting more values results in undefined behavior.
 *              It is the user's responsibility to ensure that the resultIndexMatrix and resultValueMatrix
 *              parameters in encodeToCommandBuffer are large enough to accommodate the results of this
 *              operation, otherwise the results of the encode call are undefined.
 */
@property (readwrite, nonatomic) NSUInteger numberOfTopKValues;


/*!
 *  @abstract   Initialize an MPSMatrixFindTopK object on a device for a given size.
 *
 *  @param      device              The device on which the kernel will execute.
 *  @param      numberOfTopKValues  The number of largest values to find from each row,
 *                                  must be less or equal to 16.
 *
 *  @return     A valid MPSMatrixFindTopK object or nil, if failure.
 */

// FIXME: Fix availability macros to 10.14 and 12.0 once we get there
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                    numberOfTopKValues: (NSUInteger) numberOfTopKValues
NS_DESIGNATED_INITIALIZER;



/*!
 @discussion Use the above initialization method instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;


/*!
 *  @abstract   Encode a MPSMatrixFindTopK object to a command buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix         A valid MPSMatrix object which specifies the input matrix.
 *
 *  @param      resultIndexMatrix   A valid MPSMatrix object which specifies the matrix which will
 *                                  be overwritten by the result indices.
 *                                  This matrix must have datatype MPSDataTypeUInt32.
 *  @param      resultValueMatrix   A valid MPSMatrix object which specifies the matrix which will
 *                                  be overwritten by the result values.
 *
 *  @discussion Certain constraints apply to the sizes of the matrices depending on the sizes requested at
 *              initialization time as well as the origins at the time this routine is called:
 *
 *              Both result matrices must be large enough to hold a two dimensional array of 'sourceRows' rows and
 *              'numberOfTopKValues' columns beginning at resultMatrixOrigin.
 *
 *              The source matrix must be large enough to contain at least 'numberOfTopKValues' values
 *              starting from sourceMatrixOrigin.y.
 *
 *              Each matrix within the range specified by batchStart and batchSize, which also specifies a valid
 *              set of matrices within inputMatrix, resultIndexMatrix and resultValueMatrix, will be processed.
 *
 *              The datatypes of the matrices inputMatrix and resultValueMatrix must match and be either
 *              MPSDataTypeFloat32 or MPSDataTypeFloat16.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
            resultIndexMatrix: (MPSMatrix * __nonnull) resultIndexMatrix
            resultValueMatrix: (MPSMatrix * __nonnull) resultValueMatrix
MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:resultIndexMatrix:resultValueMatrix:));



/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixFindTopK
 *  @param      device      The MTLDevice on which to make the MPSMatrixFindTopK
 *  @return     A new MPSMatrixFindTopK object, or nil if failure.
 */

// FIXME: Fix availability macros to 10.14 and 12.0 once we get there
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixFindTopK


#endif /* MPSMatrixFindTopK_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixTypes.h
//
//  MPSMatrixTypes.h
//  MPS
//
//  Created by Ian Ollmann on 8/20/16.
//  Copyright © 2016 Apple. All rights reserved.
//

#ifndef MPSMatrixTypes_h
#define MPSMatrixTypes_h

#ifndef __METAL_VERSION__
#   include <stdint.h>  // For uint32_t.
#endif
/*!
 *  @struct     MPSMatrixOffset
 *  @memberof   MPSMatrix
 *  @abstract   Specifies a row and column offset into an MPSMatrix.
 */
typedef struct
{
    uint32_t    rowOffset;        /**< offset to start of source region to read in rows */
    uint32_t    columnOffset;     /**< offset to start of source region to read in columns */
} MPSMatrixOffset;

// Hide the rest of the header from metal shading language
#ifndef __METAL_VERSION__

#import <MPSCore/MPSKernel.h>
#import <MPSCore/MPSCoreTypes.h>

#ifdef __cplusplus
extern "C" {
#endif
/*!
*  @class      MPSMatrixDescriptor
*
*  @dependency This depends on Metal.framework
*
*  @discussion A MPSMatrixDescriptor describes the sizes, strides, and data type of a
*              an array of 2-dimensional matrices.  All storage is assumed to be in 
*              "matrix-major".  See the description for MPSMatrix for further details.
*/
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSMatrixDescriptor: NSObject

/*! @property   rows
 *  @discussion The number of rows in a matrix.
 */
@property (readwrite, nonatomic) NSUInteger rows;

/*! @property   columns
 *  @discussion The number of columns in a matrix.
 */
@property (readwrite, nonatomic) NSUInteger columns;

/*! @property   matrices
 *  @discussion The number of matrices.
 */
@property (readonly, nonatomic) NSUInteger matrices MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*! @property   dataType
 *  @discussion The type of the data which makes up the values of the matrix.
 */
@property (readwrite, nonatomic) MPSDataType dataType;

/*! @property   rowBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive rows.  Must be a multiple of the element size.
 */
@property (readwrite, nonatomic) NSUInteger rowBytes;

/*! @property   matrixBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive matrices.  Must be a multiple of rowBytes.
 */
@property (readonly, nonatomic) NSUInteger matrixBytes MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Create a MPSMatrixDescriptor with the specified dimensions and data type.
 *
 *  @param      rows                The number of rows of the matrix.
 *
 *  @param      columns             The number of columns of the matrix.
 *
 *  @param      rowBytes            The number of bytes between starting elements of consecutive
 *                                  rows.  Must be a multiple of the element size.
 *
 *  @param      dataType            The type of the data to be stored in the matrix.
 *
 *  @discussion For performance considerations the optimal row stride may not necessarily be equal
 *              to the number of columns in the matrix.  The MPSMatrix class provides a method which
 *              may be used to determine this value, see the rowBytesForColumns API in the MPSMatrix
 *              class.
 *              The number of matrices described is initialized to 1.
 */
+(__nonnull instancetype) matrixDescriptorWithDimensions: (NSUInteger)              rows
                                                 columns: (NSUInteger)              columns
                                                rowBytes: (NSUInteger)              rowBytes
                                                dataType: (MPSDataType)             dataType
MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Use matrixDescriptorWithRows:columns:rowBytes:dataType instead.",
                                      ios(10.0, 11.0), tvos(10.0, 11.0));

+(__nonnull instancetype) matrixDescriptorWithRows: (NSUInteger)              rows
                                           columns: (NSUInteger)              columns
                                          rowBytes: (NSUInteger)              rowBytes
                                          dataType: (MPSDataType)             dataType
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
/*!
 *  @abstract   Create a MPSMatrixDescriptor with the specified dimensions and data type.
 *
 *  @param      rows                The number of rows of a single matrix.
 *
 *  @param      columns             The number of columns of a single matrix.
 *
 *  @param      matrices            The number of matrices in the MPSMatrix object.
 *
 *  @param      rowBytes            The number of bytes between starting elements of consecutive
 *                                  rows.  Must be a multiple of the element size.
 *
 *  @param      matrixBytes         The number of bytes between starting elements of consecutive
 *                                  matrices.  Must be a multiple of rowBytes.
 *
 *  @param      dataType            The type of the data to be stored in the matrix.
 *
 *  @discussion For performance considerations the optimal row stride may not necessarily be equal
 *              to the number of columns in the matrix.  The MPSMatrix class provides a method which
 *              may be used to determine this value, see the rowBytesForColumns API in the MPSMatrix
 *              class.
 */
+(__nonnull instancetype) matrixDescriptorWithRows: (NSUInteger)              rows
                                           columns: (NSUInteger)              columns
                                          matrices: (NSUInteger)              matrices
                                          rowBytes: (NSUInteger)              rowBytes
                                       matrixBytes: (NSUInteger)              matrixBytes
                                          dataType: (MPSDataType)             dataType
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
/*!
 *  @abstract   Return the recommended row stride, in bytes, for a given number of
 *              columns.
 *
 *  @param      columns         The number of columns in the matrix for which the recommended
 *                              row stride, in bytes, is to be determined.
 *
 *  @param      dataType        The type of matrix data values.
 *
 *  @discussion To achieve best performance the optimal stride between rows of a matrix is not
 *              necessarily equivalent to the number of columns.  This method returns the row stride, in
 *              bytes, which gives best performance for a given number of columns.  Using this row stride
 *              to construct your array is recommended, but not required (provided that the stride
 *              used is still large enough to allocate a full row of data).
 */
+(size_t) rowBytesFromColumns: (NSUInteger) columns
                     dataType: (MPSDataType) dataType
MPS_AVAILABLE_STARTING_BUT_DEPRECATED(  "Use rowBytesForColumns:dataType instead.",
                                      ios(10.0, 11.0), tvos(10.0, 11.0));

+(size_t) rowBytesForColumns:   (NSUInteger) columns
                    dataType:   (MPSDataType) dataType
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));
@end // MPSMatrixDescriptor
    
/*!
*  @class      MPSVectorDescriptor
*
*  @dependency This depends on Metal.framework
*
*  @discussion A MPSVectorDescriptor describes the length and data type of a
*              an array of 1-dimensional vectors.  All vectors are stored as
*              contiguous arrays of data.
*/
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSVectorDescriptor: NSObject

/*! @property   length
 *  @discussion The number of elements in the vector.
 */
@property (readwrite, nonatomic) NSUInteger length;

/*! @property   vectors
 *  @discussion The number of vectors.
 */
@property (readonly, nonatomic) NSUInteger vectors;

/*! @property   dataType
 *  @discussion The type of the data which makes up the values of the vector.
 */
@property (readwrite, nonatomic) MPSDataType dataType;

/*! @property   vectorBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive vectors.  Must be a multiple of the element size
 */
@property (readonly, nonatomic) NSUInteger vectorBytes;

/*!
 *  @abstract   Create a MPSVectorDescriptor with the specified length and data type.
 *
 *  @param      length              The number of elements in a single vector.
 *
 *  @param      dataType            The type of the data to be stored in the vector.
 *
 *  @discussion Use this function for creating a descriptor of a MPSVector object
 *              containing a single vector.
 */
+(__nonnull instancetype) vectorDescriptorWithLength: (NSUInteger)              length
                                            dataType: (MPSDataType)             dataType;

/*!
 *  @abstract   Create a MPSVectorDescriptor with the specified length and data type.
 *
 *  @param      length              The number of elements in a single vector.
 *
 *  @param      vectors             The number of vectors in the MPSVector object.
 *
 *  @param      vectorBytes         The number of bytes between starting elements of consecutive
 *                                  vectors.
 *
 *  @param      dataType            The type of the data to be stored in the vector.
 *
 *  @discussion For performance considerations the optimal stride between vectors may not necessarily be equal
 *              to the vector length.  The MPSVectorDescriptor class provides a method which
 *              may be used to determine this value, see the vectorBytesForLength API.
 */
+(__nonnull instancetype) vectorDescriptorWithLength: (NSUInteger)              length
                                             vectors: (NSUInteger)              vectors
                                         vectorBytes: (NSUInteger)              vectorBytes
                                            dataType: (MPSDataType)             dataType;

/*!
 *  @abstract   Return the recommended stride, in bytes, to be used for an array
 *              of vectors of a given length.
 *
 *  @param      length          The number of elements in a single vector.
 *
 *  @param      dataType        The type of vector data values.
 *
 *  @discussion To achieve best performance the optimal stride between vectors within an array of
 *              vectors is not necessarily equivalent to the number of elements per vector.  This method 
 *              returns the stride, in bytes, which gives best performance for a given vector length.  
 *              Using this stride to construct your array is recommended, but not required (provided that
 *              the stride used is still large enough to allocate a full vector of data).
 */
+(size_t) vectorBytesForLength:   (NSUInteger) length
                      dataType:   (MPSDataType) dataType;
@end // MPSVectorDescriptor
    
/*!
*  @class      MPSMatrix
*
*  @dependency This depends on Metal.framework
*
*  @discussion A MPSMatrix object describes a set of 2-dimensional arrays of data and provides storage
*              for its values.  MPSMatrix objects serve as inputs and outputs of MPSMatrixKernel
*              objects.
*
*              Implementation note:
*              A MPSMatrix object maintains its internal storage using a MTLBuffer object and thus
*              the same rules for maintaining coherency of a MTLBuffer's data between CPU memory and GPU
*              memory apply to a MPSMatrix.  An MPSMatrix object's data refers to an array of matrices.
*              Data is assumed to be ordered by matrix first, followed by row, followed by column.
*
*              For example, index [i,j] of the k'th matrix of an MPSMatrix is located at byte offset:
*                       k * matrixBytes + i * rowBytes + j * sizeof(dataType)
*               
*               Where matrixBytes is a multiple of rowBytes at least equal to rows * rowBytes.
*/
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSMatrix: NSObject

/*! @property   device
 *  @discussion The device on which the MPSMatrix will be used.
 */
@property (readonly, retain, nonatomic, nonnull) id<MTLDevice> device;

/*! @property   rows
 *  @discussion The number of rows in a matrix in the MPSMatrix.
 */
@property (readonly, nonatomic) NSUInteger rows;

/*! @property   columns
 *  @discussion The number of columns in a matrix in the MPSMatrix.
 */
@property (readonly, nonatomic) NSUInteger columns;

/*! @property   matrices
 *  @discussion The number of matrices in the MPSMatrix.
 */
@property (readonly, nonatomic) NSUInteger matrices MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*! @property   dataType
 *  @discussion The type of the MPSMatrix data.
 */
@property (readonly, nonatomic) MPSDataType dataType;

/*! @property   rowBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive rows.
 */
@property (readonly, nonatomic) NSUInteger rowBytes;

/*! @property   matrixBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive matrices.
 */
@property (readonly, nonatomic) NSUInteger matrixBytes MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*! @property   data
 *  @discussion An MTLBuffer to store the data.
 */
@property (readonly, nonnull, nonatomic) id<MTLBuffer> data;

/*!
 *  @abstract   Initialize a MPSMatrix object with a MTLBuffer.
 *
 *  @param      buffer          The MTLBuffer object which contains the data to use for the
 *                              MPSMatrix. May not be NULL.
 *
 *  @param      descriptor      The MPSMatrixDescriptor. May not be NULL.
 *
 *  @return     A valid MPSMatrix object or nil, if failure.
 *
 *  @discussion This function returns a MPSMatrix object which uses the supplied MTLBuffer.  The
 *              dimensions and stride of the matrix are specified by the MPSMatrixDescriptor object.
 *
 *              The provided MTLBuffer must have enough storage to hold
 *
 *                  (descriptor.matrices-1) * descriptor.matrixBytes +
 *                  (descriptor.rows-1) * descriptor.rowBytes +
 *                   descriptor.columns * (element size) bytes.
 *
 */
-(nonnull instancetype) initWithBuffer: (nonnull id<MTLBuffer>) buffer
                            descriptor: (nonnull MPSMatrixDescriptor*) descriptor;

/*! @abstract   Initialize a MPSMatrix object with a descriptor. Allocate the buffer.
 *  @param      device      The device with which it will be used
 *  @param      descriptor  The shape and style of the matrix
 *  @return     A valid MPSMatrix object or nil
 *  @discussion The matrix object will be created, but the storage to hold the
 *              matrix data will only be allocated when it is needed, typically
 *              when the data property is invoked.  In conjunction
 *              with -resourceSize, this will allow you to estimate storage needs
 *              without actually creating the backing store for the matrix.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            descriptor: (MPSMatrixDescriptor * __nonnull) descriptor;

/*
 * Use one of the above initialization methods instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;


/*! @abstract   Flush the underlying MTLBuffer from the device's caches, and invalidate any CPU caches if needed.
 *  @discussion This will call [id <MTLBlitEncoder> synchronizeResource: ] on the matrix's MTLBuffer, if any.
 *              This is necessary for all MTLStorageModeManaged resources. For other resources, including temporary
 *              resources (these are all MTLStorageModePrivate), and buffers that have not yet been allocated, nothing is done.
 *              It is more efficient to use this method than to attempt to do this yourself with the data property.
 *  @param      commandBuffer       The commandbuffer on which to synchronize   */
-(void) synchronizeOnCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Get the number of bytes used to allocate underyling MTLResources
 *  @discussion     This is the size of the backing store of underlying MTLResources.
 *                  It does not include all storage used by the object, for example
 *                  the storage used to hold the MPSMatrix instantiation and MTLBuffer
 *                  is not included. It only measures the size of the allocation used
 *                  to hold the matrix data in the buffer. This value is subject to
 *                  change between different devices and operating systems.
 *
 *                  Except when -initWithBuffer:descriptor: is used, most MPSMatrixes are allocated
 *                  without a backing store. The backing store is allocated lazily when
 *                  it is needed, typically when the .texture property is called.
 *                  Consequently, in most cases, it should be inexpensive to make
 *                  a MPSImage to see how much memory it will need, and release it
 *                  if it is too large.
 *
 *                  This method may fail in certain circumstances, such as when the
 *                  MPSImage is created with -initWithTexture:featureChannels:. In
 *                  such cases, 0 will be returned.
 */
-(NSUInteger)  resourceSize
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

@end // MPSMatrix
    
/*!
*  @class      MPSVector
*
*  @dependency This depends on Metal.framework
*
*  @discussion A MPSVector object describes a 1-dimensional array of data and provides storage
*              for its values.  Some MPSMatrixKernel objects operate on MPSVector objects
*              for convenience.
*/
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSVector: NSObject

/*! @property   device
 *  @discussion The device on which the MPSVector will be used.
 */
@property (readonly, retain, nonatomic, nonnull) id<MTLDevice> device;

/*! @property   length
 *  @discussion The number of elements in the vector.
 */
@property (readonly, nonatomic) NSUInteger length;

/*! @property   vectors
 *  @discussion The number of vectors in the MPSVector.
 */
@property (readonly, nonatomic) NSUInteger vectors;

/*! @property   dataType
 *  @discussion The type of the MPSVector data.
 */
@property (readonly, nonatomic) MPSDataType dataType;

/*! @property   vectorBytes
 *  @discussion The stride, in bytes, between corresponding elements of
 *              consecutive vectors.
 */
@property (readonly, nonatomic) NSUInteger vectorBytes;

/*! @property   data
 *  @discussion An MTLBuffer to store the data.
 */
@property (readonly, nonnull, nonatomic) id<MTLBuffer> data;

/*!
 *  @abstract   Initialize a MPSVector object with a MTLBuffer.
 *
 *  @param      buffer          The MTLBuffer object which contains the data to use for the
 *                              MPSVector. May not be NULL.
 *
 *  @param      descriptor      The MPSVectorDescriptor. May not be NULL.
 *
 *  @return     A valid MPSVector object or nil, if failure.
 *
 *  @discussion This function returns a MPSVector object which uses the supplied MTLBuffer.  The
 *              length, number of vectors, and stride between vectors are specified by the
 *              MPSVectorDescriptor object.
 *
 *              The provided MTLBuffer must have enough storage to hold
 *
 *                  (descriptor.vectors-1) * descriptor.vectorBytes +
 *                   descriptor.length * (element size) bytes.
 *
 */
-(nonnull instancetype) initWithBuffer: (nonnull id<MTLBuffer>) buffer
                            descriptor: (nonnull MPSVectorDescriptor*) descriptor;

/*! @abstract   Initialize a lazily backed MPSVector object with a descriptor
 *  @param      device      The device with which it will be used
 *  @param      descriptor  The shape and style of the matrix
 *  @return     A valid MPSVector object or nil
 *  @discussion The vector object will be created, but the storage to hold the
 *              vector data will only be allocated when it is needed, typically
 *              when the data property is invoked.  In conjunction
 *              with -resourceSize, this will allow you to estimate storage needs
 *              without actually creating the backing store for the vector.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                            descriptor: (MPSVectorDescriptor * __nonnull) descriptor
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*
 * Use the above initialization methods instead.
 */
-(nonnull instancetype) init NS_UNAVAILABLE;


/*! @abstract   Flush the underlying MTLBuffer from the device's caches, and invalidate any CPU caches if needed.
 *  @discussion This will call [id <MTLBlitEncoder> synchronizeResource: ] on the vector's MTLBuffer, if any.
 *              This is necessary for all MTLStorageModeManaged resources. For other resources, including temporary
 *              resources (these are all MTLStorageModePrivate), and buffers that have not yet been allocated, nothing is done.
 *              It is more efficient to use this method than to attempt to do this yourself with the data property.
 *  @param      commandBuffer       The commandbuffer on which to synchronize   */
-(void) synchronizeOnCommandBuffer: (__nonnull id <MTLCommandBuffer>) commandBuffer
        MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

/*! @abstract       Get the number of bytes used to allocate underyling MTLResources
 *  @discussion     This is the size of the backing store of underlying MTLResources.
 *                  It does not include all storage used by the object, for example
 *                  the storage used to hold the MPSVector instantiation and MTLBuffer
 *                  is not included. It only measures the size of the allocation used
 *                  to hold the vector data in the buffer. This value is subject to
 *                  change between different devices and operating systems.
 *
 *                  Except when -initWithBuffer:descriptor: is used, most MPSVectors are allocated
 *                  without a backing store. The backing store is allocated lazily when
 *                  it is needed, typically when the .texture property is called.
 *                  Consequently, in most cases, it should be inexpensive to make
 *                  a MPSMatrix to see how much memory it will need, and release it
 *                  if it is too large.
 *
 *                  This method may fail in certain circumstances, such as when the
 *                  MPSMatrix is created with -initWithBuffer:descriptor:. In
 *                  such cases, 0 will be returned.
 */
-(NSUInteger)  resourceSize
    MPS_AVAILABLE_STARTING( macos(10.13.4), ios(11.3), tvos(11.3));

@end // MPSVector
    
/*! @abstract A MPSMatrix allocated on GPU private memory. 
 *  @discussion It may alias one or more other MPSTemporaryMatrices. Undesired data destruction
 *              due to aliasing is avoided using the readCount property.
 */    
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSTemporaryMatrix : MPSMatrix

/*!
 *  @abstract   Initialize a MPSTemporaryMatrix for use on a MTLCommandBuffer
 *  @param      commandBuffer       The MTLCommandBuffer on which the MPSTemporaryMatrix will be exclusively used
 *  @param      matrixDescriptor    A valid MPSMatrixDescriptor describing the MPSMatrix format to create
 *  @return     A valid MPSTemporaryMatrix.  The object is not managed by a NSAutoreleasePool. The object will be 
 *              released when the command buffer is committed. The underlying buffer will become invalid before 
 *              this time due to the action of the readCount property.  Please read and understand the use of 
 *              the readCount property before using this object.
 */
+(nonnull instancetype) temporaryMatrixWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                        matrixDescriptor: (nonnull MPSMatrixDescriptor*) matrixDescriptor;

/*!
 *  @abstract       Help MPS decide which allocations to make ahead of time
 *  @discussion     The buffer cache that underlies the MPSTemporaryMatrix can automatically allocate new storage as
 *                  needed as you create new temporary matrices.  However, sometimes a more global view of what you
 *                  plan to make is useful for maximizing memory reuse to get the most efficient operation.
 *                  This class method hints to the cache what the list of matrices will be.
 *
 *                  It is never necessary to call this method. It is purely a performance and memory optimization.
 *
 *  @param commandBuffer        The command buffer on which the MPSTemporaryMatrix will be used
 *  @param descriptorList       A NSArray of MPSMatrixDescriptor, indicating matrices that will be created
 */
+(void) prefetchStorageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                    matrixDescriptorList: (NSArray <MPSMatrixDescriptor*> * __nonnull) descriptorList;

/* MPS can not make a temporary matrix with a outside buffer. Please use the above methods. */
/*! @abstract *** unavailable */
-(nonnull instancetype) initWithBuffer: (nonnull id<MTLBuffer>) buffer
                            descriptor: (nonnull MPSMatrixDescriptor*) descriptor NS_UNAVAILABLE;

/*!
 *  @abstract       The number of times a temporary matrix may be read by a MPSMatrix... kernel
 *                  before its contents become undefined.
 *
 *  @discussion     MPSTemporaryMatrices must release their underlying buffers for reuse
 *                  immediately after last use. So as to facilitate *prompt* convenient
 *                  memory recycling, each time a MPSTemporaryMatrix is read by a
 *                  MPSMatrix... -encode... method, its readCount is automatically
 *                  decremented. When the readCount reaches 0, the underlying buffer is
 *                  automatically made available for reuse to MPS for its own needs and for
 *                  other MPSTemporaryMatrices prior to return from the -encode.. function.
 *                  The contents of the buffer become undefined at this time.
 *
 *                  By default, the readCount is initialized to 1, indicating a matrix that
 *                  may be overwritten any number of times, but read only once.
 *
 *                  You may change the readCount as desired to allow MPSMatrixKernels to read
 *                  the MPSTemporaryMatrix additional times. However, it is an error to change
 *                  the readCount once it is zero. It is an error to read or write to a
 *                  MPSTemporaryMatrix with a zero readCount. You may set the readCount to 0
 *                  yourself to cause the underlying buffer to be returned to MPS. Writing
 *                  to a MPSTemporaryMatrix does not adjust the readCount.
 *
 *                  The Metal API Validation layer will assert if a MPSTemporaryMatrix is
 *                  deallocated with non-zero readCount to help identify cases when resources
 *                  are not returned promptly.
 */
@property (readwrite, nonatomic)  NSUInteger  readCount;

@end
    
/*! @abstract A MPSVector allocated on GPU private memory.
 *  @discussion It may alias one or more other MPSTemporaryVector objects. Undesired data destruction
 *              due to aliasing is avoided using the readCount property.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSTemporaryVector : MPSVector
/*!
 *  @abstract   Initialize a MPSTemporaryVector for use on a MTLCommandBuffer
 *  @param      commandBuffer       The MTLCommandBuffer on which the MPSTemporaryMatrix will be exclusively used
 *  @param      descriptor    A valid MPSVectorDescriptor describing the MPSVector format to create
 *  @return     A valid MPSTemporaryVector.  The object is not managed by a NSAutoreleasePool. The object will be
 *              released when the command buffer is committed. The underlying buffer will become invalid before
 *              this time due to the action of the readCount property.  Please read and understand the use of
 *              the readCount property before using this object.
 */
+(nonnull instancetype) temporaryVectorWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                                              descriptor: (nonnull MPSVectorDescriptor*) descriptor;

/*!
 *  @abstract       Help MPS decide which allocations to make ahead of time
 *  @discussion     The buffer cache that underlies the MPSTemporaryVector can automatically allocate new storage as
 *                  needed as you create new temporary vectors.  However, sometimes a more global view of what you
 *                  plan to make is useful for maximizing memory reuse to get the most efficient operation.
 *                  This class method hints to the cache what the list of matrices will be.
 *
 *                  It is never necessary to call this method. It is purely a performance and memory optimization.
 *
 *  @param commandBuffer        The command buffer on which the MPSTemporaryVector will be used
 *  @param descriptorList       A NSArray of MPSVectorDescriptor objects, indicating vectors that will be created
 */
+(void) prefetchStorageWithCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                          descriptorList: (NSArray <MPSVectorDescriptor*> * __nonnull) descriptorList;

/* MPS can not make a temporary vector with an outside buffer. Please use the above methods. */
/*! @abstract *** unavailable */
-(nonnull instancetype) initWithBuffer: (nonnull id<MTLBuffer>) buffer
                            descriptor: (nonnull MPSVectorDescriptor*) descriptor NS_UNAVAILABLE;

/*!
 *  @abstract       The number of times a temporary vector may be read by a MPSMatrix... kernel
 *                  before its contents become undefined.
 *
 *  @discussion     MPSTemporaryVector objects must release their underlying buffers for reuse
 *                  immediately after last use. So as to facilitate *prompt* convenient
 *                  memory recycling, each time a MPSTemporaryVector is read by a
 *                  MPSMatrix... -encode... method, its readCount is automatically
 *                  decremented. When the readCount reaches 0, the underlying buffer is
 *                  automatically made available for reuse to MPS for its own needs and for
 *                  other MPSTemporaryVector objects prior to return from the -encode.. function.
 *                  The contents of the buffer become undefined at this time.
 *
 *                  By default, the readCount is initialized to 1, indicating a matrix that
 *                  may be overwritten any number of times, but read only once.
 *
 *                  You may change the readCount as desired to allow MPSMatrix kernels to read
 *                  the MPSTemporaryVector additional times. However, it is an error to change
 *                  the readCount once it is zero. It is an error to read or write to a
 *                  MPSTemporaryVector with a zero readCount. You may set the readCount to 0
 *                  yourself to cause the underlying buffer to be returned to MPS. Writing
 *                  to a MPSTemporaryVector does not adjust the readCount.
 *
 *                  The Metal API Validation layer will assert if a MPSTemporaryVector is
 *                  deallocated with non-zero readCount to help identify cases when resources
 *                  are not returned promptly.
 */
@property (readwrite, nonatomic)  NSUInteger  readCount;

@end    // MPSTemporaryVector

/*!
 *  @class      MPSMatrixUnaryKernel
 *  @dependency This depends on Metal.framework
 *  @discussion A MPSMatrixUnaryKernel consumes one MPSMatrix and produces one MPSMatrix.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixUnaryKernel : MPSKernel

/*! @property   sourceMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the source matrix, at which to
 *              start reading values.  This property is modifiable and defaults to
 *              [0, 0] at initialization time.  If a different origin is desired then
 *              this should be modified prior to encoding the kernel.  The z value
 *              must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin sourceMatrixOrigin;

/*! @property   resultMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the result matrix, at which to
 *              start writing results.  This property is modifiable and defaults
 *              to [0, 0] at initialization time.  If a different origin is desired
 *              then this should be modified prior to encoding the kernel.  The z
 *              value must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin resultMatrixOrigin;

/*! @property   batchStart
 *
 *  @discussion The index of the first matrix in the batch.  This property is
 *              modifiable and defaults to 0 at initialization time.  If
 *              batch processing should begin at a different matrix this value
 *              should be modified prior to encoding the kernel.
 */
@property (readwrite, nonatomic) NSUInteger batchStart;

/*! @property   batchSize
 *
 *  @discussion The number of matrices in the batch to process.  This property
 *              is modifiable and by default allows all matrices available at
 *              encoding time to be processed.  If a single matrix should be
 *              processed set this value to 1.
 */
@property (readwrite, nonatomic) NSUInteger batchSize;

@end // MPSMatrixUnaryKernel

/*!
 *  @class      MPSMatrixBinaryKernel
 *  @dependency This depends on Metal.framework
 *  @discussion A MPSMatrixBinaryKernel consumes two MPSMatrix objects and produces
 *              one MPSMatrix object.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixBinaryKernel : MPSKernel
/*! @property   primarySourceMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the primary source matrix, at which to
 *              start reading values.  This property is modifiable and defaults to
 *              [0, 0] at initialization time.  If a different origin is desired then
 *              this should be modified prior to encoding the kernel.  The z value
 *              must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin primarySourceMatrixOrigin;

/*! @property   secondarySourceMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the secondary source matrix, at which to
 *              start reading values.  This property is modifiable and defaults to
 *              [0, 0] at initialization time.  If a different origin is desired then
 *              this should be modified prior to encoding the kernel.  The z value
 *              must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin secondarySourceMatrixOrigin;

/*! @property   resultMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the result matrix, at which to
 *              start writing results.  This property is modifiable and defaults
 *              to [0, 0] at initialization time.  If a different origin is desired
 *              then this should be modified prior to encoding the kernel.  The z
 *              value must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin resultMatrixOrigin;

/*! @property   batchStart
 *
 *  @discussion The index of the first matrix in the batch.  This property is
 *              modifiable and defaults to 0 at initialization time.  If
 *              batch processing should begin at a different matrix this value
 *              should be modified prior to encoding the kernel.
 */
@property (readwrite, nonatomic) NSUInteger batchStart;

/*! @property   batchSize
 *
 *  @discussion The number of matrices in the batch to process.  This property
 *              is modifiable and by default allows all matrices available at
 *              encoding time to be processed.  If a single matrix should be
 *              processed set this value to 1.
 */
@property (readwrite, nonatomic) NSUInteger batchSize;
@end // MPSMatrixBinaryKernel

#ifdef __cplusplus
}   // extern "C"
#endif

#endif // __METAL_VERSION__
#endif /* MPSMatrixTypes_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixSolve.h
/*!
 *  @header MPSMatrixSolve.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2016 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders filter base classes
 */
#ifndef MPSMatrixSolve_h
#define MPSMatrixSolve_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>

/*!
 *  @class      MPSMatrixSolveTriangular
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel for computing the solution of a linear system of
 *              equations using a triangular coefficient matrix.
 *
 *  @discussion A MPSMatrixSolveTriangular finds the solution matrix to the
 *              triangular system:
 *
 *                  op(A) * X = alpha * B    or    X * op(A) = alpha * B
 *
 *              Where A is either upper or lower triangular and op(A) is A**T
 *              or A.  B is the array of right hand sides for which the
 *              equations are to be solved.  X is the resulting matrix of
 *              solutions.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixSolveTriangular : MPSMatrixBinaryKernel

/*!
 *  @abstract   Initialize an MPSMatrixSolveTriangular object on a device
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      right           A boolean value which indicates if the 
 *                              coefficient matrix is multiplied on the left
 *                              or right side of the solution.  NO indicates
 *                              the multiplication is on the left.
 *
 *  @param      upper           A boolean value which indicates if the source
 *                              is lower or upper triangular.  NO indicates
 *                              that the coefficient matrix is lower triangular.
 *
 *  @param      transpose       A boolean value which indicates if the source
 *                              matrix should be used in transposed form.  NO
 *                              indicates that the coefficient matrix is to be
 *                              used normally.
 *
 *  @param      unit            A boolean value which indicates if the source
 *                              matrix is unit triangular.
 *
 *  @param      order           The order of the source matrix and, if
 *                              right == NO, the number of rows in the solution
 *                              and right hand side matrices.  If right == YES
 *                              the number of columns in the solution and right
 *                              hand side matrices.
 *
 *  @param      numberOfRightHandSides  If right == NO, the number of columns in the
 *                                      solution and right hand side matrices.  The
 *                                      number of rows otherwise.
 *
 *  @param      alpha           A double precision value used to scale the right
 *                              hand sides.
 *
 *  @discussion This function initializes a MPSMatrixSolveTriangular object.  It
 *              may allocate device side memory.
 *
 *  @return     A valid MPSMatrixSolveTriangular object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                                 right: (BOOL) right
                                 upper: (BOOL) upper
                             transpose: (BOOL) transpose
                                  unit: (BOOL) unit
                                 order: (NSUInteger) order
                numberOfRightHandSides: (NSUInteger) numberOfRightHandSides
                                 alpha: (double) alpha;

/*!
 *  @abstract   Encode a MPSMatrixSolveTriangular kernel into a command Buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the
 *                                  encoded filter
 *
 *  @param      sourceMatrix        A valid MPSMatrix containing the source
 *                                  matrix.
 *
 *  @param      rightHandSideMatrix A valid MPSMatrix containing the right hand
 *                                  side values.
 *
 *  @param      solutionMatrix      A valid MPSMatrix to contain the result.
 *
 *  @discussion This function encodes the MPSMatrixSolveTriangular object to a
 *              valid command buffer.
 *
 *              rightHandSideMatrix and solutionMatrix must be large enough to
 *              hold at least order * numberOfRightHandSides values starting at
 *              secondarySourceMatrixOrigin and resultMatrixOrigin respectively.
 *
 *              sourceMatrix must be at least size order x order starting at
 *              primarySourceMatrixOrigin.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
          rightHandSideMatrix: (MPSMatrix * __nonnull) rightHandSideMatrix
               solutionMatrix: (MPSMatrix * __nonnull) solutionMatrix
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:rightHandSideMatrix:solutionMatrix:));

@end // MPSMatrixSolveTriangular

/*!
 *  @class      MPSMatrixSolveLU
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel for computing the solution of a linear system of equations
 *              using the LU factorization resulting from a MPSMatrixDecompositionLU
 *              kernel.
 *
 *  @discussion A MPSMatrixSolveLU finds the solution matrix to the system:
 *
 *                  op(A) * X = B
 *
 *              Where op(A) is A**T or A.  B is the array of right hand sides for which
 *              the equations are to be solved.  X is the resulting matrix of solutions.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixSolveLU : MPSMatrixBinaryKernel
/*!
 *  @abstract   Initialize an MPSMatrixSolveLU object on a device
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      transpose       A boolean value which indicates if the source
 *                              matrix should be used in transposed form.
 *
 *  @param      order           The order of the source matrix and the number of
 *                              rows in the solution and right hand side matrices.
 *
 *  @param      numberOfRightHandSides  The number of columns in the solution and right hand side
 *                                      matrices.
 *
 *  @return     A valid MPSMatrixSolveLU object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                             transpose: (BOOL) transpose
                                 order: (NSUInteger) order
                numberOfRightHandSides: (NSUInteger) numberOfRightHandSides;

/*!
 *  @abstract   Encode a MPSMatrixSolveLU kernel into a command Buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *
 *  @param      sourceMatrix        A valid MPSMatrix containing the source matrix in factored
 *                                  form as returned by a previous successful execution of a
 *                                  MPSMatrixDecompositionLU kernel.
 *
 *  @param      rightHandSideMatrix A valid MPSMatrix containing the right hand side values.
 *
 *  @param      pivotIndices        A valid MPSMatrix which contains the pivot indices as returned by
 *                                  a previous successful execution of a MPSMatrixDecompositionLU
 *                                  kernel.
 *
 *  @param      solutionMatrix      A valid MPSMatrix to contain the result.
 *
 *  @discussion This function encodes the MPSMatrixSolveLU object to a valid command buffer.
 *              sourceMatrix should contain the lower and upper triangular factors of A as
 *              results from a previous execution of MPSMatrixDecompositionLU.
 *
 *              pivotIndices is an array of pivots resulting from a previous execution of
 *              MPSMatrixDecompositionLU.
 *
 *              rightHandSideMatrix and solutionMatrix must be large enough to hold a matrix
 *              of size order x numberOfRightHandSides starting at secondarySourceMatrixOrigin and
 *              resultMatrixOrigin respectively.
 *
 *              sourceMatrix must be at least size order x order starting at primarySourceMatrixOrigin.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
          rightHandSideMatrix: (MPSMatrix * __nonnull) rightHandSideMatrix
                 pivotIndices: (MPSMatrix * __nonnull) pivotIndices
               solutionMatrix: (MPSMatrix * __nonnull) solutionMatrix
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:rightHandSideMatrix:pivotIndices:solutionMatrix:));

@end // MPSMatrixSolveLU

/*!
 *  @class      MPSMatrixSolveCholesky
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel for computing the solution of a linear system of equations
 *              using the Cholesky factorization resulting from a
 *              MPSMatrixDecompositionCholesky kernel.
 *
 *  @discussion A MPSMatrixSolveCholesky finds the solution matrix to the system:
 *
 *                  A * X = B
 *
 *              Where A is symmetric positive definite.  B is the array of
 *              right hand sides for which the equations are to be solved.
 *              X is the resulting matrix of solutions.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixSolveCholesky : MPSMatrixBinaryKernel
/*!
 *  @abstract   Initialize an MPSMatrixSolveCholesky object on a device
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      upper           A boolean value which indicates if the source
 *                              matrix stores the lower or upper triangular
 *                              factors.
 *
 *  @param      order           The order of the source matrix and the number of
 *                              rows in the solution and right hand side matrices.
 *
 *  @param      numberOfRightHandSides  The number of columns in the solution and right hand side
 *                                      matrices.
 *
 *  @return     A valid MPSMatrixSolveCholesky object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                                 upper: (BOOL) upper
                                 order: (NSUInteger) order
                numberOfRightHandSides: (NSUInteger) numberOfRightHandSides;

/*!
 *  @abstract   Encode a MPSMatrixSolveCholesky kernel into a command Buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded filter
 *
 *  @param      sourceMatrix        A valid MPSMatrix containing the source matrix in factored
 *                                  form as returned by a previous successful execution of a
 *                                  MPSMatrixDecompositionCholesky kernel.
 *
 *  @param      rightHandSideMatrix A valid MPSMatrix containing the right hand side values.
 *
 *  @param      solutionMatrix      A valid MPSMatrix to contain the result.
 *
 *  @discussion This function encodes the MPSMatrixSolveCholesky object to a valid
 *              command buffer. sourceMatrix should contain either the lower or upper triangular
 *              factors corresponding to the factorization returned by a previous execution
 *              of MPSMatrixDecompositionCholesky.
 *
 *              rightHandSideMatrix and solutionMatrix must be large enough to hold a matrix
 *              of size order x numberOfRightHandSides starting at secondarySourceMatrixOrigin and
 *              resultMatrixOrigin respectively.
 *
 *              sourceMatrix must be at least size order x order starting at primarySourceMatrixOrigin.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
          rightHandSideMatrix: (MPSMatrix * __nonnull) rightHandSideMatrix
               solutionMatrix: (MPSMatrix * __nonnull) solutionMatrix
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:rightHandSideMatrix:solutionMatrix:));

@end // MPSMatrixSolveCholesky

#endif /* MPSMatrixSolve_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixDecomposition.h
/*!
 *  @header MPSMatrixDecomposition.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders filter base classes
 */
#ifndef MPSMatrixDecomposition_h
#define MPSMatrixDecomposition_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>

/*!
 *  @typedef    MPSMatrixDecompositionStatus
 *
 *  @abstract   A value to indicate the status of a matrix decomposition.
 *
 *  @constant   MPSMatrixDecompositionStatusSuccess        The decomposition was
 *              performed successfully.
 *
 *  @constant   MPSMatrixDecompositionStatusFailure         The decomposition was
 *              not able to be completed.
 *
 *  @constant   MPSMatrixDecompositionStatusSingular       The resulting decomposition
 *              is not suitable for use in a subsequent system solve.
 *
 *  @constant   MPSMatrixDecompositionStatusNonPositiveDefinite       A
 *              non-positive-definite pivot value was calculated.
 */
#ifdef DOXYGEN
typedef enum MPSMatrixDecompositionStatus
#else
typedef NS_ENUM( int, MPSMatrixDecompositionStatus)
#endif
{
    MPSMatrixDecompositionStatusSuccess             MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))  =   0,
    MPSMatrixDecompositionStatusFailure             MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))  =  -1,
    MPSMatrixDecompositionStatusSingular            MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))  =  -2,
    MPSMatrixDecompositionStatusNonPositiveDefinite MPS_ENUM_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0))  =  -3,
}
#ifdef DOXYGEN
    MPSMatrixDecompositionStatus
#endif
;

/*!
 *  @class      MPSMatrixDecompositionLU
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel for computing the LU factorization of a matrix using
 *              partial pivoting with row interchanges.
 *
 *  @discussion A MPSMatrixDecompositionLU object computes an LU factorization:
 *
 *                  P * A = L * U
 *
 *              A is a matrix for which the LU factorization is to be computed.
 *              L is a unit lower triangular matrix and U is an upper triangular
 *              matrix.  P is a permutation matrix.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixDecompositionLU : MPSMatrixUnaryKernel

/*!
 *  @abstract   Initialize an MPSMatrixDecompositionLU object on a device
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      rows            The number of rows in the source matrix.
 *
 *  @param      columns         The number of columns in the source matrix.
 *
 *  @return     A valid MPSMatrixDecompositionLU object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                                  rows: (NSUInteger) rows
                               columns: (NSUInteger) columns;

/*!
 *  @abstract   Encode a MPSMatrixDecompositionLU kernel into a command Buffer.
 *
 *  @param      commandBuffer           A valid MTLCommandBuffer to receive the encoded filter
 *
 *  @param      sourceMatrix            A valid MPSMatrix containing the source data.  Must have
 *                                      enough space to hold a rows x columns matrix.
 *
 *  @param      resultMatrix            A valid MPSMatrix to contain the result.  Must have enough
 *                                      space to hold a rows x columns matrix.
 *
 *  @param      pivotIndices            A valid MPSMatrix to contain the pivot indices. Must have enough space
 *                                      to hold an array of size 1xmin(rows, columns) values.
 *                                      Element type must be MPSDataTypeUInt32.
 *
 *  @param      status                  A MTLBuffer which indicates the resulting MPSMatrixDecompositionStatus
 *                                      value.
 *
 *  @discussion This function encodes the MPSMatrixDecompositionLU object to a valid
 *              command buffer.
 *
 *              Upon completion the array pivotIndices contains, for each index i,
 *              the row interchanged with row i.
 *
 *              If during the computation U[k, k], for some k, is determined to be
 *              exactly zero MPSMatrixDecompositionStatusSingular will be returned in the
 *              provided status buffer.  The data referenced by the MTLBuffer is not valid
 *              until the command buffer has completed execution.  If the matrix
 *              return status is not desired NULL may be provided.
 *
 *              Upon successful factorization, resultMatrix contains the resulting
 *              lower triangular factor (without the unit diagonal elements) in its
 *              strictly lower triangular region and the upper triangular factor in
 *              its upper triangular region.
 *
 *              This kernel functions either in-place, if the result matrix
 *              completely aliases the source matrix, or out-of-place.  If there
 *              is any partial overlap between input and output data the results
 *              are undefined.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
                 pivotIndices: (MPSMatrix * __nonnull) pivotIndices
                       status: (__nullable id <MTLBuffer>) status
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:resultMatrix:pivotIndices:info:));

@end // MPSMatrixDecompositionLU

/*!
 *  @class      MPSMatrixDecompositionCholesky
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A kernel for computing the Cholesky factorization of a matrix.
 *
 *  @discussion A MPSMatrixDecompositionLU object computes one of the following
 *              factorizations of a matrix A:
 *
 *                  A = L * L**T
 *                  A = U**T * U
 *
 *              A is a symmetric positive-definite matrix for which the
 *              factorization is to be computed. L and U are lower and upper
 *              triangular matrices respectively.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixDecompositionCholesky : MPSMatrixUnaryKernel

/*!
 *  @abstract   Initialize an MPSMatrixDecompositionCholesky object on a device
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      lower           A boolean value indicating if the lower triangular
 *                              part of the source matrix is stored.  If lower = YES
 *                              the lower triangular part will be used and the factor
 *                              will be written to the lower triangular part of the
 *                              result, otherwise the upper triangular part will be used
 *                              and the factor will be written to the upper triangular
 *                              part.
 *
 *  @param      order           The number of rows and columns in the source matrix.
 *
 *  @return     A valid MPSMatrixDecompositionCholesky object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                                 lower: (BOOL) lower
                                 order: (NSUInteger) order;

/*!
 *  @abstract   Encode a MPSMatrixDecompositionCholesky kernel into a command Buffer.
 *
 *  @param      commandBuffer           A valid MTLCommandBuffer to receive the encoded filter
 *
 *  @param      sourceMatrix            A valid MPSMatrix containing the source data.  Must have
 *                                      enough space to hold a order x order matrix.
 *
 *  @param      resultMatrix            A valid MPSMatrix to contain the result.  Must have enough
 *                                      space to hold a order x order matrix.
 *
 *  @param      status                  A MTLBuffer which indicates the resulting MPSMatrixDecompositionStatus
 *                                      value.
 *
 *  @discussion This function encodes the MPSMatrixDecompositionCholesky object to a valid
 *              command buffer.
 *
 *              If during the factorization a leading minor of the matrix is found to be
 *              not positive definite, MPSMatrixDecompositionNonPositiveDefinite will be returned
 *              in the provided status buffer.  Previously computed pivots and the non positive
 *              pivot are written to the result, but the factorization does not complete.
 *              The data referenced by the MTLBuffer is not valid until the command buffer has completed
 *              execution.  If the matrix return status is not desired NULL may be provided.
 *
 *              If the return status is MPSMatrixDecompositionStatusSuccess, resultMatrix
 *              contains the resulting factors in its lower or upper triangular regions
 *              respectively.
 *
 *              This kernel functions either in-place, if the result matrix
 *              completely aliases the source matrix, or out-of-place.  If there
 *              is any partial overlap between input and output data the results
 *              are undefined.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                 sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
                       status: (__nullable id <MTLBuffer>) status
MPS_SWIFT_NAME(encode(commandBuffer:sourceMatrix:resultMatrix:status:));

@end // MPSMatrixDecompositionCholesky

#endif /* MPSMatrixDecomposition_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixMultiplication.h
/*!
 *  @header MPSMatrixMultiplication.h
 *  @framework MetalPerformanceShaders.framework
 *
 *  @copyright Copyright (c) 2016 Apple Inc. All rights reserved.
 *  @abstract MetalPerformanceShaders filter base classes
 */
#ifndef MPSMatrixMultiplication_h
#define MPSMatrixMultiplication_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>

/*!
 *  @class      MPSMatrixMultiplication
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A matrix multiplication kernel.
 *
 *  @discussion A MPSMatrixMultiplication object computes:
 *
 *                  C = alpha * op(A) * op(B) + beta * C
 *
 *              A, B, and C are matrices which are represented by MPSMatrix
 *              objects. alpha and beta are scalar values (of the same data type
 *              as values of C) which are applied as shown above.  A and B may
 *              each have an optional transposition operation applied.
 *
 *              A, B, and C (also referred to in later discussions as the left input
 *              matrix, the right input matrix, and the result matrix respectively).
 *
 *              A MPSMatrixMultiplication object is initialized with the transpose
 *              operators to apply to A and B, sizes for the operation to perform,
 *              and the scalar values alpha and beta.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(10.0), tvos(10.0))
@interface MPSMatrixMultiplication : MPSKernel
/*! @property   resultMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the result matrix, at which to
 *              start writing (and reading if necessary) results.  This property is
 *              modifiable and defaults to [0, 0] at initialization time.  If a
 *              different origin is desired then this should be modified prior to
 *              encoding the kernel.  The z value must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin resultMatrixOrigin;

/*! @property   leftMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the left input matrix, at which to
 *              start reading values.  This property is modifiable and defaults to
 *              [0, 0] at initialization time.  If a different origin is desired then
 *              this should be modified prior to encoding the kernel.  The z value
 *              must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin leftMatrixOrigin;

/*! @property   rightMatrixOrigin
 *
 *  @discussion The origin, relative to [0, 0] in the right input matrix, at which to
 *              start reading values.  This property is modifiable and defaults to
 *              [0, 0] at initialization time.  If a different origin is desired then
 *              this should be modified prior to encoding the kernel.  The z value
 *              must be 0.
 */
@property (readwrite, nonatomic) MTLOrigin rightMatrixOrigin;

/*! @property   batchStart
 *
 *  @discussion The index of the first matrix in the batch.  This property is
 *              modifiable and defaults to 0 at initialization time.  If
 *              batch processing should begin at a different matrix this value
 *              should be modified prior to encoding the kernel.
 */
@property (readwrite, nonatomic) NSUInteger batchStart;

/*! @property   batchSize
 *
 *  @discussion The number of matrices in the batch to process.  This property
 *              is modifiable and by default allows all matrices available at
 *              encoding time to be processed.
 */
@property (readwrite, nonatomic) NSUInteger batchSize;

/*!
 *  @abstract   Initialize an MPSMatrixMultiplication object on a device for a given size
 *              and desired transpose and scale values.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      transposeLeft   A boolean value which indicates if the left input matrix should be
 *                              used in transposed form.  If 'YES' then op(A) = A**T, otherwise
 *                              op(A) = A.
 *
 *  @param      transposeRight  A boolean value which indicates if the right input matrix should be
 *                              used in transposed form.  If 'YES' then op(B) = B**T, otherwise
 *                              op(B) = B.
 *
 *  @param      resultRows      The number of rows in the result matrix, M in BLAS GEMM description.
 *
 *  @param      resultColumns   The number of columns in the result matrix, N in BLAS GEMM description.
 *
 *  @param      interiorColumns The number of columns of the left input matrix after the
 *                              appropriate transpose operation has been applied. K in BLAS
 *                              GEMM description.
 *
 *  @param      alpha           The scale factor to apply to the product.  Specified in double
 *                              precision.  Will be converted to the appropriate precision in the
 *                              implementation subject to rounding and/or clamping as necessary.
 *
 *  @param      beta            The scale factor to apply to the initial values of C.  Specified
 *                              in double precision.  Will be converted to the appropriate precision in the
 *                              implementation subject to rounding and/or clamping as necessary.
 *
 *  @return     A valid MPSMatrixMultiplication object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                         transposeLeft: (BOOL) transposeLeft
                        transposeRight: (BOOL) transposeRight
                            resultRows: (NSUInteger) resultRows
                         resultColumns: (NSUInteger) resultColumns
                       interiorColumns: (NSUInteger) interiorColumns
                                 alpha: (double) alpha
                                  beta: (double) beta;

/*!
 *  @abstract   Convenience initialization for a matrix-matrix multiplication
 *              with no transpositions, unit scaling of the product, and no
 *              accumulation of the result.  The scaling factors alpha and beta
 *              are taken to be 1.0 and 0.0 respectively.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      resultRows      The number of rows in the result matrix, M in BLAS GEMM description.
 *
 *  @param      resultColumns   The number of columns in the result matrix, N in BLAS GEMM description.
 *
 *  @param      interiorColumns The number of columns of the left input matrix. K in BLAS
 *                              GEMM description.
 *
 *  @return     A valid MPSMatrixMultiplication object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                            resultRows: (NSUInteger) resultRows
                         resultColumns: (NSUInteger) resultColumns
                       interiorColumns: (NSUInteger) interiorColumns
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 @discussion Use the above initialization method instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract   Encode a MPSMatrixMultiplication object to a command buffer.
 *
 *  @param      commandBuffer   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      leftMatrix      A valid MPSMatrix object which specifies the left input matrix.
 *
 *  @param      rightMatrix     A valid MPSMatrix object which specifies the right input matrix.
 *
 *  @param      resultMatrix    A valid MPSMatrix object which specifies the addend matrix which will
 *                              also be overwritten by the result.
 *
 *  @discussion Certain constraints apply to the sizes of the matrices depending on the transposition
 *              operations and sizes requested at initialization time as well as the origins at the time
 *              this routine is called:
 *
 *              The left input matrix must be large enough to hold an array of size resultRows x interiorColumns
 *              elements beginning at leftMatrixOrigin.
 *
 *              The right input matrix must be large enough to hold an array of size interiorColumns x resultColumns
 *              elements beginning at rightMatrixOrigin.
 *
 *              The result matrix must be large enough to hold an array of size resultRows x resultColumns
 *              elements beginning at resultMatrixOrigin.
 *
 *              Each matrix within the range specified by batchStart and batchSize, which also specifies
 *              a valid set of matrices within leftMatrix, rightMatrix, and resultMatrix, will
 *              be processed.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                   leftMatrix: (MPSMatrix * __nonnull) leftMatrix
                  rightMatrix: (MPSMatrix * __nonnull) rightMatrix
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
                    MPS_SWIFT_NAME(encode(commandBuffer:leftMatrix:rightMatrix:resultMatrix:));



@end // MPSMatrixMultiplication

/*!
 *  @class      MPSMatrixVectorMultiplication
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A matrix-vector multiplication kernel.
 *
 *  @discussion A MPSMatrixVectorMultiplication object computes:
 *
 *                  y = alpha * op(A) * x + beta * y
 *
 *              A is a matrix represented by a MPSMatrix object. alpha and beta
 *              are scalar values (of the same data type as values of y) which are
 *              applied as shown above.  A may have an optional transposition
 *              operation applied.
 *
 *              A MPSMatrixVectorMultiplication object is initialized with the transpose
 *              operator to apply to A, sizes for the operation to perform,
 *              and the scalar values alpha and beta.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixVectorMultiplication : MPSMatrixBinaryKernel

/*!
 *  @abstract   Initialize an MPSMatrixVectorMultiplication object on a device for a given size
 *              and desired transpose and scale values.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      transpose       A boolean value which indicates if the input matrix should be
 *                              used in transposed form.  if 'YES' then op(A) == A**T, otherwise
 *                              op(A) == A.
 *
 *  @param      rows            The number of rows in the input matrix op(A), and the number of elements
 *                              in the vector y.
 *
 *  @param      columns         The number of columns in the input matrix op(A), and the number of
 *                              elements in the input vector x.
 *
 *  @param      alpha           The scale factor to apply to the product.  Specified in double
 *                              precision.  Will be converted to the appropriate precision in the
 *                              implementation subject to rounding and/or clamping as necessary.
 *
 *  @param      beta            The scale factor to apply to the initial values of y.  Specified
 *                              in double precision.  Will be converted to the appropriate precision in the
 *                              implementation subject to rounding and/or clamping as necessary.
 *
 *  @return     A valid MPSMatrixVectorMultiplication object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                             transpose: (BOOL)                  transpose
                                  rows: (NSUInteger)            rows
                               columns: (NSUInteger)            columns
                                 alpha: (double)                alpha
                                  beta: (double)                beta;

/*!
 *  @abstract   Convenience initialization for a matrix-vector multiplication
 *              with no transposition, unit scaling of the product, and no
 *              accumulation of the result.  The scaling factors alpha and beta
 *              are taken to be 1.0 and 0.0 respectively.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @param      rows            The number of rows in the input matrix A, and the number of elements
 *                              in the vector y.
 *
 *  @param      columns         The number of columns in the input matrix A, and the number of
 *                              elements in the input vector x.
 *
 *  @return     A valid MPSMatrixVectorMultiplication object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
                                  rows: (NSUInteger)            rows
                               columns: (NSUInteger)            columns;

/*!
 @discussion Use the above initialization method instead.
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*!
 *  @abstract   Encode a MPSMatrixVectorMultiplication object to a command buffer.
 *
 *  @param      commandBuffer   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix     A valid MPSMatrix object which specifies the input matrix A.
 *
 *  @param      inputVector     A valid MPSVector object which specifies the input vector x.
 *
 *  @param      resultVector    A valid MPSVector object which specifies the addend vector which will
 *                              also be overwritten by the result.
 *
 *  @discussion The left input matrix must be large enough to hold an array of size (rows x columns)
 *              elements beginning at primarySourceMatrixOrigin.
 *
 *              The input vector must be large enough to hold an array of size (columns)
 *              elements beginning at secondarySourceMatrixOrigin.x  secondarySourceMatrixOrigin.y and
 *              secondarySourceMatrixOrigin.z must be zero.
 *
 *              The result vector must be large enough to hold an array of size (rows)
 *              elements beginning at resultMatrixOrigin.x.  resultMatrixOrigin.y and
 *              resultMatrixOrigin.z must be zero.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>)  commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull)     inputMatrix
                  inputVector: (MPSVector * __nonnull)     inputVector
                 resultVector: (MPSVector * __nonnull)           resultVector
                    MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:inputVector:resultVector:));

@end // MPSMatrixVectorMultiplication
#endif /* MPSMatrixMultiplication_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixCombination.h
//
//  MPSMatrixCombination.h
//  MPSMatrix
//
//  Created by Ian Ollmann on 6/5/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSMatrixCombination_h
#define MPSMatrixCombination_h


#ifndef __METAL_VERSION__
#   include <stdint.h>
#endif

/*!
 *  @struct     MPSMatrixCopyOffsets
 *  @memberof   MPSMatrixCopy
 *  @abstract   A description of each copy operation
 */

typedef struct
{
    uint32_t    sourceRowOffset;        /**< offset to start of source region to read in rows */
    uint32_t    sourceColumnOffset;     /**< offset to start of source region to read in columns */
    uint32_t    destinationRowOffset;   /**< offset to start of destination region to read in rows */
    uint32_t    destinationColumnOffset;/**< offset to start of destination region to read in columns */
} MPSMatrixCopyOffsets;

// Hide the rest of the header from metal shading language
#ifndef __METAL_VERSION__

#import <MPSMatrix/MPSMatrixTypes.h>

#ifdef __cplusplus
extern "C" {
#endif


/*! @abstract   A list of copy operations
 *  @discussion The MPSMatrixCopy filter can do multiple copy operations.  For RNN filters, these
 *              copies are often small, and are more efficient when grouped together.
 *              The MPSMatriceCopyDescriptor provides a container to list the operations.
 *              The operations occur in any order, and may not alias.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface  MPSMatrixCopyDescriptor : NSObject
/*! @abstract   convenience allocator for single copies */
+(nonnull instancetype) descriptorWithSourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
                                 destinationMatrix: (MPSMatrix * __nonnull) destinationMatrix
                                           offsets: (MPSMatrixCopyOffsets) offsets
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

/*! @abstract       initialize a MPSMatrixCopyDescriptor with default values.
 *  @discussion     Use -setCopyOperationAtIndex:sourceMatrix:destinationMatrix:copyOffsets
 *                  to initialize. All indices must be initialized before use.
 *  @param          device    The device on which the copy will be performed
 *  @param          count     The number of copy operations the object will encode
 *  @return     A MPSMatrixCopyDescriptor. It still needs to be initialized with
 *              -setCopyOperationAtIndex:sourceMatrix:destinationMatrix:copyOffsets */
-(nonnull instancetype)     initWithDevice: (nonnull id <MTLDevice>) device
                                     count: (NSUInteger) count
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

/*! @abstract    Initialize a MPSMatrixCopyDescriptor using offsets generated on the CPU
 *  @discussion  This is for one at a time intialization of the copy operations
 *  @param  index               The index of the copy operation
 *  @param  sourceMatrix        The source matrix for this copy operation
 *  @param  destinationMatrix   The destination matrix for this copy operation
 *  @param  offsets             The offsets to use for the copy operation */
-(void) setCopyOperationAtIndex: (NSUInteger) index
                   sourceMatrix: (MPSMatrix * __nonnull) sourceMatrix
              destinationMatrix: (MPSMatrix * __nonnull) destinationMatrix
                        offsets: (MPSMatrixCopyOffsets) offsets
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );


/*! @abstract       Initialize a MPSMatrixCopyDescriptor using offsets generated on the GPU
 *  @discussion     Use this method when the offsets needed are coming from GPU based computation.
 *  @param          sourceMatrices      A list of matrices from which the matrix data is read
 *  @param          destinationMatrices A list of matrices to which to write the data. The count
 *                                      must match the number of source matrices.
 *  @param          offsets         A MPSVector of type MPSDataTypeUInt32 containing the list of
 *                                  offsets, stored as a packed array of MPSMatrixCopyOffsets.
 *  @param          byteOffset      A byte offset into the offsets vector where the data starts in 'offsets'.
 *                                  This value must be a multiple of 16.
 *  @result         A valid MPSMatrixCopyDescriptor to represent the list of copy operations
 */
-(nonnull instancetype) initWithSourceMatrices: (NSArray<MPSMatrix*>*__nonnull) sourceMatrices
                           destinationMatrices: (NSArray<MPSMatrix*>*__nonnull) destinationMatrices
                                  offsetVector: (MPSVector * __nullable) offsets
                                        offset: (NSUInteger) byteOffset
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );


-(nonnull instancetype) init    NS_UNAVAILABLE;

@end

MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixCopy : MPSKernel

/*
 * Use initWithDevice:rnnDescriptor instead
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device NS_UNAVAILABLE;

/*! @abstract   Initialize a copy operator
 *  @param      copyRows        The number of rows to copy for each copy operation
 *  @param      copyColumns     The number of matrix columns to copy in each copy operation
 *  @param      sourcesAreTransposed       If YES, the sources are in row major storage order
 *  @param      destinationsAreTransposed  If YES, the destinations are in row major storage order
 */
-(nonnull instancetype) initWithDevice: (nonnull id <MTLDevice>) device
                              copyRows: (NSUInteger) copyRows
                           copyColumns: (NSUInteger) copyColumns
                  sourcesAreTransposed: (BOOL) sourcesAreTransposed
             destinationsAreTransposed: (BOOL) destinationsAreTransposed
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );

/*! @abstract   The number of rows to copy for each copy operation */
@property (nonatomic, readonly) NSUInteger copyRows;

/*! @abstract   The number of columns to copy for each copy operation */
@property (nonatomic, readonly) NSUInteger copyColumns;

/*! @abstract   If YES, the sources are in row major storage order */
@property (nonatomic, readonly) BOOL sourcesAreTransposed;

/*! @abstract   If YES, the destinations are in row major storage order */
@property (nonatomic, readonly) BOOL destinationsAreTransposed;

/*! @abstract   Encode the copy operations to the command buffer
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      copyDescriptor      The descriptor that defines the copy operator
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               copyDescriptor: (MPSMatrixCopyDescriptor*__nonnull) copyDescriptor
MPS_SWIFT_NAME( encode(commandBuffer:copyDescriptor:));


/*! @abstract   Encode the copy operations to the command buffer.
 *              This of the encode version support permuting the outputs with custom vectors of indices.
 *              The permutations are defined on the destination indices and are the same for each copy
 *              operation.
 *
 *  @param      commandBuffer           A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      copyDescriptor          The descriptor that defines the copy operator
 *
 *  @param      rowPermuteIndices       If not nil then the output row index is
 *                                      'rowPermuteIndices[i] + rowOffset' instead of 'i + rowOffset',
 *                                      where 'i' is the local row index of the copy operation.
 *                                      Note: if destinationsAreTransposed is set to YES then the destination
 *                                      transpose is performed before permutations.
 *
 *  @param      rowPermuteOffset        Offset in numbers to apply to the 'rowPermuteIndices' vector.
 *
 *  @param      columnPermuteIndices    If not nil then the output column index is
 *                                      'columnPermuteIndices[i] + columnOffset' instead of 'i + columnOffset',
 *                                      where 'i' is the local column index of the copy operation.
 *                                      Note: if destinationsAreTransposed is set to YES then the destination
 *                                      transpose is performed before permutations.
 *
 *  @param      columnPermuteOffset     Offset in numbers to apply to the 'columnPermuteIndices' vector.
 *
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               copyDescriptor: (MPSMatrixCopyDescriptor*__nonnull) copyDescriptor
            rowPermuteIndices: (MPSVector * __nullable) rowPermuteIndices
             rowPermuteOffset: (NSUInteger) rowPermuteOffset
         columnPermuteIndices: (MPSVector * __nullable) columnPermuteIndices
          columnPermuteOffset: (NSUInteger) columnPermuteOffset
MPS_SWIFT_NAME( encode(commandBuffer:copyDescriptor:rowPermuteIndices:rowPermuteOffset:columnPermuteIndices:columnPermuteOffset:));



/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixLookUpAndCopy
 *  @param      device      The MTLDevice on which to make the MPSMatrixLookUpAndCopy
 *  @return     A new MPSMatrixLookUpAndCopy object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

@end


#ifdef __cplusplus
}
#endif

#endif // __METAL_VERSION__

#endif /* MPSMatrixCombination_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrixSoftMax.h
//
//  MPSMatrixSoftmax.h
//  MPSMatrix
//
//  Created by Teemu Rantalaiho on 7/23/17.
//  Copyright © 2017 Apple. All rights reserved.
//

#ifndef MPSMatrixSoftmax_h
#define MPSMatrixSoftmax_h

#import <MPSCore/MPSKernel.h>
#import <MPSMatrix/MPSMatrixTypes.h>


/*!
 *  @class      MPSMatrixSoftMax
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A softmax kernel that operates on matrices.
 *
 *  @discussion A MPSMatrixSoftMax object computes:
 *
 *                  B_ij = Exp { A_ij } / ( Sum_k Exp { A_ik } )
 *
 *              A and B are matrices which are represented by MPSMatrix
 *              objects. This filter computes the same result for MPSMatrices as
 *              MPSCNNSoftMax filter does for MPSImages by interpreting the columns
 *              of the matrix as feature channels, that is the sum runs over column indices.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixSoftMax : MPSMatrixUnaryKernel

/*! @property   sourceRows
 *
 *  @discussion The number of rows to consider from the source in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrix available starting from sourceMatrixOrigin.x,
 *              indicating that by default the whole source matrix is used.
 *              If a different size is desired then this should be modified prior to
 *              encoding the kernel. It is the user's responsibility to ensure that the
 *              resultMatrix parameter in encodeToCommandBuffer is large enough
 *              to accommodate the results of this operation, otherwise the results of
 *              the encode call are undefined.
 *              NOTE: sourceMatrixOrigin and resultMatrixOrigin from MPSMatrixUnaryKernel
 *              can be used to control the starting points in the source and destination
 *              at kernel encode time (see encodeToCommandBuffer).
 */
@property (readwrite, nonatomic) NSUInteger sourceRows;

/*! @property   sourceColumns
 *
 *  @discussion The number of columns to consider from the source in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrix available starting from sourceMatrixOrigin.y,
 *              indicating that by default the whole source matrix is used.
 *              If a different size is desired then this should be modified prior to
 *              encoding the kernel. It is the user's responsibility to ensure that the
 *              resultMatrix parameter in encodeToCommandBuffer is large enough
 *              to accommodate the results of this operation, otherwise the results of
 *              the encode call are undefined.
 *              NOTE: sourceMatrixOrigin and resultMatrixOrigin from MPSMatrixUnaryKernel
 *              can be used to control the starting points in the source and destination
 *              at kernel encode time (see encodeToCommandBuffer).
 */
@property (readwrite, nonatomic) NSUInteger sourceColumns;


/*!
 *  @abstract   Initialize an MPSMatrixSoftMax object on a device for a given size.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @return     A valid MPSMatrixSoftMax object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device
NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0) );


/*!
 *  @abstract   Encode a MPSMatrixSoftMax object to a command buffer.
 *
 *  @param      commandBuffer   A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      inputMatrix     A valid MPSMatrix object which specifies the input matrix.
 *
 *  @param      resultMatrix    A valid MPSMatrix object which specifies the matrix which will
 *                              be overwritten by the result.
 *
 *  @discussion Certain constraints apply to the sizes of the matrices depending on the sizes requested at
 *              initialization time as well as the origins at the time this routine is called:
 *
 *              The result matrix must be large enough to hold a two dimensional array of 'sourceRows' rows and
 *              'sourceColumns' columns beginning at resultMatrixOrigin.
 *
 *              Each matrix within the range specified by batchStart and batchSize, which also specifies
 *              a valid set of matrices within inputMatrix and resultMatrix, will
 *              be processed.
 *
 *              The datatypes of the matrices inputMatrix and resultMatrix must match and be either
 *              MPSDataTypeFloat32 or MPSDataTypeFloat16.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
                  inputMatrix: (MPSMatrix * __nonnull) inputMatrix
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix
MPS_SWIFT_NAME(encode(commandBuffer:inputMatrix:resultMatrix:));


/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixSoftMax
 *  @param      device      The MTLDevice on which to make the MPSMatrixSoftMax
 *  @return     A new MPSMatrixSoftMax object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER
MPS_AVAILABLE_STARTING(macos(10.13), ios(11.0), tvos(11.0));

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixSoftMax


/*!
 *  @class      MPSMatrixLogSoftMax
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   A logarithmic softmax kernel that operates on matrices.
 *
 *  @discussion A MPSMatrixLogSoftMax object computes:
 *
 *                  B_ij = ln { Exp { A_ij } / ( Sum_k Exp { A_ik } ) } = A_ij - ln { Sum_k Exp { A_ik } }
 *
 *              A and B are matrices which are represented by MPSMatrix
 *              objects. This filter computes the same result for MPSMatrices as
 *              MPSCNNLogSoftMax filter does for MPSImages by interpreting the columns
 *              of the matrix as feature channels, that is the sum runs over column indices.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.13), ios(11.0), tvos(11.0))
@interface MPSMatrixLogSoftMax : MPSMatrixSoftMax

@end // MPSMatrixLogSoftMax

/*!
 *  @class      MPSMatrixSoftMaxGradient
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   Computes the gradient corresponding to a forward MPSMatrixSoftMax object.
 *
 *  @discussion A MPSMatrixSoftMaxGradient object computes:
 *
 *                  dL_dX_ij = Y_ij * (dL_dY_ij - sum_k(dL_dY_ik * Y_ik)
 *
 *              Where dL_dX is the resulting gradient of the loss function with respect to
 *              the original input to the forward MPSMatrixSoftMax operation, Y is
 *              the output of the forward MPSMatrixSoftMax operation, and dL_dY is the
 *              gradient of the loss function with respect to Y.
 *
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixSoftMaxGradient : MPSMatrixBinaryKernel

/*! @property   sourceRows
 *
 *  @discussion The number of rows to consider from the sources in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrices available starting from
 *              [primary/secondary]SourceMatrixOrigin.x, indicating that by default the
 *              whole source matrix is used. If a different size is desired then this should
 *              be modified prior to encoding the kernel. It is the user's responsibility to
 *              ensure that the resultMatrix parameter in encodeToCommandBuffer is large enough
 *              to accommodate the results of this operation, otherwise the results of
 *              the encode call are undefined.
 *              NOTE: primarySourceMatrixOrigin, secondarySourceMatrixOrigin and resultMatrixOrigin
 *              from MPSMatrixBinaryKernel can be used to control the starting points in the primary
 *              source, secondary source, and result matrices respectively.
 */
@property (readwrite, nonatomic) NSUInteger sourceRows;

/*! @property   sourceColumns
 *
 *  @discussion The number of columns to consider from the sources in the operation.
 *              This property is modifiable and defaults to NSUIntegerMax and the number is
 *              adjusted dynamically at kernel encode time (see encodeToCommandBuffer) to
 *              fit into the source matrices available starting from [primary/secondary]SourceMatrixOrigin.y,
 *              indicating that by default the whole source matrix is used.
 *              If a different size is desired then this should be modified prior to
 *              encoding the kernel. It is the user's responsibility to ensure that the
 *              resultMatrix parameter in encodeToCommandBuffer is large enough
 *              to accommodate the results of this operation, otherwise the results of
 *              the encode call are undefined.
 *              NOTE: primarySourceMatrixOrigin, secondarySourceMatrixOrigin and resultMatrixOrigin
 *              from MPSMatrixBinaryKernel can be used to control the starting points in the primary
 *              source, secondary source, and result matrices respectively.
 */
@property (readwrite, nonatomic) NSUInteger sourceColumns;


/*!
 *  @abstract   Initialize an MPSMatrixSoftMaxGradient object on a device.
 *
 *  @param      device          The device on which the kernel will execute.
 *
 *  @return     A valid MPSMatrixSoftMaxGradient object or nil, if failure.
 */
-(nonnull instancetype) initWithDevice: (nonnull id<MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Encode a MPSMatrixSoftMaxGradient object to a command buffer.
 *
 *  @param      commandBuffer       A valid MTLCommandBuffer to receive the encoded kernel.
 *
 *  @param      gradientMatrix      A MPSMatrix object containing gradient values with respect
 *                                  to the forward operation's output.  dL_dY in the class
 *                                  description.
 *
 *  @param      forwardOutputMatrix A MPSMatrix object containing the output values from the
 *                                  forward operation.  Y in the class description.
 *
 *  @param      resultMatrix        The MPSMatrix object to hold the resulting gradient values
 *                                  with respect to the forward operation's input.  dL_dX in the
 *                                  class description.
 */
-(void) encodeToCommandBuffer: (nonnull id <MTLCommandBuffer>) commandBuffer
               gradientMatrix: (MPSMatrix * __nonnull) gradientMatrix
          forwardOutputMatrix: (MPSMatrix * __nonnull) forwardOutputMatrix
                 resultMatrix: (MPSMatrix * __nonnull) resultMatrix;


/*! @abstract NSSecureCoding compatability
 *  @discussion See @ref MPSKernel#initWithCoder.
 *  @param      aDecoder    The NSCoder subclass with your serialized MPSMatrixSoftMaxGradient
 *  @param      device      The MTLDevice on which to make the MPSMatrixSoftMaxGradient
 *  @return     A new MPSMatrixSoftMaxGradient object, or nil if failure.
 */
-(nullable instancetype) initWithCoder:(NSCoder * __nonnull)aDecoder
                                device:(nonnull id <MTLDevice>) device NS_DESIGNATED_INITIALIZER;

/*!
 *  @abstract   Make a copy of this kernel for a new device - @see MPSKernel
 *  @param      zone        The NSZone in which to allocate the object
 *  @param      device      The device for the new MPSKernel. If nil, then use
 *                          self.device.
 *  @result     a pointer to a copy of this MPSKernel. This will fail, returning
 *              nil if the device is not supported. Devices must be
 *              MTLFeatureSet_iOS_GPUFamily2_v1 or later.
 */

- (nonnull instancetype) copyWithZone:(nullable NSZone *)zone
                               device:(nullable id <MTLDevice>) device;

@end // MPSMatrixSoftMaxGradient

/*!
 *  @class      MPSMatrixLogSoftMaxGradient
 *
 *  @dependency This depends on Metal.framework.
 *
 *  @abstract   Computes the gradient corresponding to a forward MPSMatrixLogSoftMax object.
 *
 *  @discussion A MPSMatrixLogSoftMaxGradient object computes:
 *
 *                  dL_dX_ij = dL_dY_ij - exp(Y_ij * sum_k(dL_dY_ik))
 *
 *              Where dL_dX is the resulting gradient of the loss function with respect to
 *              the original input to the forward MPSMatrixLogSoftMax operation, Y is
 *              the output of the forward MPSMatrixLogSoftMax operation, and dL_dY is the
 *              gradient of the loss function with respect to Y.
 */
MPS_CLASS_AVAILABLE_STARTING( macos(10.14), ios(12.0), tvos(12.0))
@interface MPSMatrixLogSoftMaxGradient : MPSMatrixSoftMaxGradient

@end // MPSMatrixLogSoftMaxGradient

#endif /* MPSMatrixSoftmax_h */
// ==========  MetalPerformanceShaders.framework/Frameworks/MPSMatrix.framework/Headers/MPSMatrix.h
/*!
 *  @header MPSMatrix.h
 *  @framework MPSMatrix
 *
 *  @copyright Copyright (c) 2017 Apple Inc. All rights reserved.
 */

#import <MPSMatrix/MPSMatrixTypes.h>
#import <MPSMatrix/MPSMatrixMultiplication.h>
#import <MPSMatrix/MPSMatrixSolve.h>
#import <MPSMatrix/MPSMatrixDecomposition.h>
#import <MPSMatrix/MPSMatrixCombination.h>
#import <MPSMatrix/MPSMatrixSoftMax.h>
#import <MPSMatrix/MPSMatrixFindTopK.h>

