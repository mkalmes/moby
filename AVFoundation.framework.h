// ==========  AVFoundation.framework/Headers/AVAudioUnitSampler.h
/*
	File:           AVAudioUnitSampler.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitSampler.h>

// ==========  AVFoundation.framework/Headers/AVAudioMix.h
/*
	File:  AVAudioMix.h
 
	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.
 
 */

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAudioProcessingSettings.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMBase.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <MediaToolbox/MTAudioProcessingTap.h>

/*!
 
 @class          AVAudioMix
 
 @abstract       Allows custom audio processing to be performed on audio tracks during playback or other operations.
 
*/

@class AVAudioMixInternal;
@class AVAudioMixInputParameters;

NS_ASSUME_NONNULL_BEGIN

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAudioMix : NSObject <NSCopying, NSMutableCopying> {
@private
    AVAudioMixInternal    *_audioMix;
}

/* Indicates parameters for inputs to the mix; an NSArray of instances of AVAudioMixInputParameters. Note that an instance of AVAudioMixInputParameters is not required for each audio track that contributes to the mix; audio for those without associated AVAudioMixInputParameters will be included in the mix, processed according to default behavior.  */
@property (nonatomic, readonly, copy) NSArray<AVAudioMixInputParameters *> *inputParameters;

@end


@class AVMutableAudioMixInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableAudioMix : AVAudioMix {
@private
    AVMutableAudioMixInternal    *_mutableAudioMix __attribute__((unused));
}

/*  
 @method		audioMix
 @abstract		Returns a new instance of AVMutableAudioMix with a nil array of inputParameters.
*/
+ (instancetype)audioMix;

/*!
 @property		inputParameters
 @abstract		Indicates parameters for inputs to the mix; an NSArray of instances of AVAudioMixInputParameters.
 @discussion	Note that an instance of AVAudioMixInputParameters is not required for each audio track that contributes to the mix; audio for those without associated AVAudioMixInputParameters will be included in the mix, processed according to default behavior.
*/
@property (nonatomic, copy) NSArray<AVAudioMixInputParameters *> *inputParameters;

@end


/*!
 
 @class          AVAudioMixInputParameters
 
 @abstract       Provides time-varying parameters to apply to an input of an audio mix. Audio volume is currently supported as a time-varying parameter.
 
 @discussion
 
 Use an instance of AVAudioMixInputParameters to apply audio volume ramps for an input to an audio mix.
 AVAudioMixInputParameters are associated with audio tracks via the trackID property.
 
 Notes on audio volume ramps:
 
 Before the first time at which a volume is set, a volume of 1.0 used; after the last time for which a volume has been set, the last volume is used.
 Within the timeRange of a volume ramp, the volume is interpolated between the startVolume and endVolume of the ramp.
 For example, setting the volume to 1.0 at time 0 and also setting a volume ramp from a volume of 0.5 to 0.2 with a timeRange of [4.0, 5.0]
 results in an audio volume parameters that hold the volume constant at 1.0 from 0.0 sec to 4.0 sec, then cause it to jump to 0.5 and
 descend to 0.2 from 4.0 sec to 9.0 sec, holding constant at 0.2 thereafter.
*/

@class AVAudioMixInputParametersInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAudioMixInputParameters : NSObject <NSCopying, NSMutableCopying> {
@private
    AVAudioMixInputParametersInternal    *_inputParameters;
}

/*!
 @property		trackID
 @abstract		Indicates the trackID of the audio track to which the parameters should be applied.
*/
@property (nonatomic, readonly) CMPersistentTrackID trackID;

/*!
 @property		audioTimePitchAlgorithm
 @abstract		Indicates the processing algorithm used to manage audio pitch at varying rates and for scaled audio edits.
 @discussion
   Constants for various time pitch algorithms, e.g. AVAudioTimePitchSpectral, are defined in AVAudioProcessingSettings.h.
   Can be nil, in which case the audioTimePitchAlgorithm set on the AVPlayerItem, AVAssetExportSession, or AVAssetReaderAudioMixOutput on which the AVAudioMix is set will be used for the associated track.
*/
@property (nonatomic, readonly, copy, nullable) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_10, 7_0);

/*!
 @property		audioTapProcessor
 @abstract		Indicates the audio processing tap that will be used for the audio track.
*/
@property (nonatomic, readonly, retain, nullable) __attribute__((NSObject)) MTAudioProcessingTapRef audioTapProcessor NS_AVAILABLE(10_9, 6_0);

/*  
 @method		getVolumeRampForTime:startVolume:endVolume:timeRange:
 @abstract		Obtains the volume ramp that includes the specified time.
 @param			time
   If a ramp with a timeRange that contains the specified time has been set, information about the effective ramp for that time is supplied.
   Otherwise, information about the first ramp that starts after the specified time is supplied.
 @param			startVolume
   A pointer to a float to receive the starting volume value for the volume ramp. May be NULL.
 @param			endVolume
   A pointer to a float to receive the ending volume value for the volume ramp. May be NULL.
 @param			timeRange
   A pointer to a CMTimeRange to receive the timeRange of the volume ramp. May be NULL.
 @result
   An indication of success. NO will be returned if the specified time is beyond the duration of the last volume ramp that has been set.
*/
- (BOOL)getVolumeRampForTime:(CMTime)time startVolume:(nullable float *)startVolume endVolume:(nullable float *)endVolume timeRange:(nullable CMTimeRange *)timeRange;

@end


@class AVAssetTrack;
@class AVPlayerItemTrack;
@class AVMutableAudioMixInputParametersInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableAudioMixInputParameters : AVAudioMixInputParameters {
@private
    AVMutableAudioMixInputParametersInternal    *_mutableInputParameters __attribute__((unused));
}

/*  
 @method		audioMixInputParametersWithTrack:
 @abstract		Returns a new instance of AVMutableAudioMixInputParameters with no volume ramps and a trackID set to the specified track's trackID.
 @param			track
   A reference to an AVAssetTrack.
*/
+ (instancetype)audioMixInputParametersWithTrack:(nullable AVAssetTrack *)track;

/*  
 @method		audioMixInputParameters
 @abstract		Returns a new instance of AVMutableAudioMixInputParameters with no volume ramps and a trackID initialized to kCMPersistentTrackID_Invalid.
*/
+ (instancetype)audioMixInputParameters;

/*!
 @property		trackID
 @abstract		Indicates the trackID of the audio track to which the parameters should be applied.
*/
@property (nonatomic) CMPersistentTrackID trackID;

/*!
 @property		audioTimePitchAlgorithm
 @abstract		Indicates the processing algorithm used to manage audio pitch at varying rates and for scaled audio edits.
 @discussion
   Constants for various time pitch algorithms, e.g. AVAudioTimePitchSpectral, are defined in AVAudioProcessingSettings.h.
   Can be nil, in which case the audioTimePitchAlgorithm set on the AVPlayerItem, AVAssetExportSession, or AVAssetReaderAudioMixOutput on which the AVAudioMix is set will be used for the associated track.
*/
@property (nonatomic, copy, nullable) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_10, 7_0);

/*!
 @property		audioTapProcessor
 @abstract		Indicates the audio processing tap that will be used for the audio track.
*/
@property (nonatomic, retain, nullable) __attribute__((NSObject)) MTAudioProcessingTapRef audioTapProcessor NS_AVAILABLE(10_9, 6_0);

/*  
 @method		setVolumeRampFromStartVolume:toEndVolume:timeRange:
 @abstract		Sets a volume ramp to apply during the specified timeRange.
*/
- (void)setVolumeRampFromStartVolume:(float)startVolume toEndVolume:(float)endVolume timeRange:(CMTimeRange)timeRange;

/*  
 @method		setVolume:atTime:
 @abstract		Sets the value of the audio volume at a specific time.
*/
- (void)setVolume:(float)volume atTime:(CMTime)time;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Headers/AVAssetCache.h
/*
	File:  AVAssetCache.h
 
	Framework:  AVFoundation
 
	Copyright 2016 Apple Inc. All rights reserved.
 
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVMediaSelectionGroup;
@class AVMediaSelectionOption;

/*!
	@class		AVAssetCache

	@abstract
		AVAssetCache is a class vended by an AVAsset used for the inspection of locally available media data.

	@discussion
		AVAssetCaches are vended by AVURLAsset's assetCache property.

*/
NS_CLASS_AVAILABLE(10_12, 10_0)
@interface AVAssetCache : NSObject

/*
	@property	playableOffline
	@abstract
		Returns YES if a complete rendition of an AVAsset is available to be played without a network connection.
	@discussion
		An answer of YES does not indicate that any given media selection is available for offline playback. To determine if a specific media selection is available offline, see mediaSelectionOptionsInMediaSelectionGroup:.
*/
@property (nonatomic, readonly, getter=isPlayableOffline) BOOL playableOffline;

/*
	@method		mediaSelectionOptionsInMediaSelectionGroup:
	@abstract
		Returns an array of AVMediaSelectionOptions in an AVMediaSelectionGroup that are available for offline operations, e.g. playback.
*/
- (NSArray<AVMediaSelectionOption *> *)mediaSelectionOptionsInMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup;

AV_INIT_UNAVAILABLE

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureAudioDataOutput.h
/*
    File:  AVCaptureAudioDataOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureAudioDataOutput

@class AVCaptureAudioDataOutputInternal;
@protocol AVCaptureAudioDataOutputSampleBufferDelegate;

/*!
 @class AVCaptureAudioDataOutput
 @abstract
    AVCaptureAudioDataOutput is a concrete subclass of AVCaptureOutput that can be used to process uncompressed or compressed samples from the audio being captured.
 
 @discussion
    Instances of AVCaptureAudioDataOutput produce audio sample buffers suitable for processing using other media APIs. Applications can access the sample buffers with the captureOutput:didOutputSampleBuffer:fromConnection: delegate method.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureAudioDataOutput : AVCaptureOutput
{
@private
    AVCaptureAudioDataOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method setSampleBufferDelegate:queue:
 @abstract
    Sets the receiver's delegate that will accept captured buffers and dispatch queue on which the delegate will be called.
 
 @param sampleBufferDelegate
    An object conforming to the AVCaptureAudioDataOutputSampleBufferDelegate protocol that will receive sample buffers after they are captured.
 @param sampleBufferCallbackQueue
    A dispatch queue on which all sample buffer delegate methods will be called.
 
 @discussion
    When a new audio sample buffer is captured it will be vended to the sample buffer delegate using the captureOutput:didOutputSampleBuffer:fromConnection: delegate method. All delegate methods will be called on the specified dispatch queue. If the queue is blocked when new samples are captured, those samples will be automatically dropped when they become sufficiently late. This allows clients to process existing samples on the same queue without having to manage the potential memory usage increases that would otherwise occur when that processing is unable to keep up with the rate of incoming samples.
 
    Clients that need to minimize the chances of samples being dropped should specify a queue on which a sufficiently small amount of processing is being done outside of receiving sample buffers. However, if such clients migrate extra processing to another queue, they are responsible for ensuring that memory usage does not grow without bound from samples that have not been processed.
 
    A serial dispatch queue must be used to guarantee that audio samples will be delivered in order. The sampleBufferCallbackQueue parameter may not be NULL, except when setting sampleBufferDelegate to nil.
 */
- (void)setSampleBufferDelegate:(nullable id<AVCaptureAudioDataOutputSampleBufferDelegate>)sampleBufferDelegate queue:(nullable dispatch_queue_t)sampleBufferCallbackQueue;

/*!
 @property sampleBufferDelegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureAudioDataOutputSampleBufferDelegate protocol that will receive sample buffers after they are captured. The delegate is set using the setSampleBufferDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) id<AVCaptureAudioDataOutputSampleBufferDelegate> sampleBufferDelegate;

/*!
 @property sampleBufferCallbackQueue
 @abstract
    The dispatch queue on which all sample buffer delegate methods will be called.
 
 @discussion
    The value of this property is a dispatch_queue_t. The queue is set using the setSampleBufferDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) dispatch_queue_t sampleBufferCallbackQueue;

/*!
 @property audioSettings
 @abstract
    Specifies the settings used to decode or re-encode audio before it is output by the receiver.
 
 @discussion
    The value of this property is an NSDictionary containing values for audio settings keys defined in AVAudioSettings.h. When audioSettings is set to nil, the AVCaptureAudioDataOutput vends samples in their device native format.
 */
@property(nonatomic, copy, null_resettable) NSDictionary<NSString *, id> *audioSettings API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @method recommendedAudioSettingsForAssetWriterWithOutputFileType:
 @abstract
    Specifies the recommended settings for use with an AVAssetWriterInput.
 
 @param outputFileType
    Specifies the UTI of the file type to be written (see AVMediaFormat.h for a list of file format UTIs).
 @result
    A fully populated dictionary of keys and values that are compatible with AVAssetWriter.
 
 @discussion
    The value of this property is an NSDictionary containing values for compression settings keys defined in AVAudioSettings.h. This dictionary is suitable for use as the "outputSettings" parameter when creating an AVAssetWriterInput, such as,
 
       [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio outputSettings:outputSettings sourceFormatHint:hint];
 
    The dictionary returned contains all necessary keys and values needed by AVAssetWriter (see AVAssetWriterInput.h, -initWithMediaType:outputSettings: for a more in depth discussion). For QuickTime movie and ISO files, the recommended audio settings will always produce output comparable to that of AVCaptureMovieFileOutput.
 
    Note that the dictionary of settings is dependent on the current configuration of the receiver's AVCaptureSession and its inputs. The settings dictionary may change if the session's configuration changes. As such, you should configure your session first, then query the recommended audio settings.
 */
- (nullable NSDictionary *)recommendedAudioSettingsForAssetWriterWithOutputFileType:(AVFileType)outputFileType API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

@end


/*!
 @protocol AVCaptureAudioDataOutputSampleBufferDelegate
 @abstract
    Defines an interface for delegates of AVCaptureAudioDataOutput to receive captured audio sample buffers.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureAudioDataOutputSampleBufferDelegate <NSObject>

@optional

/*!
 @method captureOutput:didOutputSampleBuffer:fromConnection:
 @abstract
    Called whenever an AVCaptureAudioDataOutput instance outputs a new audio sample buffer.
 
 @param output
    The AVCaptureAudioDataOutput instance that output the samples.
 @param sampleBuffer
    A CMSampleBuffer object containing the audio samples and additional information about them, such as their format and presentation time.
 @param connection
    The AVCaptureConnection from which the audio was received.
 
 @discussion
    Delegates receive this message whenever the output captures and outputs new audio samples, decoding or re-encoding as specified by the audioSettings property. Delegates can use the provided sample buffer in conjunction with other APIs for further processing. This method will be called on the dispatch queue specified by the output's sampleBufferCallbackQueue property. This method is called periodically, so it must be efficient to prevent capture performance problems, including dropped audio samples.
 
    Clients that need to reference the CMSampleBuffer object outside of the scope of this method must CFRetain it and then CFRelease it when they are finished with it.
 */
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetTrackGroup.h
/*
	File:  AVAssetTrackGroup.h

	Framework:  AVFoundation

	Copyright 2010-2016 Apple Inc. All rights reserved.

 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAssetTrackGroupInternal;

/*!
 @class AVAssetTrackGroup
 @abstract
	A class whose instances describe a group of tracks in an asset.
 
 @discussion
	Instances of AVAssetTrackGroup describe a single group of related tracks in an asset. For example, a track group can
	describe a set of alternate tracks, which are tracks containing variations of the same content, such as content
	translated into different languages, out of which only one track should be played at a time.
 
	Clients can inspect the track groups contained in an AVAsset by loading and obtaining the value of its trackGroups property.
 */

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVAssetTrackGroup : NSObject <NSCopying>
{
@private
	AVAssetTrackGroupInternal	*_assetTrackGroup;
}

/*!
 @property trackIDs
 @abstract
	The IDs of all of the tracks in the group.
 
 @discussion
	The value of this property is an NSArray of NSNumbers interpreted as CMPersistentTrackIDs, one for each track in the
	group.
 */
@property (nonatomic, readonly) NSArray<NSNumber *> *trackIDs;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureInput.h
/*
    File:  AVCaptureInput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMFormatDescription.h>
#import <CoreMedia/CMSync.h>
#if TARGET_OS_OSX
    #import <CoreGraphics/CGDirectDisplay.h>
#endif

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureInput

@class AVCaptureInputPort;
@class AVCaptureInputInternal;
@class AVTimedMetadataGroup;

/*!
 @class AVCaptureInput
 @abstract
    AVCaptureInput is an abstract class that provides an interface for connecting capture input sources to an AVCaptureSession.
 
 @discussion
    Concrete instances of AVCaptureInput representing input sources such as cameras can be added to instances of AVCaptureSession using the -[AVCaptureSession addInput:] method. An AVCaptureInput vends one or more streams of media data. For example, input devices can provide both audio and video data. Each media stream provided by an input is represented by an AVCaptureInputPort object. Within a capture session, connections are made between AVCaptureInput instances and AVCaptureOutput instances via AVCaptureConnection objects that define the mapping between a set of AVCaptureInputPort objects and a single AVCaptureOutput.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureInput : NSObject
{
@private
    AVCaptureInputInternal *_inputInternal;
}

AV_INIT_UNAVAILABLE

/*!
 @property ports
 @abstract
    The ports owned by the receiver.
 
 @discussion
    The value of this property is an array of AVCaptureInputPort objects, each exposing an interface to a single stream of media data provided by an input.
 */
@property(nonatomic, readonly) NSArray<AVCaptureInputPort *> *ports;

@end


/*!
 @constant AVCaptureInputPortFormatDescriptionDidChangeNotification
 @abstract
    This notification is posted when the value of an AVCaptureInputPort instance's formatDescription property changes.
 
 @discussion
    The notification object is the AVCaptureInputPort instance whose format description changed.
 */
AVF_EXPORT NSString *const AVCaptureInputPortFormatDescriptionDidChangeNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVCaptureInputPort

@class AVCaptureInputPortInternal;

/*!
 @class AVCaptureInputPort
 @abstract
    An AVCaptureInputPort describes a single stream of media data provided by an AVCaptureInput and provides an interface for connecting that stream to AVCaptureOutput instances via AVCaptureConnection.
 
 @discussion
    Instances of AVCaptureInputPort cannot be created directly. An AVCaptureInput exposes its input ports via its ports property. Input ports provide information about the format of their media data via the mediaType and formatDescription properties, and allow clients to control the flow of data via the enabled property. Input ports are used by an AVCaptureConnection to define the mapping between inputs and outputs in an AVCaptureSession.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureInputPort : NSObject
{
@private
    AVCaptureInputPortInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property input
 @abstract
    The input that owns the receiver.
 
 @discussion
    The value of this property is an AVCaptureInput instance that owns the receiver.
 */
@property(nonatomic, readonly) AVCaptureInput *input;

/*!
 @property mediaType
 @abstract
    The media type of the data provided by the receiver.
 
 @discussion
    The value of this property is a constant describing the type of media, such as AVMediaTypeVideo or AVMediaTypeAudio, provided by the receiver. Media type constants are defined in AVMediaFormat.h.
 */
@property(nonatomic, readonly) AVMediaType mediaType;

/*!
 @property formatDescription
 @abstract
    The format of the data provided by the receiver.
 
 @discussion
    The value of this property is a CMFormatDescription that describes the format of the media data currently provided by the receiver. Clients can be notified of changes to the format by observing the AVCaptureInputPortFormatDescriptionDidChangeNotification.
 */
@property(nonatomic, readonly, nullable) __attribute__((NSObject)) CMFormatDescriptionRef formatDescription;

/*!
 @property enabled
 @abstract
    Whether the receiver should provide data.
 
 @discussion
    The value of this property is a BOOL that determines whether the receiver should provide data to outputs when a session is running. Clients can set this property to fine tune which media streams from a given input will be used during capture. The default value is YES.
 */
@property(nonatomic, getter=isEnabled) BOOL enabled;

/*!
 @property clock
 @abstract
    Provides access to the "native" clock used by the input port.
 
 @discussion
    The clock is read-only.
 */
@property(nonatomic, readonly, nullable) __attribute__((NSObject)) CMClockRef clock API_AVAILABLE(macos(10.9), ios(7.0));

@end


#pragma mark - AVCaptureDeviceInput

@class AVCaptureDevice;
@class AVCaptureDeviceInputInternal;

/*!
 @class AVCaptureDeviceInput
 @abstract
    AVCaptureDeviceInput is a concrete subclass of AVCaptureInput that provides an interface for capturing media from an AVCaptureDevice.
 
 @discussion
    Instances of AVCaptureDeviceInput are input sources for AVCaptureSession that provide media data from devices connected to the system, represented by instances of AVCaptureDevice.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDeviceInput : AVCaptureInput
{
@private
    AVCaptureDeviceInputInternal *_internal;
}

/*!
 @method deviceInputWithDevice:error:
 @abstract
    Returns an AVCaptureDeviceInput instance that provides media data from the given device.
 
 @param device
    An AVCaptureDevice instance to be used for capture.
 @param outError
    On return, if the given device cannot be used for capture, points to an NSError describing the problem.
 @result
    An AVCaptureDeviceInput instance that provides data from the given device, or nil, if the device could not be used for capture.
 
 @discussion
    This method returns an instance of AVCaptureDeviceInput that can be used to capture data from an AVCaptureDevice in an AVCaptureSession. This method attempts to open the device for capture, taking exclusive control of it if necessary. If the device cannot be opened because it is no longer available or because it is in use, for example, this method returns nil, and the optional outError parameter points to an NSError describing the problem.
 */
+ (nullable instancetype)deviceInputWithDevice:(AVCaptureDevice *)device error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method initWithDevice:error:
 @abstract
    Creates an AVCaptureDeviceInput instance that provides media data from the given device.
 
 @param device
    An AVCaptureDevice instance to be used for capture.
 @param outError
    On return, if the given device cannot be used for capture, points to an NSError describing the problem.
 @result
    An AVCaptureDeviceInput instance that provides data from the given device, or nil, if the device could not be used for capture.
 
 @discussion
    This method creates an instance of AVCaptureDeviceInput that can be used to capture data from an AVCaptureDevice in an AVCaptureSession. This method attempts to open the device for capture, taking exclusive control of it if necessary. If the device cannot be opened because it is no longer available or because it is in use, for example, this method returns nil, and the optional outError parameter points to an NSError describing the problem.
 */
- (nullable instancetype)initWithDevice:(AVCaptureDevice *)device error:(NSError * _Nullable * _Nullable)outError;

/*!
 @property device
 @abstract
    The device from which the receiver provides data.
 
 @discussion
    The value of this property is the AVCaptureDevice instance that was used to create the receiver.
 */
@property(nonatomic, readonly) AVCaptureDevice *device;

/*!
 @property unifiedAutoExposureDefaultsEnabled
 @abstract
    Specifies whether the source device should use the same default auto exposure behaviors for -[AVCaptureSession setSessionPreset:] and -[AVCaptureDevice setActiveFormat:].
 
 @discussion
    AVCaptureDevice's activeFormat property may be set two different ways. 1) You set it directly using one of the formats in the device's -formats array, or 2) the AVCaptureSession sets it on your behalf when you set the AVCaptureSession's sessionPreset property. Depending on the device and format, the default auto exposure behavior may be configured differently when you use one method or the other, resulting in non-uniform auto exposure behavior. Auto exposure defaults include min frame rate, max frame rate, and max exposure duration. If you wish to ensure that consistent default behaviors are applied to the device regardless of the API you use to configure the activeFormat, you may set the device input's unifiedAutoExposureDefaultsEnabled property to YES. Default value for this property is NO.
 
    Note that if you manually set the device's min frame rate, max frame rate, or max exposure duration, your custom values will override the device defaults regardless of whether you've set this property to YES.
 */
@property(nonatomic) BOOL unifiedAutoExposureDefaultsEnabled API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

@end


#pragma mark - AVCaptureScreenInput

@class AVCaptureScreenInputInternal;

/*!
 @class AVCaptureScreenInput
 @abstract
    AVCaptureScreenInput is a concrete subclass of AVCaptureInput that provides an interface for capturing media from a screen or portion thereof.
 
 @discussion
    Instances of AVCaptureScreenInput are input sources for AVCaptureSession that provide media data from one of the screens connected to the system, represented by CGDirectDisplayIDs.
 */
API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos)
@interface AVCaptureScreenInput : AVCaptureInput
{
@private
    AVCaptureScreenInputInternal *_internal;
}

/*!
 @method init
 @abstract
    Creates an AVCaptureScreenInput instance that provides media data from the main display.
 
 @discussion
    This method creates an instance of AVCaptureScreenInput using the main display whose id is returned from CGMainDisplayID().
 */
- (instancetype)init;

+ (instancetype)new;

#if TARGET_OS_OSX

/*!
 @method initWithDisplayID:
 @abstract
    Creates an AVCaptureScreenInput instance that provides media data from the given display.
 
 @param displayID
    The id of the display from which to capture video. CGDirectDisplayID is defined in <CoreGraphics/CGDirectDisplay.h>
 @result
    An AVCaptureScreenInput instance that provides data from the given screen, or nil, if the screen could not be used for capture.
 
 @discussion
    This method creates an instance of AVCaptureScreenInput that can be used to capture data from a display in an AVCaptureSession. This method validates the displayID. If the display cannot be used because it is not available on the system, for example, this method returns nil.
 */
- (nullable instancetype)initWithDisplayID:(CGDirectDisplayID)displayID;

#endif // TARGET_OS_OSX

/*!
 @property minFrameDuration
 @abstract
    A property indicating the screen input's minimum frame duration.
 
 @discussion
    An AVCaptureScreenInput's minFrameDuration is the reciprocal of its maximum frame rate. This property may be used to request a maximum frame rate at which the input produces video frames. The requested rate may not be achievable due to overall bandwidth, so actual frame rates may be lower.
 */
@property(nonatomic) CMTime minFrameDuration;

/*!
 @property cropRect
 @abstract
    A property indicating the bounding rectangle of the screen area to be captured in pixels.
 
 @discussion
    By default, AVCaptureScreenInput captures the entire area of the displayID with which it is associated. To limit the capture rectangle to a subsection of the screen, set the cropRect property, which defines a smaller section of the screen in the screen's coordinate system. The origin (0,0) is the bottom-left corner of the screen.
 */
@property(nonatomic) CGRect cropRect;

/*!
 @property scaleFactor
 @abstract
    A property indicating the factor by which video buffers captured from the screen are to be scaled.
 
 @discussion
    By default, AVCaptureScreenInput captures the video buffers from the display at a scale factor of 1.0 (no scaling). Set this property to scale the buffers by a given factor. For instance, a 320x240 capture area with a scaleFactor of 2.0f produces video buffers at 640x480.
 */
@property(nonatomic) CGFloat scaleFactor;

/*!
 @property capturesMouseClicks
 @abstract
    A property indicating whether mouse clicks should be highlighted in the captured output.
 
 @discussion
    By default, AVCaptureScreenInput does not highlight mouse clicks in its captured output. If this property is set to YES, mouse clicks are highlighted (a circle is drawn around the mouse for the duration of the click) in the captured output.
 */
@property(nonatomic) BOOL capturesMouseClicks;

/*!
 @property capturesCursor
 @abstract
    A property indicating whether the cursor should be rendered to the captured output.
 
 @discussion
    By default, AVCaptureScreenInput draws the cursor in its captured output. If this property is set to NO, the captured output contains only the windows on the screen. Cursor is omitted. Note that cursor position and mouse button state at the time of capture is preserved in CMSampleBuffers emitted from AVCaptureScreenInput. See the inline documentation for kCMIOSampleBufferAttachmentKey_MouseAndKeyboardModifiers in <CoreMediaIO/CMIOSampleBuffer.h>
 */
@property(nonatomic) BOOL capturesCursor API_AVAILABLE(macos(10.8));

/*!
 @property removesDuplicateFrames
 @abstract
    A property indicating whether duplicate frames should be removed by the input.
 
 @discussion
    If this property is set to YES, AVCaptureScreenInput performs frame differencing and when it detects duplicate frames, it drops them. If set to NO, the captured output receives all frames from the input. Prior to 10.9 this value defaulted to YES. In 10.9 and later, it defaults to NO, as modern platforms support frame differencing in hardware-based encoders.
 
    As of 10.10, this property has been deprecated and is ignored. Clients wishing to re-create this functionality can use an AVCaptureVideoDataOutput and compare frame contents in their own code. If they wish to write a movie file, they can then pass the unique frames to an AVAssetWriterInput.
 */
@property(nonatomic) BOOL removesDuplicateFrames API_DEPRECATED("No longer supported.", macos(10.8, 10.10));

@end


#pragma mark - AVCaptureMetadataInput

@class AVCaptureMetadataInputInternal;

/*!
 @class AVCaptureMetadataInput
 @abstract
    AVCaptureMetadataInput is a concrete subclass of AVCaptureInput that provides a way for clients to supply AVMetadataItems to an AVCaptureSession.
 
 @discussion
    Instances of AVCaptureMetadataInput are input sources for AVCaptureSession that provide AVMetadataItems to an AVCaptureSession. AVCaptureMetadataInputs present one and only one AVCaptureInputPort, which currently may only be connected to an AVCaptureMovieFileOutput. The metadata supplied over the input port is provided by the client, and must conform to a client-supplied CMFormatDescription. The AVMetadataItems are supplied in an AVTimedMetadataGroup.
 */
API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureMetadataInput : AVCaptureInput
{
@private
    AVCaptureMetadataInputInternal *_internal;
}

/*!
 @method metadataInputWithFormatDescription:clock:
 @abstract
    Returns an AVCaptureMetadataInput instance that allows a client to provide AVTimedMetadataGroups to an AVCaptureSession.
 
 @param desc
    A CMFormatDescription that defines the metadata to be supplied by the client. Throws an NSInvalidArgumentException if NULL is passed.
 @param clock
    A CMClock that provided the timebase for the supplied samples. Throws an NSInvalidArgumentException if NULL is passed.
 @result
    An AVCaptureMetadataInput instance.
 
 @discussion
    This method returns an instance of AVCaptureMetadataInput that can be used to capture AVTimedMetadataGroups supplied by the client to an AVCaptureSession.
 */
+ (instancetype)metadataInputWithFormatDescription:(CMMetadataFormatDescriptionRef)desc clock:(CMClockRef)clock;

/*!
 @method initWithFormatDescription:clock:
 @abstract
    Creates an AVCaptureMetadataInput instance that allows a client to provide AVTimedMetadataGroups to an AVCaptureSession.
 
 @param desc
    A CMFormatDescription that defines the metadata to be supplied by the client. Throws NSInvalidArgumentException if NULL is passed.
 @param clock
    A CMClock that provided the timebase for the supplied samples. Throws NSInvalidArgumentException if NULL is passed.
 @result
    An AVCaptureMetadataInput instance, or nil, if the device could not be used for capture.
 
 @discussion
    This method creates an instance of AVCaptureMetadataInput that can be used to capture AVTimedMetadataGroups supplied by the client to an AVCaptureSession.
 */
- (instancetype)initWithFormatDescription:(CMMetadataFormatDescriptionRef)desc clock:(CMClockRef)clock;

/*!
 @method appendTimedMetadataGroup:
 @abstract
    Provides metadata to the AVCaptureSession.
 
 @param metadata
    An AVTimedMetadataGroup of metadata. Will throw an exception if nil. In order to denote a period of no metadata, an empty AVTimedMetadataGroup should be passed.
 
 @discussion
    The provided AVTimedMetadataGroup will be provided to the AVCaptureSession. The group's presentation timestamp is expressed in the context of the clock supplied to the initializer. It is not required that the AVTimedMetadataGroup have a duration; an empty AVTimedMetadataGroup can be supplied to denote a period of no metadata.
 */
- (BOOL)appendTimedMetadataGroup:(AVTimedMetadataGroup *)metadata error:(NSError * _Nullable * _Nullable)outError;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetDownloadTask.h
/*
	File:  AVAssetDownloadTask.h

	Framework:  AVFoundation

	Copyright 2015-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import	<AVFoundation/AVAsset.h>
#import <AVFoundation/AVMediaSelection.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

// Keys for options dictionary for use with -[AVAssetDownloadURLSession assetDownloadTaskWithURLAsset:assetTitle:assetArtworkData:options:]

/*!
 @constant		AVAssetDownloadTaskMinimumRequiredMediaBitrateKey
 @abstract		The lowest media bitrate greater than or equal to this value will be selected. Value should be a NSNumber in bps. If no suitable media bitrate is found, the highest media bitrate will be selected.
				The value for this key should be a NSNumber.
 @discussion	By default, the highest media bitrate will be selected for download.
*/
AVF_EXPORT NSString *const AVAssetDownloadTaskMinimumRequiredMediaBitrateKey API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @constant		AVAssetDownloadTaskMediaSelectionKey
 @abstract		The media selection for this download.
				The value for this key should be an AVMediaSelection.
 @discussion	By default, media selections for AVAssetDownloadTask will be automatically selected.
*/
AVF_EXPORT NSString *const AVAssetDownloadTaskMediaSelectionKey API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @class			AVAssetDownloadTask
 @abstract		A NSURLSessionTask that accepts remote AVURLAssets to download locally.
 @discussion	Should be created with -[AVAssetDownloadURLSession assetDownloadTaskWithURLAsset:assetTitle:assetArtworkData:options:]. To utilize local data for playback for downloads that are in-progress, re-use the URLAsset supplied in initialization. An AVAssetDownloadTask may be instantiated with a destinationURL pointing to an existing asset on disk, for the purpose of completing or augmenting a downloaded asset.
*/

NS_CLASS_AVAILABLE_IOS(9_0) __TVOS_PROHIBITED __WATCHOS_PROHIBITED
@interface AVAssetDownloadTask : NSURLSessionTask

/*!
 @property		URLAsset
 @abstract		The asset supplied to the download task upon initialization.
*/
@property (nonatomic, readonly) AVURLAsset *URLAsset;

/*!
 @property		destinationURL
 @abstract		The file URL supplied to the download task upon initialization.
 @discussion	This URL may have been appended with the appropriate extension for the asset.
*/
@property (nonatomic, readonly) NSURL *destinationURL NS_DEPRECATED_IOS(9_0, 10_0);

/*!
 @property		options
 @abstract		The options supplied to the download task upon initialization.
*/
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *options;

/*!
 @property		loadedTimeRanges
 @abstract		This property provides a collection of time ranges for which the download task has media data already downloaded and playable. The ranges provided might be discontinuous.
 @discussion	Returns an NSArray of NSValues containing CMTimeRanges.
*/
@property (nonatomic, readonly) NSArray<NSValue *> *loadedTimeRanges;

// NSURLRequest and NSURLResponse objects are not available for AVAssetDownloadTask
AV_INIT_UNAVAILABLE
@property (readonly, copy) NSURLRequest *originalRequest NS_UNAVAILABLE;
@property (readonly, copy) NSURLRequest *currentRequest NS_UNAVAILABLE;
@property (readonly, copy) NSURLResponse *response NS_UNAVAILABLE;

@end

/*!
 @class			AVAggregateAssetDownloadTask
 @abstract		An AVAssetDownloadTask used for downloading multiple AVMediaSelections for a single AVAsset, under the umbrella of a single download task.
 @discussion	Should be created with -[AVAssetDownloadURLSession aggregateAssetDownloadTaskWithURLAsset:mediaSelections:assetTitle:assetArtworkData:options:. For progress tracking, monitor the delegate callbacks for each childAssetDownloadTask.
*/
NS_CLASS_AVAILABLE_IOS(11_0) __TVOS_PROHIBITED __WATCHOS_PROHIBITED
@interface AVAggregateAssetDownloadTask : NSURLSessionTask

/*!
 @property		URLAsset
 @abstract		The asset supplied to the download task upon initialization.
*/
@property (nonatomic, readonly) AVURLAsset *URLAsset;

// NSURLRequest and NSURLResponse objects are not available for AVAggregateAssetDownloadTask
AV_INIT_UNAVAILABLE
@property (readonly, copy) NSURLRequest *originalRequest NS_UNAVAILABLE;
@property (readonly, copy) NSURLRequest *currentRequest NS_UNAVAILABLE;
@property (readonly, copy) NSURLResponse *response NS_UNAVAILABLE;

@end

/*!
 @protocol		AVAssetDownloadDelegate
 @abstract		Delegate methods to implement when adopting AVAssetDownloadTask.
*/

__TVOS_PROHIBITED __WATCHOS_PROHIBITED
@protocol AVAssetDownloadDelegate <NSURLSessionTaskDelegate>
@optional
/*!
 @method		URLSession:assetDownloadTask:didFinishDownloadingToURL:
 @abstract		Sent when a download task that has completed a download.
 @discussion	Unlike NSURLSessionDownloadDelegate, the delegate should NOT move the file from this directory after it has been called. Downloaded assets must remain at the system provided URL. URLSession:task:didCompleteWithError: will still be called.
 @param			session
				The session the asset download task is on.
 @param			assetDownloadTask
				The AVAssetDownloadTask whose downloaded completed.
 @param			location
				The location the asset has been downloaded to.
*/
- (void)URLSession:(NSURLSession *)session assetDownloadTask:(AVAssetDownloadTask *)assetDownloadTask didFinishDownloadingToURL:(NSURL *)location NS_AVAILABLE_IOS(10_0);

/*!
 @method		URLSession:assetDownloadTask:didLoadTimeRange:totalTimeRangesLoaded:timeRangeExpectedToLoad:
 @abstract		Method to adopt to subscribe to progress updates of an AVAssetDownloadTask.
 @param			session
				The session the asset download task is on.
 @param			assetDownloadTask
				The AVAssetDownloadTask which is being updated.
 @param			timeRange
				A CMTimeRange indicating the time range loaded since the last time this method was called.
 @param			loadedTimeRanges
				A NSArray of NSValues of CMTimeRanges indicating all the time ranges loaded by this asset download task.
 @param			timeRangeExpectedToLoad
				A CMTimeRange indicating the single time range that is expected to be loaded when the download is complete.
*/
- (void)URLSession:(NSURLSession *)session assetDownloadTask:(AVAssetDownloadTask *)assetDownloadTask didLoadTimeRange:(CMTimeRange)timeRange totalTimeRangesLoaded:(NSArray<NSValue *> *)loadedTimeRanges timeRangeExpectedToLoad:(CMTimeRange)timeRangeExpectedToLoad NS_AVAILABLE_IOS(9_0);

/*
 @method		URLSession:assetDownloadTask:didResolveMediaSelection:
 @abstract		Method called when the media selection for the download is fully resolved, including any automatic selections.
 @param			session
				The session the asset download task is on.
 @param			assetDownloadTask
				The AVAssetDownloadTask which is being updated.
 @param			resolvedMediaSelection
				The resolved media selection for the download task. For the best chance of playing back downloaded content without further network I/O, apply this selection to subsequent AVPlayerItems.
*/
- (void)URLSession:(NSURLSession *)session assetDownloadTask:(AVAssetDownloadTask *)assetDownloadTask didResolveMediaSelection:(AVMediaSelection *)resolvedMediaSelection NS_AVAILABLE_IOS(9_0);

/*
 @method		URLSession:aggregateAssetDownloadTask:willDownloadToURL:
 @abstract		Method called when the an aggregate download task determines the location this asset will be downloaded to.
 @discussion	This URL should be saved for future instantiations of AVAsset. While an AVAsset already exists for this content, it is advisable to re-use that instance.
 @param			session
				The session the aggregate asset download task is on.
 @param			aggregateAssetDownloadTask
				The AVAggregateAssetDownloadTask.
 @param			location
				The file URL this task will download media data to.
*/
- (void)URLSession:(NSURLSession *)session aggregateAssetDownloadTask:(AVAggregateAssetDownloadTask *)aggregateAssetDownloadTask willDownloadToURL:(NSURL *)location NS_AVAILABLE_IOS(11_0);

/*
 @method		URLSession:aggregateAssetDownloadTask:didCompleteForMediaSelection:
 @abstract		Method called when a child AVAssetDownloadTask completes.
 @param			session
				The session the aggregate asset download task is on.
 @param			aggregateAssetDownloadTask
				The AVAggregateAssetDownloadTask.
 @param			mediaSelection
				The AVMediaSelection which is now fully available for offline use.
*/
- (void)URLSession:(NSURLSession *)session aggregateAssetDownloadTask:(AVAggregateAssetDownloadTask *)aggregateAssetDownloadTask didCompleteForMediaSelection:(AVMediaSelection *)mediaSelection NS_AVAILABLE_IOS(11_0);

/*
 @method		URLSession:aggregateAssetDownloadTask:didLoadTimeRange:totalTimeRangesLoaded:timeRangeExpectedToLoad:forMediaSelection:
 @abstract		Method to adopt to subscribe to progress updates of an AVAggregateAssetDownloadTask
 @param			session
				The session the asset download task is on.
 @param			aggregateAssetDownloadTask
				The AVAggregateAssetDownloadTask.
 @param			timeRange
				A CMTimeRange indicating the time range loaded for the media selection being downloaded.
 @param			loadedTimeRanges
				A NSArray of NSValues of CMTimeRanges indicating all the time ranges loaded for the media selection being downloaded.
 @param			timeRangeExpectedToLoad
				A CMTimeRange indicating the single time range that is expected to be loaded when the download is complete for the media selection being downloaded.
 @param			mediaSelection
				The media selection which has additional media data loaded for offline use.
*/
- (void)URLSession:(NSURLSession *)session aggregateAssetDownloadTask:(AVAggregateAssetDownloadTask *)aggregateAssetDownloadTask didLoadTimeRange:(CMTimeRange)timeRange totalTimeRangesLoaded:(NSArray<NSValue *> *)loadedTimeRanges timeRangeExpectedToLoad:(CMTimeRange)timeRangeExpectedToLoad forMediaSelection:(AVMediaSelection *)mediaSelection NS_AVAILABLE_IOS(11_0);

@end


/*!
 @class			AVAssetDownloadURLSession
 @abstract		A subclass of NSURLSession to support AVAssetDownloadTask.
*/
NS_CLASS_AVAILABLE_IOS(9_0) __TVOS_PROHIBITED
@interface AVAssetDownloadURLSession : NSURLSession

/*!
 @method		sessionWithConfiguration:assetDownloadDelegate:delegateQueue:
 @abstract		Creates and initializes an AVAssetDownloadURLSession for use with AVAssetDownloadTasks.
 @param			configuration
				The configuration for this URLSession. Must be a background configuration.
 @param			assetDownloadDelegate
				The delegate object to handle asset download progress updates and other session related events.
 @param			delegateQueue
				The queue to receive delegate callbacks on. If nil, a serial queue will be provided.
*/
+ (AVAssetDownloadURLSession *)sessionWithConfiguration:(NSURLSessionConfiguration *)configuration assetDownloadDelegate:(nullable id <AVAssetDownloadDelegate>)delegate delegateQueue:(nullable NSOperationQueue *)delegateQueue;

/*!
 @method		assetDownloadTaskWithURLAsset:destinationURL:options:
 @abstract		Creates and initializes an AVAssetDownloadTask to be used with this AVAssetDownloadURLSession.
 @discussion	This method may return nil if the URLSession has been invalidated.
 @param			URLAsset
				The AVURLAsset to download locally.
 @param			destinationURL
				The local URL to download the asset to. This must be a file URL.
 @param			options
				See AVAssetDownloadTask*Key above. Configures non-default behavior for the download task. Using this parameter is required for downloading non-default media selections for HLS assets.
*/
- (nullable AVAssetDownloadTask *)assetDownloadTaskWithURLAsset:(AVURLAsset *)URLAsset destinationURL:(NSURL *)destinationURL options:(nullable NSDictionary<NSString *, id> *)options NS_DEPRECATED_IOS(9_0, 10_0);

/*!
 @method		assetDownloadTaskWithURLAsset:assetTitle:assetArtworkData:options:
 @abstract		Creates and initializes an AVAssetDownloadTask to be used with this AVAssetDownloadURLSession.
 @discussion	This method may return nil if the URLSession has been invalidated.
 @param			URLAsset
				The AVURLAsset to download locally.
 @param			assetTitle
				A human readable title for this asset, expected to be as suitable as possible for the user's preferred languages. Will show up in the usage pane of the settings app.
 @param			assetArtworkData
				NSData representing artwork data for this asset. Optional. Will show up in the usage pane of the settings app. Must work with +[UIImage imageWithData:].
 @param			options
				See AVAssetDownloadTask*Key above. Configures non-default behavior for the download task. Using this parameter is required for downloading non-default media selections for HLS assets.
*/
- (nullable AVAssetDownloadTask *)assetDownloadTaskWithURLAsset:(AVURLAsset *)URLAsset assetTitle:(NSString *)title assetArtworkData:(nullable NSData *)artworkData options:(nullable NSDictionary<NSString *, id> *)options NS_AVAILABLE_IOS(10_0);

/*!
 @method		aggregateAssetDownloadTaskWithURLAsset:mediaSelections:assetTitle:assetArtworkData:options:
 @abstract		Creates and initializes an AVAggregateAssetDownloadTask to download multiple AVMediaSelections on an AVURLAsset.
 @discussion	This method may return nil if the URLSession has been invalidated. The value of AVAssetDownloadTaskMediaSelectionKey will be ignored.
 @param			URLAsset
				The AVURLAsset to download locally.
 @param			mediaSelections
				A list of AVMediaSelections. Each AVMediaSelection will correspond to a childAssetDownloadTask. Use -[AVAsset allMediaSelections] to download all AVMediaSelections on this AVAsset.
 @param			assetTitle
				A human readable title for this asset, expected to be as suitable as possible for the user's preferred languages. Will show up in the usage pane of the settings app.
 @param			assetArtworkData
				Artwork data for this asset. Optional. Will show up in the usage pane of the settings app.
 @param			options
				See AVAssetDownloadTask*Key above. Configures non-default behavior for the download task.
*/
- (nullable AVAggregateAssetDownloadTask *)aggregateAssetDownloadTaskWithURLAsset:(AVURLAsset *)URLAsset mediaSelections:(NSArray <AVMediaSelection *> *)mediaSelections assetTitle:(NSString *)title assetArtworkData:(nullable NSData *)artworkData options:(nullable NSDictionary<NSString *, id> *)options NS_AVAILABLE_IOS(11_0);

// only AVAssetDownloadTasks can be created with AVAssetDownloadURLSession
AV_INIT_UNAVAILABLE
+ (NSURLSession *)sharedSession NS_UNAVAILABLE;
+ (NSURLSession *)sessionWithConfiguration:(NSURLSessionConfiguration *)configuration NS_UNAVAILABLE;
+ (NSURLSession *)sessionWithConfiguration:(NSURLSessionConfiguration *)configuration delegate:(nullable id <NSURLSessionDelegate>)delegate delegateQueue:(nullable NSOperationQueue *)queue NS_UNAVAILABLE;
- (NSURLSessionDataTask *)dataTaskWithRequest:(NSURLRequest *)request NS_UNAVAILABLE;
- (NSURLSessionDataTask *)dataTaskWithURL:(NSURL *)url NS_UNAVAILABLE;
- (NSURLSessionUploadTask *)uploadTaskWithRequest:(NSURLRequest *)request fromFile:(NSURL *)fileURL NS_UNAVAILABLE;
- (NSURLSessionUploadTask *)uploadTaskWithRequest:(NSURLRequest *)request fromData:(NSData *)bodyData NS_UNAVAILABLE;
- (NSURLSessionUploadTask *)uploadTaskWithStreamedRequest:(NSURLRequest *)request NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithRequest:(NSURLRequest *)request NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithURL:(NSURL *)url NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithResumeData:(NSData *)resumeData NS_UNAVAILABLE;
- (NSURLSessionDataTask *)dataTaskWithRequest:(NSURLRequest *)request completionHandler:(void (^)(NSData *data, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionDataTask *)dataTaskWithURL:(NSURL *)url completionHandler:(void (^)(NSData *data, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionUploadTask *)uploadTaskWithRequest:(NSURLRequest *)request fromFile:(NSURL *)fileURL completionHandler:(void (^)(NSData *data, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionUploadTask *)uploadTaskWithRequest:(NSURLRequest *)request fromData:(nullable NSData *)bodyData completionHandler:(void (^)(NSData *data, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithRequest:(NSURLRequest *)request completionHandler:(void (^)(NSURL *location, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithURL:(NSURL *)url completionHandler:(void (^)(NSURL *location, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;
- (NSURLSessionDownloadTask *)downloadTaskWithResumeData:(NSData *)resumeData completionHandler:(void (^)(NSURL *location, NSURLResponse *response, NSError *error))completionHandler NS_UNAVAILABLE;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureVideoPreviewLayer.h
/*
    File:  AVCaptureVideoPreviewLayer.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <QuartzCore/QuartzCore.h>
#import <AVFoundation/AVCaptureSession.h>
#import <AVFoundation/AVAnimation.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureVideoPreviewLayer

@class AVMetadataObject;
@class AVCaptureVideoPreviewLayerInternal;

/*!
 @class AVCaptureVideoPreviewLayer
 @abstract
    A CoreAnimation layer subclass for previewing the visual output of an AVCaptureSession.
 
 @discussion
    An AVCaptureVideoPreviewLayer instance is a subclass of CALayer and is therefore suitable for insertion in a layer hierarchy as part of a graphical interface. One creates an AVCaptureVideoPreviewLayer instance with the capture session to be previewed, using +layerWithSession: or -initWithSession:. Using the @"videoGravity" property, one can influence how content is viewed relative to the layer bounds. On some hardware configurations, the orientation of the layer can be manipulated using @"orientation" and @"mirrored".
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureVideoPreviewLayer : CALayer
{
@private
    AVCaptureVideoPreviewLayerInternal *_internal;
}

/*!
 @method layerWithSession:
 @abstract
    Creates an AVCaptureVideoPreviewLayer for previewing the visual output of the specified AVCaptureSession.
 
 @param session
    The AVCaptureSession instance to be previewed.
 @result
    A newly initialized AVCaptureVideoPreviewLayer instance.
 */
+ (instancetype)layerWithSession:(AVCaptureSession *)session;

/*!
 @method initWithSession:
 @abstract
    Creates an AVCaptureVideoPreviewLayer for previewing the visual output of the specified AVCaptureSession.
 
 @param session
    The AVCaptureSession instance to be previewed.
 @result
    A newly initialized AVCaptureVideoPreviewLayer instance.
 */
- (instancetype)initWithSession:(AVCaptureSession *)session;

/*!
 @method layerWithSessionWithNoConnection:
 @abstract
    Creates an AVCaptureVideoPreviewLayer for previewing the visual output of the specified AVCaptureSession, but creates no connections to any of the session's eligible video inputs. Only use this initializer if you intend to manually form a connection between a desired AVCaptureInputPort and the receiver using AVCaptureSession's -addConnection: method.
 
 @param session
    The AVCaptureSession instance to be previewed.
 @result
    A newly initialized AVCaptureVideoPreviewLayer instance.
 */
+ (instancetype)layerWithSessionWithNoConnection:(AVCaptureSession *)session API_AVAILABLE(ios(8.0));

/*!
 @method initWithSessionWithNoConnection:
 @abstract
    Creates an AVCaptureVideoPreviewLayer for previewing the visual output of the specified AVCaptureSession, but creates no connections to any of the session's eligible video inputs. Only use this initializer if you intend to manually form a connection between a desired AVCaptureInputPort and the receiver using AVCaptureSession's -addConnection: method.
 
 @param session
    The AVCaptureSession instance to be previewed.
 @result
    A newly initialized AVCaptureVideoPreviewLayer instance.
 */
- (instancetype)initWithSessionWithNoConnection:(AVCaptureSession *)session API_AVAILABLE(ios(8.0));

/*!
 @property session
 @abstract
    The AVCaptureSession instance being previewed by the receiver.
 
 @discussion
    The session is retained by the preview layer.
 */
@property(nonatomic, retain, nullable) AVCaptureSession *session;

/*!
 method setSessionWithNoConnection:
 @abstract
    Attaches the receiver to a given session without implicitly forming a connection to the first eligible video AVCaptureInputPort. Only use this setter if you intend to manually form a connection between a desired AVCaptureInputPort and the receiver using AVCaptureSession's -addConnection: method.
 
 @discussion
    The session is retained by the preview layer.
 */
- (void)setSessionWithNoConnection:(AVCaptureSession *)session API_AVAILABLE(ios(8.0));

/*!
 @property connection
 @abstract
    The AVCaptureConnection instance describing the AVCaptureInputPort to which the receiver is connected.
 
 @discussion
    When calling initWithSession: or setSession: with a valid AVCaptureSession instance, a connection is formed to the first eligible video AVCaptureInput. If the receiver is detached from a session, the connection property becomes nil.
 */
@property(nonatomic, readonly, nullable) AVCaptureConnection *connection API_AVAILABLE(ios(6.0));

/*!
 @property videoGravity
 @abstract
    A string defining how the video is displayed within an AVCaptureVideoPreviewLayer bounds rect.
 
 @discussion
    Options are AVLayerVideoGravityResize, AVLayerVideoGravityResizeAspect and AVLayerVideoGravityResizeAspectFill. AVLayerVideoGravityResizeAspect is default. See <AVFoundation/AVAnimation.h> for a description of these options.
 */
@property(copy) AVLayerVideoGravity videoGravity;

/*!
 @method captureDevicePointOfInterestForPoint:
 @abstract
    Converts a point in layer coordinates to a point of interest in the coordinate space of the capture device providing input to the layer.
 
 @param pointInLayer
    A CGPoint in layer coordinates.
 @result
    A CGPoint in the coordinate space of the capture device providing input to the layer.
 
 @discussion
    AVCaptureDevice pointOfInterest is expressed as a CGPoint where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a point in the coordinate space of the receiver to a point of interest in the coordinate space of the AVCaptureDevice providing input to the receiver. The conversion takes frameSize and videoGravity into consideration.
 */
- (CGPoint)captureDevicePointOfInterestForPoint:(CGPoint)pointInLayer API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @method pointForCaptureDevicePointOfInterest:
 @abstract
    Converts a point of interest in the coordinate space of the capture device providing input to the layer to a point in layer coordinates.
 
 @param captureDevicePointOfInterest
    A CGPoint in the coordinate space of the capture device providing input to the layer.
 @result
    A CGPoint in layer coordinates.
 
 @discussion
    AVCaptureDevice pointOfInterest is expressed as a CGPoint where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a point in the coordinate space of the AVCaptureDevice providing input to the coordinate space of the receiver. The conversion takes frame size and videoGravity into consideration.
 */
- (CGPoint)pointForCaptureDevicePointOfInterest:(CGPoint)captureDevicePointOfInterest API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @method metadataOutputRectOfInterestForRect:
 @abstract
    Converts a rectangle in layer coordinates to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the layer.
 
 @param rectInLayerCoordinates
    A CGRect in layer coordinates.
 @result
    A CGRect in the coordinate space of the metadata output whose capture device is providing input to the layer.
 
 @discussion
    AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of the receiver to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the receiver. The conversion takes frame size and videoGravity into consideration.
 */
- (CGRect)metadataOutputRectOfInterestForRect:(CGRect)rectInLayerCoordinates API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method rectForMetadataOutputRectOfInterest:
 @abstract
    Converts a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the layer to a rectangle in layer coordinates.
 
 @param rectInMetadataOutputCoordinates
    A CGRect in the coordinate space of the metadata output whose capture device is providing input to the layer.
 @result
    A CGRect in layer coordinates.
 
 @discussion
    AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the coordinate space of the receiver. The conversion takes frame size and videoGravity into consideration.
 */
- (CGRect)rectForMetadataOutputRectOfInterest:(CGRect)rectInMetadataOutputCoordinates API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method transformedMetadataObjectForMetadataObject:
 @abstract
    Converts an AVMetadataObject's visual properties to layer coordinates.
 
 @param metadataObject
    An AVMetadataObject originating from the same AVCaptureInput as the preview layer.
 @result
    An AVMetadataObject whose properties are in layer coordinates.
 
 @discussion
    AVMetadataObject bounds may be expressed as a rect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. Face metadata objects likewise express yaw and roll angles with respect to an unrotated picture. -transformedMetadataObjectForMetadataObject: converts the visual properties in the coordinate space of the supplied AVMetadataObject to the coordinate space of the receiver. The conversion takes orientation, mirroring, layer bounds and videoGravity into consideration. If the provided metadata object originates from an input source other than the preview layer's, nil will be returned.
 */
- (nullable AVMetadataObject *)transformedMetadataObjectForMetadataObject:(AVMetadataObject *)metadataObject API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @property orientationSupported
 @abstract
    Specifies whether or not the preview layer supports orientation.
 
 @discussion
    Changes in orientation are not supported on all hardware configurations. An application should check the value of @"orientationSupported" before attempting to manipulate the orientation of the receiver. This property is deprecated. Use AVCaptureConnection's -isVideoOrientationSupported instead.
 */
@property(nonatomic, readonly, getter=isOrientationSupported) BOOL orientationSupported API_DEPRECATED("Use AVCaptureConnection's isVideoOrientationSupported instead.", ios(4.0, 6.0)) API_UNAVAILABLE(macos);

/*!
 @property orientation
 @abstract
    Specifies the orientation of the preview layer.
 
 @discussion
    AVCaptureVideoOrientation and its constants are defined in AVCaptureSession.h. The value of @"orientationSupported" must be YES in order to set @"orientation". An exception will be raised if this requirement is ignored. This property is deprecated. Use AVCaptureConnection's -videoOrientation instead.
 */
@property(nonatomic) AVCaptureVideoOrientation orientation API_DEPRECATED("Use AVCaptureConnection's videoOrientation instead.", ios(4.0, 6.0)) API_UNAVAILABLE(macos);

/*!
 @property mirroringSupported
 @abstract
    Specifies whether or not the preview layer supports mirroring.
 
 @discussion
    Mirroring is not supported on all hardware configurations. An application should check the value of @"mirroringSupported" before attempting to manipulate mirroring on the receiver. This property is deprecated. Use AVCaptureConnection's -isVideoMirroringSupported instead.
 */
@property(nonatomic, readonly, getter=isMirroringSupported) BOOL mirroringSupported API_DEPRECATED("Use AVCaptureConnection's isVideoMirroringSupported instead.", ios(4.0, 6.0)) API_UNAVAILABLE(macos);

/*!
 @property automaticallyAdjustsMirroring
 @abstract
    Specifies whether or not the value of @"mirrored" can change based on configuration of the session.
 
 @discussion
    For some session configurations, preview will be mirrored by default. When the value of this property is YES, the value of @"mirrored" may change depending on the configuration of the session, for example after switching to a different AVCaptureDeviceInput. The default value is YES. This property is deprecated. Use AVCaptureConnection's -automaticallyAdjustsVideoMirroring instead.
 */
@property(nonatomic) BOOL automaticallyAdjustsMirroring API_DEPRECATED("Use AVCaptureConnection's automaticallyAdjustsVideoMirroring instead.", ios(4.0, 6.0)) API_UNAVAILABLE(macos);

/*!
 @property mirrored
 @abstract
    Specifies whether or not the preview is flipped over a vertical axis.
 
 @discussion
    For most applications, it is unnecessary to manipulate preview mirroring manually if @"automaticallyAdjustsMirroring" is set to YES. The value of @"automaticallyAdjustsMirroring" must be NO in order to set @"mirrored". The value of @"mirroringSupported" must be YES in order to set @"mirrored". An exception will be raised if the value of @"mirrored" is mutated without respecting these requirements. This property is deprecated. Use AVCaptureConnection's -videoMirrored instead.
 */
@property(nonatomic, getter=isMirrored) BOOL mirrored API_DEPRECATED("Use AVCaptureConnection's videoMirrored instead.", ios(4.0, 6.0)) API_UNAVAILABLE(macos);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureSessionPreset.h
/*
    File:  AVCaptureSessionPreset.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @group AVCaptureSessionPreset string constants
 
 @discussion
    Clients may use an AVCaptureSessionPreset to set the format for output on an AVCaptureSession.
 */
typedef NSString * AVCaptureSessionPreset NS_STRING_ENUM API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetPhoto
 @abstract
    An AVCaptureSession preset suitable for high resolution photo quality output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetPhoto for full resolution photo quality output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetPhoto API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetHigh
 @abstract
    An AVCaptureSession preset suitable for high quality video and audio output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetHigh to achieve high quality video and audio output. AVCaptureSessionPresetHigh is the default sessionPreset value.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetHigh API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetMedium
 @abstract
    An AVCaptureSession preset suitable for medium quality output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetMedium to achieve output video and audio bitrates suitable for sharing over WiFi.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetMedium API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetLow
 @abstract
    An AVCaptureSession preset suitable for low quality output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetLow to achieve output video and audio bitrates suitable for sharing over 3G.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetLow API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPreset320x240
 @abstract
    An AVCaptureSession preset suitable for 320x240 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset320x240 to achieve 320x240 output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset320x240 API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @constant AVCaptureSessionPreset352x288
 @abstract
    An AVCaptureSession preset suitable for 352x288 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset352x288 to achieve CIF quality (352x288) output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset352x288 API_AVAILABLE(macos(10.7), ios(5.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPreset640x480
 @abstract
    An AVCaptureSession preset suitable for 640x480 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset640x480 to achieve VGA quality (640x480) output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset640x480 API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPreset960x540
 @abstract
    An AVCaptureSession preset suitable for 960x540 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset960x540 to achieve quarter HD quality (960x540) output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset960x540 API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @constant AVCaptureSessionPreset1280x720
 @abstract
    An AVCaptureSession preset suitable for 1280x720 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset1280x720 to achieve 1280x720 output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset1280x720 API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPreset1920x1080
 @abstract
    An AVCaptureSession preset suitable for 1920x1080 video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset1920x1080 to achieve 1920x1080 output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset1920x1080 API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPreset3840x2160
 @abstract
    An AVCaptureSession preset suitable for 3840x2160 (UHD 4K) video output.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPreset3840x2160 to achieve 3840x2160 output.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPreset3840x2160 API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetiFrame960x540
 @abstract
    An AVCaptureSession preset producing 960x540 Apple iFrame video and audio content.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetiFrame960x540 to achieve 960x540 quality iFrame H.264 video at ~30 Mbits/sec with AAC audio. QuickTime movies captured in iFrame format are optimal for editing applications.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetiFrame960x540 API_AVAILABLE(macos(10.9), ios(5.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetiFrame1280x720
 @abstract
    An AVCaptureSession preset producing 1280x720 Apple iFrame video and audio content.
 
 @discussion
    Clients may set an AVCaptureSession instance's sessionPreset to AVCaptureSessionPresetiFrame1280x720 to achieve 1280x720 quality iFrame H.264 video at ~40 Mbits/sec with AAC audio. QuickTime movies captured in iFrame format are optimal for editing applications.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetiFrame1280x720 API_AVAILABLE(macos(10.9), ios(5.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionPresetInputPriority
 @abstract
    An AVCaptureSession preset indicating that the formats of the session's inputs are being given priority.
 
 @discussion
    By calling -setSessionPreset:, clients can easily configure an AVCaptureSession to produce a desired quality of service level. The session configures its inputs and outputs optimally to produce the QoS level indicated. Clients who need to ensure a particular input format is chosen can use AVCaptureDevice's -setActiveFormat: method. When a client sets the active format on a device, the associated session's -sessionPreset property automatically changes to AVCaptureSessionPresetInputPriority. This change indicates that the input format selected by the client now dictates the quality of service level provided at the outputs. When a client sets the session preset to anything other than AVCaptureSessionPresetInputPriority, the session resumes responsibility for configuring inputs and outputs, and is free to change its inputs' activeFormat as needed.
 */
AVF_EXPORT AVCaptureSessionPreset const AVCaptureSessionPresetInputPriority API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMediaSelectionGroup.h
/*
	File:  AVMediaSelectionGroup.h

	Framework:  AVFoundation
 
	Copyright 2011-2018 Apple Inc. All rights reserved.

*/

/*!
 @class			AVMediaSelectionGroup

 @abstract		AVMediaSelectionGroup provides a collection of mutually exclusive options for the presentation of media within an asset.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVMediaSelectionOption;
@class AVMediaSelectionGroupInternal;

NS_CLASS_AVAILABLE(10_8, 5_0)
@interface AVMediaSelectionGroup : NSObject <NSCopying> {
@private
	AVMediaSelectionGroupInternal	*_mediaSelectionGroup;
}

/*!
 @property		options
 @abstract		A collection of mutually exclusive media selection options.
 @discussion	An NSArray of AVMediaSelectionOption*.
*/
@property (nonatomic, readonly) NSArray<AVMediaSelectionOption *> *options;

/*!
 @property		defaultOption
 @abstract		Indicates the default option in the group, i.e. the option that's intended for use in the absence of a specific end-user selection or preference.
 @discussion
	Can be nil, indicating that without a specific end-user selection or preference, no option in the group is intended to be selected.
*/
@property (nonatomic, readonly, nullable) AVMediaSelectionOption *defaultOption NS_AVAILABLE(10_10, 8_0);

/*!
 @property		allowsEmptySelection
 @abstract		Indicates whether it's possible to present none of the options in the group when an associated AVPlayerItem is played.
 @discussion
	If allowsEmptySelection is YES, all of the available media options in the group can be deselected by passing nil
	as the specified AVMediaSelectionOption to -[AVPlayerItem selectMediaOption:inMediaSelectionGroup:].
*/
@property (nonatomic, readonly) BOOL allowsEmptySelection;

/*!
  @method		mediaSelectionOptionWithPropertyList:
  @abstract		Returns the instance of AVMediaSelectionOption with properties that match the specified property list.
  @param		plist
  				A property list previously obtained from an option in the group via -[AVMediaSelectionOption propertyList].
  @result		If the specified properties match those of an option in the group, an instance of AVMediaSelectionOption. Otherwise nil.
*/
- (nullable AVMediaSelectionOption *)mediaSelectionOptionWithPropertyList:(id)plist;

@end


/*!
  @category		AVMediaSelectionOptionFiltering
  @abstract		Filtering of media selection options.
  @discussion
	The AVMediaSelectionOptionFiltering category is provided for convenience in filtering the media selection options in a group
	according to playability, locale, and media characteristics.
	Note that it's possible to implement additional filtering behaviors by using -[NSArray indexesOfObjectsPassingTest:].
*/
@interface AVMediaSelectionGroup (AVMediaSelectionOptionFiltering)

/*!
  @method		playableMediaSelectionOptionsFromArray:
  @abstract		Filters an array of AVMediaSelectionOptions according to whether they are playable.
  @param		mediaSelectionOptions
  				An array of AVMediaSelectionOption to be filtered according to whether they are playable.
  @result		An instance of NSArray containing the media selection options of the specified NSArray that are playable.
*/
+ (NSArray<AVMediaSelectionOption *> *)playableMediaSelectionOptionsFromArray:(NSArray<AVMediaSelectionOption *> *)mediaSelectionOptions;

/*!
 @method		mediaSelectionOptionsFromArray:filteredAndSortedAccordingToPreferredLanguages:
 @abstract		Filters an array of AVMediaSelectionOptions according to whether their locales match any language identifier in the specified array of preferred languages. The returned array is sorted according to the order of preference of the language each matches.
 @param			mediaSelectionOptions
				An array of AVMediaSelectionOptions to be filtered and sorted.
 @param			preferredLanguages
				An array of language identifiers in order of preference, each of which is an IETF BCP 47 (RFC 4646) language identifier. Use +[NSLocale preferredLanguages] to obtain the user's list of preferred languages.
 @result		An instance of NSArray containing media selection options of the specified NSArray that match a preferred language, sorted according to the order of preference of the language each matches.
*/
+ (NSArray<AVMediaSelectionOption *> *)mediaSelectionOptionsFromArray:(NSArray<AVMediaSelectionOption *> *)mediaSelectionOptions filteredAndSortedAccordingToPreferredLanguages:(NSArray<NSString *> *)preferredLanguages NS_AVAILABLE(10_8, 6_0);

/*!
  @method		mediaSelectionOptionsFromArray:withLocale:
  @abstract		Filters an array of AVMediaSelectionOptions according to locale.
  @param		mediaSelectionOptions
				An array of AVMediaSelectionOption to be filtered by locale.
  @param		locale
  				The NSLocale that must be matched for a media selection option to be copied to the output array.
  @result		An instance of NSArray containing the media selection options of the specified NSArray that match the specified locale.
*/
+ (NSArray<AVMediaSelectionOption *> *)mediaSelectionOptionsFromArray:(NSArray<AVMediaSelectionOption *> *)mediaSelectionOptions withLocale:(NSLocale *)locale;

/*!
  @method		mediaSelectionOptionsFromArray:withMediaCharacteristics:
  @abstract		Filters an array of AVMediaSelectionOptions according to one or more media characteristics.
  @param		mediaSelectionOptions
  				An array of AVMediaSelectionOptions to be filtered by media characteristic.
  @param		mediaCharacteristics
  				The media characteristics that must be matched for a media selection option to be copied to the output array.
  @result		An instance of NSArray containing the media selection options of the specified NSArray that match the specified
				media characteristics.
*/
+ (NSArray<AVMediaSelectionOption *> *)mediaSelectionOptionsFromArray:(NSArray<AVMediaSelectionOption *> *)mediaSelectionOptions withMediaCharacteristics:(NSArray<AVMediaCharacteristic> *)mediaCharacteristics;

/*!
  @method		mediaSelectionOptionsFromArray:withoutMediaCharacteristics:
  @abstract		Filters an array of AVMediaSelectionOptions according to whether they lack one or more media characteristics.
  @param		mediaSelectionOptions
  				An array of AVMediaSelectionOptions to be filtered by media characteristic.
  @param		mediaCharacteristics
  				The media characteristics that must not be present for a media selection option to be copied to the output array.
  @result		An instance of NSArray containing the media selection options of the specified NSArray that lack the specified
				media characteristics.
*/
+ (NSArray<AVMediaSelectionOption *> *)mediaSelectionOptionsFromArray:(NSArray<AVMediaSelectionOption *> *)mediaSelectionOptions withoutMediaCharacteristics:(NSArray<AVMediaCharacteristic> *)mediaCharacteristics;

@end


/*!
 @class			AVMediaSelectionOption

 @abstract		AVMediaSelectionOption represents a specific option for the presentation of media within a group of options.

*/

@class AVMediaSelectionOptionInternal;
@class AVMetadataItem;

NS_CLASS_AVAILABLE(10_8, 5_0)
@interface AVMediaSelectionOption : NSObject <NSCopying> {
@private
	AVMediaSelectionOptionInternal	*_mediaSelectionOption;
}

/*!
 @property		mediaType
 @abstract		The media type of the media data, e.g. AVMediaTypeAudio, AVMediaTypeSubtitle, etc.
*/
@property (nonatomic, readonly) AVMediaType mediaType;

/*!
 @property		mediaSubTypes
 @abstract		The mediaSubTypes of the media data associated with the option. 
 @discussion
	An NSArray of NSNumbers carrying four character codes (of type FourCharCode) as defined in CoreAudioTypes.h for audio media and in CMFormatDescription.h for video media.
	Also see CMFormatDescriptionGetMediaSubType in CMFormatDescription.h for more information about media subtypes.
	
	Note that if no information is available about the encoding of the media presented when a media option is selected, the value of mediaSubTypes will be an empty array. This can occur, for example, with streaming media. In these cases the value of mediaSubTypes should simply not be used as a criteria for selection.
*/
@property (nonatomic, readonly) NSArray<NSNumber *> *mediaSubTypes;

/*!
  @method		hasMediaCharacteristic:
  @abstract		Reports whether the media selection option includes media with the specified media characteristic.
  @param		mediaCharacteristic
  				The media characteristic of interest, e.g. AVMediaCharacteristicVisual, AVMediaCharacteristicAudible, AVMediaCharacteristicLegible, etc.
  @result		YES if the media selection option includes media with the specified characteristic, otherwise NO.
*/
- (BOOL)hasMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

/*!
 @property		playable
 @abstract		Indicates whether a media selection option is playable.
 @discussion	If the media data associated with the option cannot be decoded or otherwise rendered, playable is NO.
*/
@property (nonatomic, readonly, getter=isPlayable) BOOL playable;

/*!
 @property		extendedLanguageTag
 @abstract		Indicates the RFC 4646 language tag associated with the option. May be nil.
 */
@property (nonatomic, readonly, nullable) NSString *extendedLanguageTag NS_AVAILABLE(10_9, 7_0);

/*!
 @property		locale
 @abstract		Indicates the locale for which the media option was authored.
 @discussion
 	Use -[NSLocale objectForKey:NSLocaleLanguageCode] to obtain the language code of the locale. See NSLocale.h for additional information.
*/
@property (nonatomic, readonly, nullable) NSLocale *locale;

/*!
 @property		commonMetadata
 @abstract		Provides an array of AVMetadataItems for each common metadata key for which a value is available.
 @discussion
   The array of AVMetadataItems can be filtered according to language via +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:], according to locale via +[AVMetadataItem metadataItemsFromArray:withLocale:],
   or according to key via +[AVMetadataItem metadataItemsFromArray:withKey:keySpace:].
   Example: to obtain the name (or title) of a media selection option in any of the user's preferred languages.

	NSString *title = nil;
	NSArray *titles = [AVMetadataItem metadataItemsFromArray:[mediaSelectionOption commonMetadata] withKey:AVMetadataCommonKeyTitle keySpace:AVMetadataKeySpaceCommon];
	if ([titles count] > 0)
	{
		// Try to get a title that matches one of the user's preferred languages.
		NSArray *titlesForPreferredLanguages = [AVMetadataItem metadataItemsFromArray:titles filteredAndSortedAccordingToPreferredLanguages:[NSLocale preferredLanguages]];
		if ([titlesForPreferredLanguages count] > 0)
		{
			title = [[titlesForPreferredLanguages objectAtIndex:0] stringValue];
		}
		
		// No matches in any of the preferred languages. Just use the primary title metadata we find.
		if (title == nil)
		{
			title = [[titles objectAtIndex:0] stringValue];
		}
	}

*/
@property (nonatomic, readonly) NSArray<AVMetadataItem *> *commonMetadata;

/*!
 @property		availableMetadataFormats
 @abstract		Provides an NSArray of NSStrings, each representing a metadata format that contains metadata associated with the option (e.g. ID3, iTunes metadata, etc.).
 @discussion
   Metadata formats are defined in AVMetadataFormat.h.
*/
@property (nonatomic, readonly) NSArray<NSString *> *availableMetadataFormats;

/*!
  @method		metadataForFormat:
  @abstract		Provides an NSArray of AVMetadataItems, one for each metadata item in the container of the specified format.
  @param		format
  				The metadata format for which items are requested.
  @result		An NSArray containing AVMetadataItems.
*/
- (NSArray<AVMetadataItem *> *)metadataForFormat:(NSString *)format;

/*!
  @method		associatedMediaSelectionOptionInMediaSelectionGroup
  @abstract		If a media selection option in another group is associated with the specified option, returns a reference to the associated option.
  @param		mediaSelectionGroup
  				A media selection group in which an associated option is to be sought.
  @result		An instance of AVMediaSelectionOption.
 @discussion
   Audible media selection options often have associated legible media selection options; in particular, audible options are typically associated with forced-only subtitle options with the same locale. See AVMediaCharacteristicContainsOnlyForcedSubtitles in AVMediaFormat.h for a discussion of forced-only subtitles.
*/
- (nullable AVMediaSelectionOption *)associatedMediaSelectionOptionInMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup;

/*!
  @method		propertyList
  @abstract		Returns a serializable property list that can be used to obtain an instance of AVMediaSelectionOption representing the same option as the receiver via -[AVMediaSelectionGroup mediaSelectionOptionWithPropertyList:].
  @result		A serializable property list that's sufficient to identify the option within its group. For serialization utilities, see NSPropertyList.h.
*/
- (id)propertyList;

/*!
  @method		displayNameWithLocale
  @abstract		Provides an NSString suitable for display.
  @param		locale
  				Localize manufactured portions of the string using the specificed locale.
  @discussion
   May use this option's common metadata, media characteristics and locale properties in addition to the provided locale to formulate an NSString intended for display. Will only consider common metadata with the specified locale.
*/
- (NSString *)displayNameWithLocale:(NSLocale *)locale NS_AVAILABLE(10_9, 7_0);

/*!
  @property		displayName
  @abstract		Provides an NSString suitable for display using the current system locale.
  @discussion
   May use this option's common metadata, media characteristics and locale properties in addition to the current system locale to formulate an NSString intended for display.
   In the event that common metadata is not available in the specified locale, displayName will fall back to considering locales with the multilingual ("mul") then undetermined ("und") locale identifiers.
   For a display name strictly with the specified locale use displayNameWithLocale: instead.
*/
@property (nonatomic, readonly) NSString *displayName NS_AVAILABLE(10_9, 7_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioEngine.h
/*
	File:           AVAudioEngine.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioEngine.h>

// ==========  AVFoundation.framework/Headers/AVSampleBufferDisplayLayer.h
/*
	File:  AVSampleBufferDisplayLayer.h

	Framework:  AVFoundation
 
	Copyright 2011-2017 Apple Inc. All rights reserved.

*/

/*!
    @class			AVSampleBufferDisplayLayer

    @abstract		AVSampleBufferDisplayLayer is a subclass of CALayer that can decompress and display compressed or uncompressed video frames.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAnimation.h>
#import <AVFoundation/AVQueuedSampleBufferRendering.h>
#import <QuartzCore/CoreAnimation.h>
#import <CoreMedia/CMSync.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

@class AVSampleBufferDisplayLayerInternal;

AVF_EXPORT NSString *const AVSampleBufferDisplayLayerFailedToDecodeNotification API_AVAILABLE(macos(10.10), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED; // decode failed, see NSError in notification payload
AVF_EXPORT NSString *const AVSampleBufferDisplayLayerFailedToDecodeNotificationErrorKey API_AVAILABLE(macos(10.10), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED; // NSError

API_AVAILABLE(macos(10.8), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED
@interface AVSampleBufferDisplayLayer : CALayer
{
@private
	AVSampleBufferDisplayLayerInternal		*_sampleBufferDisplayLayerInternal;
}

/*!
	@property		controlTimebase
	@abstract		The layer's control timebase, which governs how time stamps are interpreted.
	@discussion		By default, this property is NULL, in which case time stamps will be interpreted
					according to the host time clock (mach_absolute_time with the appropriate timescale
					conversion; this is the same as Core Animation's CACurrentMediaTime).  With no 
					control timebase, once frames are enqueued, it is not possible to adjust exactly 
					when they are displayed.
					
					If a non-NULL control timebase is set, it will be used to interpret time stamps.
					You can control the timing of frame display by setting the rate and time of the
					control timebase.  
					If you are synchronizing video to audio, you can use a timebase whose master clock
					is a CMAudioDeviceClock for the appropriate audio device to prevent drift.
					
					Note that prior to OSX 10.10 and iOS 8.0, the control timebase could not be changed after enqueueSampleBuffer: was called.  As of OSX 10.10 and iOS 8.0, the control timebase may be changed at any time.
*/
@property (retain, nullable) __attribute__((NSObject)) CMTimebaseRef controlTimebase;

/*!
	@property		videoGravity
	@abstract		A string defining how the video is displayed within an AVSampleBufferDisplayLayer bounds rect.
	@discusssion	Options are AVLayerVideoGravityResizeAspect, AVLayerVideoGravityResizeAspectFill 
 					and AVLayerVideoGravityResize. AVLayerVideoGravityResizeAspect is default. 
					See <AVFoundation/AVAnimation.h> for a description of these options.
 */
@property(copy) AVLayerVideoGravity videoGravity;

@end


@interface AVSampleBufferDisplayLayer (AVSampleBufferDisplayLayerQueueManagement) <AVQueuedSampleBufferRendering>

/*!
	@property		status
	@abstract		The ability of the display layer to be used for enqueuing sample buffers.
	@discussion		The value of this property is an AVQueuedSampleBufferRenderingStatus that indicates whether the receiver can be used for enqueuing and rendering sample buffers. When the value of this property is AVQueuedSampleBufferRenderingStatusFailed, clients can check the value of the error property to determine the failure. To resume rendering sample buffers using the display layer after a failure, clients must first reset the status to AVQueuedSampleBufferRenderingStatusUnknown. This can be achieved by invoking -flush on the display layer.
		
					This property is key value observable.
 */
@property (nonatomic, readonly) AVQueuedSampleBufferRenderingStatus status API_AVAILABLE(macos(10.10), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
	@property		error
	@abstract		If the display layer's status is AVQueuedSampleBufferRenderingStatusFailed, this describes the error that caused the failure.
	@discussion		The value of this property is an NSError that describes what caused the display layer to no longer be able to enqueue sample buffers. If the status is not AVQueuedSampleBufferRenderingStatusFailed, the value of this property is nil.
 */
@property (nonatomic, readonly, nullable) NSError *error API_AVAILABLE(macos(10.10), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
	@method			enqueueSampleBuffer:
	@abstract		Sends a sample buffer for display.
	@discussion		If sampleBuffer has the kCMSampleAttachmentKey_DoNotDisplay attachment set to
					kCFBooleanTrue, the frame will be decoded but not displayed.
					Otherwise, if sampleBuffer has the kCMSampleAttachmentKey_DisplayImmediately
					attachment set to kCFBooleanTrue, the decoded image will be displayed as soon 
					as possible, replacing all previously enqueued images regardless of their timestamps.
					Otherwise, the decoded image will be displayed at sampleBuffer's output presentation
					timestamp, as interpreted by the control timebase (or the mach_absolute_time timeline
					if there is no control timebase).
					
					To schedule the removal of previous images at a specific timestamp, enqueue 
					a marker sample buffer containing no samples, with the
					kCMSampleBufferAttachmentKey_EmptyMedia attachment set to kCFBooleanTrue.
					
					IMPORTANT NOTE: attachments with the kCMSampleAttachmentKey_ prefix must be set via
					CMSampleBufferGetSampleAttachmentsArray and CFDictionarySetValue. 
					Attachments with the kCMSampleBufferAttachmentKey_ prefix must be set via
					CMSetAttachment.
*/
- (void)enqueueSampleBuffer:(CMSampleBufferRef)sampleBuffer;

/*!
	@method			flush
	@abstract		Instructs the layer to discard pending enqueued sample buffers.
	@discussion		It is not possible to determine which sample buffers have been decoded, 
					so the next frame passed to enqueueSampleBuffer: should be an IDR frame
					(also known as a key frame or sync sample).
*/
- (void)flush;

/*!
	@method			flushAndRemoveImage
	@abstract		Instructs the layer to discard pending enqueued sample buffers and remove any
					currently displayed image.
	@discussion		It is not possible to determine which sample buffers have been decoded, 
					so the next frame passed to enqueueSampleBuffer: should be an IDR frame
					(also known as a key frame or sync sample).
*/
- (void)flushAndRemoveImage;

/*!
	@property		readyForMoreMediaData
	@abstract		Indicates the readiness of the layer to accept more sample buffers.
	@discussion		AVSampleBufferDisplayLayer keeps track of the occupancy levels of its internal queues
					for the benefit of clients that enqueue sample buffers from non-real-time sources --
					i.e., clients that can supply sample buffers faster than they are consumed, and so
					need to decide when to hold back.
					
					Clients enqueueing sample buffers from non-real-time sources may hold off from
					generating or obtaining more sample buffers to enqueue when the value of
					readyForMoreMediaData is NO.  
					
					It is safe to call enqueueSampleBuffer: when readyForMoreMediaData is NO, but 
					it is a bad idea to enqueue sample buffers without bound.
					
					To help with control of the non-real-time supply of sample buffers, such clients can use
					-requestMediaDataWhenReadyOnQueue:usingBlock
					in order to specify a block that the layer should invoke whenever it's ready for 
					sample buffers to be appended.
 
					The value of readyForMoreMediaData will often change from NO to YES asynchronously, 
					as previously supplied sample buffers are decoded and displayed.
	
					This property is not key value observable.
*/
@property (readonly, getter=isReadyForMoreMediaData) BOOL readyForMoreMediaData;

/*!
	@method			requestMediaDataWhenReadyOnQueue:usingBlock:
	@abstract		Instructs the target to invoke a client-supplied block repeatedly, 
					at its convenience, in order to gather sample buffers for display.
	@discussion		The block should enqueue sample buffers to the layer either until the layer's
					readyForMoreMediaData property becomes NO or until there is no more data 
					to supply. When the layer has decoded enough of the media data it has received 
					that it becomes ready for more media data again, it will invoke the block again 
					in order to obtain more.
					If this function is called multiple times, only the last call is effective.
					Call stopRequestingMediaData to cancel this request.
					Each call to requestMediaDataWhenReadyOnQueue:usingBlock: should be paired
					with a corresponding call to stopRequestingMediaData:. Releasing the
					AVSampleBufferDisplayLayer without a call to stopRequestingMediaData will result
					in undefined behavior.
*/
- (void)requestMediaDataWhenReadyOnQueue:(dispatch_queue_t)queue usingBlock:(void (^)(void))block;

/*!
	@method			stopRequestingMediaData
	@abstract		Cancels any current requestMediaDataWhenReadyOnQueue:usingBlock: call.
	@discussion		This method may be called from outside the block or from within the block.
*/
- (void)stopRequestingMediaData;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetWriterInput.h
/*
	File:  AVAssetWriterInput.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <CoreMedia/CMSampleBuffer.h>
#import <CoreVideo/CVPixelBuffer.h>
#import <CoreMedia/CMFormatDescription.h>

@class AVMetadataItem;
@class AVAssetWriterInputInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @class AVAssetWriterInput
 @abstract
	AVAssetWriterInput defines an interface for appending either new media samples or references to existing media samples packaged as CMSampleBuffer objects to a single track of the output file of an AVAssetWriter.
 
 @discussion
	Clients that need to write multiple concurrent tracks of media data should use one AVAssetWriterInput instance per track. In order to write multiple concurrent tracks with ideal interleaving of media data, clients should observe the value returned by the readyForMoreMediaData property of each AVAssetWriterInput instance.
	
	AVAssetWriterInput also supports writing per-track metadata collections to the output file.

	As of OS X 10.10 and iOS 8.0 AVAssetWriterInput can also be used to create tracks that are not self-contained.  Such tracks reference sample data that is located in another file. This is currently supported only for instances of AVAssetWriterInput attached to an instance of AVAssetWriter that writes files of type AVFileTypeQuickTimeMovie.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetWriterInput : NSObject
{
@private
	AVAssetWriterInputInternal	*_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetWriterInputWithMediaType:outputSettings:
 @abstract
	Creates a new input of the specified media type to receive sample buffers for writing to the output file.

 @param mediaType
	The media type of samples that will be accepted by the input. Media types are defined in AVMediaFormat.h.
 @param outputSettings
	The settings used for encoding the media appended to the output.  See AVAudioSettings.h for AVMediaTypeAudio or AVVideoSettings.h for AVMediaTypeVideo and for more information on how to construct an output settings dictionary.  If you only require simple preset-based output settings, see AVOutputSettingsAssistant.
 @result
	An instance of AVAssetWriterInput.

 @discussion
	Each new input accepts data for a new track of the AVAssetWriter's output file.  Inputs are added to an asset writer using -[AVAssetWriter addInput:].
	
	Passing nil for output settings instructs the input to pass through appended samples, doing no processing before they are written to the output file.  This is useful if, for example, you are appending buffers that are already in a desirable compressed format.  However, if not writing to a QuickTime Movie file (i.e. the AVAssetWriter was initialized with a file type other than AVFileTypeQuickTimeMovie), AVAssetWriter only supports passing through a restricted set of media types and subtypes.  In order to pass through media data to files other than AVFileTypeQuickTimeMovie, a non-NULL format hint must be provided using +assetWriterInputWithMediaType:outputSettings:sourceFormatHint: instead of this method.
 
	For AVMediaTypeAudio the following keys are not currently supported in the outputSettings dictionary: AVSampleRateConverterAudioQualityKey.  When using this method to construct a new instance, an audio settings dictionary must be fully specified, meaning that it must contain AVFormatIDKey, AVSampleRateKey, and AVNumberOfChannelsKey.  If no other channel layout information is available, a value of 1 for AVNumberOfChannelsKey will result in mono output and a value of 2 will result in stereo output.  If AVNumberOfChannelsKey specifies a channel count greater than 2, the dictionary must also specify a value for AVChannelLayoutKey.  For kAudioFormatLinearPCM, all relevant AVLinearPCM*Key keys must be included, and for kAudioFormatAppleLossless, AVEncoderBitDepthHintKey keys must be included.  See +assetWriterInputWithMediaType:outputSettings:sourceFormatHint: for a way to avoid having to specify a value for each of those keys.
 
	For AVMediaTypeVideo, any output settings dictionary must request a compressed video format.  This means that the value passed in for outputSettings must follow the rules for compressed video output, as laid out in AVVideoSettings.h.  When using this method to construct a new instance, a video settings dictionary must be fully specified, meaning that it must contain AVVideoCodecKey, AVVideoWidthKey, and AVVideoHeightKey.  See +assetWriterInputWithMediaType:outputSettings:sourceFormatHint: for a way to avoid having to specify a value for each of those keys.  On iOS, the only values currently supported for AVVideoCodecKey are AVVideoCodecTypeH264 and AVVideoCodecTypeJPEG.  AVVideoCodecTypeH264 is not supported on iPhone 3G.  For AVVideoScalingModeKey, the value AVVideoScalingModeFit is not supported.
 */
+ (instancetype)assetWriterInputWithMediaType:(AVMediaType)mediaType outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings;

/*!
 @method assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
 @abstract
	Creates a new input of the specified media type to receive sample buffers for writing to the output file.
 
 @param mediaType
	The media type of samples that will be accepted by the input. Media types are defined in AVMediaFormat.h.
 @param outputSettings
	The settings used for encoding the media appended to the output.  See AVAudioSettings.h for AVMediaTypeAudio or AVVideoSettings.h for AVMediaTypeVideo and for more information on how to construct an output settings dictionary.  If you only require simple preset-based output settings, see AVOutputSettingsAssistant.
 @param sourceFormatHint
	A hint about the format of media data that will be appended to the new input.
 @result
	An instance of AVAssetWriterInput.
 
 @discussion
	A version of +assetWriterInputWithMediaType:outputSettings: that includes the ability to hint at the format of media data that will be appended to the new instance of AVAssetWriterInput.  When a source format hint is provided, the outputSettings dictionary is not required to be fully specified.  For AVMediaTypeAudio, this means that AVFormatIDKey is the only required key.  For AVMediaTypeVideo, this means that AVVideoCodecKey is the only required key.  Values for the remaining keys will be chosen by the asset writer input, with consideration given to the attributes of the source format.  To guarantee successful file writing, clients who specify a format hint should ensure that subsequently-appended buffers are of the specified format.
 
	An NSInvalidArgumentException will be thrown if the media type of the format description does not match the media type string passed into this method.
 */
+ (instancetype)assetWriterInputWithMediaType:(AVMediaType)mediaType outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings sourceFormatHint:(nullable CMFormatDescriptionRef)sourceFormatHint NS_AVAILABLE(10_8, 6_0);

/*!
 @method initWithMediaType:outputSettings:
 @abstract
	Creates a new input of the specified media type to receive sample buffers for writing to the output file.

 @param mediaType
	The media type of samples that will be accepted by the input. Media types are defined in AVMediaFormat.h.
 @param outputSettings
	The settings used for encoding the media appended to the output.  See AVAudioSettings.h for AVMediaTypeAudio or AVVideoSettings.h for AVMediaTypeVideo and for more information on how to construct an output settings dictionary.  If you only require simple preset-based output settings, see AVOutputSettingsAssistant.
 @result
	An instance of AVAssetWriterInput.

 @discussion
	Each new input accepts data for a new track of the AVAssetWriter's output file.  Inputs are added to an asset writer using -[AVAssetWriter addInput:].
	
	Passing nil for output settings instructs the input to pass through appended samples, doing no processing before they are written to the output file.  This is useful if, for example, you are appending buffers that are already in a desirable compressed format.  However, if not writing to a QuickTime Movie file (i.e. the AVAssetWriter was initialized with a file type other than AVFileTypeQuickTimeMovie), AVAssetWriter only supports passing through a restricted set of media types and subtypes.  In order to pass through media data to files other than AVFileTypeQuickTimeMovie, a non-NULL format hint must be provided using -initWithMediaType:outputSettings:sourceFormatHint: instead of this method.
 
	For AVMediaTypeAudio the following keys are not currently supported in the outputSettings dictionary: AVSampleRateConverterAudioQualityKey.  When using this initializer, an audio settings dictionary must be fully specified, meaning that it must contain AVFormatIDKey, AVSampleRateKey, and AVNumberOfChannelsKey.  If no other channel layout information is available, a value of 1 for AVNumberOfChannelsKey will result in mono output and a value of 2 will result in stereo output.  If AVNumberOfChannelsKey specifies a channel count greater than 2, the dictionary must also specify a value for AVChannelLayoutKey.  For kAudioFormatLinearPCM, all relevant AVLinearPCM*Key keys must be included, and for kAudioFormatAppleLossless, AVEncoderBitDepthHintKey keys must be included.  See -initWithMediaType:outputSettings:sourceFormatHint: for a way to avoid having to specify a value for each of those keys.
 
	For AVMediaTypeVideo, any output settings dictionary must request a compressed video format.  This means that the value passed in for outputSettings must follow the rules for compressed video output, as laid out in AVVideoSettings.h.  When using this initializer, a video settings dictionary must be fully specified, meaning that it must contain AVVideoCodecKey, AVVideoWidthKey, and AVVideoHeightKey.  See -initWithMediaType:outputSettings:sourceFormatHint: for a way to avoid having to specify a value for each of those keys.  On iOS, the only values currently supported for AVVideoCodecKey are AVVideoCodecTypeH264 and AVVideoCodecTypeJPEG.  AVVideoCodecTypeH264 is not supported on iPhone 3G.  For AVVideoScalingModeKey, the value AVVideoScalingModeFit is not supported.
 */
- (instancetype)initWithMediaType:(AVMediaType)mediaType outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings;

/*!
 @method initWithMediaType:outputSettings:sourceFormatHint:
 @abstract
	Creates a new input of the specified media type to receive sample buffers for writing to the output file.  This is the designated initializer of AVAssetWriterInput.
 
 @param mediaType
	The media type of samples that will be accepted by the input. Media types are defined in AVMediaFormat.h.
 @param outputSettings
	The settings used for encoding the media appended to the output.  See AVAudioSettings.h for AVMediaTypeAudio or AVVideoSettings.h for AVMediaTypeVideo and for more information on how to construct an output settings dictionary.  If you only require simple preset-based output settings, see AVOutputSettingsAssistant.
 @param sourceFormatHint
	A hint about the format of media data that will be appended to the new input.
 @result
	An instance of AVAssetWriterInput.
 
 @discussion
	A version of -initWithMediaType:outputSettings: that includes the ability to hint at the format of media data that will be appended to the new instance of AVAssetWriterInput.  When a source format hint is provided, the outputSettings dictionary is not required to be fully specified.  For AVMediaTypeAudio, this means that AVFormatIDKey is the only required key.  For AVMediaTypeVideo, this means that AVVideoCodecKey is the only required key.  Values for the remaining keys will be chosen by the asset writer input, with consideration given to the attributes of the source format.  To guarantee successful file writing, clients who specify a format hint should ensure that subsequently-appended buffers are of the specified format.
 
	An NSInvalidArgumentException will be thrown if the media type of the format description does not match the media type string passed into this method.
 */
- (instancetype)initWithMediaType:(AVMediaType)mediaType outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings sourceFormatHint:(nullable CMFormatDescriptionRef)sourceFormatHint NS_AVAILABLE(10_8, 6_0) NS_DESIGNATED_INITIALIZER;

/*!
 @property mediaType
 @abstract
	The media type of the samples that can be appended to the receiver.
 
 @discussion
	The value of this property is one of the media type strings defined in AVMediaFormat.h.
 */
@property (nonatomic, readonly) AVMediaType mediaType;

/*!
 @property outputSettings
 @abstract
	The settings used for encoding the media appended to the output.
 
 @discussion
	The value of this property is an NSDictionary that contains values for keys as specified by either AVAudioSettings.h for AVMediaTypeAudio or AVVideoSettings.h for AVMediaTypeVideo.  A value of nil indicates that the receiver will pass through appended samples, doing no processing before they are written to the output file.
*/
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *outputSettings;

/*!
 @property sourceFormatHint
 @abstract
	 The hint given at initialization time about the format of incoming media data.
 
 @discussion
	AVAssetWriterInput may be able to use this hint to fill in missing output settings or perform more upfront validation.  To guarantee successful file writing, clients who specify a format hint should ensure that subsequently-appended media data are of the specified format.
 */
@property (nonatomic, readonly, nullable) __attribute__((NSObject)) CMFormatDescriptionRef sourceFormatHint NS_AVAILABLE(10_8, 6_0);

/*!
 @property metadata
 @abstract
	A collection of metadata to be written to the track corresponding to the receiver. 

 @discussion
	The value of this property is an array of AVMetadataItem objects representing the collection of track-level metadata to be written in the output file.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic, copy) NSArray<AVMetadataItem *> *metadata;

/*!
 @property readyForMoreMediaData
 @abstract
	Indicates the readiness of the input to accept more media data.
 
 @discussion
    When there are multiple inputs, AVAssetWriter tries to write media data in an ideal interleaving pattern for efficiency in storage and playback. Each of its inputs signals its readiness to receive media data for writing according to that pattern via the value of readyForMoreMediaData. You can append media data to an input only while its readyForMoreMediaData property is YES.
 
    Clients writing media data from a non-real-time source, such as an instance of AVAssetReader, should hold off on generating or obtaining more media data to append to an input when the value of readyForMoreMediaData is NO. To help with control of the supply of non-real-time media data, such clients can use -requestMediaDataWhenReadyOnQueue:usingBlock in order to specify a block that the input should invoke whenever it's ready for input to be appended.

    Clients writing media data from a real-time source, such as an instance of AVCaptureOutput, should set the input's expectsMediaDataInRealTime property to YES to ensure that the value of readyForMoreMediaData is calculated appropriately. When expectsMediaDataInRealTime is YES, readyForMoreMediaData will become NO only when the input cannot process media samples as quickly as they are being provided by the client. If readyForMoreMediaData becomes NO for a real-time source, the client may need to drop samples or consider reducing the data rate of appended samples.
 
	When the value of canPerformMultiplePasses is YES for any input attached to this input's asset writer, the value for this property may start as NO and/or be NO for long periods of time.
 
    The value of readyForMoreMediaData will often change from NO to YES asynchronously, as previously supplied media data is processed and written to the output.  It is possible for all of an AVAssetWriter's AVAssetWriterInputs temporarily to return NO for readyForMoreMediaData.
	
    This property is key value observable. Observers should not assume that they will be notified of changes on a specific thread.
 */
@property (nonatomic, readonly, getter=isReadyForMoreMediaData) BOOL readyForMoreMediaData;

/*!
 @property expectsMediaDataInRealTime
 @abstract
	Indicates whether the input should tailor its processing of media data for real-time sources.

 @discussion
    Clients appending media data to an input from a real-time source, such as an AVCaptureOutput, should set expectsMediaDataInRealTime to YES. This will ensure that readyForMoreMediaData is calculated appropriately for real-time usage.
 
	For best results, do not set both this property and performsMultiPassEncodingIfSupported to YES.
 
	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic) BOOL expectsMediaDataInRealTime;

/*!
 @method requestMediaDataWhenReadyOnQueue:usingBlock:
 @abstract
	Instructs the receiver to invoke a client-supplied block repeatedly, at its convenience, in order to gather media data for writing to the output file.

 @param queue
	The queue on which the block should be invoked.
 @param block
	The block the input should invoke to obtain media data.

 @discussion
	The block should append media data to the input either until the input's readyForMoreMediaData property becomes NO or until there is no more media data to supply (at which point it may choose to mark the input as finished via -markAsFinished). The block should then exit. After the block exits, if the input has not been marked as finished, once the input has processed the media data it has received and becomes ready for more media data again, it will invoke the block again in order to obtain more.
 
    A typical use of this method, with a block that supplies media data to an input while respecting the input's readyForMoreMediaData property, might look like this:

    [myAVAssetWriterInput requestMediaDataWhenReadyOnQueue:myInputSerialQueue usingBlock:^{
        while ([myAVAssetWriterInput isReadyForMoreMediaData])
        {
            CMSampleBufferRef nextSampleBuffer = [self copyNextSampleBufferToWrite];
            if (nextSampleBuffer)
            {
                [myAVAssetWriterInput appendSampleBuffer:nextSampleBuffer];
                CFRelease(nextSampleBuffer);
            }
            else
            {
                [myAVAssetWriterInput markAsFinished];
                break;
            }
        }
    }];
 
	This method is not recommended for use with a push-style buffer source, such as AVCaptureAudioDataOutput or AVCaptureVideoDataOutput, because such a combination will likely require intermediate queueing of buffers.  Instead, this method is better suited to a pull-style buffer source such as AVAssetReaderOutput, as illustrated in the above example.
 
	When using a push-style buffer source, it is generally better to immediately append each buffer to the AVAssetWriterInput, directly via -[AVAssetWriter appendSampleBuffer:], as it is received.  Using this strategy, it is often possible to avoid  having to queue up buffers in between the buffer source and the AVAssetWriterInput.  Note that many of these push-style buffer sources also produce buffers in real-time, in which case the client should set expectsMediaDataInRealTime to YES.
 
	Before calling this method, you must ensure that the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.
 */
- (void)requestMediaDataWhenReadyOnQueue:(dispatch_queue_t)queue usingBlock:(void (^)(void))block;

/*!
 @method appendSampleBuffer:
 @abstract
	Appends samples to the receiver.

 @param sampleBuffer
	The CMSampleBuffer to be appended.
 @result
	A BOOL value indicating success of appending the sample buffer. If a result of NO is returned, clients can check the value of AVAssetWriter.status to determine whether the writing operation completed, failed, or was cancelled.  If the status is AVAssetWriterStatusFailed, AVAsset.error will contain an instance of NSError that describes the failure.
 
 @discussion
	The timing information in the sample buffer, considered relative to the time passed to -[AVAssetWriter startSessionAtSourceTime:], will be used to determine the timing of those samples in the output file.

	For track types other than audio tracks, to determine the duration of all samples in the output file other than the very last sample that's appended, the difference between the sample buffer's output DTS and the following sample buffer's output DTS will be used. The duration of the last sample is determined as follows:
	1. If a marker sample buffer with kCMSampleBufferAttachmentKey_EndsPreviousSampleDuration is appended following the last media-bearing sample, the difference between the output DTS of the marker sample buffer and the output DTS of the last media-bearing sample will be used.
	2. If the marker sample buffer is not provided and if the output duration of the last media-bearing sample is valid, it will be used.
	3. if the output duration of the last media-bearing sample is not valid, the duration of the second-to-last sample will be used.

	For audio tracks, the properties of each appended sample buffer are used to determine corresponding output durations.

	The receiver will retain the CMSampleBuffer until it is done with it, and then release it.  Do not modify a CMSampleBuffer or its contents after you have passed it to this method.
 
	If the sample buffer contains audio data and the AVAssetWriterInput was intialized with an outputSettings dictionary then the format must be linear PCM. If the outputSettings dictionary was nil then audio data can be provided in a compressed format, and it will be passed through to the output without any re-compression. Note that advanced formats like AAC will have encoder delay present in their bitstreams. This data is inserted by the encoder and is necessary for proper decoding, but it is not meant to be played back. Clients who provide compressed audio bitstreams must use kCMSampleBufferAttachmentKey_TrimDurationAtStart to mark the encoder delay (generally restricted to the first sample buffer). Packetization can cause there to be extra audio frames in the last packet which are not meant to be played back. These remainder frames should be marked with kCMSampleBufferAttachmentKey_TrimDurationAtEnd. CMSampleBuffers obtained from AVAssetReader will already have the necessary trim attachments. Please see http://developer.apple.com/mac/library/technotes/tn2009/tn2258.html for more information about encoder delay. When attaching trims make sure that the output PTS of the sample buffer is what you expect. For example if you called -[AVAssetWriter startSessionAtSourceTime:kCMTimeZero] and you want your audio to start at time zero in the output file then make sure that the output PTS of the first non-fully trimmed audio sample buffer is kCMTimeZero.
	
	If the sample buffer contains a CVPixelBuffer then the choice of pixel format will affect the performance and quality of the encode. For optimal performance the format of the pixel buffer should match one of the native formats supported by the selected video encoder. Below are some recommendations:
 
	The H.264 encoder natively supports kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange and kCVPixelFormatType_420YpCbCr8BiPlanarFullRange, which should be used with video and full range input respectively. The JPEG encoder on iOS natively supports kCVPixelFormatType_422YpCbCr8FullRange. For other video codecs on OSX, kCVPixelFormatType_422YpCbCr8 is the preferred pixel format for video and is generally the most performant when encoding. If you need to work in the RGB domain then kCVPixelFormatType_32BGRA is recommended on iOS and kCVPixelFormatType_32ARGB is recommended on OSX.

	Pixel buffers not in a natively supported format will be converted internally prior to encoding when possible. Pixel format conversions within the same range (video or full) are generally faster than conversions between different ranges.
 
	The ProRes encoders can preserve high bit depth sources, supporting up to 12bits/ch. ProRes 4444 can contain a mathematically lossless alpha channel and it doesn't do any chroma subsampling. This makes ProRes 4444 ideal for quality critical applications. If you are working with 8bit sources ProRes is also a good format to use due to its high image quality. Use either of the recommended pixel formats above. Note that RGB pixel formats by definition have 4:4:4 chroma sampling.
 
 	If you are working with high bit depth sources the following yuv pixel formats are recommended when encoding to ProRes: kCVPixelFormatType_4444AYpCbCr16, kCVPixelFormatType_422YpCbCr16, and kCVPixelFormatType_422YpCbCr10. When working in the RGB domain kCVPixelFormatType_64ARGB is recommended. Scaling and color matching are not currently supported when using AVAssetWriter with any of these high bit depth pixel formats. Please make sure that your track's output settings dictionary specifies the same width and height as the buffers you will be appending. Do not include AVVideoScalingModeKey or AVVideoColorPropertiesKey.

	As of OS X 10.10 and iOS 8.0, this method can be used to add sample buffers that reference existing data in a file instead of containing media data to be appended to the file. This can be used to generate tracks that are not self-contained. In order to append such a sample reference to the track create a CMSampleBufferRef with a NULL dataBuffer and dataReady set to true and set the kCMSampleBufferAttachmentKey_SampleReferenceURL and kCMSampleBufferAttachmentKey_SampleReferenceByteOffset attachments on the sample buffer. Further documentation on how to create such a "sample reference" sample buffer can be found in the description of the kCMSampleBufferAttachmentKey_SampleReferenceURL and kCMSampleBufferAttachmentKey_SampleReferenceByteOffset attachment keys in the CMSampleBuffer documentation.

	Before calling this method, you must ensure that the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.  It is an error to invoke this method before starting a session (via -[AVAssetWriter startSessionAtSourceTime:]) or after ending a session (via -[AVAssetWriter endSessionAtSourceTime:]).
 */
- (BOOL)appendSampleBuffer:(CMSampleBufferRef)sampleBuffer;

/*!
 @method markAsFinished
 @abstract
	Indicates to the AVAssetWriter that no more buffers will be appended to this input.

 @discussion
	Clients that are monitoring each input's readyForMoreMediaData value must call markAsFinished on an input when they are done appending buffers to it.  This is necessary to prevent other inputs from stalling, as they may otherwise wait forever for that input's media data, attempting to complete the ideal interleaving pattern.
 
	After invoking this method from the serial queue passed to -requestMediaDataWhenReadyOnQueue:usingBlock:, the receiver is guaranteed to issue no more invocations of the block passed to that method.  The same is true of -respondToEachPassDescriptionOnQueue:usingBlock:.
 
	Before calling this method, you must ensure that the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.
 */
- (void)markAsFinished;

@end


@interface AVAssetWriterInput (AVAssetWriterInputLanguageProperties)

/*!
 @property languageCode
 @abstract
	Indicates the language to associate with the track corresponding to the receiver, as an ISO 639-2/T language code; can be nil.
 
 @discussion
	Also see extendedLanguageTag below.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic, copy, nullable) NSString *languageCode NS_AVAILABLE(10_9, 7_0);

/*!
 @property extendedLanguageTag
 @abstract
	Indicates the language tag to associate with the track corresponding to the receiver, as an IETF BCP 47 (RFC 4646) language identifier; can be nil.
 
 @discussion
	Extended language tags are normally set only when an ISO 639-2/T language code by itself is ambiguous, as in cases in which media data should be distinguished not only by language but also by the regional dialect in use or the writing system employed.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.	
 */
@property (nonatomic, copy, nullable) NSString *extendedLanguageTag NS_AVAILABLE(10_9, 7_0);

@end


@interface AVAssetWriterInput (AVAssetWriterInputPropertiesForVisualCharacteristic)

/*!
 @property naturalSize
 @abstract
	The size specified in the output file as the natural dimensions of the visual media data for display purposes.
 
 @discussion
	If the default value, CGSizeZero, is specified, the naturalSize of the track corresponding to the receiver is set according to dimensions indicated by the format descriptions that are ultimately written to the output track.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
*/
@property (nonatomic) CGSize naturalSize NS_AVAILABLE(10_9, 7_0);

/*!
 @property transform
 @abstract
	The transform specified in the output file as the preferred transformation of the visual media data for display purposes.
 
 @discussion
	If no value is specified, the identity transform is used.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
*/
@property (nonatomic) CGAffineTransform transform;

@end


@interface AVAssetWriterInput (AVAssetWriterInputPropertiesForAudibleCharacteristic)

/*!
 @property preferredVolume
 @abstract
	The preferred volume level to be stored in the output file.
 
 @discussion
	The value for this property should typically be in the range of 0.0 to 1.0.  The default value is 1.0, which is equivalent to a "normal" volume level for audio media type. For all other media types the default value is 0.0.
 
	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic) float preferredVolume NS_AVAILABLE(10_9, 7_0);

@end


@interface AVAssetWriterInput (AVAssetWriterInputFileTypeSpecificProperties)

/*!
 @property marksOutputTrackAsEnabled
 @abstract
	For file types that support enabled and disabled tracks, such as QuickTime Movie files, specifies whether the track corresponding to the receiver should be enabled by default for playback and processing. The default value is YES.
 
 @discussion
	When an input group is added to an AVAssetWriter (see -[AVAssetWriter addInputGroup:]), the value of marksOutputTrackAsEnabled will automatically be set to YES for the default input and set to NO for all of the other inputs in the group.  In this case, if a new value is set on this property then an exception will be raised.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic) BOOL marksOutputTrackAsEnabled NS_AVAILABLE(10_9, 7_0);

/*!
 @property mediaTimeScale
 @abstract
	For file types that support media time scales, such as QuickTime Movie files, specifies the media time scale to be used.

 @discussion
	The default value is 0, which indicates that the receiver should choose a convenient value, if applicable.  It is an error to set a value other than 0 if the receiver has media type AVMediaTypeAudio.

	This property cannot be set after writing has started.
 */
@property (nonatomic) CMTimeScale mediaTimeScale NS_AVAILABLE(10_7, 4_3);

/*!
 @property preferredMediaChunkDuration
 @abstract
	For file types that support media chunk duration, such as QuickTime Movie files, specifies the duration to be used for each chunk of sample data in the output file.
 
 @discussion
	Chunk duration can influence the granularity of the I/O performed when reading a media file, e.g. during playback.  A larger chunk duration can result in fewer reads from disk, at the potential expense of a higher memory footprint.
 
	A "chunk" contains one or more samples.  The total duration of the samples in a chunk is no greater than this preferred chunk duration, or the duration of a single sample if the sample's duration is greater than this preferred chunk duration.
 
	The default value is kCMTimeInvalid, which means that the receiver will choose an appropriate default value.  It is an error to set a chunk duration that is negative or non-numeric.

	This property cannot be set after -startWriting has been called on the receiver.
 */
@property (nonatomic) CMTime preferredMediaChunkDuration NS_AVAILABLE(10_10, 8_0);

/*!
 @property preferredMediaChunkAlignment
 @abstract
	For file types that support media chunk alignment, such as QuickTime Movie files, specifies the boundary for media chunk alignment in bytes (e.g. 512).
 
 @discussion
	The default value is 0, which means that the receiver will choose an appropriate default value.  A value of 1 implies that no padding should be used to achieve a particular chunk alignment.  It is an error to set a negative value for chunk alignment.
 
	This property cannot be set after -startWriting has been called on the receiver.
 */
@property (nonatomic) NSInteger preferredMediaChunkAlignment NS_AVAILABLE(10_10, 8_0);

/*!
 @property sampleReferenceBaseURL
 @abstract
	For file types that support writing sample references, such as QuickTime Movie files, specifies the base URL sample references are relative to.

 @discussion
	If the value of this property can be resolved as an absolute URL, the sample locations written to the file when appending sample references will be relative to this URL. The URL must point to a location that is in a directory that is a parent of the sample reference location. 

	Usage example:

	Setting the sampleReferenceBaseURL property to "file:///User/johnappleseed/Movies/" and appending sample buffers with the kCMSampleBufferAttachmentKey_SampleReferenceURL attachment set to "file:///User/johnappleseed/Movies/data/movie1.mov" will cause the sample reference "data/movie1.mov" to be written to the movie.

	If the value of the property cannot be resolved as an absolute URL or if it points to a location that is not in a parent directory of the sample reference location, the location referenced in the sample buffer will be written unmodified.

 	The default value is nil, which means that the location referenced in the sample buffer will be written unmodified.
 
	This property cannot be set after -startWriting has been called on the receiver.
 */
@property (nonatomic, copy, nullable) NSURL *sampleReferenceBaseURL NS_AVAILABLE(10_10, 8_0);

typedef NSString *AVAssetWriterInputMediaDataLocation NS_STRING_ENUM NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData
	Indicates that the media data should be interleaved with all other media data with this constant.
 */
AVF_EXPORT AVAssetWriterInputMediaDataLocation const AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVAssetWriterInputMediaDataLocationBeforeMainMediaDataNotInterleaved
	Indicates that the media data should be laid out before all the media data with AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData and not be interleaved.
 */
AVF_EXPORT AVAssetWriterInputMediaDataLocation const AVAssetWriterInputMediaDataLocationBeforeMainMediaDataNotInterleaved NS_AVAILABLE(10_13, 11_0);

/*!
 @property mediaDataLocation
 @abstract
	Specifies where the media data will be laid out and whether the media data will be interleaved as the main media data.

 @discussion
	If this value is set to AVAssetWriterInputMediaDataLocationBeforeMainMediaDataNotInterleaved, AVAssetWriter tries to write the media data for this track before all the media data for AVAssetWriterInputs with this property set to AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData.

	Use of this property is recommended for optimizing tracks that contain a small amount of data that is needed all at once, independent of playback time, such as chapter name tracks and chapter image tracks.
	Keep it set to AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData for tracks whose media data that's needed only as its presentation time is approaching and, when multiple inputs are present that supply media data that will be played concurrently, should be interleaved for optimal access.

	For file types that support preloading media data such as QuickTime movie file, if this value is set to AVAssetWriterInputMediaDataLocationBeforeMainMediaDataNotInterleaved, AVAssetWriter will write an indication such as 'load' atom that the whole media data should be preloaded.

	The default value is AVAssetWriterInputMediaDataLocationInterleavedWithMainMediaData, which means that the receiver will not write the indication and that the media data will be interleaved.

	This property cannot be set after -startWriting has been called on the receiver.
 */
@property (nonatomic, copy) AVAssetWriterInputMediaDataLocation mediaDataLocation NS_AVAILABLE(10_13, 11_0);

@end


@interface AVAssetWriterInput (AVAssetWriterInputTrackAssociations)

/*!
 @method canAddTrackAssociationWithTrackOfInput:type:
 @abstract
	Tests whether an association between the tracks corresponding to a pair of inputs is valid.

 @param input
	The instance of AVAssetWriterInput with a corresponding track to associate with track corresponding with the receiver.
 @param trackAssociationType
	The type of track association to test. Common track association types, such as AVTrackAssociationTypeTimecode, are defined in AVAssetTrack.h.

 @discussion
	If the type of association requires tracks of specific media types that don't match the media types of the inputs, or if the output file type does not support track associations, -canAddTrackAssociationWithTrackOfInput:type: will return NO.
 */
- (BOOL)canAddTrackAssociationWithTrackOfInput:(AVAssetWriterInput *)input type:(NSString *)trackAssociationType NS_AVAILABLE(10_9, 7_0);

/*!
 @method addTrackAssociationWithTrackOfInput:type:
 @abstract
	Associates the track corresponding to the specified input with the track corresponding with the receiver.

 @param input
	The instance of AVAssetWriterInput with a corresponding track to associate with track corresponding to the receiver.
 @param trackAssociationType
	The type of track association to add. Common track association types, such as AVTrackAssociationTypeTimecode, are defined in AVAssetTrack.h.

 @discussion
	If the type of association requires tracks of specific media types that don't match the media types of the inputs, or if the output file type does not support track associations, an NSInvalidArgumentException is raised.

	Track associations cannot be added after writing on the receiver's AVAssetWriter has started.
 */
- (void)addTrackAssociationWithTrackOfInput:(AVAssetWriterInput *)input type:(NSString *)trackAssociationType NS_AVAILABLE(10_9, 7_0);

@end


@class AVAssetWriterInputPassDescription;

@interface AVAssetWriterInput (AVAssetWriterInputMultiPass)

/*!
 @property performsMultiPassEncodingIfSupported
 @abstract
	Indicates whether the input should attempt to encode the source media data using multiple passes.
 
 @discussion
	The input may be able to achieve higher quality and/or lower data rate by performing multiple passes over the source media.  It does this by analyzing the media data that has been appended and re-encoding certain segments with different parameters.  In order to do this re-encoding, the media data for these segments must be appended again.  See -markCurrentPassAsFinished and the property currentPassDescription for the mechanism by which the input nominates segments for re-appending.
 
	When the value of this property is YES, the value of readyForMoreMediaData for other inputs attached to the same AVAssetWriter may be NO more often and/or for longer periods of time.  In particular, the value of readyForMoreMediaData for inputs that do not (or cannot) perform multiple passes may start out as NO after -[AVAssetWriter startWriting] has been called and may not change to YES until after all multi-pass inputs have completed their final pass.
 
	When the value of this property is YES, the input may store data in one or more temporary files before writing compressed samples to the output file.  Use the AVAssetWriter property directoryForTemporaryFiles if you need to control the location of temporary file writing.
 
	The default value is NO, meaning that no additional analysis will occur and no segments will be re-encoded.  Not all asset writer input configurations (for example, inputs configured with certain media types or to use certain encoders) can benefit from performing multiple passes over the source media.  To determine whether the selected encoder can perform multiple passes, query the value of canPerformMultiplePasses after calling -startWriting.
 
	For best results, do not set both this property and expectsMediaDataInRealTime to YES.

	This property cannot be set after writing on the receiver's AVAssetWriter has started.
 */
@property (nonatomic) BOOL performsMultiPassEncodingIfSupported NS_AVAILABLE(10_10, 8_0);

/*!
 @property canPerformMultiplePasses
 @abstract
	Indicates whether the input might perform multiple passes over appended media data.

 @discussion
	When the value for this property is YES, your source for media data should be configured for random access.  After appending all of the media data for the current pass (as specified by the currentPassDescription property), call -markCurrentPassAsFinished to start the process of determining whether additional passes are needed.  Note that it is still possible in this case for the input to perform only the initial pass, if it determines that there will be no benefit to performing multiple passes.
 
	When the value for this property is NO, your source for media data only needs to support sequential access.  In this case, append all of the source media once and call -markAsFinished.
 
	In the default configuration of AVAssetWriterInput, the value for this property will be NO.  Currently the only way for this property to become YES is when performsMultiPassEncodingIfSupported has been set to YES.  The final value will be available after -startWriting is called, when a specific encoder has been choosen.
 
	This property is key-value observable.
 */
@property (nonatomic, readonly) BOOL canPerformMultiplePasses NS_AVAILABLE(10_10, 8_0);

/*!
 @property currentPassDescription
 @abstract
	Provides an object that describes the requirements, such as source time ranges to append or re-append, for the current pass.
 
 @discussion
	If the value of this property is nil, it means there is no request to be fulfilled and -markAsFinished should be called on the asset writer input.
 
	During the first pass, the request will contain a single time range from zero to positive infinity, indicating that all media from the source should be appended.  This will also be true when canPerformMultiplePasses is NO, in which case only one pass will be performed.
 
	The value of this property will be nil before -startWriting is called on the attached asset writer.  It will transition to an initial non-nil value during the call to -startWriting.  After that, the value of this property will change only after a call to -markCurrentPassAsFinished.  For an easy way to be notified at the beginning of each pass, see -respondToEachPassDescriptionOnQueue:usingBlock:.
 
	This property is key-value observable.  Observers should not assume that they will be notified of changes on a specific thread.
 */
@property (readonly, nullable) AVAssetWriterInputPassDescription *currentPassDescription NS_AVAILABLE(10_10, 8_0);

/*!
 @method respondToEachPassDescriptionOnQueue:usingBlock:
 @abstract
	Instructs the receiver to invoke a client-supplied block whenever a new pass has begun.
 
 @param queue
	The queue on which the block should be invoked.
 @param block
	A block the receiver should invoke whenever a new pass has begun.

 @discussion
	A typical block passed to this method will perform the following steps:

		1. Query the value of the receiver's currentPassDescription property and reconfigure the source of media data (e.g. AVAssetReader) accordingly
		2. Call -requestMediaDataWhenReadyOnQueue:usingBlock: to begin appending data for the current pass
		3. Exit

	When all media data has been appended for the current request, call -markCurrentPassAsFinished to begin the process of determining whether an additional pass is warranted.  If an additional pass is warranted, the block passed to this method will be invoked to begin the next pass.  If no additional passes are needed, the block passed to this method will be invoked one final time so the client can invoke -markAsFinished in response to the value of currentPassDescription becoming nil.
 
	Before calling this method, you must ensure that the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.
 */
- (void)respondToEachPassDescriptionOnQueue:(dispatch_queue_t)queue usingBlock:(dispatch_block_t)block NS_AVAILABLE(10_10, 8_0);

/*!
 @method markCurrentPassAsFinished
 @abstract
	Instructs the receiver to analyze the media data that has been appended and determine whether the results could be improved by re-encoding certain segments.
 
 @discussion
	When the value of canPerformMultiplePasses is YES, call this method after you have appended all of your media data.  After the receiver analyzes whether an additional pass is warranted, the value of currentPassDescription will change (usually asynchronously) to describe how to set up for the next pass.  Although it is possible to use key-value observing to determine when the value of currentPassDescription has changed, it is typically more convenient to invoke -respondToEachPassDescriptionOnQueue:usingBlock: in order to start the work for each pass.
 
	After re-appending the media data for all of the time ranges of the new pass, call this method again to determine whether additional segments should be re-appended in another pass.
 
	Calling this method effectively cancels any previous invocation of -requestMediaDataWhenReadyOnQueue:usingBlock:, meaning that -requestMediaDataWhenReadyOnQueue:usingBlock: can be invoked again for each new pass.  -respondToEachPassDescriptionOnQueue:usingBlock: provides a convenient way to consolidate these invocations in your code.
 
	After each pass, you have the option of keeping the most recent results by calling -markAsFinished instead of this method.  If the value of currentPassDescription is nil at the beginning of a pass, call -markAsFinished to tell the receiver to not expect any further media data.
 
	If the value of canPerformMultiplePasses is NO, the value of currentPassDescription will immediately become nil after calling this method.

	Before calling this method, you must ensure that the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.
 */
- (void)markCurrentPassAsFinished NS_AVAILABLE(10_10, 8_0);

@end


@class AVAssetWriterInputPassDescriptionInternal;

/*!
 @class AVAssetWriterInputPassDescription
 @abstract
	Defines an interface for querying information about the requirements of the current pass, such as the time ranges of media data to append.
 */
NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVAssetWriterInputPassDescription : NSObject
{
@private
	AVAssetWriterInputPassDescriptionInternal *_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @property sourceTimeRanges
 @abstract
	An NSArray of NSValue objects wrapping CMTimeRange structures, each representing one source time range.
 
 @discussion
	The value of this property is suitable for using as a parameter for -[AVAssetReaderOutput resetForReadingTimeRanges:].
 */
@property (nonatomic, readonly) NSArray<NSValue *> *sourceTimeRanges;

@end


@class AVAssetWriterInputPixelBufferAdaptorInternal;

/*!
 @class AVAssetWriterInputPixelBufferAdaptor
 @abstract
	Defines an interface for appending video samples packaged as CVPixelBuffer objects to a single AVAssetWriterInput object.
 
 @discussion
	Instances of AVAssetWriterInputPixelBufferAdaptor provide a CVPixelBufferPool that can be used to allocate pixel buffers for writing to the output file.  Using the provided pixel buffer pool for buffer allocation is typically more efficient than appending pixel buffers allocated using a separate pool.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetWriterInputPixelBufferAdaptor : NSObject
{
@private
	AVAssetWriterInputPixelBufferAdaptorInternal	*_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetWriterInputPixelBufferAdaptorWithAssetWriterInput:sourcePixelBufferAttributes:
 @abstract
	Creates a new pixel buffer adaptor to receive pixel buffers for writing to the output file.

 @param input
	An instance of AVAssetWriterInput to which the receiver should append pixel buffers.  Currently, only asset writer inputs that accept media data of type AVMediaTypeVideo can be used to initialize a pixel buffer adaptor.
 @param sourcePixelBufferAttributes
	Specifies the attributes of pixel buffers that will be vended by the input's CVPixelBufferPool.
 @result
	An instance of AVAssetWriterInputPixelBufferAdaptor.

 @discussion
	In order to take advantage of the improved efficiency of appending buffers created from the adaptor's pixel buffer pool, clients should specify pixel buffer attributes that most closely accommodate the source format of the video frames being appended.

	Pixel buffer attributes keys for the pixel buffer pool are defined in <CoreVideo/CVPixelBuffer.h>. To specify the pixel format type, the pixelBufferAttributes dictionary should contain a value for kCVPixelBufferPixelFormatTypeKey.  For example, use [NSNumber numberWithInt:kCVPixelFormatType_32BGRA] for 8-bit-per-channel BGRA. See the discussion under appendPixelBuffer:withPresentationTime: for advice on choosing a pixel format.

	Clients that do not need a pixel buffer pool for allocating buffers should set sourcePixelBufferAttributes to nil.
	
	It is an error to initialize an instance of AVAssetWriterInputPixelBufferAdaptor with a sample buffer input that is already attached to another instance of AVAssetWriterInputPixelBufferAdaptor.
 */
+ (instancetype)assetWriterInputPixelBufferAdaptorWithAssetWriterInput:(AVAssetWriterInput *)input sourcePixelBufferAttributes:(nullable NSDictionary<NSString *, id> *)sourcePixelBufferAttributes;

/*!
 @method initWithAssetWriterInput:sourcePixelBufferAttributes:
 @abstract
	Creates a new pixel buffer adaptor to receive pixel buffers for writing to the output file.

 @param input
	An instance of AVAssetWriterInput to which the receiver should append pixel buffers.  Currently, only asset writer inputs that accept media data of type AVMediaTypeVideo can be used to initialize a pixel buffer adaptor.
 @param sourcePixelBufferAttributes
	Specifies the attributes of pixel buffers that will be vended by the input's CVPixelBufferPool.
 @result
	An instance of AVAssetWriterInputPixelBufferAdaptor.

 @discussion
	In order to take advantage of the improved efficiency of appending buffers created from the adaptor's pixel buffer pool, clients should specify pixel buffer attributes that most closely accommodate the source format of the video frames being appended.

	Pixel buffer attributes keys for the pixel buffer pool are defined in <CoreVideo/CVPixelBuffer.h>. To specify the pixel format type, the pixelBufferAttributes dictionary should contain a value for kCVPixelBufferPixelFormatTypeKey.  For example, use [NSNumber numberWithInt:kCVPixelFormatType_32BGRA] for 8-bit-per-channel BGRA. See the discussion under appendPixelBuffer:withPresentationTime: for advice on choosing a pixel format.

	Clients that do not need a pixel buffer pool for allocating buffers should set sourcePixelBufferAttributes to nil.
	
	It is an error to initialize an instance of AVAssetWriterInputPixelBufferAdaptor with an asset writer input that is already attached to another instance of AVAssetWriterInputPixelBufferAdaptor.  It is also an error to initialize an instance of AVAssetWriterInputPixelBufferAdaptor with an asset writer input whose asset writer has progressed beyond AVAssetWriterStatusUnknown.
 */
- (instancetype)initWithAssetWriterInput:(AVAssetWriterInput *)input sourcePixelBufferAttributes:(nullable NSDictionary<NSString *, id> *)sourcePixelBufferAttributes NS_DESIGNATED_INITIALIZER;

/*!
 @property assetWriterInput
 @abstract
	The asset writer input to which the receiver should append pixel buffers.
 */
@property (nonatomic, readonly) AVAssetWriterInput *assetWriterInput;

/*!
 @property sourcePixelBufferAttributes
 @abstract
	The pixel buffer attributes of pixel buffers that will be vended by the receiver's CVPixelBufferPool.

 @discussion
	The value of this property is a dictionary containing pixel buffer attributes keys defined in <CoreVideo/CVPixelBuffer.h>.
 */
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *sourcePixelBufferAttributes;

/*!
 @property pixelBufferPool
 @abstract
	A pixel buffer pool that will vend and efficiently recycle CVPixelBuffer objects that can be appended to the receiver.

 @discussion
	For maximum efficiency, clients should create CVPixelBuffer objects for appendPixelBuffer:withPresentationTime: by using this pool with the CVPixelBufferPoolCreatePixelBuffer() function.
	
	The value of this property will be NULL before -[AVAssetWriter startWriting] is called on the associated AVAssetWriter object.
	
	This property is key value observable.
 */
@property (nonatomic, readonly, nullable) CVPixelBufferPoolRef pixelBufferPool;

/*!
 @method appendPixelBuffer:withPresentationTime:
 @abstract
	Appends a pixel buffer to the receiver.

 @param pixelBuffer
	The CVPixelBuffer to be appended.
 @param presentationTime
	The presentation time for the pixel buffer to be appended.  This time will be considered relative to the time passed to -[AVAssetWriter startSessionAtSourceTime:] to determine the timing of the frame in the output file.
 @result
	A BOOL value indicating success of appending the pixel buffer. If a result of NO is returned, clients can check the value of AVAssetWriter.status to determine whether the writing operation completed, failed, or was cancelled.  If the status is AVAssetWriterStatusFailed, AVAsset.error will contain an instance of NSError that describes the failure.

 @discussion
	The receiver will retain the CVPixelBuffer until it is done with it, and then release it.  Do not modify a CVPixelBuffer or its contents after you have passed it to this method.
	
	For optimal performance the format of the pixel buffer should match one of the native formats supported by the selected video encoder. Below are some recommendations:
 
	The H.264 encoder natively supports kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange and kCVPixelFormatType_420YpCbCr8BiPlanarFullRange, which should be used with video and full range input respectively. The JPEG encoder on iOS natively supports kCVPixelFormatType_422YpCbCr8FullRange. For other video codecs on OSX, kCVPixelFormatType_422YpCbCr8 is the preferred pixel format for video and is generally the most performant when encoding. If you need to work in the RGB domain then kCVPixelFormatType_32BGRA is recommended on iOS and kCVPixelFormatType_32ARGB is recommended on OSX.

	Pixel buffers not in a natively supported format will be converted internally prior to encoding when possible. Pixel format conversions within the same range (video or full) are generally faster than conversions between different ranges.

	The ProRes encoders can preserve high bit depth sources, supporting up to 12bits/ch. ProRes 4444 can contain a mathematically lossless alpha channel and it doesn't do any chroma subsampling. This makes ProRes 4444 ideal for quality critical applications. If you are working with 8bit sources ProRes is also a good format to use due to its high image quality. Use either of the recommended pixel formats above. Note that RGB pixel formats by definition have 4:4:4 chroma sampling.
 
 	If you are working with high bit depth sources the following yuv pixel formats are recommended when encoding to ProRes: kCVPixelFormatType_4444AYpCbCr16, kCVPixelFormatType_422YpCbCr16, and kCVPixelFormatType_422YpCbCr10. When working in the RGB domain kCVPixelFormatType_64ARGB is recommended. Scaling and color matching are not currently supported when using AVAssetWriter with any of these high bit depth pixel formats. Please make sure that your track's output settings dictionary specifies the same width and height as the buffers you will be appending. Do not include AVVideoScalingModeKey or AVVideoColorPropertiesKey.
 
	Before calling this method, you must ensure that the input that underlies the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.  It is an error to invoke this method before starting a session (via -[AVAssetWriter startSessionAtSourceTime:]) or after ending a session (via -[AVAssetWriter endSessionAtSourceTime:]).
 */
- (BOOL)appendPixelBuffer:(CVPixelBufferRef)pixelBuffer withPresentationTime:(CMTime)presentationTime;

@end


@class AVTimedMetadataGroup;
@class AVAssetWriterInputMetadataAdaptorInternal;

/*!
 @class AVAssetWriterInputMetadataAdaptor
 @abstract
	Defines an interface for writing metadata, packaged as instances of AVTimedMetadataGroup, to a single AVAssetWriterInput object.
 */

NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVAssetWriterInputMetadataAdaptor : NSObject {
    AVAssetWriterInputMetadataAdaptorInternal	*_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetWriterInputMetadataAdaptorWithAssetWriterInput:
 @abstract
	Creates a new timed metadata group adaptor to receive instances of AVTimedMetadataGroup for writing to the output file.
 
 @param input
	An instance of AVAssetWriterInput to which the receiver should append groups of timed metadata.  Only asset writer inputs that accept media data of type AVMediaTypeMetadata can be used to initialize a timed metadata group adaptor.
 @result
	An instance of AVAssetWriterInputMetadataAdaptor.
 
 @discussion
	The instance of AVAssetWriterInput passed in to this method must have been created with a format hint indicating all possible combinations of identifier (or, alternatively, key and keySpace), dataType, and extendedLanguageTag that will be appended to the metadata adaptor.  It is an error to append metadata items not represented in the input's format hint.
 
	It is an error to initialize an instance of AVAssetWriterInputMetadataAdaptor with an asset writer input that is already attached to another instance of AVAssetWriterInputMetadataAdaptor.  It is also an error to initialize an instance of AVAssetWriterInputMetadataAdaptor with an asset writer input whose asset writer has progressed beyond AVAssetWriterStatusUnknown.
 */
+ (instancetype)assetWriterInputMetadataAdaptorWithAssetWriterInput:(AVAssetWriterInput *)input;

/*!
 @method initWithAssetWriterInput:
 @abstract
	Creates a new timed metadator group adaptor to receive instances of AVTimedMetadataGroup for writing to the output file.
 
 @param input
	An instance of AVAssetWriterInput to which the receiver should append groups of timed metadata. Only asset writer inputs that accept media data of type AVMediaTypeMetadata can be used to initialize a timed metadata group adaptor.
 @result
	An instance of AVAssetWriterInputMetadataAdaptor.
 
 @discussion
	The instance of AVAssetWriterInput passed in to this method must have been created with a format hint indicating all possible combinations of identifier (or, alternatively, key and keySpace), dataType, and extendedLanguageTag that will be appended to the metadata adaptor.  It is an error to append metadata items not represented in the input's format hint.  For help creating a suitable format hint, see -[AVTimedMetadataGroup copyFormatDescription].

	It is an error to initialize an instance of AVAssetWriterInputMetadataAdaptor with an asset writer input that is already attached to another instance of AVAssetWriterInputMetadataAdaptor.  It is also an error to initialize an instance of AVAssetWriterInputMetadataAdaptor with an asset writer input whose asset writer has progressed beyond AVAssetWriterStatusUnknown.
 */
- (instancetype)initWithAssetWriterInput:(AVAssetWriterInput *)input NS_DESIGNATED_INITIALIZER;

/*!
 @property assetWriterInput
 @abstract
	The asset writer input to which the receiver should append timed metadata groups.
 */
@property (nonatomic, readonly) AVAssetWriterInput *assetWriterInput;

/*!
 @method appendTimedMetadataGroup:
 @abstract
	Appends a timed metadata group to the receiver.
 
 @param timedMetadataGroup
	The AVTimedMetadataGroup to be appended.
 @result
	A BOOL value indicating success of appending the timed metadata group.  If a result of NO is returned, AVAssetWriter.error will contain more information about why apending the timed metadata group failed.
 
 @discussion
	The receiver will retain the AVTimedMetadataGroup until it is done with it, and then release it.
 
	The timing of the metadata items in the output asset will correspond to the timeRange of the AVTimedMetadataGroup, regardless of the values of the time and duration properties of the individual items.
 
	Before calling this method, you must ensure that the input that underlies the receiver is attached to an AVAssetWriter via a prior call to -addInput: and that -startWriting has been called on the asset writer.  It is an error to invoke this method before starting a session (via -[AVAssetWriter startSessionAtSourceTime:]) or after ending a session (via -[AVAssetWriter endSessionAtSourceTime:]).
 */
- (BOOL)appendTimedMetadataGroup:(AVTimedMetadataGroup *)timedMetadataGroup;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMetadataIdentifiers.h
/*
    File:		AVMetadataIdentifiers.h

    Framework:  AVFoundation
 
    Copyright 2014-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

typedef NSString * AVMetadataIdentifier NS_EXTENSIBLE_STRING_ENUM;

// CommonMetadata
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierTitle                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierCreator                                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierSubject                                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierDescription                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierPublisher                                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierContributor                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierCreationDate                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierLastModifiedDate                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierType                                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierFormat                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierAssetIdentifier                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierSource                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierLanguage                                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierRelation                                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierLocation                                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierCopyrights                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierAlbumName                                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierAuthor                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierArtist                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierArtwork                                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierMake                                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierModel                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataCommonIdentifierSoftware                                   NS_AVAILABLE(10_10, 8_0);

// QuickTimeUserData
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataAlbum                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataArranger                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataArtist                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataAuthor                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataChapter                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataComment                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataComposer                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataCopyright                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataCreationDate                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataDescription                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataDirector                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataDisclaimer                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataEncodedBy                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataFullName                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataGenre                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataHostComputer                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataInformation                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataKeywords                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataMake                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataModel                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataOriginalArtist                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataOriginalFormat                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataOriginalSource                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataPerformers                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataProducer                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataPublisher                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataProduct                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataSoftware                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataSpecialPlaybackRequirements     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataTrack                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataWarning                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataWriter                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataURLLink                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataLocationISO6709                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataTrackName                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataCredits                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataPhonogramRights                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeUserDataTaggedCharacteristic            NS_AVAILABLE(10_10, 8_0);

// ISO UserData (includes 3GPP)
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierISOUserDataCopyright                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierISOUserDataDate                                  NS_AVAILABLE(10_12, 10_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierISOUserDataTaggedCharacteristic                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataCopyright                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataAuthor                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataPerformer                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataGenre                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataRecordingYear                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataLocation                              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataTitle                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataDescription                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataCollection                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataUserRating                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataThumbnail                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataAlbumAndTrack                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataKeywordList                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataMediaClassification                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifier3GPUserDataMediaRating                           NS_AVAILABLE(10_10, 8_0);

// QuickTimeMetadata. For more information, see the QuickTime File Format Specification, available as part of the Mac OS X Reference Library at http://developer.apple.com/library/mac/navigation/
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataAuthor                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataComment                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCopyright                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCreationDate                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDirector                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDisplayName                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataInformation                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataKeywords                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataProducer                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataPublisher                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataAlbum                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataArtist                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataArtwork                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDescription                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataSoftware                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataYear                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataGenre                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataiXML                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationISO6709                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataMake                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataModel                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataArranger                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataEncodedBy                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataOriginalArtist                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataPerformer                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataComposer                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCredits                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataPhonogramRights                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCameraIdentifier                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCameraFrameReadoutTime          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataTitle		                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataCollectionUser                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataRatingUser                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationName                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationBody                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationNote                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationRole                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataLocationDate                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDirectionFacing                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDirectionMotion                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataPreferredAffineTransform        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataDetectedFace                    NS_AVAILABLE(10_11, 9_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataVideoOrientation                NS_AVAILABLE(10_11, 9_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierQuickTimeMetadataContentIdentifier               NS_AVAILABLE(10_11, 9_0);

// iTunesMetadata
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAlbum                              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataArtist                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataUserComment                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataCoverArt                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataCopyright                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataReleaseDate                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataEncodedBy                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataPredefinedGenre                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataUserGenre                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataSongName                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataTrackSubTitle                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataEncodingTool                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataComposer                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAlbumArtist                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAccountKind                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAppleID                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataArtistID                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataSongID                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataDiscCompilation                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataDiscNumber                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataGenreID                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataGrouping                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataPlaylistID                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataContentRating                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataBeatsPerMin                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataTrackNumber                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataArtDirector                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataArranger                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAuthor                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataLyrics                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataAcknowledgement                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataConductor                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataDescription                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataDirector                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataEQ                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataLinerNotes                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataRecordCompany                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataOriginalArtist                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataPhonogramRights                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataProducer                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataPerformer                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataPublisher                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataSoundEngineer                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataSoloist                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataCredits                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataThanks                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataOnlineExtras                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifieriTunesMetadataExecProducer                       NS_AVAILABLE(10_10, 8_0);

// ID3Metadata
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataAudioEncryption                       /* AENC Audio encryption */                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataAttachedPicture                       /* APIC Attached picture */                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataAudioSeekPointIndex                   /* ASPI Audio seek point index */                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataComments                              /* COMM Comments */                                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataCommercial                            /* COMR Commercial frame */                                     NS_AVAILABLE(10_11, 9_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataCommerical                            /* COMR Commercial frame */                                     NS_DEPRECATED(10_10, 10_11, 8_0, 9_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEncryption                            /* ENCR Encryption method registration */                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEqualization                          /* EQUA Equalization */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEqualization2                         /* EQU2 Equalisation (2) */                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEventTimingCodes                      /* ETCO Event timing codes */                                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataGeneralEncapsulatedObject             /* GEOB General encapsulated object */                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataGroupIdentifier                       /* GRID Group identification registration */                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInvolvedPeopleList_v23                /* IPLS Involved people list */                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataLink                                  /* LINK Linked information */                                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataMusicCDIdentifier                     /* MCDI Music CD identifier */                                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataMPEGLocationLookupTable               /* MLLT MPEG location lookup table */                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOwnership                             /* OWNE Ownership frame */                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPrivate                               /* PRIV Private frame */                                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPlayCounter                           /* PCNT Play counter */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPopularimeter                         /* POPM Popularimeter */                                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPositionSynchronization               /* POSS Position synchronisation frame */                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataRecommendedBufferSize                 /* RBUF Recommended buffer size */                              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataRelativeVolumeAdjustment              /* RVAD Relative volume adjustment */                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataRelativeVolumeAdjustment2             /* RVA2 Relative volume adjustment (2) */                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataReverb                                /* RVRB Reverb */                                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSeek                                  /* SEEK Seek frame */                                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSignature                             /* SIGN Signature frame */                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSynchronizedLyric                     /* SYLT Synchronized lyric/text */                              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSynchronizedTempoCodes                /* SYTC Synchronized tempo codes */                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataAlbumTitle                            /* TALB Album/Movie/Show title */                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataBeatsPerMinute                        /* TBPM BPM (beats per minute) */                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataComposer                              /* TCOM Composer */                                             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataContentType                           /* TCON Content type */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataCopyright                             /* TCOP Copyright message */                                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataDate                                  /* TDAT Date */                                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEncodingTime                          /* TDEN Encoding time */                                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPlaylistDelay                         /* TDLY Playlist delay */                                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalReleaseTime                   /* TDOR Original release time */                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataRecordingTime                         /* TDRC Recording time */                                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataReleaseTime                           /* TDRL Release time */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTaggingTime                           /* TDTG Tagging time */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEncodedBy                             /* TENC Encoded by */                                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataLyricist                              /* TEXT Lyricist/Text writer */                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataFileType                              /* TFLT File type */                                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTime                                  /* TIME Time */                                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInvolvedPeopleList_v24                /* TIPL Involved people list */                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataContentGroupDescription               /* TIT1 Content group description */                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTitleDescription                      /* TIT2 Title/songname/content description */                   NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSubTitle                              /* TIT3 Subtitle/Description refinement */                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInitialKey                            /* TKEY Initial key */                                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataLanguage                              /* TLAN Language(s) */                                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataLength                                /* TLEN Length */                                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataMusicianCreditsList                   /* TMCL Musician credits list */                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataMediaType                             /* TMED Media type */                                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataMood                                  /* TMOO Mood */                                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalAlbumTitle                    /* TOAL Original album/movie/show title */                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalFilename                      /* TOFN Original filename */                                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalLyricist                      /* TOLY Original lyricist(s)/text writer(s) */                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalArtist                        /* TOPE Original artist(s)/performer(s) */                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOriginalReleaseYear                   /* TORY Original release year */                                NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataFileOwner                             /* TOWN File owner/licensee */                                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataLeadPerformer                         /* TPE1 Lead performer(s)/Soloist(s) */                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataBand                                  /* TPE2 Band/orchestra/accompaniment */                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataConductor                             /* TPE3 Conductor/performer refinement */                       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataModifiedBy                            /* TPE4 Interpreted, remixed, or otherwise modified by */       NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPartOfASet                            /* TPOS Part of a set */                                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataProducedNotice                        /* TPRO Produced notice */                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPublisher                             /* TPUB Publisher */                                            NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTrackNumber                           /* TRCK Track number/Position in set */                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataRecordingDates                        /* TRDA Recording dates */                                      NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInternetRadioStationName              /* TRSN Internet radio station name */                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInternetRadioStationOwner             /* TRSO Internet radio station owner */                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSize                                  /* TSIZ Size */                                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataAlbumSortOrder                        /* TSOA Album sort order */                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPerformerSortOrder                    /* TSOP Performer sort order */                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTitleSortOrder                        /* TSOT Title sort order */                                     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataInternationalStandardRecordingCode    /* TSRC ISRC (international standard recording code) */         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataEncodedWith                           /* TSSE Software/Hardware and settings used for encoding */     NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataSetSubtitle                           /* TSST Set subtitle */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataYear                                  /* TYER Year */                                                 NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataUserText                              /* TXXX User defined text information frame */                  NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataUniqueFileIdentifier                  /* UFID Unique file identifier */                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataTermsOfUse                            /* USER Terms of use */                                         NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataUnsynchronizedLyric                   /* USLT Unsynchronized lyric/text transcription */              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataCommercialInformation                 /* WCOM Commercial information */                               NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataCopyrightInformation                  /* WCOP Copyright/Legal information */                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOfficialAudioFileWebpage              /* WOAF Official audio file webpage */                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOfficialArtistWebpage                 /* WOAR Official artist/performer webpage */                    NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOfficialAudioSourceWebpage            /* WOAS Official audio source webpage */                        NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOfficialInternetRadioStationHomepage  /* WORS Official Internet radio station homepage */             NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataPayment                               /* WPAY Payment */                                              NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataOfficialPublisherWebpage              /* WPUB Publishers official webpage */                          NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierID3MetadataUserURL                               /* WXXX User defined URL link frame */                          NS_AVAILABLE(10_10, 8_0);

AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierIcyMetadataStreamTitle                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataIdentifier const AVMetadataIdentifierIcyMetadataStreamURL                             NS_AVAILABLE(10_10, 8_0);

// ==========  AVFoundation.framework/Headers/AVAudioUnitGenerator.h
/*
	File:           AVAudioUnitGenerator.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitGenerator.h>

// ==========  AVFoundation.framework/Headers/AVAssetWriter.h
/*
	File:  AVAssetWriter.h

	Framework:  AVFoundation
 
	Copyright 2010-2016 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVMediaSelectionGroup.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMBase.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMSampleBuffer.h>

@class AVAssetWriterInput;
@class AVMetadataItem;

NS_ASSUME_NONNULL_BEGIN

/*!
 @enum AVAssetWriterStatus
 @abstract
	These constants are returned by the AVAssetWriter status property to indicate whether it can successfully write samples to its output file.

 @constant	 AVAssetWriterStatusUnknown
	Indicates that the status of the asset writer is not currently known.
 @constant	 AVAssetWriterStatusWriting
	Indicates that the asset writer is successfully writing samples to its output file.
 @constant	 AVAssetWriterStatusCompleted
	Indicates that the asset writer has successfully written all samples following a call to finishWriting.
 @constant	 AVAssetWriterStatusFailed
	Indicates that the asset writer can no longer write samples to its output file because of an error. The error is described by the value of the asset writer's error property.
 @constant	 AVAssetWriterStatusCancelled
	Indicates that the asset writer can no longer write samples because writing was canceled with the cancelWriting method.
 */
typedef NS_ENUM(NSInteger, AVAssetWriterStatus) {
	AVAssetWriterStatusUnknown = 0,
	AVAssetWriterStatusWriting,
	AVAssetWriterStatusCompleted,
	AVAssetWriterStatusFailed,
	AVAssetWriterStatusCancelled
};

@class AVAssetWriterInternal;

/*!
 @class AVAssetWriter
 @abstract
	 AVAssetWriter provides services for writing media data to a new file,
 
 @discussion
	Instances of AVAssetWriter can write media to new files in formats such as the QuickTime movie file format or the MPEG-4 file format. AVAssetWriter has support for automatic interleaving of media data for multiple concurrent tracks. Source media data can be obtained from instances of AVAssetReader for one or more assets or from other sources outside of AVFoundation.

	Instances of AVAssetWriter can re-encode media samples as they are written. Instances of AVAssetWriter can also optionally write metadata collections to the output file.
 
	A single instance of AVAssetWriter can be used once to write to a single file. Clients that wish to write to files multiple times must use a new instance of AVAssetWriter each time.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetWriter : NSObject
{
@private
	AVAssetWriterInternal	*_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetWriterWithURL:fileType:error:
 @abstract
	Returns an instance of AVAssetWriter configured to write to a file in a specified container format.
 
 @param URL
	The location of the file to be written. The URL must be a file URL.
 @param fileType
	A UTI indicating the format of the file to be written.
 @param outError
	On return, if initialization of the AVAssetWriter fails, points to an NSError describing the nature of the failure.
 @result
	An instance of AVAssetWriter.
 
 @discussion
	Writing will fail if a file already exists at the specified URL.
	
	UTIs for container formats that can be written are declared in AVMediaFormat.h.
 */
+ (nullable instancetype)assetWriterWithURL:(NSURL *)outputURL fileType:(AVFileType)outputFileType error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method initWithURL:fileType:error:
 @abstract
	Creates an instance of AVAssetWriter configured to write to a file in a specified container format.
 
 @param URL
	The location of the file to be written. The URL must be a file URL.
 @param fileType
	A UTI indicating the format of the file to be written.
 @param outError
	On return, if initialization of the AVAssetWriter fails, points to an NSError describing the nature of the failure.
 @result
	An instance of AVAssetWriter.
 
 @discussion
	Writing will fail if a file already exists at the specified URL.
	
	UTIs for container formats that can be written are declared in AVMediaFormat.h.
 */
- (nullable instancetype)initWithURL:(NSURL *)outputURL fileType:(AVFileType)outputFileType error:(NSError * _Nullable * _Nullable)outError NS_DESIGNATED_INITIALIZER;

/*!
 @property outputURL
 @abstract
	The location of the file for which the instance of AVAssetWriter was initialized for writing.
 @discussion
	You may use UTTypeCopyPreferredTagWithClass(outputFileType, kUTTagClassFilenameExtension) to obtain an appropriate path extension for the outputFileType you have specified. For more information about UTTypeCopyPreferredTagWithClass and kUTTagClassFilenameExtension, on iOS see <MobileCoreServices/UTType.h> and on Mac OS X see <LaunchServices/UTType.h>.
 */
@property (nonatomic, copy, readonly) NSURL *outputURL;

/*!
 @property outputFileType
 @abstract
	The UTI of the file format of the file for which the instance of AVAssetWriter was initialized for writing.
 */
@property (nonatomic, copy, readonly) AVFileType outputFileType;

/*!
 @property availableMediaTypes
 @abstract
	The media types for which inputs can be added to the receiver.

 @discussion
	Some media types may not be accepted within the file format with which an AVAssetWriter was initialized.
 */
@property (nonatomic, readonly) NSArray<AVMediaType> *availableMediaTypes;

/*!
 @property status
 @abstract
	The status of writing samples to the receiver's output file.

 @discussion
	The value of this property is an AVAssetWriterStatus that indicates whether writing is in progress, has completed successfully, has been canceled, or has failed. Clients of AVAssetWriterInput objects should check the value of this property after appending samples fails to determine why no more samples could be written. This property is thread safe.
 */
@property (readonly) AVAssetWriterStatus status;

/*!
 @property error
 @abstract
	If the receiver's status is AVAssetWriterStatusFailed, this describes the error that caused the failure.

 @discussion
	The value of this property is an NSError that describes what caused the receiver to no longer be able to write to its output file. If the receiver's status is not AVAssetWriterStatusFailed, the value of this property is nil. This property is thread safe.
 */
@property (readonly, nullable) NSError *error;

/*!
 @property metadata
 @abstract
	A collection of metadata to be written to the receiver's output file.

 @discussion
	The value of this property is an array of AVMetadataItem objects representing the collection of top-level metadata to be written in the output file.
	
	This property cannot be set after writing has started.
 */
@property (nonatomic, copy) NSArray<AVMetadataItem *> *metadata;

/*!
 @property shouldOptimizeForNetworkUse
 @abstract
	Specifies whether the output file should be written in way that makes it more suitable for playback over a network
 
 @discussion
	When the value of this property is YES, the output file will be written in such a way that playback can start after only a small amount of the file is downloaded.
	
	This property cannot be set after writing has started.
 */
@property (nonatomic) BOOL shouldOptimizeForNetworkUse;

/*!
 @property directoryForTemporaryFiles
 @abstract 
	Specifies a directory that is suitable for containing temporary files generated during the process of writing an asset.
 
 @discussion
	AVAssetWriter may need to write temporary files when configured in certain ways, such as when performsMultiPassEncodingIfSupported is set to YES on one or more of its inputs.  This property can be used to control where in the filesystem those temporary files are created.  All temporary files will be deleted when asset writing is completed, is canceled, or fails.
 
	When the value of this property is nil, the asset writer will choose a suitable location when writing temporary files.  The default value is nil.
	
	This property cannot be set after writing has started.  The asset writer will fail if a file cannot be created in this directory (for example, due to insufficient permissions).
 */
@property (nonatomic, copy, nullable) NSURL *directoryForTemporaryFiles NS_AVAILABLE(10_10, 8_0);

/*!
 @property inputs
 @abstract
	The inputs from which the asset writer receives media data.
 @discussion
	The value of this property is an NSArray containing concrete instances of AVAssetWriterInput. Inputs can be added to the receiver using the addInput: method.
 */
@property (nonatomic, readonly) NSArray<AVAssetWriterInput *> *inputs;

/*!
 @method canApplyOutputSettings:forMediaType:
 @abstract
	Tests whether output settings for a specific media type are supported by the receiver's file format.

 @param outputSettings
	The output settings that are to be tested.
 @param mediaType
	The media type for which the output settings are to be tested. Media types are defined in AVMediaFormat.h.
 @result
	A BOOL indicating whether the given output settings can be used for the given media type.
 
 @discussion
	This method determines whether the output settings for the specified media type can be used with the receiver's file format. For example, video compression settings that specify H.264 compression are not compatible with file formats that cannot contain H.264-compressed video.
 
	Attempting to add an input with output settings and a media type for which this method returns NO will cause an exception to be thrown.
*/
- (BOOL)canApplyOutputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings forMediaType:(AVMediaType)mediaType;

/*!
 @method canAddInput:
 @abstract
	Tests whether an input can be added to the receiver.

 @param input
	The AVAssetWriterInput object to be tested.
 @result
	A BOOL indicating whether the input can be added to the receiver.

 @discussion
	An input that accepts media data of a type that is not compatible with the receiver, or with output settings that are not compatible with the receiver, cannot be added.
 */
- (BOOL)canAddInput:(AVAssetWriterInput *)input;

/*!
 @method addInput:
 @abstract
	Adds an input to the receiver.

 @param input
	The AVAssetWriterInput object to be added.

 @discussion
	Inputs are created with a media type and output settings. These both must be compatible with the receiver.
	
	Inputs cannot be added after writing has started.
 */
- (void)addInput:(AVAssetWriterInput *)input;

/*!
 @method startWriting
 @abstract
	Prepares the receiver for accepting input and for writing its output to its output file.

 @result
	A BOOL indicating whether writing successfully started.
 
 @discussion
	This method must be called after all inputs have been added and other configuration properties have been set in order to tell the receiver to prepare for writing. After this method is called, clients can start writing sessions using startSessionAtSourceTime: and can write media samples using the methods provided by each of the receiver's inputs.
 
	If writing cannot be started, this method returns NO. Clients can check the values of the status and error properties for more information on why writing could not be started.
 
	On iOS, if the status of an AVAssetWriter is AVAssetWriterStatusWriting when the client app goes into the background, its status will change to AVAssetWriterStatusFailed and appending to any of its inputs will fail.  You may want to use -[UIApplication beginBackgroundTaskWithExpirationHandler:] to avoid being interrupted in the middle of a writing session and to finish writing the data that has already been appended.  For more information about executing code in the background, see the iOS Application Programming Guide.
 */
- (BOOL)startWriting;

/*!
 @method startSessionAtSourceTime:
 @abstract
	Initiates a sample-writing session for the receiver.
 
 @param startTime
	The starting asset time for the sample-writing session, in the timeline of the source samples.

 @discussion
	Sequences of sample data appended to the asset writer inputs are considered to fall within "sample-writing sessions", initiated with this method. Accordingly, this method must be called after writing has started (using -startWriting) but before any sample data is appended to the receiver's inputs.
	
	Each writing session has a start time which, where allowed by the file format being written, defines the mapping from the timeline of source samples to the timeline of the written file. In the case of the QuickTime movie file format, the first session begins at movie time 0, so a sample appended with timestamp T will be played at movie time (T-startTime).  Samples with timestamps earlier than startTime will still be added to the output file but will be edited out (i.e. not presented during playback). If the earliest appended sample for an input has a timestamp later than than startTime, an empty edit will be inserted to preserve synchronization between tracks of the output asset.
	
	To end the session started by use of this method, use -endSessionAtSourceTime: or -finishWritingWithCompletionHandler:.  It is an error to invoke -startSessionAtSourceTime: twice in a row without invoking -endSessionAtSourceTime: in between.
 
	NOTE: Multiple sample-writing sessions are currently not supported. It is an error to call -startSessionAtSourceTime: a second time after calling -endSessionAtSourceTime:.
 */
- (void)startSessionAtSourceTime:(CMTime)startTime;

/*!
 @method endSessionAtSourceTime:
 @abstract
	Concludes a sample-writing session.

 @param endTime
	The ending asset time for the sample-writing session, in the timeline of the source samples.

 @discussion
	Call this method to complete a session started with -startSessionAtSourceTime:.
 
	The endTime defines the moment on the timeline of source samples at which the session ends. In the case of the QuickTime movie file format, each sample-writing session's startTime...endTime pair corresponds to a period of movie time into which the session's samples are inserted. Samples with timestamps that are later than the session end time will still be added to the written file but will be edited out (i.e. not presented during playback). So if the first session has duration D1 = endTime - startTime, it will be inserted into the written file at time 0 through D1; the second session would be inserted into the written file at time D1 through D1+D2, etc. It is legal to have a session with no samples; this will cause creation of an empty edit of the prescribed duration.
	
	It is not mandatory to call -endSessionAtSourceTime:; if -finishWritingWithCompletionHandler: is called without first invoking -endSessionAtSourceTime:, the session's effective end time will be the latest end timestamp of the session's appended samples (i.e. no samples will be edited out at the end).
 
	It is an error to append samples outside of a sample-writing session.  To append more samples after invoking -endSessionAtSourceTime:, you must first start a new session using -startSessionAtSourceTime:.
	
	NOTE: Multiple sample-writing sessions are currently not supported. It is an error to call -startSessionAtSourceTime: a second time after calling -endSessionAtSourceTime:.
 */
- (void)endSessionAtSourceTime:(CMTime)endTime;

/*!
 @method cancelWriting
 @abstract
	Cancels the creation of the output file.
 
 @discussion
	If the status of the receiver is "failed" or "completed," -cancelWriting is a no-op.  Otherwise, this method will block until writing is canceled.
 
	If an output file was created by the receiver during the writing process, -cancelWriting will delete the file.
	
	This method should not be called concurrently with -[AVAssetWriterInput appendSampleBuffer:] or -[AVAssetWriterInputPixelBufferAdaptor appendPixelBuffer:withPresentationTime:].
*/
- (void)cancelWriting;

/*!
 @method finishWriting
 @abstract
	Completes the writing of the output file.
 
 @result
	A BOOL indicating whether writing successfully finished.
 
 @discussion
	This method is deprecated.  Use finishWritingWithCompletionHandler: instead.

	This method will block until writing is finished. When this method returns successfully, the file being written by the receiver is complete and ready to use.

	Because this method is blocking and can take a long time to execute (especially with shouldOptimizeForNetworkUse set to YES), it should not be called from the main thread.  Doing so can cause the finishWriting operation to fail.

	If writing cannot be finished, this method returns NO. Clients can check the values of the status and error properties for more information on why writing could not be finished.
	
	This method should not be called concurrently with -[AVAssetWriterInput appendSampleBuffer:] or -[AVAssetWriterInputPixelBufferAdaptor appendPixelBuffer:withPresentationTime:].
*/
- (BOOL)finishWriting NS_DEPRECATED(10_7, 10_9, 4_1, 6_0);

/*!
 @method finishWritingWithCompletionHandler:
 @abstract
	Marks all unfinished inputs as finished and completes the writing of the output file.

 @discussion
	This method returns immediately and causes its work to be performed asynchronously.
	
	When the writing of the output file is finished, or if a failure or a cancellation occurs in the meantime, the specified handler will be invoked to indicate completion of the operation. To determine whether the operation succeeded, your handler can check the value of AVAssetWriter.status. If the status is AVAssetWriterStatusFailed, AVAsset.error will contain an instance of NSError that describes the failure.
	
	To guarantee that all sample buffers are successfully written, ensure all calls to -[AVAssetWriterInput appendSampleBuffer:] or -[AVAssetWriterInputPixelBufferAdaptor appendPixelBuffer:withPresentationTime:] have returned before invoking this method.
*/
- (void)finishWritingWithCompletionHandler:(void (^)(void))handler NS_AVAILABLE(10_9, 6_0);

@end


@interface AVAssetWriter (AVAssetWriterFileTypeSpecificProperties)

/*!
 @property movieFragmentInterval
 @abstract
	For file types that support movie fragments, specifies the frequency at which movie fragments should be written.
 
 @discussion
	When movie fragments are used, a partially written asset whose writing is unexpectedly interrupted can be successfully opened and played up to multiples of the specified time interval. The default value of this property is kCMTimeInvalid, which indicates that movie fragments should not be used.

	This property cannot be set after writing has started.
 */
@property (nonatomic) CMTime movieFragmentInterval;

/*!
 @property overallDurationHint
 @abstract
	For file types that support movie fragments, provides a hint of the final duration of the file to be written
 
 @discussion
	The value of this property must be a nonnegative, numeric CMTime.  Alternatively, if the value of this property is an invalid CMTime (e.g. kCMTimeInvalid), no overall duration hint will be written to the file.  The default value is kCMTimeInvalid.
 
	This property is currently ignored if movie fragments are not being written.  Use the movieFragmentInterval property to enable movie fragments.
 
	This property cannot be set after writing has started.
 */
@property (nonatomic) CMTime overallDurationHint;

/*!
 @property movieTimeScale
 @abstract
	For file types that contain a 'moov' atom, such as QuickTime Movie files, specifies the asset-level time scale to be used. 

 @discussion
	The default value is 0, which indicates that the receiver should choose a convenient value, if applicable.
 
	This property cannot be set after writing has started.
 */
@property (nonatomic) CMTimeScale movieTimeScale NS_AVAILABLE(10_7, 4_3);

@end


@class AVAssetWriterInputGroup;

@interface AVAssetWriter (AVAssetWriterInputGroups)

/*!
 @method canAddInputGroup:
 @abstract
	Tests whether an input group can be added to the receiver.

 @param inputGroup
	The AVAssetWriterInputGroup object to be tested.
 @result
	A BOOL indicating whether the input group can be added to the receiver.

 @discussion
	If outputFileType specifies a container format that does not support mutually exclusive relationships among tracks, or if the specified instance of AVAssetWriterInputGroup contains inputs with media types that cannot be related, the group cannot be added to the AVAssetWriter.
 */
- (BOOL)canAddInputGroup:(AVAssetWriterInputGroup *)inputGroup NS_AVAILABLE(10_9, 7_0);

/*
 @method addInputGroup:
 @abstract
	Adds an instance of AVAssetWriterInputGroup to the AVAssetWriter.  The AVAssetWriter will mark the tracks associated with grouped inputs as mutually exclusive to each other for playback or other processing, if the output container format supports mutually exlusive relationships among tracks.

 @param inputGroup
	The collection of AVAssetWriterInputs to be grouped together.
 
 @discussion
	When an input group is added to an AVAssetWriter, the value of marksOutputTrackAsEnabled will automatically be set to YES for the default input and set to NO for all of the other inputs in the group.

	Input groups cannot be added after writing has started.
 */
- (void)addInputGroup:(AVAssetWriterInputGroup *)inputGroup NS_AVAILABLE(10_9, 7_0);

/*!
 @property inputGroups
 @abstract
	The instances of AVAssetWriterInputGroup that have been added to the AVAssetWriter.
 
 @discussion
	The value of this property is an NSArray containing concrete instances of AVAssetWriterInputGroup.  Input groups can be added to the receiver using the addInputGroup: method.
 */
@property (nonatomic, readonly) NSArray<AVAssetWriterInputGroup *> *inputGroups NS_AVAILABLE(10_9, 7_0);

@end


@class AVAssetWriterInputGroupInternal;

/*
 @class AVAssetWriterInputGroup
 @abstract Associates tracks corresponding to inputs with each other in a mutually exclusive relationship.

 @discussion
	This class is used to associate tracks corresponding to multiple AVAssetWriterInputs as mutually exclusive to each other for playback or other processing.  For example, if you are creating an asset with multiple audio tracks using different spoken languages, only one of which should be played at a time, group the inputs corresponding to those tracks into a single instance of AVAssetWriterInputGroup and add the group to the AVAssetWriter via -[AVAssetWriter addInputGroup:].  If the output format supports mutually exlusive relationships among tracks, the AVAssetWriter will mark the tracks as mutually exclusive to each other.
 
	Note that because AVAssetWriterInputGroup is a subclass of AVMediaSelectionGroup, clients can examine the media selection options that will be available on the output asset before the asset is written.  Best results for examining the options of the AVAssetWriterInputGroup will be obtained after associating the AVAssetWriterInputs of the AVAsset as appropriate via -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:] and by initializing each AVAssetWriterInput with a source format hint, where appropriate.
 */

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVAssetWriterInputGroup : AVMediaSelectionGroup
{
@private
    AVAssetWriterInputGroupInternal	*_internal;
}
AV_INIT_UNAVAILABLE

/*
 @method assetWriterInputGroupWithInputs:defaultInput:
 @abstract
	Creates an instance of AVAssetWriterInputGroup, for use with -[AVAssetWriter addInputGroup:].

 @param inputs
	The collection of AVAssetWriterInputs to be grouped together.
 @param defaultInput
	The instance of AVAssetWriterInput in the group to designate as the default.  When the input group is added to an AVAssetWriter via -addInputGroup:, the value of marksOutputTrackAsEnabled will automatically be set to YES for the default input and set to NO for all of the other inputs in the group.
 @result
	An instance of AVAssetWriterInputGroup, for use with -[AVAssetWriter addInputGroup:].
 */
+ (instancetype)assetWriterInputGroupWithInputs:(NSArray<AVAssetWriterInput *> *)inputs defaultInput:(nullable AVAssetWriterInput *)defaultInput;

/*
 @method initWithInputs:defaultInput:
 @abstract
	Creates an instance of AVAssetWriterInputGroup, for use with -[AVAssetWriter addInputGroup:].

 @param inputs
	The collection of AVAssetWriterInputs to be grouped together.
 @param defaultInput
	The instance of AVAssetWriterInput in the group to designate as the default.  When the input group is added to an AVAssetWriter via -addInputGroup:, the value of marksOutputTrackAsEnabled will automatically be set to YES for the default input and set to NO for all of the other inputs in the group.
 @result
	An instance of AVAssetWriterInputGroup, for use with -[AVAssetWriter addInputGroup:].
 */
- (instancetype)initWithInputs:(NSArray<AVAssetWriterInput *> *)inputs defaultInput:(nullable AVAssetWriterInput *)defaultInput NS_DESIGNATED_INITIALIZER;

/*!
 @property inputs
 @abstract
	The inputs grouped together by the receiver.
 
 @discussion
	The value of this property is an NSArray containing concrete instances of AVAssetWriterInput.
 */
@property (nonatomic, readonly) NSArray<AVAssetWriterInput *> *inputs;

/*!
 @property defaultInput
 @abstract
	The input designated at the defaultInput of the receiver.
 
 @discussion
	The value of this property is a concrete instance of AVAssetWriterInput.
 */
@property (nonatomic, readonly, nullable) AVAssetWriterInput *defaultInput;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioTime.h
/*
	File:           AVAudioTime.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioTime.h>

// ==========  AVFoundation.framework/Headers/AVCaptureMetadataOutput.h
/*
    File:  AVCaptureMetadataOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVMetadataObject.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureMetadataOutput

@class AVCaptureMetadataOutputInternal;
@protocol AVCaptureMetadataOutputObjectsDelegate;

/*!
 @class AVCaptureMetadataOutput
 @abstract
    AVCaptureMetadataOutput is a concrete subclass of AVCaptureOutput that can be used to process metadata objects from an attached connection.
 
 @discussion
    Instances of AVCaptureMetadataOutput emit arrays of AVMetadataObject instances (see AVMetadataObject.h), such as detected faces. Applications can access the metadata objects with the captureOutput:didOutputMetadataObjects:fromConnection: delegate method.
 */
API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureMetadataOutput : AVCaptureOutput
{
@private
    AVCaptureMetadataOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method setMetadataObjectsDelegate:queue:
 @abstract
    Sets the receiver's delegate that will accept metadata objects and dispatch queue on which the delegate will be called.
 
 @param objectsDelegate
    An object conforming to the AVCaptureMetadataOutputObjectsDelegate protocol that will receive metadata objects after they are captured.
 @param objectsCallbackQueue
    A dispatch queue on which all delegate methods will be called.
 
 @discussion
    When new metadata objects are captured in the receiver's connection, they will be vended to the delegate using the captureOutput:didOutputMetadataObjects:fromConnection: delegate method. All delegate methods will be called on the specified dispatch queue.
 
    Clients that need to minimize the chances of metadata being dropped should specify a queue on which a sufficiently small amount of processing is performed along with receiving metadata objects.
 
    A serial dispatch queue must be used to guarantee that metadata objects will be delivered in order. The objectsCallbackQueue parameter may not be NULL, except when setting the objectsDelegate to nil.
 */
- (void)setMetadataObjectsDelegate:(nullable id<AVCaptureMetadataOutputObjectsDelegate>)objectsDelegate queue:(nullable dispatch_queue_t)objectsCallbackQueue;

/*!
 @property metadataObjectsDelegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureMetadataOutputObjectsDelegate protocol that will receive metadata objects after they are captured. The delegate is set using the setMetadataObjectsDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) id<AVCaptureMetadataOutputObjectsDelegate> metadataObjectsDelegate;

/*!
 @property metadataObjectsCallbackQueue
 @abstract
    The dispatch queue on which all metadata object delegate methods will be called.
 
 @discussion
    The value of this property is a dispatch_queue_t. The queue is set using the setMetadataObjectsDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) dispatch_queue_t metadataObjectsCallbackQueue;

/*!
 @property availableMetadataObjectTypes
 @abstract
    Indicates the receiver's supported metadata object types.
 
 @discussion
    The value of this property is an NSArray of NSStrings corresponding to AVMetadataObjectType strings defined in AVMetadataObject.h -- one for each metadata object type supported by the receiver. Available metadata object types are dependent on the capabilities of the AVCaptureInputPort to which this receiver's AVCaptureConnection is connected. Clients may specify the types of objects they would like to process by calling setMetadataObjectTypes:. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<AVMetadataObjectType> *availableMetadataObjectTypes;

/*!
 @property metadataObjectTypes
 @abstract
    Specifies the types of metadata objects that the receiver should present to the client.
 
 @discussion
    AVCaptureMetadataOutput may detect and emit multiple metadata object types. For apps linked before iOS 7.0, the receiver defaults to capturing face metadata objects if supported (see -availableMetadataObjectTypes). For apps linked on or after iOS 7.0, the receiver captures no metadata objects by default. -setMetadataObjectTypes: throws an NSInvalidArgumentException if any elements in the array are not present in the -availableMetadataObjectTypes array.
 */
@property(nonatomic, copy, null_resettable) NSArray<AVMetadataObjectType> *metadataObjectTypes;

/*!
 @property rectOfInterest
 @abstract
    Specifies a rectangle of interest for limiting the search area for visual metadata.
 
 @discussion
    The value of this property is a CGRect that determines the receiver's rectangle of interest for each frame of video. The rectangle's origin is top left and is relative to the coordinate space of the device providing the metadata. Specifying a rectOfInterest may improve detection performance for certain types of metadata. The default value of this property is the value CGRectMake(0, 0, 1, 1). Metadata objects whose bounds do not intersect with the rectOfInterest will not be returned.
 */
@property(nonatomic) CGRect rectOfInterest API_AVAILABLE(ios(7.0));

@end


/*!
 @protocol AVCaptureMetadataOutputObjectsDelegate
 @abstract
    Defines an interface for delegates of AVCaptureMetadataOutput to receive emitted objects.
 */
API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureMetadataOutputObjectsDelegate <NSObject>

@optional

/*!
 @method captureOutput:didOutputMetadataObjects:fromConnection:
 @abstract
    Called whenever an AVCaptureMetadataOutput instance emits new objects through a connection.
 
 @param output
    The AVCaptureMetadataOutput instance that emitted the objects.
 @param metadataObjects
    An array of AVMetadataObject subclasses (see AVMetadataObject.h).
 @param connection
    The AVCaptureConnection through which the objects were emitted.
 
 @discussion
    Delegates receive this message whenever the output captures and emits new objects, as specified by its metadataObjectTypes property. Delegates can use the provided objects in conjunction with other APIs for further processing. This method will be called on the dispatch queue specified by the output's metadataObjectsCallbackQueue property. This method may be called frequently, so it must be efficient to prevent capture performance problems, including dropped metadata objects.
 
    Clients that need to reference metadata objects outside of the scope of this method must retain them and then release them when they are finished with them.
 */
- (void)captureOutput:(AVCaptureOutput *)output didOutputMetadataObjects:(NSArray<__kindof AVMetadataObject *> *)metadataObjects fromConnection:(AVCaptureConnection *)connection;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitMIDIInstrument.h
/*
	File:           AVAudioUnitMIDIInstrument.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitMIDIInstrument.h>

// ==========  AVFoundation.framework/Headers/AVAudioUnitEffect.h
/*
	File:           AVAudioUnitEffect.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitEffect.h>

// ==========  AVFoundation.framework/Headers/AVVideoCompositing.h
/*
	File:  AVVideoCompositing.h

	Framework:  AVFoundation
 
	Copyright 2013-2016 Apple Inc. All rights reserved.
*/

#import <Foundation/Foundation.h>
#import <AVFoundation/AVBase.h>
#import <CoreVideo/CVPixelBufferPool.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

typedef struct {
	NSInteger	horizontalSpacing;
	NSInteger	verticalSpacing;
} AVPixelAspectRatio;

typedef struct {
	CGFloat		left;
	CGFloat		top;
	CGFloat		right;
	CGFloat		bottom;
} AVEdgeWidths;

/*!
	@class		AVVideoCompositionRenderContext
 
	@abstract	The AVVideoCompositionRenderContext class defines the context within which custom compositors render new output pixels buffers.
 
	@discussion
		An instance of AVVideoCompositionRenderContext provides size and scaling information and offers a service for efficiently providing pixel buffers from a managed pool of buffers.
*/

@class AVVideoComposition;
@class AVAsynchronousVideoCompositionRequest;
@protocol AVVideoCompositionInstruction;

@class AVVideoCompositionRenderContextInternal;

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVVideoCompositionRenderContext : NSObject {
@private
	AVVideoCompositionRenderContextInternal	*_internal;
}

/* indicates the width and height for rendering frames. */
@property (nonatomic, readonly) CGSize size;

/* Transform to apply to the source image to incorporate renderScale, pixelAspectRatio, edgeWidths.
   The coordinate system origin is the top left corner of the buffer. */
@property (nonatomic, readonly) CGAffineTransform renderTransform; // incorporates renderScale, pixelAspectRatio, edgeWidths

/* indicates a scaling ratio that should be applied when rendering frames. */
@property (nonatomic, readonly) float renderScale;

/* indicates the pixel aspect ratio for rendered frames. */
@property (nonatomic, readonly) AVPixelAspectRatio pixelAspectRatio;

/* indicates the thickness of the edge processing region on the left, top, right and bottom edges, in pixels. */
@property (nonatomic, readonly) AVEdgeWidths edgeWidths;

/* hints the custom compositor that it may use higher quality, potentially slower algorithms.
   Generally true for non real time use cases. */
@property (nonatomic, readonly) BOOL highQualityRendering;

/* The AVVideoComposition being rendered. */
@property (nonatomic, readonly) AVVideoComposition *videoComposition;

/*!
	@method			newPixelBuffer
	@abstract		Vends a CVPixelBuffer to use for rendering
	@discussion
					The buffer will have its kCVImageBufferCleanApertureKey and kCVImageBufferPixelAspectRatioKey attachments set to match the current composition processor properties.
					 
*/
- (nullable CVPixelBufferRef)newPixelBuffer CF_RETURNS_RETAINED; // caller must CFRelease

@end


/*!
	@protocol		AVVideoCompositing
	@abstract		Defines properties and methods for custom video compositors
	@discussion
		For each AVFoundation object of class AVPlayerItem, AVAssetExportSession, AVAssetImageGenerator, or AVAssetReaderVideoCompositionOutput that has a non-nil value for its videoComposition property, when the value of the customVideoCompositorClass property of the AVVideoComposition is not Nil, AVFoundation creates and uses an instance of that custom video compositor class to process the instructions contained in the AVVideoComposition. The custom video compositor instance will be created when you invoke -setVideoComposition: with an instance of AVVideoComposition that's associated with a different custom video compositor class than the object was previously using.

		When creating instances of custom video compositors, AVFoundation initializes them by calling -init and then makes them available to you for further set-up or communication, if any is needed, as the value of the customVideoCompositor property of the object on which -setVideoComposition: was invoked.

		Custom video compositor instances will then be retained by the AVFoundation object for as long as the value of its videoComposition property indicates that an instance of the same custom video compositor class should be used, even if the value is changed from one instance of AVVideoComposition to another instance that's associated with the same custom video compositor class.
*/
NS_CLASS_AVAILABLE(10_9, 7_0)
@protocol AVVideoCompositing<NSObject>

@required

/* Indicates the kinds of source frame pixel buffer attributes a video compositor can accept as input.
   The property is required to provide kCVPixelBufferPixelFormatTypeKey along with the attributes
   for which the compositor needs specific values to work properly. If the attribute kCVPixelBufferPixelFormatTypeKey
   is missing an exception will be raised. If the custom compositor is meant to be used with an AVVideoCompositionCoreAnimationTool
   created using the videoCompositionCoreAnimationToolWithAdditionalLayer constructor, kCVPixelFormatType_32BGRA 
   should be indicated as one of the supported pixel format types.
   Missing attributes will be set by the composition engine to values allowing the best performance.
   This property is queried once before any composition request is sent to the compositor. Changing
   source buffer attributes afterwards is not supported.
*/
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *sourcePixelBufferAttributes;

/* Indicates the pixel buffer attributes required by the video compositor for new buffers that it creates
 for processing. The property is required to provide kCVPixelBufferPixelFormatTypeKey along with attributes for which the compositor needs specific values to work properly. Omitted attributes will be supplied by the composition engine to allow for the best performance. If the attribute kCVPixelBufferPixelFormatTypeKey is missing an exception will be raised.
 The getter for requiredPixelBufferAttributesForRenderContext is typically invoked prior to the creation of
 a new render context; the combination of the attributes in the returned value and the additional attributes
 supplied by the composition engine will be used in the creation of subsequent render context's pixelBuffers.
 This property is queried once before any composition request is sent to the compositor. Changing
 required buffer attributes afterwards is not supported.
 */
@property (nonatomic, readonly) NSDictionary<NSString *, id> *requiredPixelBufferAttributesForRenderContext;

/*!
    @method			renderContextChanged:
	@abstract       Called to notify the custom compositor that a composition will switch to a different render context
	@param			newRenderContext
					The render context that will be handling the video composition from this point
    @discussion
					Instances of classes implementing the AVVideoComposting protocol can implement this method to be notified when
					the AVVideoCompositionRenderContext instance handing a video composition changes. AVVideoCompositionRenderContext instances
					being immutable, such a change will occur every time there is a change in the video composition parameters.
*/
- (void)renderContextChanged:(AVVideoCompositionRenderContext *)newRenderContext;

/*!
	@method			startVideoCompositionRequest:
	@abstract		Directs a custom video compositor object to create a new pixel buffer composed asynchronously from a collection of sources.
	@param			asyncVideoCompositionRequest
    				An instance of AVAsynchronousVideoCompositionRequest that provides context for the requested composition.
	@discussion
		The custom compositor is expected to invoke, either subsequently or immediately, either:
		-[AVAsynchronousVideoCompositionRequest finishWithComposedVideoFrame:] or
		-[AVAsynchronousVideoCompositionRequest finishWithError:]. If you intend to finish rendering the frame after your
		handling of this message returns, you must retain the instance of AVAsynchronousVideoCompositionRequest until after composition is finished.
		Note that if the custom compositor's implementation of -startVideoCompositionRequest: returns without finishing the composition immediately,
		it may be invoked again with another composition request before the prior request is finished; therefore in such cases the custom compositor should
		be prepared to manage multiple composition requests.

		If the rendered frame is exactly the same as one of the source frames, with no letterboxing, pillboxing or cropping needed,
		then the appropriate source pixel buffer may be returned (after CFRetain has been called on it).
*/
- (void)startVideoCompositionRequest:(AVAsynchronousVideoCompositionRequest *)asyncVideoCompositionRequest;

@optional

/*!
	@method			cancelAllPendingVideoCompositionRequests	
	@abstract		Directs a custom video compositor object to cancel or finish all pending video composition requests
	@discussion
		When receiving this message, a custom video compositor must block until it has either cancelled all pending frame requests,
		and called the finishCancelledRequest callback for each of them, or, if cancellation is not possible, finished processing of all the frames
		and called the finishWithComposedVideoFrame: callback for each of them.
*/
- (void)cancelAllPendingVideoCompositionRequests;

/*!
 @property supportsWideColorSourceFrames
 @abstract
	Indicates that clients can handle frames that contains wide color properties.
 
 @discussion 
	Controls whether the client will receive frames that contain wide color information. Care should be taken to avoid clamping.
 */
@property (nonatomic, readonly) BOOL supportsWideColorSourceFrames NS_AVAILABLE(10_12, 10_0);

@end

/*!
	@class		AVAsynchronousVideoCompositionRequest
 
	@abstract	An AVAsynchronousVideoCompositionRequest instance contains the information necessary for a video compositor to render an output pixel buffer. The video compositor must implement the AVVideoCompositing protocol.
*/

@class AVAsynchronousVideoCompositionRequestInternal;

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVAsynchronousVideoCompositionRequest : NSObject <NSCopying> {
@private
	AVAsynchronousVideoCompositionRequestInternal *_internal;
}

/* The AVVideoCompositionRenderContext making the request */
@property (nonatomic, readonly) AVVideoCompositionRenderContext *renderContext;

/* The time for which the frame should be composed */
@property (nonatomic, readonly) CMTime compositionTime;

/* Track ID of all the source buffers that are available to compose the frame. */
@property (nonatomic, readonly) NSArray<NSNumber *> *sourceTrackIDs;

/* The AVVideoCompositionInstruction to use to compose the frame. */
@property (nonatomic, readonly) id<AVVideoCompositionInstruction> videoCompositionInstruction;

/*!
    @method			sourceFrameByTrackID:
	@abstract       Returns the source CVPixelBufferRef for the given track ID
	@param			trackID
					The track ID for the requested source frame
*/
- (nullable CVPixelBufferRef)sourceFrameByTrackID:(CMPersistentTrackID)trackID CF_RETURNS_NOT_RETAINED;

/* callback the custom compositor should call when composition succeeded */
- (void)finishWithComposedVideoFrame:(CVPixelBufferRef)composedVideoFrame;

/* callback the custom compositor should call when composition failed. The error parameter should describe the actual error. */
- (void)finishWithError:(NSError *)error;

/* callback the custom compositor should call for a request that has been cancelled. */
- (void)finishCancelledRequest;

@end

/*!
	@class		AVAsynchronousCIImageFilteringRequest
 
	@abstract	An AVAsynchronousCIImageFilteringRequest instance contains the information necessary for a filter to render an output CIImage.
*/

@class AVAsynchronousCIImageFilteringRequestInternal;
@class CIImage;
@class CIContext;

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVAsynchronousCIImageFilteringRequest : NSObject <NSCopying> {
@private
	AVAsynchronousCIImageFilteringRequestInternal *_internal;
}

/* Width and height for rendering frames. */
@property (nonatomic, readonly) CGSize renderSize;

/* The time for which the frame should be filtered */
@property (nonatomic, readonly) CMTime compositionTime;

/* CIImage for the first enabled source video track. The pixel format will be kCIFormatBGRA8 (kCVPixelFormatType_32BGRA). Unlike AVAsynchronousVideoCompositionRequest, renderContext.renderTransform is already applied to the source image. */
@property (nonatomic, readonly) CIImage *sourceImage;

/*
Callback the filter should call when filtering succeeded. The pixel format of the filteredImage must be kCIFormatBGRA8 (kCVPixelFormatType_32BGRA). If context is nil then a default context will be used, GPU-accelerated if possible.

It is safe to pass in the sourceImage in which case the filter will appear to have no effect, essentially functioning as a pass-through.
*/
- (void)finishWithImage:(CIImage *)filteredImage context:(nullable CIContext *)context;

/* Callback the filter should call when filtering failed. The error parameter should describe the actual error. */
- (void)finishWithError:(NSError *)error;

@end


/*!
	@protocol	AVVideoCompositionInstruction
 
	@abstract	The AVVideoCompositionInstruction protocol is implemented by objects to represent operations to be performed by a compositor.
*/
NS_CLASS_AVAILABLE(10_9, 7_0)
@protocol AVVideoCompositionInstruction<NSObject>

@required

/* Indicates the timeRange during which the instruction is effective. Note requirements for the timeRanges of instructions described in connection with AVVideoComposition's instructions key above. */
@property (nonatomic, readonly) CMTimeRange timeRange;

/* If NO, indicates that post-processing should be skipped for the duration of this instruction.
   See +[AVVideoCompositionCoreAnimationTool videoCompositionToolWithPostProcessingAsVideoLayer:inLayer:].*/
@property (nonatomic, readonly) BOOL enablePostProcessing;

/* If YES, rendering a frame from the same source buffers and the same composition instruction at 2 different
   compositionTime may yield different output frames. If NO, 2 such compositions would yield the
   same frame. The media pipeline may me able to avoid some duplicate processing when containsTweening is NO */
@property (nonatomic, readonly) BOOL containsTweening;

/* List of video track IDs required to compose frames for this instruction. If the value of this property is nil, all source tracks will be considered required for composition */
@property (nonatomic, readonly, nullable) NSArray<NSValue *> *requiredSourceTrackIDs;

/* If for the duration of the instruction, the video composition result is one of the source frames, this property should
   return the corresponding track ID. The compositor won't be run for the duration of the instruction and the proper source
   frame will be used instead. The dimensions, clean aperture and pixel aspect ratio of the source buffer will be
   matched to the required values automatically */
@property (nonatomic, readonly) CMPersistentTrackID passthroughTrackID; // kCMPersistentTrackID_Invalid if not a passthrough instruction

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVTimedMetadataGroup.h
/*
    File:  AVTimedMetadataGroup.h

	Framework:  AVFoundation
 
	Copyright 2010-2016 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTimeRange.h>
#import <CoreMedia/CMFormatDescription.h>
#import <CoreMedia/CMSampleBuffer.h>

@class AVTimedMetadataGroupInternal;
@class AVDateRangeMetadataGroupInternal;
@class AVMetadataItem;

NS_ASSUME_NONNULL_BEGIN

/*!
	@class		AVMetadataGroup
 
	@abstract	AVMetadataGroup is the common superclass for AVTimedMetadataGroup and AVDateRangeMetadataGroup; each represents a collection of metadata items associated with a segment of a timeline. AVTimedMetadataGroup is typically used with content that defines an independent timeline, while AVDateRangeMetadataGroup is typically used with content that's associated with a specific range of dates.
*/

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVMetadataGroup : NSObject

@property (nonatomic, readonly, copy) NSArray<AVMetadataItem *> *items;

@end

@interface AVMetadataGroup (AVMetadataGroupIdentification)

/* indicates the classifyingLabel of the group; nil if no classifyingLabel is indicated */
@property (nonatomic, readonly, nullable) NSString *classifyingLabel NS_AVAILABLE(10_11_3, 9_3);

/* indicates the unique identifier of the group; nil if no unique identifier is indicated */
@property (nonatomic, readonly, nullable) NSString *uniqueID NS_AVAILABLE(10_11_3, 9_3);

@end

/*!
	@class		AVTimedMetadataGroup
 
	@abstract	AVTimedMetadataGroup is used to represent a collection of metadata items that are valid for use during a specific range of time. For example, AVTimedMetadataGroups are used to represent chapters, optionally containing metadata items for chapter titles and chapter images.
*/

NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVTimedMetadataGroup : AVMetadataGroup <NSCopying, NSMutableCopying>
{
@private
	AVTimedMetadataGroupInternal *_priv;
}

/*!
	@method		initWithItems:timeRange:
	@abstract	Initializes an instance of AVTimedMetadataGroup with a collection of metadata items.
	@param		items
				An NSArray of AVMetadataItems.
	@param		timeRange
				The timeRange of the collection of AVMetadataItems.
	@result		An instance of AVTimedMetadataGroup.
*/
- (instancetype)initWithItems:(NSArray<AVMetadataItem *> *)items timeRange:(CMTimeRange)timeRange;

/*!
	@method		initWithSampleBuffer:
	@abstract	Initializes an instance of AVTimedMetadataGroup with a sample buffer.
	@param		sampleBuffer
				A CMSampleBuffer with media type kCMMediaType_Metadata.
	@result		An instance of AVTimedMetadataGroup.
*/
- (nullable instancetype)initWithSampleBuffer:(CMSampleBufferRef)sampleBuffer NS_AVAILABLE(10_10, 8_0);

/* indicates the time range of the timed metadata */
@property (nonatomic, readonly) CMTimeRange timeRange;

/* an array of AVMetadataItems */
@property (nonatomic, readonly, copy) NSArray<AVMetadataItem *> *items;

@end

@interface AVTimedMetadataGroup (AVTimedMetadataGroupSerializationSupport)

/*!
	@method		copyFormatDescription
	@abstract	Creates a format description based on the receiver's items.
	@result		An instance of CMMetadataFormatDescription sufficient to describe the contents of all the items referenced by the receiver.
	@discussion
		The returned format description is suitable for use as the format hint parameter when creating an instance of AVAssetWriterInput.
 
		Each item referenced by the receiver must carry a non-nil value for its dataType property.  An exception will be thrown if any item does not have a data type.
*/
- (nullable CMMetadataFormatDescriptionRef)copyFormatDescription NS_AVAILABLE(10_10, 8_0) CF_RETURNS_RETAINED;

@end

/*!
	@class		AVMutableTimedMetadataGroup
 
	@abstract	AVMutableTimedMetadataGroup is used to represent a mutable collection of metadata items that are valid for use during a specific range of time.
*/

NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVMutableTimedMetadataGroup : AVTimedMetadataGroup
{
@private
	AVTimedMetadataGroupInternal	*_mutablePriv;
}

/* indicates the time range of the timed metadata */
@property (nonatomic, readwrite) CMTimeRange timeRange;

/* an array of AVMetadataItems */
@property (nonatomic, readwrite, copy) NSArray<AVMetadataItem *> *items;

@end

/*!
	@class		AVDateRangeMetadataGroup
 
	@abstract	AVDateRangeMetadataGroup is used to represent a collection of metadata items that are valid for use within a specific range of dates.
*/

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVDateRangeMetadataGroup : AVMetadataGroup <NSCopying, NSMutableCopying>
{
@private
	AVDateRangeMetadataGroupInternal *_priv;
}

/*!
	@method		initWithItems:startDate:endDate:
	@abstract	Initializes an instance of AVDateRangeMetadataGroup with a collection of metadata items.
	@param		items
				An NSArray of AVMetadataItems.
	@param		startDate
				The start date of the collection of AVMetadataItems.
	@param		endDate
				The end date of the collection of AVMetadataItems. If the receiver is intended to represent information about an instantaneous event, the value of endDate should be equal to the value of startDate. A value of nil for endDate indicates that the endDate is indefinite.
	@result		An instance of AVDateRangeMetadataGroup.
*/
- (instancetype)initWithItems:(NSArray<AVMetadataItem *> *)items startDate:(NSDate *)startDate endDate:(nullable NSDate *)endDate;

/* indicates the start date of the metadata */
@property (nonatomic, readonly, copy) NSDate *startDate;

/* indicates the end date of the metadata */
@property (nonatomic, readonly, copy, nullable) NSDate *endDate;

/* an array of AVMetadataItems */
@property (nonatomic, readonly, copy) NSArray<AVMetadataItem *> *items;

@end

/*!
	@class		AVMutableDateRangeMetadataGroup
 
	@abstract	AVMutableDateRangeMetadataGroup is used to represent a mutable collection of metadata items that are valid for use within a specific range of dates.
*/

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVMutableDateRangeMetadataGroup : AVDateRangeMetadataGroup
{
@private
	AVDateRangeMetadataGroupInternal *_mutablePriv;
}

/* indicates the start date of the metadata */
@property (nonatomic, readwrite, copy) NSDate *startDate;

/* indicates the end date of the metadata */
@property (nonatomic, readwrite, copy, nullable) NSDate *endDate;

/* an array of AVMetadataItems */
@property (nonatomic, readwrite, copy) NSArray<AVMetadataItem *> *items;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioSettings.h
/*
	File:           AVAudioSettings.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioSettings.h>

// ==========  AVFoundation.framework/Headers/AVAudioUnitComponent.h
/*
	File:           AVAudioUnitComponent.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitComponent.h>

// ==========  AVFoundation.framework/Headers/AVAudioSequencer.h
/*
	File:           AVAudioSequencer.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioSequencer.h>

// ==========  AVFoundation.framework/Headers/AVPortraitEffectsMatte.h
/*
    File:  AVPortraitMatte.h
 
    Framework:  AVFoundation
 
    Copyright 2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreVideo/CVPixelBufferPool.h>
#import <ImageIO/CGImageProperties.h>

NS_ASSUME_NONNULL_BEGIN

@class AVPortraitEffectsMatteInternal;

/*!
 @class AVPortraitEffectsMatte
 @abstract
    An object wrapping a matting image used for high quality rendering of portrait style effects onto an image (i.e. shallow depth of field, stage lighting, etc).
 
 @discussion
    The pixel data in the matting image is represented in CVPixelBuffers as kCVPixelFormatType_OneComponent8 ('L008'). It's stored in image files as an auxiliary image, accessible using CGImageSourceCopyAuxiliaryDataInfoAtIndex with the data type kCGImageAuxiliaryDataTypePortraitEffectsMatte (see <ImageIO/CGImageProperties.h>).
 */
API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0))
@interface AVPortraitEffectsMatte : NSObject
{
@private
    AVPortraitEffectsMatteInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @method portraitEffectsMatteFromDictionaryRepresentation:error:
 @abstract
    Returns an AVPortraitEffectsMatte instance from auxiliary image information in an image file.
 
 @param imageSourceAuxDataInfoDictionary
    A dictionary of primitive portrait effects matte related information obtained from CGImageSourceCopyAuxiliaryDataInfoAtIndex.
 @param outError
    On return, if the portrait effects matte cannot be created, points to an NSError describing the problem.
 @result
    An AVPortraitEffectsMatte instance, or nil if the auxiliary data info dictionary was malformed.
 
 @discussion
    When using ImageIO framework's CGImageSource API to read from a HEIF or JPEG file containing a portrait effects matte, AVPortraitEffectsMatte can be instantiated using the result of CGImageSourceCopyAuxiliaryDataInfoAtIndex, which returns a CFDictionary of primitive map information.
 */
+ (nullable instancetype)portraitEffectsMatteFromDictionaryRepresentation:(NSDictionary *)imageSourceAuxDataInfoDictionary error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method portraitEffectsMatteByApplyingExifOrientation:
 @abstract
    Returns a derivative AVPortraitEffectsMatte instance in which the specified Exif orientation has been applied.
 
 @param exifOrientation
    One of the 8 standard Exif orientation tags expressing how the portrait effects matte should be rotated / mirrored.
 @result
    An AVPortraitEffectsMatte instance.
 
 @discussion
    When applying simple 90 degree rotation or mirroring edits to media containing a portrait effects matte, you may use this initializer to create a derivative copy of the portrait effects matte in which the specified orientation is applied. This method throws an NSInvalidArgumentException if you pass an unrecognized exifOrientation.
 */
- (instancetype)portraitEffectsMatteByApplyingExifOrientation:(CGImagePropertyOrientation)exifOrientation;

/*!
 @method portraitEffectsMatteByReplacingPortraitEffectsMatteWithPixelBuffer:error:
 @abstract
    Returns an AVPortraitEffectsMatte instance wrapping the replacement pixel buffer.
 
 @param pixelBuffer
    A pixel buffer containing a portrait effects matting image, represented as kCVPixelFormatType_OneComponent8 with kCVImageBufferColorPrimaries_ITU_R_709_2 color primaries and a kCVImageBufferTransferFunction_Linear transfer function.
 @param outError
    On return, if the AVPortraitEffectsMatte cannot be created, points to an NSError describing the problem.
 @result
    An AVPortraitEffectsMatte instance, or nil if the pixel buffer is malformed.
 
 @discussion
    When applying complex edits to media containing a portrait effects matte, you may create a derivative matte with arbitrary transforms applied to it, then use this initializer to create a new AVPortraitEffectsMatte.
 */
- (nullable instancetype)portraitEffectsMatteByReplacingPortraitEffectsMatteWithPixelBuffer:(CVPixelBufferRef)pixelBuffer error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method dictionaryRepresentationForAuxiliaryDataType:
 @abstract
    Returns a dictionary of primitive map information to be used when writing an image file with a portrait effects matte.
 
 @param outAuxDataType
    On output, the auxiliary data type to be used when calling CGImageDestinationAddAuxiliaryDataInfo. Currently the only supported auxiliary data type is kCGImageAuxiliaryDataTypePortraitEffectsMatte.
 @result
    A dictionary of CGImageDestination compatible portrait effects matte information, or nil if the auxDataType is unsupported.
 
 @discussion
    When using ImageIO framework's CGImageDestination API to write portrait effects matte information to a HEIF or JPEG file, you may use this method to generate a dictionary of primitive map information consumed by CGImageDestinationAddAuxiliaryDataInfo.
 */
- (nullable NSDictionary *)dictionaryRepresentationForAuxiliaryDataType:(NSString * _Nullable * _Nullable)outAuxDataType;

/*!
 @property pixelFormatType
 @abstract
    Specifies the pixel format type of this object's internal matting image.
 
 @discussion
    Currently the only supported CV pixel format type for the matting image is kCVPixelFormatType_OneComponent8.
 */
@property(readonly) OSType pixelFormatType;

/*!
 @property mattingImage
 @abstract
    Provides access to the portrait effects matte's internal image.
 
 @discussion
    The pixel format can be queried using the pixelFormatType property.
 */
@property(readonly) __attribute__((NSObject)) CVPixelBufferRef mattingImage NS_RETURNS_INNER_POINTER;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetImageGenerator.h
/*
	File:  AVAssetImageGenerator.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/

/*!
    @class			AVAssetImageGenerator

    @abstract		AVAssetImageGenerator provides thumbnail or preview images of assets independently of playback.
	
	@discussion		Generating a single image in isolation can require the decoding of a large number of video frames
					with complex interdependencies. Whenever a series of images is required, far greater efficiency
					can be achieved by use of the asynchronous method, -generateCGImagesAsynchronouslyForTimes:completionHandler:,
					which employs decoding efficiencies similar to those used during playback.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>

#import <CoreGraphics/CoreGraphics.h>

@class AVAsset;
@class AVVideoComposition;
@class AVAssetImageGeneratorInternal;
@protocol AVVideoCompositing;

NS_ASSUME_NONNULL_BEGIN

/*!
 @typedef AVAssetImageGeneratorApertureMode
 @abstract
    The type of an aperture mode.
*/
typedef NSString * AVAssetImageGeneratorApertureMode NS_STRING_ENUM;

/*!
	@constant		AVAssetImageGeneratorApertureModeCleanAperture
	@abstract		Both pixel aspect ratio and clean aperture will be applied.
	@discussion
		An image's clean aperture is a region of video free from transition artifacts caused by the encoding of the signal.
*/
AVF_EXPORT AVAssetImageGeneratorApertureMode const AVAssetImageGeneratorApertureModeCleanAperture NS_AVAILABLE(10_7, 4_0);

/*!
	@constant		AVAssetImageGeneratorApertureModeProductionAperture
	@abstract		Only pixel aspect ratio will be applied.
	@discussion
		The image is not cropped to the clean aperture region, but it is scaled according to the pixel aspect ratio. Use this option when you want to see all the pixels in your video, including the edges.
*/
AVF_EXPORT AVAssetImageGeneratorApertureMode const AVAssetImageGeneratorApertureModeProductionAperture NS_AVAILABLE(10_7, 4_0);

/*!
	@constant		AVAssetImageGeneratorApertureModeEncodedPixels
	@abstract		Neither pixel aspect ratio nor clean aperture will be applied.
	@discussion
		The image is not cropped to the clean aperture region and is not scaled according to the pixel aspect ratio. The encoded dimensions of the image description are displayed.
*/
AVF_EXPORT AVAssetImageGeneratorApertureMode const AVAssetImageGeneratorApertureModeEncodedPixels NS_AVAILABLE(10_7, 4_0);

typedef NS_ENUM(NSInteger, AVAssetImageGeneratorResult)
{
	AVAssetImageGeneratorSucceeded,
	AVAssetImageGeneratorFailed,
	AVAssetImageGeneratorCancelled,
};

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAssetImageGenerator : NSObject
{
@private
	AVAssetImageGeneratorInternal		*_priv;
}
AV_INIT_UNAVAILABLE

/* Indicates the instance of AVAsset with which the AVAssetImageGenerator was initialized  */ 
@property (nonatomic, readonly) AVAsset *asset NS_AVAILABLE(10_9, 6_0);

/* Specifies whether or not to apply the track's preferredTransform (see -[AVAssetTrack preferredTransform]) when extracting an image from the asset.
   Default is NO.  Only rotation by 90, 180, or 270 degrees is supported. */
@property (nonatomic) BOOL appliesPreferredTrackTransform;

/* Specifies the maximum dimensions for generated image.  Default (CGSizeZero) is the asset's unscaled dimensions.
   AVAssetImageGenerator will scale images such that they fit within the defined bounding box.
   Images will never be scaled up.  The aspect ratio of the scaled image will be defined by the apertureMode property. */
@property (nonatomic) CGSize maximumSize;

/* Specifies the aperture mode for the generated image.  Default is AVAssetImageGeneratorApertureModeCleanAperture. */
@property (nonatomic, copy, nullable) AVAssetImageGeneratorApertureMode apertureMode;

/* Specifies the video composition to use when extracting images from assets with multiple video tracks.
   If no videoComposition is specified, only the first enabled video track will be used.
   If a videoComposition is specified, the value of appliesPreferredTrackTransform is ignored. */
@property (nonatomic, copy, nullable) AVVideoComposition *videoComposition;

/* Indicates the custom video compositor instance used, if any */
@property (nonatomic, readonly, nullable) id <AVVideoCompositing> customVideoCompositor NS_AVAILABLE(10_9, 7_0);

/* The actual time of the generated images will be within the range [requestedTime-toleranceBefore, requestedTime+toleranceAfter] and may differ from the requested time for efficiency.
   Pass kCMTimeZero for both toleranceBefore and toleranceAfter to request frame-accurate image generation; this may incur additional decoding delay.
   Default is kCMTimePositiveInfinity. */
@property (nonatomic) CMTime requestedTimeToleranceBefore NS_AVAILABLE(10_7, 5_0);
@property (nonatomic) CMTime requestedTimeToleranceAfter NS_AVAILABLE(10_7, 5_0);

/*!
	@method			assetImageGeneratorWithAsset:
	@abstract		Returns an instance of AVAssetImageGenerator for use with the specified asset.
	@param			asset
					The asset from which images will be extracted.
	@result			An instance of AVAssetImageGenerator
	@discussion		This method may succeed even if the asset possesses no visual tracks at the time of initialization.
					Clients may wish to test whether an asset has any tracks with the visual characteristic via
					-[AVAsset tracksWithMediaCharacteristic:].
					
					Note also that assets that belong to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie,
					may gain visual tracks after initialization of an associated AVAssetImageGenerator.
					
					However, the results of image generation are undefined if mutations of the asset occur while images
					are being generated. 

					AVAssetImageGenerator will use the default enabled video track(s) to generate images.
*/
+ (instancetype)assetImageGeneratorWithAsset:(AVAsset *)asset;

/*!
	@method			initWithAsset:
	@abstract		Initializes an instance of AVAssetImageGenerator for use with the specified asset.
	@param			asset
					The asset from which images will be extracted.
	@result			An instance of AVAssetImageGenerator
	@discussion		This method may succeed even if the asset possesses no visual tracks at the time of initialization.
					Clients may wish to test whether an asset has any tracks with the visual characteristic via
					-[AVAsset tracksWithMediaCharacteristic:].
					
					Note also that assets that belong to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie,
					may gain visual tracks after initialization of an associated AVAssetImageGenerator.
					
					However, the results of image generation are undefined if mutations of the asset occur while images
					are being generated. 

					AVAssetImageGenerator will use the default enabled video track(s) to generate images.
*/
- (instancetype)initWithAsset:(AVAsset *)asset NS_DESIGNATED_INITIALIZER;

/*!
	@method			copyCGImageAtTime:actualTime:error:
	@abstract		Returns a CFRetained CGImageRef for an asset at or near the specified time.
	@param			requestedTime
					The time at which the image of the asset is to be created.
	@param			actualTime
					A pointer to a CMTime to receive the time at which the image was actually generated. If you are not interested
					in this information, pass NULL.
	@param			outError
					An error object describing the reason for failure, in the event that this method returns NULL.
	@result			A CGImageRef.
	@discussion		Returns the CGImage synchronously. Ownership follows the Create Rule.
*/
- (nullable CGImageRef)copyCGImageAtTime:(CMTime)requestedTime actualTime:(nullable CMTime *)actualTime error:(NSError * _Nullable * _Nullable)outError CF_RETURNS_RETAINED;

/* error object indicates the reason for failure if the result is AVAssetImageGeneratorFailed */
typedef void (^AVAssetImageGeneratorCompletionHandler)(CMTime requestedTime, CGImageRef _Nullable image, CMTime actualTime, AVAssetImageGeneratorResult result, NSError * _Nullable error);

/*!
	@method			generateCGImagesAsynchronouslyForTimes:completionHandler:
	@abstract		Returns a series of CGImageRefs for an asset at or near the specified times.
	@param			requestedTimes
					An NSArray of NSValues, each containing a CMTime, specifying the asset times at which an image is requested.
	@param			handler
					A block that will be called when an image request is complete.
	@discussion		Employs an efficient "batch mode" for getting images in time order.
					The client will receive exactly one handler callback for each requested time in requestedTimes.
					Changes to generator properties (snap behavior, maximum size, etc...) will not affect outstanding asynchronous image generation requests.
					The generated image is not retained.  Clients should retain the image if they wish it to persist after the completion handler returns.
*/
- (void)generateCGImagesAsynchronouslyForTimes:(NSArray<NSValue *> *)requestedTimes completionHandler:(AVAssetImageGeneratorCompletionHandler)handler;

/*!
	@method			cancelAllCGImageGeneration
	@abstract		Cancels all outstanding image generation requests.
	@discussion		Calls the handler block with AVAssetImageGeneratorCancelled for each image time in every previous invocation of -generateCGImagesAsynchronouslyForTimes:completionHandler:
					for which images have not yet been supplied.
*/
- (void)cancelAllCGImageGeneration;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMIDIPlayer.h
/*
	File:           AVMIDIPlayer.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVMIDIPlayer.h>

// ==========  AVFoundation.framework/Headers/AVAudioSession.h
/*
	File:           AVAudioSession.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#if __has_include(<AVFAudio/AVAudioSession.h>)
#import <AVFAudio/AVAudioSession.h>
#endif

// ==========  AVFoundation.framework/Headers/AVComposition.h
/*
	File:  AVComposition.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/

/*!
    @class          AVComposition

    @abstract       An AVComposition combines media data from multiple local file-based sources in a custom temporal arrangement, in order to present or process media data from multiple sources together. All local file-based audiovisual assets are eligible to be combined, regardless of container type.

    @discussion
      At its top-level, AVComposition is a collection of tracks, each presenting media of a specific media type, e.g. audio or video, according to a timeline. Each track is represented by an instance of AVCompositionTrack.

      Each track is comprised of an array of track segments, each of which present a portion of the media data stored in a source container, specified by URL, a track identifier, and a time mapping, as represented by an instance of AVCompositionTrackSegment.

      The URL specifies the source container, and the track identifier indicates the track of the source container to be presented.

      The time mapping specifies the temporal range of the source track that's to be presented and also specifies the temporal range of its presentation in the composition track. If the durations of the source and destination ranges of the time mapping are the same, the media data for the segment will be presented at its natural rate. Otherwise, the segment will be presented at a rate equal to the ratio source.duration / target.duration.

      The track segments of a track are available via AVCompositionTrack's trackSegment property, an array of AVCompositionTrackSegment. The collection of tracks with media type information for each, and each with its array of track segments (URL, track identifier, and time mapping), form a complete low-level representation of a composition.

      This representation can be written out by clients in any convenient form, and subsequently the composition can be reconstituted by instantiating a new AVMutableComposition with AVMutableCompositionTracks of the appropriate media type, each with its trackSegment property set according to the stored array of URL, track identifier, and time mapping.

      A higher-level interface for constructing compositions is also presented by AVMutableComposition and AVMutableCompositionTrack, offering insertion, removal, and scaling operations without direct manipulation of the trackSegment arrays of composition tracks. This interface makes use of higher-level constructs such as AVAsset and AVAssetTrack, allowing the client to make use of the same references to candidate sources that it would have created in order to inspect or preview them prior to inclusion in a composition.

      Immutable Snapshots

        To make an immutable snapshot of a mutable composition for playback or inspection:

            // myMutableComposition is a mutable composition; the client wants to inspect and play it in its current state
            AVComposition *immutableSnapshotOfMyComposition = [myMutableComposition copy];

            // inspect and play at will, e.g.
            AVPlayerItem *playerItemForSnapshottedComposition = [[AVPlayerItem alloc] initWithAsset:immutableSnapshotOfMyComposition];

      Compositing Of Video Tracks

        During playback or other processing, such as export, without the use of an AVVideoComposition only the first enabled video track will be processed. Other video tracks are effectively ignored. To control the compositing of multiple enabled video tracks, you must create and configure an instance of AVVideoComposition and set it as the value of the videoComposition property of the AVFoundation object you're using to control processing, such as an AVPlayerItem or AVAssetExportSession.

      Mixing Of Audio Tracks

        During playback or other processing, without the use of an AVAudioMix all of the asset's enabled audio tracks are mixed together at equal levels. To control the mixing of enabled audio tracks, you must create and configure an instance of AVAudioMix and set it as the value of the audioMix property of the AVFoundation object you're using to control processing, such as an AVPlayerItem or AVAssetExportSession.
*/

#import <AVFoundation/AVAsset.h>
#import <AVFoundation/AVCompositionTrack.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

@class AVCompositionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVComposition : AVAsset <NSMutableCopying>
{
@private
    AVCompositionInternal    *_priv;
}

/*!
    @property       tracks
    @abstract       Provides the array of AVCompositionTracks contained by the composition.
*/
@property (nonatomic, readonly) NSArray<AVCompositionTrack *> *tracks;

/*!
    @property       naturalSize
    @abstract       Indicates the authored size of the visual portion of the composition.
*/
@property (nonatomic, readonly) CGSize naturalSize;

/*!
    @property       URLAssetInitializationOptions
    @abstract       Specifies the initialization options for the creation of AVURLAssets by the receiver, e.g. AVURLAssetPreferPreciseDurationAndTimingKey. The default behavior for creation of AVURLAssets by an AVComposition is equivalent to the behavior of +[AVURLAsset URLAssetWithURL:options:] when specifying no initialization options.
    @discussion
      AVCompositions create AVURLAssets internally for URLs specified by AVCompositionTrackSegments of AVCompositionTracks, as needed, whenever AVCompositionTrackSegments were originally added to a track via -[AVMutableCompositionTrack setSegments:] rather than by inserting timeranges of already existing AVAssets or AVAssetTracks.
      The value of URLAssetInitializationOptions can be specified at the time an AVMutableComposition is created via +compositionWithURLAssetInitializationOptions:.
 */
@property (nonatomic, readonly, copy) NSDictionary<NSString *, id> *URLAssetInitializationOptions NS_AVAILABLE(10_11, 9_0);

@end

@interface AVComposition (AVCompositionTrackInspection)

/*!
    @method         trackWithTrackID:
    @abstract       Provides an instance of AVCompositionTrack that represents the track of the specified trackID.
    @param          trackID
                    The trackID of the requested AVCompositionTrack.
    @result         An instance of AVCompositionTrack; may be nil if no track of the specified trackID is available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (nullable AVCompositionTrack *)trackWithTrackID:(CMPersistentTrackID)trackID;

/*!
    @method         tracksWithMediaType:
    @abstract       Provides an array of AVCompositionTracks of the asset that present media of the specified media type.
    @param          mediaType
                    The media type according to which the receiver filters its AVCompositionTracks. (Media types are defined in AVMediaFormat.h)
    @result         An NSArray of AVCompositionTracks; may be empty if no tracks of the specified media type are available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVCompositionTrack *> *)tracksWithMediaType:(AVMediaType)mediaType;

/*!
    @method         tracksWithMediaCharacteristic:
    @abstract       Provides an array of AVCompositionTracks of the asset that present media with the specified characteristic.
    @param          mediaCharacteristic
                    The media characteristic according to which the receiver filters its AVCompositionTracks. (Media characteristics are defined in AVMediaFormat.h)
    @result         An NSArray of AVCompositionTracks; may be empty if no tracks with the specified characteristic are available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVCompositionTrack *> *)tracksWithMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

@end


@class AVMutableCompositionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableComposition : AVComposition
{
@private
    AVMutableCompositionInternal    *_mutablePriv __attribute__((unused));
}

/*!
    @property       tracks
    @abstract       Provides the array of AVMutableCompositionTracks contained by the composition.
*/
@property (nonatomic, readonly) NSArray<AVMutableCompositionTrack *> *tracks;

/*!
    @property       naturalSize
    @abstract       Indicates the authored size of the visual portion of the asset.
    @discussion
      If not set, the value is the size of the composition's first video track. Set to CGSizeZero to revert to default behavior.
*/
@property (nonatomic) CGSize naturalSize;

/*!
    @method         composition
    @abstract       Returns an empty AVMutableComposition.
*/
+ (instancetype)composition;

/*!
    @method         compositionWithURLAssetInitializationOptions:
    @abstract       Returns an empty AVMutableComposition.
    @param          URLAssetInitializationOptions
                    Specifies the initialization options that the receiver should use when creating AVURLAssets internally, e.g. AVURLAssetPreferPreciseDurationAndTimingKey. The default behavior for creation of AVURLAssets by an AVMutableComposition is equivalent to the behavior of +[AVURLAsset URLAssetWithURL:options:] when specifying no initialization options.
    @discussion
      AVMutableCompositions create AVURLAssets internally for URLs specified by AVCompositionTrackSegments of AVMutableCompositionTracks, as needed, whenever AVCompositionTrackSegments are added to tracks via -[AVMutableCompositionTrack setSegments:] rather than by inserting timeranges of already existing AVAssets or AVAssetTracks.
 */
+ (instancetype)compositionWithURLAssetInitializationOptions:(nullable NSDictionary<NSString *, id> *)URLAssetInitializationOptions NS_AVAILABLE(10_11, 9_0);

@end


@interface AVMutableComposition (AVMutableCompositionCompositionLevelEditing)

/*!
    @method         insertTimeRange:ofAsset:atTime:error:
    @abstract       Inserts all the tracks of a timeRange of an asset into a composition.
    @param          timeRange
                    Specifies the timeRange of the asset to be inserted.
    @param          asset
                    Specifies the asset that contains the tracks that are to be inserted. Only instances of AVURLAsset and AVComposition are supported (AVComposition starting in MacOS X 10.10 and iOS 8.0).
    @param          startTime
                    Specifies the time at which the inserted tracks are to be presented by the composition.
    @param          outError
                    Describes failures that may be reported to the user, e.g. the asset that was selected for insertion in the composition is restricted by copy-protection.
    @result         A BOOL value indicating the success of the insertion.
    @discussion
      You provide a reference to an AVAsset and the timeRange within it that you want to insert. You specify the start time in the destination composition at which the timeRange should be inserted.

      This method may add new tracks to ensure that all tracks of the asset are represented in the inserted timeRange.

      Note that the media data for the inserted timeRange will be presented at its natural duration and rate. It can be scaled to a different duration and presented at a different rate via -scaleTimeRange:toDuration:.

      Existing content at the specified startTime will be pushed out by the duration of timeRange.
*/
- (BOOL)insertTimeRange:(CMTimeRange)timeRange ofAsset:(AVAsset *)asset atTime:(CMTime)startTime error:(NSError * _Nullable * _Nullable)outError;

/*!
    @method         insertEmptyTimeRange:
    @abstract       Adds or extends an empty timeRange within all tracks of the composition.
    @param          timeRange
                    Specifies the empty timeRange to be inserted.
    @discussion
     If you insert an empty timeRange into the composition, any media that was presented during that interval prior to the insertion will be presented instead immediately afterward. You can use this method to reserve an interval in which you want a subsequently created track to present its media.
      Note that you cannot add empty time ranges to the end of a composition.
*/
- (void)insertEmptyTimeRange:(CMTimeRange)timeRange;

/*!
    @method         removeTimeRange:
    @abstract       Removes a specified timeRange from all tracks of the composition.
    @param          timeRange
                    Specifies the timeRange to be removed.
    @discussion
      Removal of a time range does not cause any existing tracks to be removed from the composition, even if removing timeRange results in an empty track. Instead, it removes or truncates track segments that intersect with the timeRange.

      After removing, existing content after timeRange will be pulled in.
*/
- (void)removeTimeRange:(CMTimeRange)timeRange;

/*!
    @method         scaleTimeRange:toDuration:
    @abstract       Changes the duration of a timeRange of all tracks.
    @param          timeRange
                    Specifies the timeRange of the composition to be scaled.
    @param          duration
                    Specifies the new duration of the timeRange.
    @discussion
      Each trackSegment affected by the scaling operation will be presented at a rate equal to source.duration / target.duration of its resulting timeMapping.
*/
- (void)scaleTimeRange:(CMTimeRange)timeRange toDuration:(CMTime)duration;

@end


@interface AVMutableComposition (AVMutableCompositionTrackLevelEditing)

/*!
    @method         addMutableTrackWithMediaType:preferredTrackID:
    @abstract       Adds an empty track to a mutable composition.
    @param          mediaType
                    The media type of the new track.
    @param          preferredTrackID
                    Specifies the preferred track ID for the new track. If you do not need to specify a preferred track ID, pass kCMPersistentTrackID_Invalid. Otherwise the preferred track ID will be used for the new track, provided that it is not currently in use and has not previously been used.
    @result         An instance of AVMutableCompositionTrack representing the new track. Its actual trackID is available via its @"trackID" key.
    @discussion
      If the specified preferred track ID is not available, or kCMPersistentTrackID_Invalid was passed in, a unique track ID will be generated.
*/
- (nullable AVMutableCompositionTrack *)addMutableTrackWithMediaType:(AVMediaType)mediaType preferredTrackID:(CMPersistentTrackID)preferredTrackID;

/*!
    @method         removeTrack:
    @abstract       Removes a track of a mutable composition.
    @param          track
                    A reference to the AVCompositionTrack to be removed.
    @discussion
      If you retain a reference to the removed track, note that its @"composition" key will have the value nil, and the values of its other properties are undefined.
*/
- (void)removeTrack:(AVCompositionTrack *)track;

/*!
    @method         mutableTrackCompatibleWithTrack:
    @abstract       Provides a reference to a track of a mutable composition into which any timeRange of an AVAssetTrack can be inserted (via -[AVMutableCompositionTrack insertTimeRange:ofTrack:atTime:error:]).
    @param          track
                    A reference to the AVAssetTrack from which a timeRange may be inserted.
    @result         An AVMutableCompositionTrack that can accommodate the insertion, or, if no such track is available, nil.
    @discussion
      If a compatible track is desired but the result of this method is nil, a new track of the same mediaType as the AVAssetTrack can be created via -addMutableTrackWithMediaType:preferredTrackID:, and this new track will be compatible.

      For best performance, the number of tracks of a composition should be kept to a minimum, corresponding to the number for which media data must be presented in parallel. If media data of the same type is to be presented serially, even from multiple assets, a single track of that media type should be used. This method, -mutableTrackCompatibleWithTrack:, can help the client to identify an existing target track for an insertion.

      Similar to -[AVAsset compatibleTrackForCompositionTrack:].
*/
- (nullable AVMutableCompositionTrack *)mutableTrackCompatibleWithTrack:(AVAssetTrack *)track;

@end

@interface AVMutableComposition (AVMutableCompositionTrackInspection)

/*!
    @method         trackWithTrackID:
    @abstract       Provides an instance of AVMutableCompositionTrack that represents the track of the specified trackID.
    @param          trackID
                    The trackID of the requested AVMutableCompositionTrack.
    @result         An instance of AVMutableCompositionTrack; may be nil if no track of the specified trackID is available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (nullable AVMutableCompositionTrack *)trackWithTrackID:(CMPersistentTrackID)trackID;

/*!
    @method         tracksWithMediaType:
    @abstract       Provides an array of AVMutableCompositionTracks of the asset that present media of the specified media type.
    @param          mediaType
                    The media type according to which the receiver filters its AVMutableCompositionTracks. (Media types are defined in AVMediaFormat.h)
    @result         An NSArray of AVMutableCompositionTracks; may be empty if no tracks of the specified media type are available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVMutableCompositionTrack *> *)tracksWithMediaType:(AVMediaType)mediaType;

/*!
    @method         tracksWithMediaCharacteristic:
    @abstract       Provides an array of AVMutableCompositionTracks of the asset that present media with the specified characteristic.
    @param          mediaCharacteristic
                    The media characteristic according to which the receiver filters its AVMutableCompositionTracks. (Media characteristics are defined in AVMediaFormat.h)
    @result         An NSArray of AVMutableCompositionTracks; may be empty if no tracks with the specified characteristic are available.
    @discussion
      Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVMutableCompositionTrack *> *)tracksWithMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVSpeechSynthesis.h
/*
	File:           AVSpeechSynthesis.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVSpeechSynthesis.h>

// ==========  AVFoundation.framework/Headers/AVFoundation.h
/*
	File:  AVFoundation.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

	To report bugs, go to:  http://developer.apple.com/bugreporter/

*/

#import <TargetConditionals.h>
#if TARGET_OS_WATCH
#if ! __has_include(<AVFoundation/AVAnimation.h>)
#define AVF_IS_WATCHOS_SDK 1
#endif
#endif

#import <AVFoundation/AVBase.h>

#if ! AVF_IS_WATCHOS_SDK
#import <AVFoundation/AVAnimation.h>
#import <AVFoundation/AVAsset.h>
#import <AVFoundation/AVAssetCache.h>
#import <AVFoundation/AVAssetExportSession.h>
#import <AVFoundation/AVAssetImageGenerator.h>
#import <AVFoundation/AVAssetReader.h>
#import <AVFoundation/AVAssetReaderOutput.h>
#import <AVFoundation/AVAssetResourceLoader.h>
#import <AVFoundation/AVContentKeySession.h>
#import <AVFoundation/AVAssetTrack.h>
#import <AVFoundation/AVAssetTrackGroup.h>
#import <AVFoundation/AVAssetTrackSegment.h>
#import <AVFoundation/AVAssetWriter.h>
#import <AVFoundation/AVAssetWriterInput.h>
#import <AVFoundation/AVAsynchronousKeyValueLoading.h>
#import <AVFoundation/AVAudioMix.h>
#import <AVFoundation/AVAudioProcessingSettings.h>

#if TARGET_OS_IPHONE
#if __has_include(<AVFoundation/AVAssetDownloadTask.h>)
#import <AVFoundation/AVAssetDownloadTask.h>
#endif
#if __has_include(<AVFoundation/AVAssetDownloadStorageManager.h>)
#import <AVFoundation/AVAssetDownloadStorageManager.h>
#endif
#endif

#if (TARGET_OS_IPHONE || defined(__MAC_10_7))
#import <AVFoundation/AVCaptureDataOutputSynchronizer.h>
#import <AVFoundation/AVCaptureDevice.h>
#import <AVFoundation/AVCaptureInput.h>
#import <AVFoundation/AVCaptureOutput.h>
#import <AVFoundation/AVCaptureSession.h>
#import <AVFoundation/AVCaptureSystemPressure.h>
#import <AVFoundation/AVCaptureVideoPreviewLayer.h>
#endif

#import <AVFoundation/AVComposition.h>
#import <AVFoundation/AVCompositionTrack.h>
#import <AVFoundation/AVCompositionTrackSegment.h>
#import <AVFoundation/AVDepthData.h>
#import <AVFoundation/AVPortraitEffectsMatte.h>
#import <AVFoundation/AVError.h>
#endif

#import <AVFoundation/AVFAudio.h>

#if ! AVF_IS_WATCHOS_SDK
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVMediaSelection.h>
#import <AVFoundation/AVMediaSelectionGroup.h>
#import <AVFoundation/AVMetadataFormat.h>
#import <AVFoundation/AVMetadataIdentifiers.h> 
#import <AVFoundation/AVMetadataItem.h>
#import <AVFoundation/AVMetadataObject.h>
#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))
#import <AVFoundation/AVMovie.h>
#import <AVFoundation/AVMovieTrack.h>
#endif
#import <AVFoundation/AVOutputSettingsAssistant.h>
#import <AVFoundation/AVPlayer.h>
#import <AVFoundation/AVPlayerItem.h>
#import <AVFoundation/AVPlayerItemMediaDataCollector.h>
#import <AVFoundation/AVPlayerItemOutput.h>
#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))
#import <AVFoundation/AVPlayerItemProtectedContentAdditions.h>
#endif
#import <AVFoundation/AVPlayerItemTrack.h>
#import <AVFoundation/AVPlayerLayer.h>
#import <AVFoundation/AVPlayerLooper.h>
#import <AVFoundation/AVPlayerMediaSelectionCriteria.h>
#import <AVFoundation/AVQueuedSampleBufferRendering.h>
#import <AVFoundation/AVRouteDetector.h>
#import <AVFoundation/AVSampleBufferAudioRenderer.h>
#import <AVFoundation/AVSampleBufferDisplayLayer.h>
#import <AVFoundation/AVSampleBufferRenderSynchronizer.h>
#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))
#import <AVFoundation/AVSampleBufferGenerator.h>
#import <AVFoundation/AVSampleCursor.h>
#endif
#import <AVFoundation/AVSynchronizedLayer.h>
#import <AVFoundation/AVTextStyleRule.h>
#import <AVFoundation/AVTime.h>
#import <AVFoundation/AVTimedMetadataGroup.h>
#import <AVFoundation/AVUtilities.h>
#import <AVFoundation/AVVideoCompositing.h>
#import <AVFoundation/AVVideoComposition.h>
#import <AVFoundation/AVVideoSettings.h>
#endif
#if TARGET_OS_TV
#import <AVFoundation/AVDisplayCriteria.h>
#endif
// ==========  AVFoundation.framework/Headers/AVSampleBufferRenderSynchronizer.h
/*
	File:  AVSampleBufferRenderSynchronizer.h

	Framework:  AVFoundation
 
	Copyright 2013-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMSync.h>

@protocol AVQueuedSampleBufferRendering;
@class AVSampleBufferRenderSynchronizerInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
  @class		AVSampleBufferRenderSynchronizer
  @abstract		AVSampleBufferRenderSynchronizer can synchronize multiple objects conforming to AVQueuedSampleBufferRendering to a single timebase.
*/
API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED
@interface AVSampleBufferRenderSynchronizer : NSObject
{
@private
	AVSampleBufferRenderSynchronizerInternal *_synchronizerInternal;
}

/*!
	@property		timebase
	@abstract		The synchronizer's rendering timebase, which governs how time stamps are interpreted.
	@discussion
		By default, this timebase will be driven by the clock of an added AVSampleBufferAudioRenderer.
	
		If no AVSampleBufferAudioRenderer has been added, the master clock will be the host time clock (mach_absolute_time with the appropriate timescale conversion; this is the same as Core Animation's CACurrentMediaTime).
					
		The timebase is a read-only timebase.  Use the rate property and corresponding methods to adjust the timebase.
*/
@property (retain, readonly) __attribute__((NSObject)) CMTimebaseRef timebase;

/*!
	@property		rate
	@abstract		Playback rate.
	@discussion
		Indicates the current rate of rendering.  A value of 0.0 means "stopped"; a value of 1.0 means "play at the natural rate of the media".  Must be greater than or equal to 0.0.
*/
@property (nonatomic, readwrite) float rate;

/*!
	@constant		AVSampleBufferRenderSynchronizerRateDidChangeNotification
	@abstract		A notification that fires whenever the value of the "rate" property changes.
	@discussion
		The rate can change as a result of setting the rate property, either by directly setting the property or calling -setRate:time:.  The rate can also change at any time, without any action by the client of the render synchronizer.  For example, on iOS if the app's playback is interrupted (e.g. by a phone call or another non-mixable app starting playback), the rate will automatically be set to zero.  This notification will be sent in all of those cases.
 */
AVF_EXPORT NSNotificationName const AVSampleBufferRenderSynchronizerRateDidChangeNotification API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0)) __WATCHOS_PROHIBITED;

/*!
	@method			currentTime
	@abstract		Returns the current time of the synchronizer.
	@result			A CMTime
	@discussion		Returns the current time of the synchronizer. Not key-value observable; use -addPeriodicTimeObserverForInterval:queue:usingBlock: instead.
*/
- (CMTime)currentTime API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0)) __WATCHOS_PROHIBITED;

/*!
	@method			setRate:time:
	@abstract		Sets the timebase's time and rate.
	@param			rate
		A new timebase rate to set.  Must be greater than or equal to 0.0
	@param			time
		A new time to set.  Must be greater than or equal to kCMTimeZero, or kCMTimeInvalid
	@discussion
		Sets the timebase's time to time and then sets the rendering rate to rate.  A rate value of 0.0 means "stopped"; a rate value of 1.0 means "play at the natural rate of the media".  Use kCMTimeInvalid for time to not modify the timebase's time.
*/
- (void)setRate:(float)rate time:(CMTime)time;

@end

@interface AVSampleBufferRenderSynchronizer (AVSampleBufferRenderSynchronizerRendererManagement)

/*!
	@property		renderers
	@abstract		Array of id<AVQueuedSampleBufferRendering> currently attached to the synchronizer.
	@discussion
		A list of renderers added to and not removed from the synchronizer.  The list also includes renderers that have been scheduled to be removed but have not yet been removed.
	
		This property is not KVO observable.
*/
@property (atomic, readonly) NSArray<__kindof id <AVQueuedSampleBufferRendering>> *renderers;

/*!
	@method			addRenderer:
	@abstract		Adds a renderer to the list of renderers under the synchronizer's control.
	@param			renderer
		An object conforming to AVQueuedSampleBufferRendering to be synchronized by this synchronizer.
	@discussion
		Adds a renderer to begin operating with the synchronizer's timebase.

		This method can be called while rate is non-0.0.
*/
- (void)addRenderer:(id <AVQueuedSampleBufferRendering>)renderer;

/*!
	@method			removeRenderer:atTime:completionHandler:
	@abstract		Removes a renderer from the list of renderers under the synchronizer's control.
	@param			renderer
		An object conforming to AVQueuedSampleBufferRendering currently synchronized by this synchronizer to no longer be synchronized by the synchronizer.
	@param			time
		The time on the timebase's timeline at which the renderer should be removed.
	@param			completionHandler
		Optional.  A block called when the renderer is removed from the synchronizer.  If provided, this block will always be called with didRemoveRenderer indicating whether the renderer was removed by this scheduled removal.
	@discussion
		This method can be called while rate is non-0.0.

		time is used to schedule future removals.  If the time is in the past, the renderer will be removed immediately.  kCMTimeInvalid can also be used to force immediate removal.
	
		This method removes the renderer asynchronously.  The method can be called more than once, with a subsequent scheduled removal replacing a previously scheduled removal.
					
		Clients may provide an optional completionHandler block to be notified when the scheduled removal completes.  If provided, completionHandler will always be called with the following values for didRemoveRenderer:
			
			- If the renderer has not been added to this synchronizer, completionHandler will be called and didRemoveRenderer will be NO.
			- If a removal of a particular renderer is scheduled after another removal of that same renderer has already been scheduled but not yet occurred, the previously-scheduled removal's completionHandler will be called and didRemoveRenderer will be NO.  The new scheduled removal's completionHandler will not be called until it is replaced by another scheduled removal or the renderer is actually removed.
			- When the renderer is removed due to a scheduled removal, the completionHandler provided when that removal was scheduled will be called and didRemoveRenderer will be YES.
*/
- (void)removeRenderer:(id <AVQueuedSampleBufferRendering>)renderer atTime:(CMTime)time completionHandler:(nullable void (^)(BOOL didRemoveRenderer))completionHandler;

@end

@interface AVSampleBufferRenderSynchronizer (AVSampleBufferRenderSynchronizerTimeObservation)

/*!
	@method			addPeriodicTimeObserverForInterval:queue:usingBlock:
	@abstract		Requests invocation of a block during rendering to report changing time.
	@param			interval
		The interval of invocation of the block during normal rendering, according to progress of the current time of the timebase.
	@param			queue
		The serial queue onto which block should be enqueued.  If you pass NULL, the main queue (obtained using dispatch_get_main_queue()) will be used.  Passing a concurrent queue to this method will result in undefined behavior.
	@param			block
		The block to be invoked periodically.
	@result
		An object conforming to the NSObject protocol.  You must retain this returned value as long as you want the time observer to be invoked by the synchronizer.  Pass this object to -removeTimeObserver: to cancel time observation.
	@discussion
		The block is invoked periodically at the interval specified, interpreted according to the timeline of the timebase.  The block is also invoked whenever time jumps and whenever rendering starts or stops.
 
		If the interval corresponds to a very short interval in real time, the synchronizer may invoke the block less frequently than requested. Even so, the synchronizer will invoke the block sufficiently often for the client to update indications of the current time appropriately in its end-user interface.
 
		Each call to -addPeriodicTimeObserverForInterval:queue:usingBlock: should be paired with a corresponding call to -removeTimeObserver:.  Releasing the observer object without a call to -removeTimeObserver: will result in undefined behavior.
*/
- (id)addPeriodicTimeObserverForInterval:(CMTime)interval queue:(nullable dispatch_queue_t)queue usingBlock:(void (^)(CMTime time))block;

/*!
	@method			addBoundaryTimeObserverForTimes:queue:usingBlock:
	@abstract		Requests invocation of a block when specified times are traversed during normal rendering.
	@param			times
		The times for which the observer requests notification, supplied as an array of NSValues carrying CMTimes.
	@param			queue
		The serial queue onto which block should be enqueued.  If you pass NULL, the main queue (obtained using dispatch_get_main_queue()) will be used.  Passing a
	  concurrent queue to this method will result in undefined behavior.
	@param			block
		The block to be invoked when any of the specified times is crossed during normal rendering.
	@result
		An object conforming to the NSObject protocol.  You must retain this returned value as long as you want the time observer to be invoked by the synchronizer.  Pass this object to -removeTimeObserver: to cancel time observation.
	@discussion
		Each call to -addPeriodicTimeObserverForInterval:queue:usingBlock: should be paired with a corresponding call to -removeTimeObserver:.  Releasing the observer object without a call to -removeTimeObserver: will result in undefined behavior.
*/
- (id)addBoundaryTimeObserverForTimes:(NSArray<NSValue *> *)times queue:(nullable dispatch_queue_t)queue usingBlock:(void (^)(void))block;

/*!
	@method			removeTimeObserver:
	@abstract		Cancels a previously registered time observer.
	@param			observer
		An object returned by a previous call to -addPeriodicTimeObserverForInterval:queue:usingBlock: or -addBoundaryTimeObserverForTimes:queue:usingBlock:.
	@discussion
		Upon return, the caller is guaranteed that no new time observer blocks will begin executing.  Depending on the calling thread and the queue used to add the time observer, an in-flight block may continue to execute after this method returns.  You can guarantee synchronous time observer removal by enqueuing the call to -removeTimeObserver: on that queue.  Alternatively, call dispatch_sync(queue, ^{}) after -removeTimeObserver: to wait for any in-flight blocks to finish executing.  -removeTimeObserver: should be used to explicitly cancel each time observer added using -addPeriodicTimeObserverForInterval:queue:usingBlock: and -addBoundaryTimeObserverForTimes:queue:usingBlock:.
*/
- (void)removeTimeObserver:(id)observer;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioFile.h
/*
	File:           AVAudioFile.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioFile.h>

// ==========  AVFoundation.framework/Headers/AVMediaSelection.h
/*
	File:  AVMediaSelection.h

	Framework:  AVFoundation

	Copyright 2015 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import	<AVFoundation/AVAsset.h>
#import <AVFoundation/AVMediaSelectionGroup.h>

NS_ASSUME_NONNULL_BEGIN

@class AVMediaSelectionInternal;
@class AVMutableMediaSelectionInternal;

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVMediaSelection : NSObject <NSCopying, NSMutableCopying>
{
@private
	AVMediaSelectionInternal *_mediaSelection;
}

/*
 @property		asset
 @abstract		The asset associated with the receiver.
*/
@property (nonatomic, readonly, weak) AVAsset *asset;

/*!
 @method		selectedMediaOptionInMediaSelectionGroup:
 @abstract		Indicates the media selection option that's currently selected from the specified group. May be nil.
 @param 		mediaSelectionGroup
				A media selection group obtained from the receiver's asset.
 @result		An instance of AVMediaSelectionOption that describes the currently selection option in the group.
 @discussion
				If the value of the property allowsEmptySelection of the AVMediaSelectionGroup is YES, the currently selected option in the group may be nil.
*/
- (nullable AVMediaSelectionOption *)selectedMediaOptionInMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup;

/*!
 @method		mediaSelectionCriteriaCanBeAppliedAutomaticallyToMediaSelectionGroup:
 @abstract		Indicates that specified media selection group is subject to automatic media selection.
 @param 		mediaSelectionGroup
				A media selection group obtained from the receiver's asset.
 @result		YES if the group is subject to automatic media selection.
 @discussion	Automatic application of media selection criteria is suspended in any group in which a specific selection has been made via an invocation of -selectMediaOption:inMediaSelectionGroup:.
*/
- (BOOL)mediaSelectionCriteriaCanBeAppliedAutomaticallyToMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup;

@end

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVMutableMediaSelection : AVMediaSelection
/*!
 @method		selectMediaOption:inMediaSelectionGroup:
 @abstract		Selects the media option described by the specified instance of AVMediaSelectionOption in the specified AVMediaSelectionGroup and deselects all other options in that group.
 @param			mediaSelectionOption
				The option to select.
 @param			mediaSelectionGroup
				The media selection group, obtained from the receiver's asset, that contains the specified option.
 @discussion
				If the specified media selection option isn't a member of the specified media selection group, no change in presentation state will result.
				If the value of the property allowsEmptySelection of the AVMediaSelectionGroup is YES, you can pass nil for mediaSelectionOption to deselect all media selection options in the group.
*/
- (void)selectMediaOption:(nullable AVMediaSelectionOption *)mediaSelectionOption inMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetTrackSegment.h
/*
	File:  AVAssetTrackSegment.h

	Framework:  AVFoundation
 
	Copyright 2010-2015 Apple Inc. All rights reserved.

*/

/*!
	@class			AVAssetTrackSegment

	@abstract		AVAssetTrackSegment represents a segment of an AVAssetTrack, comprising of a
					time mapping from the source to the asset track timeline.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAssetTrackSegment : NSObject
{
@private
	CMTimeMapping	_timeMapping;
}
AV_INIT_UNAVAILABLE

/* indicates the timeRange of the track of the container file of the media presented by the AVAssetTrackSegment */
@property (nonatomic, readonly) CMTimeMapping timeMapping;

/* indicates whether the AVAssetTrackSegment is an empty segment */
@property (nonatomic, readonly, getter=isEmpty) BOOL empty;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVTextStyleRule.h
/*
	File:  AVTextStyleRule.h

	Framework:  AVFoundation
 
	Copyright 2012-2016 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

/*!
  @class		AVTextStyleRule

  @abstract		AVTextStyleRule represents a set of text styling attributes that can be applied to some or all of the text of legible media, such as subtitles and closed captions.
*/

@class AVTextStyleRuleInternal;

NS_CLASS_AVAILABLE(10_9, 6_0)
@interface AVTextStyleRule : NSObject <NSCopying> {
@private
	AVTextStyleRuleInternal *_textStyleRule;
}
AV_INIT_UNAVAILABLE

/*!
 @method		propertyListForTextStyleRules:
 @abstract		Converts an NSArray of AVTextStyleRules into a serializable property list that can be used for persistent storage.
 @param			textStyleRules
 				An array of AVTextStyleRules.
 @result		A serializable property list.
 @discussion	For serialization utilities, see NSPropertyList.h.
*/
+ (id)propertyListForTextStyleRules:(NSArray<AVTextStyleRule *> *)textStyleRules;

/*!
 @method		textStyleRulesFromPropertyList:
 @abstract		Converts a property list into an NSArray of AVTextStyleRules.
 @param			plist
 				A property list, normally obtained previously via an invocation of +propertyListForTextStyleRules:.
 @result		An NSArray of AVTextStyleRules
*/
+ (nullable NSArray<AVTextStyleRule *> *)textStyleRulesFromPropertyList:(id)plist;

/*!
 @method		textStyleRuleWithTextMarkupAttributes:
 @abstract		Creates an instance of AVTextStyleRule with the specified text markup attributes.
 @param			textMarkupAttributes
 				An NSDictionary with keys representing text style attributes that are specifiable in text markup. Eligible keys are defined in <CoreMedia/CMTextMarkup.h>.
 @result		An instance of AVTextStyleRule
 @discussion	Equivalent to invoking +textStyleRuleWithTextMarkupAttributes:textSelector: with a value of nil for textSelector.
*/
+ (nullable AVTextStyleRule *)textStyleRuleWithTextMarkupAttributes:(NSDictionary<NSString *, id> *)textMarkupAttributes;

/*!
 @method		textStyleRuleWithTextMarkupAttributes:textSelector:
 @abstract		Creates an instance of AVTextStyleRule with the specified text markup attributes and an identifier for the range or ranges of text to which the attributes should be applied.
 @param			textMarkupAttributes
 				An NSDictionary with keys representing text style attributes that are specifiable in text markup. Eligible keys are defined in <CoreMedia/CMTextMarkup.h>.
 @param			textSelector
				An identifier for the range or ranges of text to which the attributes should be applied. Eligible identifiers are determined by the format and content of the legible media. A value of nil indicates that the textMarkupAttributes should be applied as default styles for all text unless overridden by content markup or other applicable text selectors.
 @result		An instance of AVTextStyleRule
*/
+ (nullable AVTextStyleRule *)textStyleRuleWithTextMarkupAttributes:(NSDictionary<NSString *, id> *)textMarkupAttributes textSelector:(nullable NSString *)textSelector;

/*!
 @method		initWithTextMarkupAttributes:
 @abstract		Creates an instance of AVTextStyleRule with the specified text markup attributes.
 @param			textMarkupAttributes
 				An NSDictionary with keys representing text style attributes that are specifiable in text markup. Eligible keys are defined in <CoreMedia/CMTextMarkup.h>.
 @result		An instance of AVTextStyleRule
 @discussion	Equivalent to invoking -initWithTextMarkupAttributes:textSelector: with a value of nil for textSelector.
*/
- (nullable instancetype)initWithTextMarkupAttributes:(NSDictionary<NSString *, id> *)textMarkupAttributes;

/*!
 @method		initWithTextMarkupAttributes:textSelector:
 @abstract		Creates an instance of AVTextStyleRule with the specified text markup attributes and an identifier for the range or ranges of text to which the attributes should be applied.
 @param			textMarkupAttributes
 				An NSDictionary with keys representing text style attributes that are specifiable in text markup. Eligible keys are defined in <CoreMedia/CMTextMarkup.h>.
 @param			textSelector
				An identifier for the range or ranges of text to which the attributes should be applied. Eligible identifiers are determined by the format and content of the legible media. A value of nil indicates that the textMarkupAttributes should be applied as default styles for all text unless overridden by content markup or other applicable text selectors.
 @result		An instance of AVTextStyleRule
*/
- (nullable instancetype)initWithTextMarkupAttributes:(NSDictionary<NSString *, id> *)textMarkupAttributes textSelector:(nullable NSString *)textSelector NS_DESIGNATED_INITIALIZER;

/*!
 @property		textMarkupAttributes
 @abstract		An NSDictionary with keys representing text style attributes that are specifiable in text markup. Eligible keys and the expected types of their corresponding values are defined in <CoreMedia/CMTextMarkup.h>.
*/
@property (nonatomic, readonly) NSDictionary<NSString *, id> *textMarkupAttributes;

/*!
 @property		textSelector
 @abstract		A string that identifies the range or ranges of text to which the attributes should be applied. A value of nil indicates that the textMarkupAttributes should be applied as default styles for all text unless overridden by content markup or other applicable text selectors.
 @dicussion		The syntax of text selectors is determined by the format of the legible media. Eligible selectors may be determined by the content of the legible media (e.g. CSS selectors that are valid for a specific WebVTT document).
*/
@property (nonatomic, readonly, nullable) NSString *textSelector;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioChannelLayout.h
/*
	File:           AVAudioChannelLayout.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioChannelLayout.h>

// ==========  AVFoundation.framework/Headers/AVCaptureDataOutputSynchronizer.h
/*
    File:  AVCaptureDataOutputSynchronizer.h
 
    Framework:  AVFoundation
 
    Copyright 2016-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutput.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureDataOutputSynchronizer

@class AVCaptureDataOutputSynchronizerInternal;
@class AVCaptureSynchronizedDataCollection;
@protocol AVCaptureDataOutputSynchronizerDelegate;

/*!
 @class AVCaptureDataOutputSynchronizer
 @abstract
    AVCaptureDataOutputSynchronizer synchronizes the delivery of data from multiple capture data outputs (AVCaptureVideoDataOutput, AVCaptureDepthDataOutput, AVCaptureMetadataOutput, AVCaptureAudioDataOutput) to a single delegate callback.

 @discussion
    AVCaptureDataOutputSynchronizer is initialized with an array of data outputs (AVCaptureVideoDataOutput, AVCaptureDepthDataOutput, AVCaptureMetadataOutput, or AVCaptureAudioDataOutput) from which you'd like to receive a single, synchronized delegate callback. The first output in the array acts as the master data output and determines when the synchronized callback is delivered. When data is received for the master data output, it is held until all other data outputs have received data with an equal or later presentation time stamp, or it has been determined that there is no data for a particular output at the master data output's pts. Once all other outputs are ready, a single delegate callback is sent with all the data aligned with the master data output's data. Separate delegate callbacks are sent for any other data received with presentation time stamps earlier than the next master data output time.

    For instance, if you specify a video data output as your first (master) output and a metadata output for detected faces as your second output, your data callback will not be called until there is face data ready for a video frame, or it is assured that there is no face metadata for that particular video frame.
 
    Note that the AVCaptureDataOutputSynchronizer overrides each data output's -setSampleBufferDelegate:queue:, -setDepthDataDelegate:queue:, or -setMetadataObjectsDelegate:queue: method call. -[AVCaptureVideoDataOutput alwaysDiscardsLateVideoFrames] and -[AVCaptureDepthData alwaysDiscardsLateDepthData] properties are honored.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDataOutputSynchronizer : NSObject
{
@private
    AVCaptureDataOutputSynchronizerInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @method initWithDataOutputs:
 @abstract
    Instantiates an AVCaptureDataOutputSynchronizer from one or more capture data outputs.
 
 @param dataOutputs
    An array of capture data outputs where the first is the master.
 @result
    A newly initialized AVCaptureDataOutputSynchronizer instance.
 */
- (instancetype)initWithDataOutputs:(NSArray<AVCaptureOutput *> *)dataOutputs;

/*!
 @property dataOutputs
 @abstract
    The data outputs provided in the initializer method.
 */
@property(readonly, retain) NSArray<AVCaptureOutput *> *dataOutputs;

/*!
 @method setDelegate:queue:
 @abstract
    Sets the receiver's delegate that will accept synchronized data and the dispatch queue on which the delegate will be called.
 
 @param delegate
    An object conforming to the AVCaptureDataOutputSynchronizerDelegate protocol that will receive synchronized data from the provided data outputs.
 @param delegateCallbackQueue
    A dispatch queue on which all AVCaptureDataOutputSynchronizerDelegate methods will be called.
 
 @discussion
    AVCaptureDataOutputSynchronizer gathers data from its dataOutputs, and when it determines that all data has been received for a given timestamp, it calls the specified delegate on the specified delegateCallbackQueue. AVCaptureDataOutputSynchronizer overrides all the data outputs' delegates and callbacks. Data outputs under the control of AVCaptureDataOutputSynchronizer do not fire delegate callbacks. Delegate callbacks are restored to individual data outputs when you call this method with nil as your delegate and NULL as your delegateCallbackQueue.
 
    A serial dispatch queue must be used to guarantee that synchronized data will be delivered in order. The delegateCallbackQueue parameter may not be NULL, except when setting the delegate to nil.
 */
- (void)setDelegate:(nullable id<AVCaptureDataOutputSynchronizerDelegate>)delegate queue:(nullable dispatch_queue_t)delegateCallbackQueue;

/*!
 @property delegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureDataOutputSynchronizerDelegate protocol that will receive synchronized data output. The delegate is set using the -setDelegate:queue: method. This property is key-value observable.
 */
@property(nullable, nonatomic, readonly) id<AVCaptureDataOutputSynchronizerDelegate> delegate;

/*!
 @property delegateCallbackQueue
 @abstract
    The dispatch queue on which all AVCaptureDataOutputSynchronizerDelegate methods will be called.
 
 @discussion
    The value of this property is a dispatch_queue_t. The queue is set using the -setDelegate:queue: method.
 */
@property(nullable, nonatomic, readonly) dispatch_queue_t delegateCallbackQueue;

@end


API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureDataOutputSynchronizerDelegate <NSObject>

@required
/*!
 @method captureOutputSynchronizer:didOutputSynchronizedData:
 @abstract
    Called when an AVCaptureDataOutputSynchronizer instance outputs synchronized data from one or more data outputs.
 
 @param captureOutputSynchronizer
    The AVCaptureDataOutputSynchronizer instance delivering synchronized data.
 @param synchronizedDataCollection
    A collection of synchronized data objects indexed by data output.
 
 @discussion
    The synchronized data collection only contains synchronized data for capture outputs with synchronized data ready.
 */
- (void)dataOutputSynchronizer:(AVCaptureDataOutputSynchronizer *)synchronizer didOutputSynchronizedDataCollection:(AVCaptureSynchronizedDataCollection *)synchronizedDataCollection;

@end


#pragma mark - AVCaptureSynchronizedDataCollection

@class AVCaptureSynchronizedData;
@class AVCaptureSynchronizedDataCollectionInternal;

/*!
 @class AVCaptureSynchronizedDataCollection
 @abstract
    A collection of AVCaptureSynchronizedData objects.
 
 @discussion
    AVCaptureDataOutputSynchronizer's -dataOutputSynchronizer:didOutputSynchronizedDataCollection: delegate method delivers a collection of AVCaptureSynchronizedData objects which can be iterated by use AVCaptureOutput. AVCaptureSynchronizedDataCollection supports object subscripting and fast enumeration of the data outputs as keys.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSynchronizedDataCollection : NSObject <NSFastEnumeration>
{
@private
    AVCaptureSynchronizedDataCollectionInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @method synchronizedDataForCaptureOutput:
 @abstract
    Provides the synchronized data object for a given capture output.
 
 @param captureOutput
    The data output whose synchronized data you'd like to inspect.
 @result
    The synchronized data object associated with the provided output, or nil, if there is none.
 */
- (nullable AVCaptureSynchronizedData *)synchronizedDataForCaptureOutput:(AVCaptureOutput *)captureOutput;

/*!
 @method objectForKeyedSubscript:
 @abstract
    Method that provides support for object subscripting.
 
 @param key
    The data output whose synchronized data you'd like to inspect.
 @result
    The synchronized data object associated with the provided output, or nil, if there is none.
 
 @discussion
    AVCaptureSynchronizedDataCollection supports object subscripting. If you'd like to find the synchronized data for a given data output, simply:
        AVCaptureSynchronizedData *synchronizedData = synchronizedDataCollection[dataOutput];
 */
- (nullable AVCaptureSynchronizedData *)objectForKeyedSubscript:(AVCaptureOutput *)key;

/*!
 @property count
 @abstract
    The number of items in the collection.
 
 @discussion
    Returns the number of data output / synchronized data pairs present in the collection.
 */
@property(readonly) NSUInteger count;

@end


#pragma mark - AVCaptureSynchronizedData

@class AVCaptureSynchronizedDataInternal;

/*!
 @class AVCaptureSynchronizedData
 @abstract
    An abstract base class representing the data delivered by a data output through the AVCaptureDataOutputSynchronizer interface.
 
 @discussion
    AVCaptureDataOutputSynchronizer's -dataOutputSynchronizer:didOutputSynchronizedData: delegate callback delivers a dictionary of key/value pairs, with the keys being the AVCaptureOutput instances returning data, and the values being concrete subclasses of AVCaptureSynchronizedData.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSynchronizedData : NSObject
{
@private
    AVCaptureSynchronizedDataInternal *_synchronizedDataInternal;
}

AV_INIT_UNAVAILABLE

/*!
 @property timestamp
 @abstract
    The time at which this synchronized data was captured.
 
 @discussion
    Synchronized data is always clocked to the masterClock of the AVCaptureSession to which the data output is connected.
 */
@property(readonly) CMTime timestamp;

@end


#pragma mark - AVCaptureSynchronizedSampleBufferData

@class AVCaptureSynchronizedSampleBufferDataInternal;

/*!
 @class AVCaptureSynchronizedSampleBufferData
 @abstract
    An concrete subclass of AVCaptureSynchronizedData representing the data delivered by an AVCaptureVideoDataOutput or AVCaptureAudioDataOutput.

 @discussion
    Synchronized sample buffer data is valid for the duration of AVCaptureDataOutputSynchronizer's -dataOutputSynchronizer:didOutputSynchronizedData: delegate callback. To extend the sample buffer data beyond the callback, you must CFRetain it, and later call CFRelease when you're done with it.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSynchronizedSampleBufferData : AVCaptureSynchronizedData
{
@private
    AVCaptureSynchronizedSampleBufferDataInternal *_internal;
}

/*!
 @property sampleBuffer
 @abstract
    A sample buffer containing video or audio data.
 
 @discussion
    If sampleBufferWasDropped is YES, the returned sampleBuffer was dropped before it could be delivered to you, and thus this sample buffer is a shell containing metadata and format information, but no actual pixel data. This property is never NULL. If a data output has no data to return, it is simply not present in the dictionary of synchronized data returned by AVCaptureDataOutputSynchronizer's -dataOutputSynchronizer:didOutputSynchronizedData: delegate callback.
 */
@property(readonly) CMSampleBufferRef sampleBuffer;

/*!
 @property sampleBufferWasDropped
 @abstract
    YES if the sample buffer was dropped.
 
 @discussion
    AVCaptureVideoDataOutput has a delegate callback for dropped sample buffers. AVCaptureAudioDataOutput does not. Therefore, sampleBufferWasDropped may be YES for video, but never for audio.
 */
@property(readonly) BOOL sampleBufferWasDropped;

/*!
 @property droppedReason
 @abstract
    If sampleBufferWasDropped is YES, the reason for the drop, otherwise AVCaptureOutputDataDroppedReasonNone.
 
 @discussion
    AVCaptureOutputDataDroppedReasons are defined in AVCaptureOutputBase.h.
 */
@property(readonly) AVCaptureOutputDataDroppedReason droppedReason;

@end


#pragma mark - AVCaptureSynchronizedMetadataObjectData

@class AVCaptureSynchronizedMetadataObjectDataInternal;

/*!
 @class AVCaptureSynchronizedMetadataObjectData
 @abstract
    An concrete subclass of AVCaptureSynchronizedData representing the data delivered by an AVCaptureMetadataOutput.
 
 @discussion
    A single AVCaptureMetadataOutput may be configured to deliver multiple kinds of metadata objects (such as QRCodes and detected faces). AVCaptureSynchronizedMetadataObjectData's -metadataObjects array may contain multiple AVMetadataObject subclasses, depending on how the AVCaptureMetadataOutput was configured. All synchronized metadata objects share a common timestamp.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSynchronizedMetadataObjectData : AVCaptureSynchronizedData
{
@private
    AVCaptureSynchronizedMetadataObjectDataInternal *_internal;
}

/*!
 @property metadataObjects
 @abstract
    An array of AVMetadataObject subclasses.
 
 @discussion
    -metadataObjects is never nil. If no metadata objects are present for a given time, an empty array is returned.
 */
@property(readonly) NSArray<AVMetadataObject *> *metadataObjects;

@end


#pragma mark - AVCaptureSynchronizedDepthData

@class AVCaptureSynchronizedDepthDataInternal;

/*!
 @class AVCaptureSynchronizedDepthData
 @abstract
    An concrete subclass of AVCaptureSynchronizedData representing the data delivered by an AVCaptureDepthDataOutput.
 
 @discussion
    Depth data, like video, may be dropped if not serviced in a timely fashion.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSynchronizedDepthData : AVCaptureSynchronizedData
{
@private
    AVCaptureSynchronizedDepthDataInternal *_internal;
}

/*!
 @property depthData
 @abstract
    An instance of AVDepthData.
 
 @discussion
    If depthDataWasDropped is YES, the returned depthData was dropped before it could be delivered to you, and thus this AVDepthData is a shell containing format information and calibration data, but no actual pixel map data. This property is never nil. If a data output has no data to return, it is simply not present in the dictionary of synchronized data returned by AVCaptureDataOutputSynchronizer's -dataOutputSynchronizer:didOutputSynchronizedData: delegate callback.
 */
@property(readonly) AVDepthData *depthData;

/*!
 @property depthDataWasDropped
 @abstract
    YES if the depth data was dropped.
 
 @discussion
    If YES, inspect -droppedReason for the reason.
 */
@property(readonly) BOOL depthDataWasDropped;

/*!
 @property droppedReason
 @abstract
    If depthDataWasDropped is YES, the reason for the drop, otherwise AVCaptureOutputDataDroppedReasonNone.
 
 @discussion
    AVCaptureOutputDataDroppedReasons are defined in AVCaptureOutputBase.h.
 */
@property(readonly) AVCaptureOutputDataDroppedReason droppedReason;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitDelay.h
/*
	File:           AVAudioUnitDelay.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitDelay.h>

// ==========  AVFoundation.framework/Headers/AVAsset.h
/*
	File:  AVAsset.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <AVFoundation/AVAsynchronousKeyValueLoading.h>
#import <AVFoundation/AVContentKeySession.h>
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVMetadataFormat.h>

#import <CoreGraphics/CGAffineTransform.h>

#import <CoreMedia/CMTime.h>

#pragma mark --- AVAsset ---
/*!
  @class		AVAsset

  @abstract
	An AVAsset is an abstract class that defines AVFoundation's model for timed audiovisual media.

	Each asset contains a collection of tracks that are intended to be presented or processed together, each of a uniform media type, including but not limited to audio, video, text, closed captions, and subtitles.

  @discussion
	AVAssets are often instantiated via its concrete subclass AVURLAsset with NSURLs that refer to audiovisual media resources, such as streams (including HTTP live streams), QuickTime movie files, MP3 files, and files of other types.

	They can also be instantiated using other concrete subclasses that extend the basic model for audiovisual media in useful ways, as AVComposition does for temporal editing.

	Properties of assets as a whole are defined by AVAsset. Additionally, references to instances of AVAssetTracks representing tracks of the collection can be obtained, so that each of these can be examined independently.
					
	Because of the nature of timed audiovisual media, upon successful initialization of an AVAsset some or all of the values for its keys may not be immediately available. The value of any key can be requested at any time, and AVAsset will always return its value synchronously, although it may have to block the calling thread in order to do so.

	In order to avoid blocking, clients can register their interest in particular keys and to become notified when their values become available. For further details, see AVAsynchronousKeyValueLoading.h.

	On iOS, it is particularly important to avoid blocking.  To preserve responsiveness, a synchronous request that blocks for too long (eg, a property request on an asset on a slow HTTP server) may lead to media services being reset.

	To play an instance of AVAsset, initialize an instance of AVPlayerItem with it, use the AVPlayerItem to set up its presentation state (such as whether only a limited timeRange of the asset should be played, etc.), and provide the AVPlayerItem to an AVPlayer according to whether the items is to be played by itself or together with a collection of other items. Full details available in AVPlayerItem.h and AVPlayer.h.
					
	AVAssets can also be inserted into AVMutableCompositions in order to assemble audiovisual constructs from one or more source assets.

*/

NS_ASSUME_NONNULL_BEGIN

@class AVAssetTrack;
@class AVFragmentedAssetTrack;
@class AVMetadataItem;
@class AVMediaSelection;
@class AVCompositionTrack;
@class AVDisplayCriteria;
@class AVAssetInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAsset : NSObject <NSCopying, AVAsynchronousKeyValueLoading>
{
@private
	AVAssetInternal *_asset;
}

/*!
  @method		assetWithURL:
  @abstract		Returns an instance of AVAsset for inspection of a media resource.
  @param		URL
				An instance of NSURL that references a media resource.
  @result		An instance of AVAsset.
  @discussion	Returns a newly allocated instance of a subclass of AVAsset initialized with the specified URL.
*/
+ (instancetype)assetWithURL:(NSURL *)URL;

/*	Indicates the duration of the asset. If @"providesPreciseDurationAndTiming" is NO, a best-available estimate of the duration is returned. The degree of precision preferred for timing-related properties can be set at initialization time for assets initialized with URLs. See AVURLAssetPreferPreciseDurationAndTimingKey for AVURLAsset below.
*/
@property (nonatomic, readonly) CMTime duration;

/*	indicates the natural rate at which the asset is to be played; often but not always 1.0
*/
@property (nonatomic, readonly) float preferredRate;

/*	indicates the preferred volume at which the audible media of an asset is to be played; often but not always 1.0
*/
@property (nonatomic, readonly) float preferredVolume;

/*	indicates the preferred transform to apply to the visual content of the asset for presentation or processing; the value is often but not always the identity transform
*/
@property (nonatomic, readonly) CGAffineTransform preferredTransform;

/*	The following property is deprecated. Instead, use the naturalSize and preferredTransform, as appropriate, of the receiver's video tracks. See -tracksWithMediaType: below.
*/
@property (nonatomic, readonly) CGSize naturalSize NS_DEPRECATED(10_7, 10_8, 4_0, 5_0);

/*!
 @property	preferredDisplayCriteria
 @abstract	Guides to a display mode that is optimal for playing this particular asset.
 */
@property (nonatomic, readonly) AVDisplayCriteria *preferredDisplayCriteria API_AVAILABLE(tvos(11.2)) API_UNAVAILABLE(ios) API_UNAVAILABLE(macos, watchos);

@end


@interface AVAsset (AVAssetAsynchronousLoading)

/*	Indicates that the asset provides precise timing. See @"duration" above and AVURLAssetPreferPreciseDurationAndTimingKey below.
*/
@property (nonatomic, readonly) BOOL providesPreciseDurationAndTiming;

/*!
  @method		cancelLoading
  @abstract		Cancels the loading of all values for all observers.
  @discussion	Deallocation or finalization of an instance of AVAsset will implicitly cancel loading if any loading requests are still outstanding.
*/
- (void)cancelLoading;

@end


@interface AVAsset (AVAssetReferenceRestrictions)

/*!
  @enum			AVAssetReferenceRestrictions
  @abstract		These constants can be passed in to AVURLAssetReferenceRestrictionsKey to control the resolution of references to external media data.
 
  @constant		AVAssetReferenceRestrictionForbidNone
	Indicates that all types of references should be followed.
  @constant		AVAssetReferenceRestrictionForbidRemoteReferenceToLocal
	Indicates that references from a remote asset (e.g. referenced via http URL) to local media data (e.g. stored in a local file) should not be followed.
  @constant		AVAssetReferenceRestrictionForbidLocalReferenceToRemote
	Indicates that references from a local asset to remote media data should not be followed.
  @constant		AVAssetReferenceRestrictionForbidCrossSiteReference
	Indicates that references from a remote asset to remote media data stored at a different site should not be followed.
  @constant		AVAssetReferenceRestrictionForbidLocalReferenceToLocal
	Indicates that references from a local asset to local media data stored outside the asset's container file should not be followed.
  @constant		AVAssetReferenceRestrictionForbidAll
	Indicates that only references to media data stored within the asset's container file should be allowed.
*/
typedef NS_OPTIONS(NSUInteger, AVAssetReferenceRestrictions) {
	AVAssetReferenceRestrictionForbidNone = 0UL,
	AVAssetReferenceRestrictionForbidRemoteReferenceToLocal = (1UL << 0),
	AVAssetReferenceRestrictionForbidLocalReferenceToRemote = (1UL << 1),
	AVAssetReferenceRestrictionForbidCrossSiteReference = (1UL << 2),
	AVAssetReferenceRestrictionForbidLocalReferenceToLocal = (1UL << 3),
	AVAssetReferenceRestrictionForbidAll = 0xFFFFUL,
};

/*!
  @property		referenceRestrictions
  @abstract		Indicates the reference restrictions being used by the receiver.
  @discussion
	For AVURLAsset, this property reflects the value passed in for AVURLAssetReferenceRestrictionsKey, if any. See AVURLAssetReferenceRestrictionsKey below for a full discussion of reference restrictions. The default value for this property is AVAssetReferenceRestrictionForbidNone.
*/
@property (nonatomic, readonly) AVAssetReferenceRestrictions referenceRestrictions NS_AVAILABLE(10_7, 5_0);

@end


@class AVAssetTrackGroup;

@interface AVAsset (AVAssetTrackInspection)

/*!
  @property		tracks
  @abstract		Provides the array of AVAssetTracks contained by the asset
*/
@property (nonatomic, readonly) NSArray<AVAssetTrack *> *tracks;

/*!
  @method		trackWithTrackID:
  @abstract		Provides an instance of AVAssetTrack that represents the track of the specified trackID.
  @param		trackID
				The trackID of the requested AVAssetTrack.
  @result		An instance of AVAssetTrack; may be nil if no track of the specified trackID is available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (nullable AVAssetTrack *)trackWithTrackID:(CMPersistentTrackID)trackID;

/*!
  @method		tracksWithMediaType:
  @abstract		Provides an array of AVAssetTracks of the asset that present media of the specified media type.
  @param		mediaType
				The media type according to which AVAsset filters its AVAssetTracks. (Media types are defined in AVMediaFormat.h.)
  @result		An NSArray of AVAssetTracks; may be empty if no tracks of the specified media type are available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVAssetTrack *> *)tracksWithMediaType:(AVMediaType)mediaType;

/*!
  @method		tracksWithMediaCharacteristic:
  @abstract		Provides an array of AVAssetTracks of the asset that present media with the specified characteristic.
  @param		mediaCharacteristic
				The media characteristic according to which AVAsset filters its AVAssetTracks. (Media characteristics are defined in AVMediaFormat.h.)
  @result		An NSArray of AVAssetTracks; may be empty if no tracks with the specified characteristic are available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVAssetTrack *> *)tracksWithMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

/*!
 @property trackGroups
 @abstract
	All track groups in the receiver.
 
 @discussion
	The value of this property is an NSArray of AVAssetTrackGroups, each representing a different grouping of tracks in the receiver.
 */
@property (nonatomic, readonly) NSArray<AVAssetTrackGroup *> *trackGroups NS_AVAILABLE(10_9, 7_0);

@end


@interface AVAsset (AVAssetMetadataReading)

// high-level access to selected metadata of common interest

/* Indicates the creation date of the asset as an AVMetadataItem. May be nil. If a creation date has been stored by the asset in a form that can be converted to an NSDate, the dateValue property of the AVMetadataItem will provide an instance of NSDate. Otherwise the creation date is available only as a string value, via -[AVMetadataItem stringValue].
*/
@property (nonatomic, readonly, nullable) AVMetadataItem *creationDate NS_AVAILABLE(10_8, 5_0);

/* Provides access to the lyrics of the asset suitable for the current locale.
*/
@property (nonatomic, readonly, nullable) NSString *lyrics;

/* Provides access to an array of AVMetadataItems for each common metadata key for which a value is available; items can be filtered according to language via +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:] and according to identifier via +[AVMetadataItem metadataItemsFromArray:filteredByIdentifier:].
*/
@property (nonatomic, readonly) NSArray<AVMetadataItem *> *commonMetadata;

/* Provides access to an array of AVMetadataItems for all metadata identifiers for which a value is available; items can be filtered according to language via +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:] and according to identifier via +[AVMetadataItem metadataItemsFromArray:filteredByIdentifier:].
*/
@property (nonatomic, readonly) NSArray<AVMetadataItem *> *metadata NS_AVAILABLE(10_10, 8_0);

/* Provides an NSArray of NSStrings, each representing a metadata format that's available to the asset (e.g. ID3, iTunes metadata, etc.). Metadata formats are defined in AVMetadataFormat.h.
*/
@property (nonatomic, readonly) NSArray<AVMetadataFormat> *availableMetadataFormats;

/*!
  @method		metadataForFormat:
  @abstract		Provides an NSArray of AVMetadataItems, one for each metadata item in the container of the specified format; can subsequently be filtered according to language via +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:], according to locale via +[AVMetadataItem metadataItemsFromArray:withLocale:], or according to key via +[AVMetadataItem metadataItemsFromArray:withKey:keySpace:].
  @param		format
				The metadata format for which items are requested.
  @result		An NSArray containing AVMetadataItems; may be empty if there is no metadata of the specified format.
  @discussion	Becomes callable without blocking when the key @"availableMetadataFormats" has been loaded
*/
- (NSArray<AVMetadataItem *> *)metadataForFormat:(AVMetadataFormat)format;

@end


@class AVTimedMetadataGroup;

@interface AVAsset (AVAssetChapterInspection)

/* array of NSLocale
*/
@property (readonly) NSArray<NSLocale *> *availableChapterLocales NS_AVAILABLE(10_7, 4_3);

/*!
  @method		chapterMetadataGroupsWithTitleLocale:containingMetadataItemsWithCommonKeys:
  @abstract		Provides an array of chapters.
  @param		locale
				Locale of the metadata items carrying chapter titles to be returned (supports the IETF BCP 47 specification).
  @param		commonKeys
				Array of common keys of AVMetadataItem to be included; can be nil. 
				AVMetadataCommonKeyArtwork is the only supported key for now.
  @result		An NSArray of AVTimedMetadataGroup.
  @discussion	
	This method returns an array of AVTimedMetadataGroup objects. Each object in the array always contains an AVMetadataItem representing the chapter title; the timeRange property of the AVTimedMetadataGroup object is equal to the time range of the chapter title item.

	An AVMetadataItem with the specified common key will be added to an existing AVTimedMetadataGroup object if the time range (timestamp and duration) of the metadata item and the metadata group overlaps. The locale of items not carrying chapter titles need not match the specified locale parameter.
 
	Further filtering of the metadata items in AVTimedMetadataGroups according to language can be accomplished using +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:]; filtering of the metadata items according to locale can be accomplished using +[AVMetadataItem metadataItemsFromArray:withLocale:].
*/
- (NSArray<AVTimedMetadataGroup *> *)chapterMetadataGroupsWithTitleLocale:(NSLocale *)locale containingItemsWithCommonKeys:(nullable NSArray<AVMetadataKey> *)commonKeys NS_AVAILABLE(10_7, 4_3);

/*!
 @method		chapterMetadataGroupsBestMatchingPreferredLanguages:
 @abstract		Tests, in order of preference, for a match between language identifiers in the specified array of preferred languages and the available chapter locales, and returns the array of chapters corresponding to the first match that's found.
 @param			preferredLanguages
 An array of language identifiers in order of preference, each of which is an IETF BCP 47 (RFC 4646) language identifier. Use +[NSLocale preferredLanguages] to obtain the user's list of preferred languages.
 @result		An NSArray of AVTimedMetadataGroup.
 @discussion	
 Safe to call without blocking when the AVAsset key availableChapterLocales has status AVKeyValueStatusLoaded.
 
 Returns an array of AVTimedMetadataGroup objects. Each object in the array always contains an AVMetadataItem representing the chapter title; the timeRange property of the AVTimedMetadataGroup object is equal to the time range of the chapter title item.
 
 All of the available chapter metadata is included in the metadata groups, including items with the common key AVMetadataCommonKeyArtwork, if such items are present. Items not carrying chapter titles will be added to an existing AVTimedMetadataGroup object if the time range (timestamp and duration) of the metadata item and that of the metadata group overlaps. The locale of such items need not match the locale of the chapter titles.
 
 Further filtering of the metadata items in AVTimedMetadataGroups according to language can be accomplished using +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:]; filtering of the metadata items according to locale can be accomplished using +[AVMetadataItem metadataItemsFromArray:withLocale:].
.
*/
- (NSArray<AVTimedMetadataGroup *> *)chapterMetadataGroupsBestMatchingPreferredLanguages:(NSArray<NSString *> *)preferredLanguages NS_AVAILABLE(10_8, 6_0);


@end


@class AVMediaSelectionGroup;

@interface AVAsset (AVAssetMediaSelection)

/* Provides an NSArray of NSStrings, each NSString indicating a media characteristic for which a media selection option is available.
*/
@property (nonatomic, readonly) NSArray<AVMediaCharacteristic> *availableMediaCharacteristicsWithMediaSelectionOptions NS_AVAILABLE(10_8, 5_0);

/*!
  @method		mediaSelectionGroupForMediaCharacteristic:
  @abstract		Provides an instance of AVMediaSelectionGroup that contains one or more options with the specified media characteristic.
  @param		mediaCharacteristic
	A media characteristic for which you wish to obtain the available media selection options. AVMediaCharacteristicAudible, AVMediaCharacteristicLegible, and AVMediaCharacteristicVisual are currently supported.

	Pass AVMediaCharacteristicAudible to obtain the group of available options for audio media in various languages and for various purposes, such as descriptive audio.
	Pass AVMediaCharacteristicLegible to obtain the group of available options for subtitles in various languages and for various purposes.
	Pass AVMediaCharacteristicVisual to obtain the group of available options for video media.
  @result		An instance of AVMediaSelectionGroup. May be nil.
  @discussion
	Becomes callable without blocking when the key @"availableMediaCharacteristicsWithMediaSelectionOptions" has been loaded.

	If the asset has no AVMediaSelectionGroup containing options with the specified media characteristic, the return value will be nil.
	
	Filtering of the options in the returned AVMediaSelectionGroup according to playability, locale, and additional media characteristics can be accomplished using the category AVMediaSelectionOptionFiltering defined on AVMediaSelectionGroup.
*/
- (nullable AVMediaSelectionGroup *)mediaSelectionGroupForMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic NS_AVAILABLE(10_8, 5_0);

/*!
  @property		preferredMediaSelection
  @abstract		Provides an instance of AVMediaSelection with default selections for each of the receiver's media selection groups.
*/
@property (nonatomic, readonly) AVMediaSelection *preferredMediaSelection NS_AVAILABLE(10_11, 9_0);

/*!
  @property		allMediaSelections
  @abstract		Provides an array of all permutations of AVMediaSelection for this asset.
  @discussion
	Becomes callable without blocking when the key @"availableMediaCharacteristicsWithMediaSelectionOptions" has been loaded.
*/
@property (nonatomic, readonly) NSArray <AVMediaSelection *> *allMediaSelections NS_AVAILABLE(10_13, 11_0);

@end


@interface AVAsset (AVAssetProtectedContent)

/*!
  @property		hasProtectedContent
  @abstract		Indicates whether or not the asset has protected content.
  @discussion	Assets containing protected content may not be playable without successful authorization, even if the value of the "playable" property is YES.  See the properties in the AVAssetUsability category for details on how such an asset may be used.  On OS X, clients can use the interfaces in AVPlayerItemProtectedContentAdditions.h to request authorization to play the asset.
*/
@property (nonatomic, readonly) BOOL hasProtectedContent NS_AVAILABLE(10_7, 4_2);

@end


@interface AVAsset (AVAssetFragments)

/*!
  @property		canContainFragments
  @abstract		Indicates whether the asset is capable of being extended by fragments.
  @discussion	For QuickTime movie files and MPEG-4 files, the value of canContainFragments is YES if an 'mvex' box is present in the 'moov' box. For those types, the 'mvex' box signals the possible presence of later 'moof' boxes.
*/

@property (nonatomic, readonly) BOOL canContainFragments NS_AVAILABLE(10_11, 9_0);

/*!
  @property		containsFragments
  @abstract		Indicates whether the asset is extended by at least one fragment.
  @discussion	For QuickTime movie files and MPEG-4 files, the value of this property is YES if canContainFragments is YES and at least one 'moof' box is present after the 'moov' box.
*/
@property (nonatomic, readonly) BOOL containsFragments NS_AVAILABLE(10_11, 9_0);

/*!
  @property		overallDurationHint
  @abstract		Indicates the total duration of fragments that either exist now or may be appended in the future in order to extend the duration of the asset.
  @discussion	For QuickTime movie files and MPEG-4 files, the value of this property is obtained from the 'mehd' box of the 'mvex' box, if present. If no total fragment duration hint is available, the value of this property is kCMTimeInvalid.
*/
@property (nonatomic, readonly) CMTime overallDurationHint NS_AVAILABLE(10_12_2, 10_2);

@end


@interface AVAsset (AVAssetUsability)

/*!
 @property		playable
 @abstract		Indicates whether an AVPlayer can play the contents of the asset in a manner that meets user expectations.
 @discussion	A client can attempt playback when playable is NO, this however may lead to a substandard playback experience.
*/
@property (nonatomic, readonly, getter=isPlayable) BOOL playable NS_AVAILABLE(10_7, 4_3);

/* indicates whether an AVAssetExportSession can be used with the receiver for export
*/
@property (nonatomic, readonly, getter=isExportable) BOOL exportable NS_AVAILABLE(10_7, 4_3);

/* indicates whether an AVAssetReader can be used with the receiver for extracting media data
*/
@property (nonatomic, readonly, getter=isReadable) BOOL readable NS_AVAILABLE(10_7, 4_3);

/* indicates whether the receiver can be used to build an AVMutableComposition
*/
@property (nonatomic, readonly, getter=isComposable) BOOL composable NS_AVAILABLE(10_7, 4_3);

#if TARGET_OS_IPHONE

/* indicates whether the receiver can be written to the saved photos album
*/
@property (nonatomic, readonly, getter=isCompatibleWithSavedPhotosAlbum) BOOL compatibleWithSavedPhotosAlbum NS_AVAILABLE_IOS(5_0);

#endif	// TARGET_OS_IPHONE

/*!
  @property		compatibleWithAirPlayVideo
  @abstract		Indicates whether the asset is compatible with AirPlay Video.
  @discussion	YES if an AVPlayerItem initialized with the receiver can be played by an external device via AirPlay Video.
 */
@property (nonatomic, readonly, getter=isCompatibleWithAirPlayVideo) BOOL compatibleWithAirPlayVideo NS_AVAILABLE(10_11, 9_0);

@end


#pragma mark --- AVURLAsset ---
// Keys for options dictionary for use with -[AVURLAsset initWithURL:options:]

/*!
  @constant		AVURLAssetPreferPreciseDurationAndTimingKey
  @abstract
	Indicates whether the asset should be prepared to indicate a precise duration and provide precise random access by time.
	The value for this key is a boolean NSNumber.
  @discussion
	If nil is passed as the value of the options parameter to -[AVURLAsset initWithURL:options:], or if a dictionary that lacks a value for the key AVURLAssetPreferPreciseDurationAndTimingKey is passed instead, a default value of NO is assumed. If the asset is intended to be played only, because AVPlayer will support approximate random access by time when full precision isn't available, the default value of NO will suffice.
	Pass YES if longer loading times are acceptable in cases in which precise timing is required. If the asset is intended to be inserted into an AVMutableComposition, precise random access is typically desirable and the value of YES is recommended.
	Note that such precision may require additional parsing of the resource in advance of operations that make use of any portion of it, depending on the specifics of its container format. Many container formats provide sufficient summary information for precise timing and do not require additional parsing to prepare for it; QuickTime movie files and MPEG-4 files are examples of such formats. Other formats do not provide sufficient summary information, and precise random access for them is possible only after a preliminary examination of a file's contents.
	If you pass YES for an asset that you intend to play via an instance of AVPlayerItem and you are prepared for playback to commence before the value of -[AVPlayerItem duration] becomes available, you can omit the key @"duration" from the array of AVAsset keys you pass to -[AVPlayerItem initWithAsset:automaticallyLoadedAssetKeys:] in order to prevent AVPlayerItem from automatically loading the value of duration while the item becomes ready to play.
	If precise duration and timing is not possible for the timed media resource referenced by the asset's URL, AVAsset.providesPreciseDurationAndTiming will be NO even if precise timing is requested via the use of this key.
					
*/
AVF_EXPORT NSString *const AVURLAssetPreferPreciseDurationAndTimingKey NS_AVAILABLE(10_7, 4_0);

/*!
  @constant		AVURLAssetReferenceRestrictionsKey
  @abstract
	Indicates the restrictions used by the asset when resolving references to external media data. The value of this key is an NSNumber wrapping an AVAssetReferenceRestrictions enum value or the logical combination of multiple such values.
  @discussion
	Some assets can contain references to media data stored outside the asset's container file, for example in another file. This key can be used to specify a policy to use when these references are encountered. If an asset contains one or more references of a type that is forbidden by the reference restrictions, loading of asset properties will fail. In addition, such an asset cannot be used with other AVFoundation modules, such as AVPlayerItem or AVAssetExportSession.
*/
AVF_EXPORT NSString *const AVURLAssetReferenceRestrictionsKey NS_AVAILABLE(10_7, 5_0);

/*!
 @constant		AVURLAssetHTTPCookiesKey
 @abstract
	HTTP cookies that the AVURLAsset may send with HTTP requests
	Standard cross-site policy still applies: cookies will only be sent to domains to which they apply.
 @discussion
	By default, an AVURLAsset will only have access to cookies in the client's default cookie storage 
	that apply to the AVURLAsset's URL.  You can supplement the cookies available to the asset
	via use of this initialization option 
	
	HTTP cookies do not apply to non-HTTP(S) URLS.
	In HLS, many HTTP requests (e.g., media, crypt key, variant index) might be issued to different paths or hosts.
	In both of these cases, HTTP requests will be missing any cookies that do not apply to the AVURLAsset's URL.  
	This init option allows the AVURLAsset to use additional HTTP cookies for those HTTP(S) requests.
 */
AVF_EXPORT NSString *const AVURLAssetHTTPCookiesKey API_AVAILABLE(ios(8.0), tvos(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED;

/*
 @constant		AVURLAssetAllowsCellularAccessKey
 @abstract		Indicates whether network requests on behalf of this asset are allowed to use the cellular interface.
 @discussion
 	Default is YES.
*/
AVF_EXPORT NSString *const AVURLAssetAllowsCellularAccessKey API_AVAILABLE(ios(10.0), tvos(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED;

/*!
  @class		AVURLAsset

  @abstract		AVURLAsset provides access to the AVAsset model for timed audiovisual media referenced by URL.

  @discussion
	Note that although instances of AVURLAsset are immutable, values for its keys may not be immediately available without blocking. See the discussion of the class AVAsset above regarding the availability of values for keys and the use of AVAsynchronousKeyValueLoading.

	Once an AVURLAsset's value for a key is available, it will not change.  AVPlayerItem provides access to information that can change dynamically during playback; see AVPlayerItem.duration and AVPlayerItem.tracks.

	AVURLAssets can be initialized with NSURLs that refer to audiovisual media resources, such as streams (including HTTP live streams), QuickTime movie files, MP3 files, and files of other types.
*/
@class AVURLAssetInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVURLAsset : AVAsset
{
@private
	AVURLAssetInternal *_URLAsset;
}
AV_INIT_UNAVAILABLE

/*!
  @method		audiovisualTypes
  @abstract		Provides the file types the AVURLAsset class understands.
  @result		An NSArray of UTIs identifying the file types the AVURLAsset class understands.
*/
+ (NSArray<AVFileType> *)audiovisualTypes NS_AVAILABLE(10_7, 5_0);

/*!
  @method		audiovisualMIMETypes
  @abstract		Provides the MIME types the AVURLAsset class understands.
  @result		An NSArray of NSStrings containing MIME types the AVURLAsset class understands.
*/
+ (NSArray<NSString *> *)audiovisualMIMETypes NS_AVAILABLE(10_7, 5_0);

/*!
  @method		isPlayableExtendedMIMEType:
  @abstract		Returns YES if asset is playable with the codec(s) and container type specified in extendedMIMEType. Returns NO otherwise.
  @param		extendedMIMEType
  @result		YES or NO.
*/
+ (BOOL)isPlayableExtendedMIMEType: (NSString *)extendedMIMEType NS_AVAILABLE(10_7, 5_0);

/*!
  @method		URLAssetWithURL:options:
  @abstract		Returns an instance of AVURLAsset for inspection of a media resource.
  @param		URL
				An instance of NSURL that references a media resource.
  @param		options
				An instance of NSDictionary that contains keys for specifying options for the initialization of the AVURLAsset. See AVURLAssetPreferPreciseDurationAndTimingKey and AVURLAssetReferenceRestrictionsKey above.
  @result		An instance of AVURLAsset.
*/
+ (instancetype)URLAssetWithURL:(NSURL *)URL options:(nullable NSDictionary<NSString *, id> *)options;

/*!
  @method		initWithURL:options:
  @abstract		Initializes an instance of AVURLAsset for inspection of a media resource.
  @param		URL
				An instance of NSURL that references a media resource.
  @param		options
				An instance of NSDictionary that contains keys for specifying options for the initialization of the AVURLAsset. See AVURLAssetPreferPreciseDurationAndTimingKey and AVURLAssetReferenceRestrictionsKey above.
  @result		An instance of AVURLAsset.
*/
- (instancetype)initWithURL:(NSURL *)URL options:(nullable NSDictionary<NSString *, id> *)options NS_DESIGNATED_INITIALIZER;

/* indicates the URL with which the instance of AVURLAsset was initialized
*/
@property (nonatomic, readonly, copy) NSURL *URL;

@end


@class AVAssetResourceLoader;

@interface AVURLAsset (AVURLAssetURLHandling)

/*!
 @property resourceLoader
 @abstract
    Provides access to an instance of AVAssetResourceLoader, which offers limited control over the handling of URLs that may be loaded in the course of performing operations on the asset, such as playback.
    The loading of file URLs cannot be mediated via use of AVAssetResourceLoader.
    Note that copies of an AVAsset will vend the same instance of AVAssetResourceLoader.
*/
@property (nonatomic, readonly) AVAssetResourceLoader *resourceLoader NS_AVAILABLE(10_9, 6_0);

@end

@class AVAssetCache;

@interface AVURLAsset (AVURLAssetCache)

/*!
 @property	assetCache
 @abstract	Provides access to an instance of AVAssetCache to use for inspection of locally cached media data. Will be nil if an asset has not been configured to store or access media data from disk.
*/
@property (nonatomic, readonly, nullable) AVAssetCache *assetCache NS_AVAILABLE(10_12, 10_0);

@end

@interface AVURLAsset (AVAssetCompositionUtility )

/*!
  @method		compatibleTrackForCompositionTrack:
  @abstract		Provides a reference to an AVAssetTrack of the target from which any timeRange
				can be inserted into a mutable composition track (via -[AVMutableCompositionTrack insertTimeRange:ofTrack:atTime:error:]).
  @param		compositionTrack
				The composition track for which a compatible AVAssetTrack is requested.
  @result		an instance of AVAssetTrack
  @discussion
	Finds a track of the target with content that can be accommodated by the specified composition track.
	The logical complement of -[AVMutableComposition mutableTrackCompatibleWithTrack:].
*/
- (nullable AVAssetTrack *)compatibleTrackForCompositionTrack:(AVCompositionTrack *)compositionTrack;

@end

#pragma mark --- AVAsset change notifications ---

/*
	AVAsset change notifications are posted by instances of mutable subclasses, AVMutableComposition and AVMutableMovie.
	Some of the notifications are also posted by instances of dynamic subclasses, AVFragmentedAsset and AVFragmentedMovie, but these are capable of changing only in well-defined ways and only under specific conditions that you control. 
*/

/*!
 @constant       AVAssetDurationDidChangeNotification
 @abstract       Posted when the duration of an AVFragmentedAsset changes while it's being minded by an AVFragmentedAssetMinder, but only for changes that occur after the status of the value of @"duration" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetDurationDidChangeNotification NS_AVAILABLE(10_11, 9_0);

/*!
 @constant       AVAssetContainsFragmentsDidChangeNotification
 @abstract       Posted after the value of @"containsFragments" has already been loaded and the AVFragmentedAsset is added to an AVFragmentedAssetMinder, either when 1) fragments are detected in the asset on disk after it had previously contained none or when 2) no fragments are detected in the asset on disk after it had previously contained one or more.
*/
AVF_EXPORT NSString *const AVAssetContainsFragmentsDidChangeNotification API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos);

/*!
 @constant       AVAssetWasDefragmentedNotification
 @abstract       Posted when the asset on disk is defragmented while an AVFragmentedAsset is being minded by an AVFragmentedAssetMinder, but only if the defragmentation occurs after the status of the value of @"canContainFragments" has reached AVKeyValueStatusLoaded.
 @discussion     After this notification is posted, the value of the asset properties canContainFragments and containsFragments will both be NO.
*/
AVF_EXPORT NSString *const AVAssetWasDefragmentedNotification API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos);

/*!
 @constant       AVAssetChapterMetadataGroupsDidChangeNotification
 @abstract       Posted when the collection of arrays of timed metadata groups representing chapters of an AVAsset change and when any of the contents of the timed metadata groups change, but only for changes that occur after the status of the value of @"availableChapterLocales" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetChapterMetadataGroupsDidChangeNotification NS_AVAILABLE(10_11, 9_0);
/*!

 @constant       AVAssetMediaSelectionGroupsDidChangeNotification
 @abstract       Posted when the collection of media selection groups provided by an AVAsset changes and when any of the contents of its media selection groups change, but only for changes that occur after the status of the value of @"availableMediaCharacteristicsWithMediaSelectionOptions" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetMediaSelectionGroupsDidChangeNotification NS_AVAILABLE(10_11, 9_0);

#pragma mark --- AVFragmentedAsset ---
/*!
	@class			AVFragmentedAsset
 
	@abstract		A subclass of AVURLAsset that represents media resources that can be extended in total duration without modifying previously existing data structures.
	Such media resources include QuickTime movie files and MPEG-4 files that indicate, via an 'mvex' box in their 'moov' box, that they accommodate additional fragments. Media resources of other types may also be supported. To check whether a given instance of AVFragmentedAsset can be used to monitor the addition of fragments, check the value of the AVURLAsset property canContainFragments.
	An AVFragmentedAsset is capable of changing the values of certain of its properties and those of its tracks, while an operation that appends fragments to the underlying media resource in in progress, if the AVFragmentedAsset is associated with an instance of AVFragmentedAssetMinder.
	@discussion		While associated with an AVFragmentedAssetMinder, AVFragmentedAsset posts AVAssetDurationDidChangeNotification whenever new fragments are detected, as appropriate. It may also post AVAssetContainsFragmentsDidChangeNotification and AVAssetWasDefragmentedNotification, as discussed in documentation of those notifications.
*/

@protocol AVFragmentMinding

/*!
  @property		associatedWithFragmentMinder
  @abstract		Indicates whether an AVAsset that supports fragment minding is currently associated with a fragment minder, e.g. an instance of AVFragmentedAssetMinder.
  @discussion	AVAssets that support fragment minding post change notifications only while associated with a fragment minder.
*/
@property (nonatomic, readonly, getter=isAssociatedWithFragmentMinder) BOOL associatedWithFragmentMinder API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos);

@end

@class AVFragmentedAssetInternal;

API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos)
@interface AVFragmentedAsset : AVURLAsset <AVFragmentMinding>
{
@private
	AVFragmentedAssetInternal	*_fragmentedAsset __attribute__((unused));
}

/*!
  @method		fragmentedAssetWithURL:options:
  @abstract		Returns an instance of AVFragmentedAsset for inspection of a fragmented media resource.
  @param		URL
				An instance of NSURL that references a media resource.
  @param		options
				An instance of NSDictionary that contains keys for specifying options for the initialization of the AVFragmentedAsset. See AVURLAssetPreferPreciseDurationAndTimingKey and AVURLAssetReferenceRestrictionsKey above.
  @result		An instance of AVFragmentedAsset.
*/
+ (instancetype)fragmentedAssetWithURL:(NSURL *)URL options:(nullable NSDictionary<NSString *, id> *)options;

/*!
	@property       tracks
	@abstract       The tracks in an asset.
	@discussion     The value of this property is an array of tracks the asset contains; the tracks are of type AVFragmentedAssetTrack.
*/
@property (nonatomic, readonly) NSArray<AVFragmentedAssetTrack *> *tracks;

@end

@interface AVFragmentedAsset (AVFragmentedAssetTrackInspection)

/*!
  @method		trackWithTrackID:
  @abstract		Provides an instance of AVFragmentedAssetTrack that represents the track of the specified trackID.
  @param		trackID
				The trackID of the requested AVFragmentedAssetTrack.
  @result		An instance of AVFragmentedAssetTrack; may be nil if no track of the specified trackID is available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (nullable AVFragmentedAssetTrack *)trackWithTrackID:(CMPersistentTrackID)trackID;

/*!
  @method		tracksWithMediaType:
  @abstract		Provides an array of AVFragmentedAssetTracks of the asset that present media of the specified media type.
  @param		mediaType
				The media type according to which the receiver filters its AVFragmentedAssetTracks. (Media types are defined in AVMediaFormat.h)
  @result		An NSArray of AVFragmentedAssetTracks; may be empty if no tracks of the specified media type are available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVFragmentedAssetTrack *> *)tracksWithMediaType:(AVMediaType)mediaType;

/*!
  @method		tracksWithMediaCharacteristic:
  @abstract		Provides an array of AVFragmentedAssetTracks of the asset that present media with the specified characteristic.
  @param		mediaCharacteristic
				The media characteristic according to which the receiver filters its AVFragmentedAssetTracks. (Media characteristics are defined in AVMediaFormat.h)
  @result		An NSArray of AVFragmentedAssetTracks; may be empty if no tracks with the specified characteristic are available.
  @discussion	Becomes callable without blocking when the key @"tracks" has been loaded
*/
- (NSArray<AVFragmentedAssetTrack *> *)tracksWithMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

@end

#pragma mark --- AVFragmentedAssetMinder ---
/*!
	@class			AVFragmentedAssetMinder
	@abstract		A class that periodically checks whether additional fragments have been appended to fragmented assets.
*/

@class AVFragmentedAssetMinderInternal;

API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos)
@interface AVFragmentedAssetMinder : NSObject
{
@private
	AVFragmentedAssetMinderInternal	*_fragmentedAssetMinder;
}

/*!
	@method			fragmentedAssetMinderWithAsset:mindingInterval:
	@abstract       Creates an AVFragmentedAssetMinder, adds the specified asset to it, and sets the mindingInterval to the specified value.
	@param			asset
					An instance of AVFragmentedAsset to add to the AVFragmentedAssetMinder
	@param			mindingInterval
					The initial minding interval of the AVFragmentedAssetMinder.
	@result			A new instance of AVFragmentedAssetMinder.
*/
+ (instancetype)fragmentedAssetMinderWithAsset:(AVAsset<AVFragmentMinding> *)asset mindingInterval:(NSTimeInterval)mindingInterval;

/*!
	@method			initWithAsset:mindingInterval:
	@abstract       Creates an AVFragmentedAssetMinder, adds the specified asset to it, and sets the mindingInterval to the specified value.
	@param			asset
					An instance of AVFragmentedAsset to add to the AVFragmentedAssetMinder
	@param			mindingInterval
					The initial minding interval of the AVFragmentedAssetMinder.
	@result			A new instance of AVFragmentedAssetMinder.
*/
- (instancetype)initWithAsset:(AVAsset<AVFragmentMinding> *)asset mindingInterval:(NSTimeInterval)mindingInterval;

/*!
	@property       mindingInterval
	@abstract       An NSTimeInterval indicating how often a check for additional fragments should be performed. The default interval is 10.0.
*/
@property (nonatomic) NSTimeInterval mindingInterval;

/*!
	@property       assets
	@abstract       An NSArray of the AVFragmentedAsset objects being minded.
*/
@property (nonatomic, readonly) NSArray<AVAsset<AVFragmentMinding> *> *assets;

/*!
	@method			addFragmentedAsset:
	@abstract		Adds a fragmented asset to the array of assets being minded.
	@param			asset
					The fragmented asset to add to the minder.
*/
- (void)addFragmentedAsset:(AVAsset<AVFragmentMinding> *)asset;

/*!
	@method			removeFragmentedAsset:
	@abstract		Removes a fragmented asset from the array of assets being minded.
	@param			asset
					The fragmented asset to remove from the minder.
*/
- (void)removeFragmentedAsset:(AVAsset<AVFragmentMinding> *)asset;

@end

@interface AVURLAsset (AVURLAssetContentKeyEligibility) <AVContentKeyRecipient>

/*!
	@property 		mayRequireContentKeysForMediaDataProcessing
	@abstract		Allows AVURLAsset to be added as a content key recipient to an AVContentKeySession.
*/
@property (nonatomic, readonly) BOOL mayRequireContentKeysForMediaDataProcessing API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2), watchos(3.3));

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioEnvironmentNode.h
/*
	File:           AVAudioEnvironmentNode.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioEnvironmentNode.h>

// ==========  AVFoundation.framework/Headers/AVVideoComposition.h
/*
    File:  AVVideoComposition.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAsset.h>
#import <AVFoundation/AVVideoCompositing.h>

#import <Foundation/Foundation.h>
#import <CoreVideo/CoreVideo.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>

/*!
	@class		AVVideoComposition
 
	@abstract	An AVVideoComposition object represents an immutable video composition.
 
	@discussion	
		A video composition describes, for any time in the aggregate time range of its instructions, the number and IDs of video tracks that are to be used in order to produce a composed video frame corresponding to that time. When AVFoundation's built-in video compositor is used, the instructions an AVVideoComposition contain can specify a spatial transformation, an opacity value, and a cropping rectangle for each video source, and these can vary over time via simple linear ramping functions.
 
		A client can implement their own custom video compositor by implementing the AVVideoCompositing protocol; a custom video compositor is provided with pixel buffers for each of its video sources during playback and other operations and can perform arbitrary graphical operations on them in order to produce visual output.
*/

NS_ASSUME_NONNULL_BEGIN

@class AVVideoCompositionCoreAnimationTool;
@class AVVideoCompositionInternal;
@class AVVideoCompositionLayerInstruction;
@class AVVideoCompositionInstruction;
@class CIContext;
@class CIImage;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVVideoComposition : NSObject <NSCopying, NSMutableCopying> {
@private
    AVVideoCompositionInternal    *_videoComposition;
}

/*  
 @method		videoCompositionWithPropertiesOfAsset:
 @abstract
   Returns a new instance of AVVideoComposition with values and instructions suitable for presenting the video tracks of the specified asset according to its temporal and geometric properties and those of its tracks.
 @param			asset		An instance of AVAsset. Ensure that the duration and tracks properties of the asset are already loaded before invoking this method.
 @result		An instance of AVVideoComposition.
 @discussion
   The returned AVVideoComposition will have instructions that respect the spatial properties and timeRanges of the specified asset's video tracks.
   It will also have the following values for its properties:
   
   	- If the asset has exactly one video track, the original timing of the source video track will be used. If the asset has more than one video track, and the nominal frame rate of any of video tracks is known, the reciprocal of the greatest known nominalFrameRate will be used as the value of frameDuration. Otherwise, a default framerate of 30fps is used.
   	- If the specified asset is an instance of AVComposition, the renderSize will be set to the naturalSize of the AVComposition; otherwise the renderSize will be set to a value that encompasses all of the asset's video tracks.
   	- A renderScale of 1.0.
   	- A nil animationTool.
 
   If the specified asset has no video tracks, this method will return an AVVideoComposition instance with an empty collection of instructions.
 
*/
+ (AVVideoComposition *)videoCompositionWithPropertiesOfAsset:(AVAsset *)asset NS_AVAILABLE(10_9, 6_0);

/* indicates a custom compositor class to use. The class must implement the AVVideoCompositing protocol.
   If nil, the default, internal video compositor is used */
@property (nonatomic, readonly, nullable) Class<AVVideoCompositing> customVideoCompositorClass NS_AVAILABLE(10_9, 7_0);

/* indicates the interval which the video composition, when enabled, should render composed video frames */
@property (nonatomic, readonly) CMTime frameDuration;

/* If sourceTrackIDForFrameTiming is not kCMPersistentTrackID_Invalid, frame timing for the video composition is derived from the source asset's track with the corresponding ID. This may be used to preserve a source asset's variable frame timing. If an empty edit is encountered in the source asset’s track, the compositor composes frames as needed up to the frequency specified in frameDuration property. */
@property (nonatomic, readonly) CMPersistentTrackID sourceTrackIDForFrameTiming NS_AVAILABLE(10_13, 11_0);

/* indicates the size at which the video composition, when enabled, should render */
@property (nonatomic, readonly) CGSize renderSize;

/* indicates the scale at which the video composition should render. May only be other than 1.0 for a video composition set on an AVPlayerItem */
@property (nonatomic, readonly) float renderScale API_AVAILABLE(macos(10.14), ios(4.0), tvos(9.0)) __WATCHOS_PROHIBITED;

/* Indicates instructions for video composition via an NSArray of instances of classes implementing the AVVideoCompositionInstruction protocol.
   For the first instruction in the array, timeRange.start must be less than or equal to the earliest time for which playback or other processing will be attempted
   (note that this will typically be kCMTimeZero). For subsequent instructions, timeRange.start must be equal to the prior instruction's end time. The end time of
   the last instruction must be greater than or equal to the latest time for which playback or other processing will be attempted (note that this will often be
   the duration of the asset with which the instance of AVVideoComposition is associated).
*/
@property (nonatomic, readonly, copy) NSArray<id <AVVideoCompositionInstruction>> *instructions;

/* indicates a special video composition tool for use of Core Animation; may be nil */
@property (nonatomic, readonly, retain, nullable) AVVideoCompositionCoreAnimationTool *animationTool;

@end

/*
 @category	   AVVideoCompositionColorimetery
 @abstract
    Indicates the color space of the frames output from the video composition.
 @discussion
    Collectively the properties colorPrimaries, colorYCbCrMatrix, and colorTransferFunction define the color space that the rendered frames will be tagged with. For custom video compositing these properties are also used to specify the required color space of the source frames.

    For examples of common color spaces see AVVideoSettings.h.

    How to preserve the color space of the source frames:
 
        Decide which color space to be preserved by examining the source asset's video tracks. Copy the source track's primaries, matrix and transfer function into the video composition's colorPrimaries, colorYCbCrMatrix and colorTransferFunction respectively.

        - When using custom video compositing
			Setting these properties will cause source frames to be converted into the specified color space and tagged as such. New frames allocated using -[AVVideoCompositionRenderContext newPixelBuffer] will also be tagged correctly.

        - When using Core Image via videoCompositionWithAsset:options:applyingCIFiltersWithHandler:
			Setting these properties will cause source frames to be converted into the specified color space and tagged as such. The source frames provided as CIImages will have the appropriate CGColorSpace applied. The color space is preserved when the output CIImage is finally rendered internally.

        - When using basic compositing (i.e. AVVideoCompositionLayerInstruction)
			Setting these properties will ensure that the internal compositor renders (or passes through) frames in specified color space and are tagged as such.
*/
@interface AVVideoComposition (AVVideoCompositionColorimetery)

/*
 @property     colorPrimaries
 @abstract
    Rendering will use these primaries and frames will be tagged as such. If the value of this property is nil then the source's primaries will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoColorPrimariesKey. Generally set as a triple along with colorYCbCrMatrix and colorTransferFunction.
*/
@property (nonatomic, readonly, nullable) NSString *colorPrimaries NS_AVAILABLE(10_12, 10_0);

/*
 @property     colorYCbCrMatrix
 @abstract
    Rendering will use this matrix and frames will be tagged as such. If the value of this property is nil then the source's matrix will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoYCbCrMatrixKey. Generally set as a triple along with colorPrimaries and colorTransferFunction.
*/
@property (nonatomic, readonly, nullable) NSString *colorYCbCrMatrix NS_AVAILABLE(10_12, 10_0);

/*
 @property     colorTransferFunction
 @abstract
    Rendering will use this transfer function and frames will be tagged as such. If the value of this property is nil then the source's transfer function will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoTransferFunctionKey. Generally set as a triple along with colorYCbCrMatrix and colorYCbCrMatrix.
*/
@property (nonatomic, readonly, nullable) NSString *colorTransferFunction NS_AVAILABLE(10_12, 10_0);

@end

@interface AVVideoComposition (AVVideoCompositionFiltering)

/*  
 @method		videoCompositionWithAsset:options:applyingCIFiltersWithHandler:
 @abstract
	Returns a new instance of AVVideoComposition with values and instructions that will apply the specified handler block to video frames represented as instances of CIImage.
 @param			asset		An instance of AVAsset. For best performance, ensure that the duration and tracks properties of the asset are already loaded before invoking this method.
 @result		An instance of AVVideoComposition.
 @discussion
	The returned AVVideoComposition will cause the specified handler block to be called to filter each frame of the asset's first enabled video track. The handler block should use the properties of the provided AVAsynchronousCIImageFilteringRequest and respond using finishWithImage:context: with a "filtered" new CIImage (or the provided source image for no affect). In the event of an error, respond to the request using finishWithError:. The error can be observed via AVPlayerItemFailedToPlayToEndTimeNotification, see AVPlayerItemFailedToPlayToEndTimeErrorKey in notification payload.
 
	The video composition will also have the following values for its properties:

		- The original timing of the asset's first enabled video track will be used.
		- A renderSize that encompasses the asset's first enabled video track respecting the track's preferredTransform.
		- A renderScale of 1.0.

	The default CIContext has the following properties:

		- iOS: Device RGB color space
		- OS X: sRGB color space
 
	Example usage:

		playerItem.videoComposition = [AVVideoComposition videoCompositionWithAsset:srcAsset applyingCIFiltersWithHandler:
			^(AVAsynchronousCIImageFilteringRequest *request)
			{
				NSError *err = nil;
				CIImage *filtered = myRenderer(request, &err);
				if (filtered)
					[request finishWithImage:filtered context:nil];
				else
					[request finishWithError:err];
			}];
*/
+ (AVVideoComposition *)videoCompositionWithAsset:(AVAsset *)asset
			 applyingCIFiltersWithHandler:(void (^)(AVAsynchronousCIImageFilteringRequest *request))applier NS_AVAILABLE(10_11, 9_0);

@end


/*!
	@class		AVMutableVideoComposition
 
	@abstract	The AVMutableVideoComposition class is a mutable subclass of AVVideoComposition.
*/

@class AVMutableVideoCompositionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableVideoComposition : AVVideoComposition {
@private
    AVMutableVideoCompositionInternal    *_mutableVideoComposition __attribute__((unused));
}

/*  
 @method		videoComposition
 @abstract		Returns a new instance of AVMutableVideoComposition.
 @discussion
   The returned AVMutableVideoComposition will have a frameDuration of kCMTimeZero, a renderSize of {0.0, 0.0}, a nil array of instructions, and a nil animationTool.
*/
+ (AVMutableVideoComposition *)videoComposition;

/*  
 @method		videoCompositionWithPropertiesOfAsset:
 @abstract
   Returns a new instance of AVMutableVideoComposition with values and instructions suitable for presenting the video tracks of the specified asset according to its temporal and geometric properties and those of its tracks.
 @param			asset		An instance of AVAsset. For best performance, ensure that the duration and tracks properties of the asset are already loaded before invoking this method.
 @result		An instance of AVMutableVideoComposition.
 @discussion
   The returned AVMutableVideoComposition will have instructions that respect the spatial properties and timeRanges of the specified asset's video tracks. The client can set sourceTrackIDForFrameTiming to kCMPersistentTrackID_Invalid and frameDuration to an appropriate value in order to specify the maximum output frame rate independent of the source track timing.
   It will also have the following values for its properties:
   
   	- If the asset has exactly one video track, the original timing of the source video track will be used. If the asset has more than one video track, and the nominal frame rate of any of video tracks is known, the reciprocal of the greatest known nominalFrameRate will be used as the value of frameDuration. Otherwise, a default framerate of 30fps is used.
   	- If the specified asset is an instance of AVComposition, the renderSize will be set to the naturalSize of the AVComposition; otherwise the renderSize will be set to a value that encompasses all of the asset's video tracks.
   	- A renderScale of 1.0.
   	- A nil animationTool.

   If the specified asset has no video tracks, this method will return an AVMutableVideoComposition instance with an empty collection of instructions.
 
*/
+ (AVMutableVideoComposition *)videoCompositionWithPropertiesOfAsset:(AVAsset *)asset NS_AVAILABLE(10_9, 6_0);

/* indicates the custom compositor class to use. If nil, the default, internal video compositor is used */
@property (nonatomic, retain, nullable) Class<AVVideoCompositing> customVideoCompositorClass NS_AVAILABLE(10_9, 7_0);

/* indicates the interval which the video composition, when enabled, should render composed video frames */
@property (nonatomic) CMTime frameDuration;

/* If sourceTrackIDForFrameTiming is not kCMPersistentTrackID_Invalid, frame timing for the video composition is derived from the source asset's track with the corresponding ID. This may be used to preserve a source asset's variable frame timing. If an empty edit is encountered in the source asset’s track, the compositor composes frames as needed up to the frequency specified in frameDuration property. */
@property (nonatomic) CMPersistentTrackID sourceTrackIDForFrameTiming NS_AVAILABLE(10_13, 11_0);

/* indicates the size at which the video composition, when enabled, should render */
@property (nonatomic) CGSize renderSize;

/* indicates the scale at which the video composition should render. May only be other than 1.0 for a video composition set on an AVPlayerItem */
@property (nonatomic) float renderScale API_AVAILABLE(macos(10.14), ios(4.0), tvos(9.0)) __WATCHOS_PROHIBITED;

/* Indicates instructions for video composition via an NSArray of instances of classes implementing the AVVideoCompositionInstruction protocol.
   For the first instruction in the array, timeRange.start must be less than or equal to the earliest time for which playback or other processing will be attempted
   (note that this will typically be kCMTimeZero). For subsequent instructions, timeRange.start must be equal to the prior instruction's end time. The end time of
   the last instruction must be greater than or equal to the latest time for which playback or other processing will be attempted (note that this will often be
   the duration of the asset with which the instance of AVVideoComposition is associated).
*/
@property (nonatomic, copy) NSArray<id <AVVideoCompositionInstruction>> *instructions;

/* indicates a special video composition tool for use of Core Animation; may be nil */
@property (nonatomic, retain, nullable) AVVideoCompositionCoreAnimationTool *animationTool;

@end

/*
 @category	   AVMutableVideoCompositionColorimetery
 @abstract
    Indicates the color space of the frames output from the video composition.
 @discussion
    Collectively the properties colorPrimaries, colorYCbCrMatrix, and colorTransferFunction define the color space that the rendered frames will be tagged with. For custom video compositing these properties are also used to specify the required color space of the source frames.

    For examples of common color spaces see AVVideoSettings.h.

    How to preserve the color space of the source frames:
 
        Decide which color space to be preserved by examining the source asset's video tracks. Copy the source track's primaries, matrix and transfer function into the video composition's colorPrimaries, colorYCbCrMatrix and colorTransferFunction respectively.

        - When using custom video compositing
			Setting these properties will cause source frames to be converted into the specified color space and tagged as such. New frames allocated using -[AVVideoCompositionRenderContext newPixelBuffer] will also be tagged correctly.

        - When using Core Image via videoCompositionWithAsset:options:applyingCIFiltersWithHandler:
			Setting these properties will cause source frames to be converted into the specified color space and tagged as such. The source frames provided as CIImages will have the appropriate CGColorSpace applied. The color space is preserved when the output CIImage is finally rendered internally.

        - When using basic compositing (i.e. AVVideoCompositionLayerInstruction)
			Setting these properties will ensure that the internal compositor renders (or passes through) frames in specified color space and are tagged as such.
*/
@interface AVMutableVideoComposition (AVMutableVideoCompositionColorimetery)

/*
 @property     colorPrimaries
 @abstract
    Rendering will use these primaries and frames will be tagged as such. If the value of this property is nil then the source's primaries will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoColorPrimariesKey. Generally set as a triple along with colorYCbCrMatrix and colorTransferFunction.
*/
@property (nonatomic, copy, nullable) NSString *colorPrimaries NS_AVAILABLE(10_12, 10_0);

/*
 @property     colorYCbCrMatrix
 @abstract
    Rendering will use this matrix and frames will be tagged as such. If the value of this property is nil then the source's matrix will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoYCbCrMatrixKey. Generally set as a triple along with colorPrimaries and colorTransferFunction.
*/
@property (nonatomic, copy, nullable) NSString *colorYCbCrMatrix NS_AVAILABLE(10_12, 10_0);

/*
 @property     colorTransferFunction
 @abstract
    Rendering will use this transfer function and frames will be tagged as such. If the value of this property is nil then the source's transfer function will be propagated and used.
 @discussion
    Default is nil. Valid values are those suitable for AVVideoTransferFunctionKey. Generally set as a triple along with colorYCbCrMatrix and colorYCbCrMatrix.
*/
@property (nonatomic, copy, nullable) NSString *colorTransferFunction NS_AVAILABLE(10_12, 10_0);

@end

@interface AVMutableVideoComposition (AVMutableVideoCompositionFiltering)

/*  
 @method		videoCompositionWithAsset:options:applyingCIFiltersWithHandler:
 @abstract
	Returns a new instance of AVMutableVideoComposition with values and instructions that will apply the specified handler block to video frames represented as instances of CIImage.
 @param			asset		An instance of AVAsset. For best performance, ensure that the duration and tracks properties of the asset are already loaded before invoking this method.
 @result		An instance of AVMutableVideoComposition.
 @discussion
	The returned AVMutableVideoComposition will cause the specified handler block to be called to filter each frame of the asset's first enabled video track. The handler block should use the properties of the provided AVAsynchronousCIImageFilteringRequest and respond using finishWithImage:context: with a "filtered" new CIImage (or the provided source image for no affect). In the event of an error, respond to the request using finishWithError:. The error can be observed via AVPlayerItemFailedToPlayToEndTimeNotification, see AVPlayerItemFailedToPlayToEndTimeErrorKey in notification payload. The client can set sourceTrackIDForFrameTiming to kCMPersistentTrackID_Invalid and frameDuration to an appropriate value in order to specify the maximum output frame rate independent of the source track timing.
 
	The video composition will also have the following values for its properties:

		- The original timing of the asset's first enabled video track will be used.
		- A renderSize that encompasses the asset's first enabled video track respecting the track's preferredTransform.
		- A renderScale of 1.0.

	The default CIContext has the following properties:

		- iOS: Device RGB color space
		- OS X: sRGB color space
 
	Example usage:

		playerItem.videoComposition = [AVMutableVideoComposition videoCompositionWithAsset:srcAsset applyingCIFiltersWithHandler:
			^(AVAsynchronousCIImageFilteringRequest *request)
			{
				NSError *err = nil;
				CIImage *filtered = myRenderer(request, &err);
				if (filtered)
					[request finishWithImage:filtered context:nil];
				else
					[request finishWithError:err];
			}];
*/
+ (AVMutableVideoComposition *)videoCompositionWithAsset:(AVAsset *)asset
			 applyingCIFiltersWithHandler:(void (^)(AVAsynchronousCIImageFilteringRequest *request))applier NS_AVAILABLE(10_11, 9_0);

@end

/*!
	@class		AVVideoCompositionInstruction
 
	@abstract	An AVVideoCompositionInstruction object represents an operation to be performed by a compositor.
 
	@discussion
		An AVVideoComposition object maintains an array of instructions to perform its composition.
*/


@class AVVideoCompositionInstructionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVVideoCompositionInstruction : NSObject <NSSecureCoding, NSCopying, NSMutableCopying, AVVideoCompositionInstruction> {
@private
	AVVideoCompositionInstructionInternal	*_instruction;
}

/* Indicates the timeRange during which the instruction is effective. Note requirements for the timeRanges of instructions described in connection with AVVideoComposition's instructions key above. */
@property (nonatomic, readonly) CMTimeRange timeRange;

/* Indicates the background color of the composition. Solid BGRA colors only are supported; patterns and other color refs that are not supported will be ignored.
   If the background color is not specified the video compositor will use a default backgroundColor of opaque black.
   If the rendered pixel buffer does not have alpha, the alpha value of the backgroundColor will be ignored. */
@property (nonatomic, readonly, retain, nullable) __attribute__((NSObject)) CGColorRef backgroundColor;

/* Provides an array of instances of AVVideoCompositionLayerInstruction that specify how video frames from source tracks should be layered and composed.
   Tracks are layered in the composition according to the top-to-bottom order of the layerInstructions array; the track with trackID of the first instruction
   in the array will be layered on top, with the track with the trackID of the second instruction immediately underneath, etc.
   If this key is nil, the output will be a fill of the background color. */
@property (nonatomic, readonly, copy) NSArray<AVVideoCompositionLayerInstruction *> *layerInstructions;

/* If NO, indicates that post-processing should be skipped for the duration of this instruction.  YES by default.
   See +[AVVideoCompositionCoreAnimationTool videoCompositionToolWithPostProcessingAsVideoLayer:inLayer:].*/
@property (nonatomic, readonly) BOOL enablePostProcessing;

/* List of video track IDs required to compose frames for this instruction. The value of this property is computed from the layer instructions. */
@property (nonatomic, readonly) NSArray<NSValue *> *requiredSourceTrackIDs NS_AVAILABLE(10_9, 7_0);

/* If the video composition result is one of the source frames for the duration of the instruction, this property
   returns the corresponding track ID. The compositor won't be run for the duration of the instruction and the proper source
   frame will be used instead. The value of this property is computed from the layer instructions */
@property (nonatomic, readonly) CMPersistentTrackID passthroughTrackID NS_AVAILABLE(10_9, 7_0); // kCMPersistentTrackID_Invalid if not a passthrough instruction

@end

/*!
	@class		AVMutableVideoCompositionInstruction
 
	@abstract	An AVMutableVideoCompositionInstruction object represents an operation to be performed by a compositor.
 
	@discussion
		An AVVideoComposition object maintains an array of instructions to perform its composition.
*/

@class AVMutableVideoCompositionInstructionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableVideoCompositionInstruction : AVVideoCompositionInstruction {
@private
	AVMutableVideoCompositionInstructionInternal	*_mutableInstruction __attribute__((unused));
}

/*  
 @method		videoCompositionInstruction
 @abstract		Returns a new instance of AVMutableVideoCompositionInstruction.
 @discussion
   The returned AVMutableVideoCompositionInstruction will have a timeRange of kCMTimeRangeInvalid, a NULL backgroundColor, and a nil array of layerInstructions.
*/
+ (instancetype)videoCompositionInstruction;

/* Indicates the timeRange during which the instruction is effective. Note requirements for the timeRanges of instructions described in connection with AVVideoComposition's instructions key above. */
@property (nonatomic, assign) CMTimeRange timeRange;

/* Indicates the background color of the composition. Solid BGRA colors only are supported; patterns and other color refs that are not supported will be ignored.
   If the background color is not specified the video compositor will use a default backgroundColor of opaque black.
   If the rendered pixel buffer does not have alpha, the alpha value of the backgroundColor will be ignored. */
@property (nonatomic, retain, nullable) __attribute__((NSObject)) CGColorRef backgroundColor;

/* Provides an array of instances of AVVideoCompositionLayerInstruction that specify how video frames from source tracks should be layered and composed.
   Tracks are layered in the composition according to the top-to-bottom order of the layerInstructions array; the track with trackID of the first instruction
   in the array will be layered on top, with the track with the trackID of the second instruction immediately underneath, etc.
   If this key is nil, the output will be a fill of the background color. */
@property (nonatomic, copy) NSArray<AVVideoCompositionLayerInstruction *> *layerInstructions;

/* If NO, indicates that post-processing should be skipped for the duration of this instruction.  YES by default.
   See +[AVVideoCompositionCoreAnimationTool videoCompositionToolWithPostProcessingAsVideoLayer:inLayer:].*/
@property (nonatomic, assign) BOOL enablePostProcessing;

@end

/*!
	@class		AVVideoCompositionLayerInstruction
 
	@abstract	An AVVideoCompositionLayerInstruction object represents the transform, opacity, and cropping ramps to apply to a given track.
*/

@class AVVideoCompositionLayerInstructionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVVideoCompositionLayerInstruction : NSObject <NSSecureCoding, NSCopying, NSMutableCopying> {
@private
	AVVideoCompositionLayerInstructionInternal	*_layerInstruction;
};

/* Indicates the trackID of the source track to which the compositor will apply the instruction. */
@property (nonatomic, readonly, assign) CMPersistentTrackID trackID;

/*  
 @method		getTransformRampForTime:startTransform:endTransform:timeRange:
 @abstract		Obtains the transform ramp that includes the specified time.
 @param			time
   If a ramp with a timeRange that contains the specified time has been set, information about the effective ramp for that time is supplied.
   Otherwise, information about the first ramp that starts after the specified time is supplied.
 @param			startTransform
   A pointer to a float to receive the starting transform value for the transform ramp. May be NULL.
 @param			endTransform
   A pointer to a float to receive the ending transform value for the transform ramp. May be NULL.
 @param			timeRange
   A pointer to a CMTimeRange to receive the timeRange of the transform ramp. May be NULL.
 @result
   An indication of success. NO will be returned if the specified time is beyond the duration of the last transform ramp that has been set.
*/
- (BOOL)getTransformRampForTime:(CMTime)time startTransform:(nullable CGAffineTransform *)startTransform endTransform:(nullable CGAffineTransform *)endTransform timeRange:(nullable CMTimeRange *)timeRange;

/*  
 @method		getOpacityRampForTime:startOpacity:endOpacity:timeRange:
 @abstract		Obtains the opacity ramp that includes the specified time.
 @param			time
   If a ramp with a timeRange that contains the specified time has been set, information about the effective ramp for that time is supplied.
   Otherwise, information about the first ramp that starts after the specified time is supplied.
 @param			startOpacity
   A pointer to a float to receive the starting opacity value for the opacity ramp. May be NULL.
 @param			endOpacity
   A pointer to a float to receive the ending opacity value for the opacity ramp. May be NULL.
 @param			timeRange
   A pointer to a CMTimeRange to receive the timeRange of the opacity ramp. May be NULL.
 @result
   An indication of success. NO will be returned if the specified time is beyond the duration of the last opacity ramp that has been set.
*/
- (BOOL)getOpacityRampForTime:(CMTime)time startOpacity:(nullable float *)startOpacity endOpacity:(nullable float *)endOpacity timeRange:(nullable CMTimeRange *)timeRange;

/*  
 @method		getCropRectangleRampForTime:startCropRectangle:endCropRectangle:timeRange:
 @abstract		Obtains the crop rectangle ramp that includes the specified time.
 @param			time
   If a ramp with a timeRange that contains the specified time has been set, information about the effective ramp for that time is supplied.
   Otherwise, information about the first ramp that starts after the specified time is supplied.
 @param			startCropRectangle
   A pointer to a CGRect to receive the starting crop rectangle value for the crop rectangle ramp. May be NULL.
 @param			endCropRecrangle
   A pointer to a CGRect to receive the ending crop rectangle value for the crop rectangle ramp. May be NULL.
 @param			timeRange
   A pointer to a CMTimeRange to receive the timeRange of the crop rectangle ramp. May be NULL.
 @result
   An indication of success. NO will be returned if the specified time is beyond the duration of the last crop rectangle ramp that has been set.
*/
- (BOOL)getCropRectangleRampForTime:(CMTime)time startCropRectangle:(nullable CGRect *)startCropRectangle endCropRectangle:(nullable CGRect *)endCropRectangle timeRange:(nullable CMTimeRange *)timeRange NS_AVAILABLE(10_9, 7_0);

@end

/*!
	@class		AVMutableVideoCompositionLayerInstruction
 
	@abstract	AVMutableVideoCompositionLayerInstruction is a mutable subclass of AVVideoCompositionLayerInstruction that is used to modify the transform, cropping, and opacity ramps to apply to a given track in a composition.
*/

@class AVMutableVideoCompositionLayerInstructionInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableVideoCompositionLayerInstruction : AVVideoCompositionLayerInstruction {
@private
	AVMutableVideoCompositionLayerInstructionInternal	*_mutableLayerInstruction __attribute__((unused));
};

/*  
 @method		videoCompositionLayerInstructionWithAssetTrack:
 @abstract		Returns a new instance of AVMutableVideoCompositionLayerInstruction with no transform or opacity ramps and a trackID set to the specified track's trackID.
 @param			track
   A reference to an AVAssetTrack.
*/
+ (instancetype)videoCompositionLayerInstructionWithAssetTrack:(AVAssetTrack *)track;

/*  
 @method		videoCompositionLayerInstruction
 @abstract		Returns a new instance of AVMutableVideoCompositionLayerInstruction with no transform or opacity ramps and a trackID initialized to kCMPersistentTrackID_Invalid.
*/
+ (instancetype)videoCompositionLayerInstruction;

/* Indicates the trackID of the source track to which the compositor will apply the instruction. */
@property (nonatomic, assign) CMPersistentTrackID trackID;

/*  
 @method		setTransformRampFromStartTransform:toEndTransform:timeRange:
 @abstract		Sets a transform ramp to apply during the specified timerange.
 @param			startTransform
   The transform to be applied at the starting time of the timeRange. See the discussion below of how transforms are applied to video frames.
 @param			endTransform
   The transform to be applied at the end time of the timeRange.
 @param			timeRange
   The timeRange over which the value of the transform will be interpolated between startTransform and endTransform.
 @discussion
   For purposes of spatial positioning of video frames, the origin is in the top-left corner, so
   (a) positive translation values in an affine transform move a video frame right and down; and
   (b) with an identity transform a video frame is positioned with its top-left corner in the top-left corner of the composited frame.
   Video frames shall be interpreted at their display sizes (as described by CVImageBufferGetDisplaySize,
   ie, taking pixel aspect ratio attachments into account) before any affine transform is applied.

   During a transform ramp, the affine transform is interpolated between the values set at the ramp's start time and end time.
   Before the first specified time for which a transform is set, the affine transform is held constant at the value of CGAffineTransformIdentity;
   after the last time for which a transform is set, the affine transform is held constant at that last value;
*/
- (void)setTransformRampFromStartTransform:(CGAffineTransform)startTransform toEndTransform:(CGAffineTransform)endTransform timeRange:(CMTimeRange)timeRange;

/*  
 @method		setTransform:atTime:
 @abstract		Sets a value of the transform at a time within the timeRange of the instruction.
 @param			transform
   The transform to be applied at the specified time. See the discussion below of how transforms are applied to video frames.
 @param			time
   A time value within the timeRange of the composition instruction.
 @discussion
   For purposes of spatial positioning of video frames, the origin is in the top-left corner, so
   (a) positive translation values in an affine transform move a video frame right and down; and
   (b) with an identity transform a video frame is positioned with its top-left corner in the top-left corner of the composited frame.
   Video frames shall be interpreted at their display sizes (as described by CVImageBufferGetDisplaySize,
   ie, taking pixel aspect ratio attachments into account) before any affine transform is applied.

   Sets a fixed transform to apply from the specified time until the next time at which a transform is set; this is the same as setting a flat ramp for that time range.
   Before the first specified time for which a transform is set, the affine transform is held constant at the value of CGAffineTransformIdentity;
   after the last time for which a transform is set, the affine transform is held constant at that last value;
*/
- (void)setTransform:(CGAffineTransform)transform atTime:(CMTime)time;

/*  
 @method		setOpacityRampFromStartOpacity:toEndOpacity:timeRange:
 @abstract		Sets an opacity ramp to apply during the specified timerange.
 @param			startOpacity
   The opacity to be applied at the starting time of the timeRange. The value must be between 0.0 and 1.0.
 @param			endOpacity
   The opacity to be applied at the end time of the timeRange. The value must be between 0.0 and 1.0.
 @param			timeRange
   The timeRange over which the value of the opacity will be interpolated between startOpacity and endOpacity.
 @discussion
   During an opacity ramp, opacity is computed using a linear interpolation.
   Before the first time for which an opacity is set, the opacity is held constant at 1.0; after the last specified time, the opacity is held constant at the last value.
*/
- (void)setOpacityRampFromStartOpacity:(float)startOpacity toEndOpacity:(float)endOpacity timeRange:(CMTimeRange)timeRange;

/*  
 @method		setOpacity:atTime:
 @abstract		Sets a value of the opacity at a time within the timeRange of the instruction.
 @param			opacity
   The opacity to be applied at the specified time. The value must be between 0.0 and 1.0.
 @param			time
   A time value within the timeRange of the composition instruction.
 @discussion
   Sets a fixed opacity to apply from the specified time until the next time at which an opacity is set; this is the same as setting a flat ramp for that time range.
   Before the first time for which an opacity is set, the opacity is held constant at 1.0; after the last specified time, the opacity is held constant at the last value.
*/
- (void)setOpacity:(float)opacity atTime:(CMTime)time;

/*  
 @method		setCropRectangleRampFromStartCropRectangle:toEndCropRectangle:timeRange:
 @abstract		Sets an crop rectangle ramp to apply during the specified timerange.
 @param			startCropRectangle
   The crop rectangle to be applied at the starting time of the timeRange. See the discussion below of how crop rectangles are applied to video frames.
 @param			endCropRectangle
   The crop rectangle to be applied at the end time of the timeRange.
 @param			timeRange
   The timeRange over which the value of the opacity will be interpolated between startCropRectangle and endCropRectangle.
 @discussion
   The origin of the crop rectangle is the top-left corner of the buffer clean aperture rectangle. The crop rectangle is defined in
   square pixel space, i.e. without taking the pixel aspect ratio into account. Crop rectangles extending outside of the clean aperture,
   are cropped to the clean aperture.

   During a crop rectangle ramp, the rectangle is interpolated between the values set at the ramp's start time and end time. 
   When the starting or ending rectangle is empty, interpolations take into account the origin and size of the empty rectangle.
   Before the first specified time for which a crop rectangle is set, the crop rectangle is held constant to CGRectInfinite
   after the last time for which a crop rectangle is set, the crop rectangle is held constant at that last value.
*/
- (void)setCropRectangleRampFromStartCropRectangle:(CGRect)startCropRectangle toEndCropRectangle:(CGRect)endCropRectangle timeRange:(CMTimeRange)timeRange NS_AVAILABLE(10_9, 7_0);

/*  
 @method		setCropRectangle:atTime:
 @abstract		Sets a value of the crop rectangle at a time within the timeRange of the instruction.
 @param			cropRectangle
   The crop rectangle to be applied at the specified time. See the discussion below of how crop rectangles are applied to video frames.
 @param			time
   A time value within the timeRange of the composition instruction.
 @discussion
   The origin of the crop rectangle is the top-left corner of the buffer clean aperture rectangle. The crop rectangle is defined in
   square pixel space, i.e. without taking the pixel aspect ratio into account. Crop rectangles extending outside of the clean aperture,
   are cropped to the clean aperture.

   Sets a fixed crop rectangle to apply from the specified time until the next time at which a crop rectangle is set; this is the same as setting a flat ramp for that time range.
   Before the first specified time for which a crop rectangle is set, the crop rectangle is held constant to CGRectInfinite
   after the last time for which a crop rectangle is set, the crop rectangle is held constant at that last value.
*/
- (void)setCropRectangle:(CGRect)cropRectangle atTime:(CMTime)time NS_AVAILABLE(10_9, 7_0);

@end



/*!
    @class			AVVideoCompositionCoreAnimationTool

    @abstract		A tool for using Core Animation in a video composition.
    
 @discussion
   Instances of AVVideoCompositionCoreAnimationTool are for use with offline rendering (AVAssetExportSession and AVAssetReader), not with AVPlayer.
   To synchronize real-time playback with other CoreAnimation layers, use AVSynchronizedLayer.

   Any animations will be interpreted on the video's timeline, not real-time, so 
		(a) set animation beginTimes to small positive value such as AVCoreAnimationBeginTimeAtZero rather than 0, 
		    because CoreAnimation will replace a value of 0 with CACurrentMediaTime(); 
		(b) set removedOnCompletion to NO on animations so they are not automatically removed;
		(c) do not use layers associated with UIViews.
*/

@class CALayer;
@class AVVideoCompositionCoreAnimationToolInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVVideoCompositionCoreAnimationTool : NSObject {
@private
	AVVideoCompositionCoreAnimationToolInternal	*_videoCompositionTool;
}

/*!
 	@method						videoCompositionCoreAnimationToolWithAdditionalLayer:asTrackID:
	@abstract					Add a Core Animation layer to the video composition
	@discussion					Include a Core Animation layer as an individual track input in video composition.
								This layer should not come from, or be added to, another layer tree.
								trackID should not match any real trackID in the source. Use -[AVAsset unusedTrackID] 
								to obtain a trackID that's guaranteed not to coincide with the trackID of any track of the asset.
								AVVideoCompositionInstructions should reference trackID where the rendered animation should be included.
								For best performance, no transform should be set in the AVVideoCompositionLayerInstruction for this trackID.
								Be aware that on iOS, CALayers backing a UIView usually have their content flipped (as defined by the
								-contentsAreFlipped method). It may be required to insert a CALayer with its geometryFlipped property set
								to YES in the layer hierarchy to get the same result when attaching a CALayer to a AVVideoCompositionCoreAnimationTool
								as when using it to back a UIView.
*/
+ (instancetype)videoCompositionCoreAnimationToolWithAdditionalLayer:(CALayer *)layer asTrackID:(CMPersistentTrackID)trackID;

/*!
	@method						videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer:inLayer:
	@abstract					Compose the composited video frames with the Core Animation layer
	@discussion					Place composited video frames in videoLayer and render animationLayer 
								to produce the final frame. Normally videoLayer should be in animationLayer's sublayer tree.
								The animationLayer should not come from, or be added to, another layer tree.
								Be aware that on iOS, CALayers backing a UIView usually have their content flipped (as defined by the
								-contentsAreFlipped method). It may be required to insert a CALayer with its geometryFlipped property set
								to YES in the layer hierarchy to get the same result when attaching a CALayer to a AVVideoCompositionCoreAnimationTool
								as when using it to back a UIView.
*/
+ (instancetype)videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer:(CALayer *)videoLayer inLayer:(CALayer *)animationLayer;

/*!
	@method						videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayers:inLayer:
	@abstract					Compose the composited video frames with the Core Animation layer
	@discussion					Duplicate the composited video frames in each videoLayer and render animationLayer 
								to produce the final frame. Normally videoLayers should be in animationLayer's sublayer tree.
								The animationLayer should not come from, or be added to, another layer tree.
								Be aware that on iOS, CALayers backing a UIView usually have their content flipped (as defined by the
								-contentsAreFlipped method). It may be required to insert a CALayer with its geometryFlipped property set
								to YES in the layer hierarchy to get the same result when attaching a CALayer to a AVVideoCompositionCoreAnimationTool
								as when using it to back a UIView.
*/
+ (instancetype)videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayers:(NSArray<CALayer *> *)videoLayers inLayer:(CALayer *)animationLayer NS_AVAILABLE(10_9, 7_0);

@end


@interface AVAsset (AVAssetVideoCompositionUtility)

- (CMPersistentTrackID)unusedTrackID;

@end


@protocol AVVideoCompositionValidationHandling;

@interface AVVideoComposition (AVVideoCompositionValidation)

/*!
 @method		isValidForAsset:timeRange:validationDelegate:
 @abstract
   Indicates whether the timeRanges of the receiver's instructions conform to the requirements described for them immediately above (in connection with the instructions property) and also whether all of the layer instructions have a value for trackID that corresponds either to a track of the specified asset or to the receiver's animationTool. 
 @param			asset
    Pass a reference to an AVAsset if you wish to validate the timeRanges of the instructions against the duration of the asset and the trackIDs of the layer instructions against the asset's tracks. Pass nil to skip that validation. Clients should ensure that the keys @"tracks" and @"duration" are already loaded on the AVAsset before validation is attempted.
 @param			timeRange
   A CMTimeRange.  Only those instuctions with timeRanges that overlap with the specified timeRange will be validated. To validate all instructions that may be used for playback or other processing, regardless of timeRange, pass CMTimeRangeMake(kCMTimeZero, kCMTimePositiveInfinity).
 @param			validationDelegate
   Indicates an object implementing the AVVideoCompositionValidationHandling protocol to receive information about troublesome portions of a video composition during processing of -isValidForAsset:. May be nil.
@discussion
   In the course of validation, the receiver will invoke its validationDelegate with reference to any trouble spots in the video composition.
   An exception will be raised if the delegate modifies the receiver's array of instructions or the array of layerInstructions of any AVVideoCompositionInstruction contained therein during validation.
*/
- (BOOL)isValidForAsset:(nullable AVAsset *)asset timeRange:(CMTimeRange)timeRange validationDelegate:(nullable id<AVVideoCompositionValidationHandling>)validationDelegate NS_AVAILABLE(10_8, 5_0);

@end

@protocol AVVideoCompositionValidationHandling <NSObject>

@optional

/*!
 @method		videoComposition:shouldContinueValidatingAfterFindingInvalidValueForKey:
 @abstract
   Invoked by an instance of AVVideoComposition when validating an instance of AVVideoComposition, to report a key that has an invalid value.
 @result
   An indication of whether the AVVideoComposition should continue validation in order to report additional problems that may exist.
*/
- (BOOL)videoComposition:(AVVideoComposition *)videoComposition shouldContinueValidatingAfterFindingInvalidValueForKey:(NSString *)key NS_AVAILABLE(10_8, 5_0);

/*!
 @method		videoComposition:shouldContinueValidatingAfterFindingEmptyTimeRange:
 @abstract
   Invoked by an instance of AVVideoComposition when validating an instance of AVVideoComposition, to report a timeRange that has no corresponding video composition instruction.
 @result
   An indication of whether the AVVideoComposition should continue validation in order to report additional problems that may exist.
*/
- (BOOL)videoComposition:(AVVideoComposition *)videoComposition shouldContinueValidatingAfterFindingEmptyTimeRange:(CMTimeRange)timeRange NS_AVAILABLE(10_8, 5_0);

/*!
 @method		videoComposition:shouldContinueValidatingAfterFindingInvalidTimeRangeInInstruction:
 @abstract
   Invoked by an instance of AVVideoComposition when validating an instance of AVVideoComposition, to report a video composition instruction with a timeRange that's invalid, that overlaps with the timeRange of a prior instruction, or that contains times earlier than the timeRange of a prior instruction.
 @discussion
   Use CMTIMERANGE_IS_INVALID, defined in CMTimeRange.h, to test whether the timeRange itself is invalid. Refer to headerdoc for AVVideoComposition.instructions for a discussion of how timeRanges for instructions must be formulated.
 @result
   An indication of whether the AVVideoComposition should continue validation in order to report additional problems that may exist.
*/
- (BOOL)videoComposition:(AVVideoComposition *)videoComposition shouldContinueValidatingAfterFindingInvalidTimeRangeInInstruction:(id<AVVideoCompositionInstruction>)videoCompositionInstruction NS_AVAILABLE(10_8, 5_0);

/*!
 @method		videoComposition:shouldContinueValidatingAfterFindingInvalidTrackIDInInstruction:layerInstruction:asset:
 @abstract
   Invoked by an instance of AVVideoComposition when validating an instance of AVVideoComposition, to report a video composition layer instruction with a trackID that does not correspond either to the trackID used for the composition's animationTool or to a track of the asset specified in -[AVVideoComposition isValidForAsset:timeRange:delegate:].
 @result
   An indication of whether the AVVideoComposition should continue validation in order to report additional problems that may exist.
*/
- (BOOL)videoComposition:(AVVideoComposition *)videoComposition shouldContinueValidatingAfterFindingInvalidTrackIDInInstruction:(id<AVVideoCompositionInstruction>)videoCompositionInstruction layerInstruction:(AVVideoCompositionLayerInstruction *)layerInstruction asset:(AVAsset *)asset NS_AVAILABLE(10_8, 5_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetReaderOutput.h
/*
    File:  AVAssetReaderOutput.h

	Framework:  AVFoundation
 
    Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVVideoComposition.h>
#import <AVFoundation/AVAudioProcessingSettings.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMSampleBuffer.h>

@class AVAssetTrack;
@class AVAudioMixInputParameters;
@class AVAudioMix;
@class AVVideoComposition;
@class AVAssetReaderOutputInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @class AVAssetReaderOutput
 @abstract
	AVAssetReaderOutput is an abstract class that defines an interface for reading a single collection of samples of a common media type from an AVAssetReader.
 
 @discussion
	Clients can read the media data of an asset by adding one or more concrete instances of AVAssetReaderOutput to an AVAssetReader using the -[AVAssetReader addOutput:] method.
	
	IMPORTANT PERFORMANCE NOTE: Make sure to set the alwaysCopiesSampleData property to NO if you do not need to modify the sample data in-place, to avoid unnecessary and inefficient copying.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetReaderOutput : NSObject
{
@private
	AVAssetReaderOutputInternal	*_internal;
}

/*!
 @property mediaType
 @abstract
	The media type of the samples that can be read from the receiver.

 @discussion
	The value of this property is one of the media type strings defined in AVMediaFormat.h.
 */
@property (nonatomic, readonly) AVMediaType mediaType;

/*!
 @property alwaysCopiesSampleData
 @abstract
	Indicates whether or not the data in buffers gets copied before being vended to the client.
 
 @discussion
	When the value of this property is YES, the AVAssetReaderOutput will always vend a buffer with copied data to the client.  Data in such buffers can be freely modified by the client. When the value of this property is NO, the buffers vended to the client may not be copied.  Such buffers may still be referenced by other entities. The result of modifying a buffer whose data hasn't been copied is undefined.  Requesting buffers whose data hasn't been copied when possible can lead to performance improvements.
 
	The default value is YES.
 */
@property (nonatomic) BOOL alwaysCopiesSampleData NS_AVAILABLE(10_8, 5_0);

/*!
 @method copyNextSampleBuffer
 @abstract
	Copies the next sample buffer for the output synchronously.

 @result
	A CMSampleBuffer object referencing the output sample buffer.

 @discussion
	The client is responsible for calling CFRelease on the returned CMSampleBuffer object when finished with it. This method will return NULL if there are no more sample buffers available for the receiver within the time range specified by its AVAssetReader's timeRange property, or if there is an error that prevents the AVAssetReader from reading more media data. When this method returns NULL, clients should check the value of the associated AVAssetReader's status property to determine why no more samples could be read.
 */
- (nullable CMSampleBufferRef)copyNextSampleBuffer CF_RETURNS_RETAINED;

@end


@interface AVAssetReaderOutput (AVAssetReaderOutputRandomAccess)

/*!
 @property supportsRandomAccess
 @abstract
	Indicates whether the asset reader output supports reconfiguration of the time ranges to read.
 
 @discussion
	When the value of this property is YES, the time ranges read by the asset reader output can be reconfigured during reading using the -resetForReadingTimeRanges: method.  This also prevents the attached AVAssetReader from progressing to AVAssetReaderStatusCompleted until -markConfigurationAsFinal has been invoked.
 
	The default value is NO, which means that the asset reader output may not be reconfigured once reading has begin.  When the value of this property is NO, AVAssetReader may be able to read media data more efficiently, particularly when multiple asset reader outputs are attached.
 
	This property may not be set after -startReading has been called on the attached asset reader.
 */
@property (nonatomic) BOOL supportsRandomAccess NS_AVAILABLE(10_10, 8_0);

/*!
 @method resetForReadingTimeRanges:
 @abstract
	Starts reading over with a new set of time ranges.
 
 @param timeRanges
	An NSArray of NSValue objects, each representing a single CMTimeRange structure
 
 @discussion
	This method may only be used if supportsRandomAccess has been set to YES and may not be called after -markConfigurationAsFinal has been invoked.
 
	This method is often used in conjunction with AVAssetWriter multi-pass (see AVAssetWriterInput category AVAssetWriterInputMultiPass).  In this usage, the caller will invoke -copyNextSampleBuffer until that method returns NULL and then ask the AVAssetWriterInput for a set of time ranges from which it thinks media data should be re-encoded.  These time ranges are then given to this method to set up the asset reader output for the next pass.
 
	The time ranges set here override the time range set on AVAssetReader.timeRange.  Just as with that property, for each time range in the array the intersection of that time range and CMTimeRangeMake(kCMTimeZero, asset.duration) will take effect.  If the start times of each time range in the array are not strictly increasing or if two or more time ranges in the array overlap, an NSInvalidArgumentException will be raised.  It is an error to include a time range with a non-numeric start time or duration (see CMTIME_IS_NUMERIC), unless the duration is kCMTimePositiveInfinity.
 
	If this method is invoked after the status of the attached AVAssetReader has become AVAssetReaderStatusFailed or AVAssetReaderStatusCancelled, no change in status will occur and the result of the next call to -copyNextSampleBuffer will be NULL.
 
	If this method is invoked before all media data has been read (i.e. -copyNextSampleBuffer has not yet returned NULL), an exception will be thrown.  This method may not be called before -startReading has been invoked on the attached asset reader.
 */
- (void)resetForReadingTimeRanges:(NSArray<NSValue *> *)timeRanges NS_AVAILABLE(10_10, 8_0);

/*!
 @method markConfigurationAsFinal
 @abstract
	Informs the receiver that no more reconfiguration of time ranges is necessary and allows the attached AVAssetReader to advance to AVAssetReaderStatusCompleted.
 
 @discussion
	When the value of supportsRandomAccess is YES, the attached asset reader will not advance to AVAssetReaderStatusCompleted until this method is called.
 
	When the destination of media data vended by the receiver is an AVAssetWriterInput configured for multi-pass encoding, a convenient time to invoke this method is after the asset writer input indicates that no more passes will be performed.
 
	Once this method has been called, further invocations of -resetForReadingTimeRanges: are disallowed.
 */
- (void)markConfigurationAsFinal NS_AVAILABLE(10_10, 8_0);

@end

@class AVAssetReaderTrackOutputInternal;

/*!
 @class AVAssetReaderTrackOutput
 @abstract
	AVAssetReaderTrackOutput is a concrete subclass of AVAssetReaderOutput that defines an interface for reading media data from a single AVAssetTrack of an AVAssetReader's AVAsset.
 
 @discussion
	Clients can read the media data of an asset track by adding an instance of AVAssetReaderTrackOutput to an AVAssetReader using the -[AVAssetReader addOutput:] method. The track's media samples can either be read in the format in which they are stored in the asset, or they can be converted to a different format.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetReaderTrackOutput : AVAssetReaderOutput
{
@private
	AVAssetReaderTrackOutputInternal	*_trackOutputInternal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderTrackOutputWithTrack:outputSettings:
 @abstract
	Returns an instance of AVAssetReaderTrackOutput for reading from the specified track and supplying media data according to the specified output settings.

 @param track
	The AVAssetTrack from which the resulting AVAssetReaderTrackOutput should read sample buffers.
 @param outputSettings
	An NSDictionary of output settings to be used for sample output.  See AVAudioSettings.h for available output settings for audio tracks or AVVideoSettings.h for available output settings for video tracks and also for more information about how to construct an output settings dictionary.
 @result
	An instance of AVAssetReaderTrackOutput.

 @discussion
	The track must be one of the tracks contained by the target AVAssetReader's asset.
	
	A value of nil for outputSettings configures the output to vend samples in their original format as stored by the specified track.  Initialization will fail if the output settings cannot be used with the specified track.
	
	AVAssetReaderTrackOutput can only produce uncompressed output.  For audio output settings, this means that AVFormatIDKey must be kAudioFormatLinearPCM.  For video output settings, this means that the dictionary must follow the rules for uncompressed video output, as laid out in AVVideoSettings.h.  AVAssetReaderTrackOutput does not support the AVAudioSettings.h key AVSampleRateConverterAudioQualityKey or the following AVVideoSettings.h keys:
 
		AVVideoCleanApertureKey
		AVVideoPixelAspectRatioKey
		AVVideoScalingModeKey
	
	When constructing video output settings the choice of pixel format will affect the performance and quality of the decompression. For optimal performance when decompressing video the requested pixel format should be one that the decoder supports natively to avoid unnecessary conversions. Below are some recommendations:

	For H.264 use kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, or kCVPixelFormatType_420YpCbCr8BiPlanarFullRange if the video is known to be full range.  For JPEG on iOS, use kCVPixelFormatType_420YpCbCr8BiPlanarFullRange.

	For other codecs on OSX, kCVPixelFormatType_422YpCbCr8 is the preferred pixel format for video and is generally the most performant when decoding. If you need to work in the RGB domain then kCVPixelFormatType_32BGRA is recommended on iOS and kCVPixelFormatType_32ARGB is recommended on OSX.
 
	ProRes encoded media can contain up to 12bits/ch. If your source is ProRes encoded and you wish to preserve more than 8bits/ch during decompression then use one of the following pixel formats: kCVPixelFormatType_4444AYpCbCr16, kCVPixelFormatType_422YpCbCr16, kCVPixelFormatType_422YpCbCr10, or kCVPixelFormatType_64ARGB.  AVAssetReader does not support scaling with any of these high bit depth pixel formats. If you use them then do not specify kCVPixelBufferWidthKey or kCVPixelBufferHeightKey in your outputSettings dictionary. If you plan to append these sample buffers to an AVAssetWriterInput then note that only the ProRes encoders support these pixel formats.

	ProRes 4444 encoded media can contain a mathematically lossless alpha channel. To preserve the alpha channel during decompression use a pixel format with an alpha component such as kCVPixelFormatType_4444AYpCbCr16 or kCVPixelFormatType_64ARGB. To test whether your source contains an alpha channel check that the track's format description has kCMFormatDescriptionExtension_Depth and that its value is 32.
 */
+ (instancetype)assetReaderTrackOutputWithTrack:(AVAssetTrack *)track outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings;

/*!
 @method initWithTrack:outputSettings:
 @abstract
	Returns an instance of AVAssetReaderTrackOutput for reading from the specified track and supplying media data according to the specified output settings.

 @param track
	The AVAssetTrack from which the resulting AVAssetReaderTrackOutput should read sample buffers.
 @param outputSettings
	An NSDictionary of output settings to be used for sample output.  See AVAudioSettings.h for available output settings for audio tracks or AVVideoSettings.h for available output settings for video tracks and also for more information about how to construct an output settings dictionary.
 @result
	An instance of AVAssetReaderTrackOutput.

 @discussion
	The track must be one of the tracks contained by the target AVAssetReader's asset.
	
	A value of nil for outputSettings configures the output to vend samples in their original format as stored by the specified track.  Initialization will fail if the output settings cannot be used with the specified track.
	
	AVAssetReaderTrackOutput can only produce uncompressed output.  For audio output settings, this means that AVFormatIDKey must be kAudioFormatLinearPCM.  For video output settings, this means that the dictionary must follow the rules for uncompressed video output, as laid out in AVVideoSettings.h.  AVAssetReaderTrackOutput does not support the AVAudioSettings.h key AVSampleRateConverterAudioQualityKey or the following AVVideoSettings.h keys:
 
		AVVideoCleanApertureKey
		AVVideoPixelAspectRatioKey
		AVVideoScalingModeKey

	When constructing video output settings the choice of pixel format will affect the performance and quality of the decompression. For optimal performance when decompressing video the requested pixel format should be one that the decoder supports natively to avoid unnecessary conversions. Below are some recommendations:

	For H.264 use kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, or kCVPixelFormatType_420YpCbCr8BiPlanarFullRange if the video is known to be full range.  For JPEG on iOS, use kCVPixelFormatType_420YpCbCr8BiPlanarFullRange.

	For other codecs on OSX, kCVPixelFormatType_422YpCbCr8 is the preferred pixel format for video and is generally the most performant when decoding. If you need to work in the RGB domain then kCVPixelFormatType_32BGRA is recommended on iOS and kCVPixelFormatType_32ARGB is recommended on OSX.
 
	ProRes encoded media can contain up to 12bits/ch. If your source is ProRes encoded and you wish to preserve more than 8bits/ch during decompression then use one of the following pixel formats: kCVPixelFormatType_4444AYpCbCr16, kCVPixelFormatType_422YpCbCr16, kCVPixelFormatType_422YpCbCr10, or kCVPixelFormatType_64ARGB.  AVAssetReader does not support scaling with any of these high bit depth pixel formats. If you use them then do not specify kCVPixelBufferWidthKey or kCVPixelBufferHeightKey in your outputSettings dictionary. If you plan to append these sample buffers to an AVAssetWriterInput then note that only the ProRes encoders support these pixel formats.

	ProRes 4444 encoded media can contain a mathematically lossless alpha channel. To preserve the alpha channel during decompression use a pixel format with an alpha component such as kCVPixelFormatType_4444AYpCbCr16 or kCVPixelFormatType_64ARGB.  To test whether your source contains an alpha channel check that the track's format description has kCMFormatDescriptionExtension_Depth and that its value is 32.
 */
- (instancetype)initWithTrack:(AVAssetTrack *)track outputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings NS_DESIGNATED_INITIALIZER;

/*!
 @property track
 @abstract
	The track from which the receiver reads sample buffers.

 @discussion
	The value of this property is an AVAssetTrack owned by the target AVAssetReader's asset.
 */
@property (nonatomic, readonly) AVAssetTrack *track;

/*!
 @property outputSettings
 @abstract
	The output settings used by the receiver.

 @discussion
	The value of this property is an NSDictionary that contains values for keys as specified by either AVAudioSettings.h for audio tracks or AVVideoSettings.h for video tracks.  A value of nil indicates that the receiver will vend samples in their original format as stored in the target track.
 */ 
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *outputSettings;

/*!
 @property audioTimePitchAlgorithm
 @abstract
	Indicates the processing algorithm used to manage audio pitch for scaled audio edits.
 
 @discussion
	Constants for various time pitch algorithms, e.g. AVAudioTimePitchAlgorithmSpectral, are defined in AVAudioProcessingSettings.h.  An NSInvalidArgumentException will be raised if this property is set to a value other than the constants defined in that file.
 
	The default value is AVAudioTimePitchAlgorithmSpectral.
 */
@property (nonatomic, copy) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_9, 7_0);

@end


@class AVAssetReaderAudioMixOutputInternal;

/*!
 @class AVAssetReaderAudioMixOutput
 @abstract
	AVAssetReaderAudioMixOutput is a concrete subclass of AVAssetReaderOutput that defines an interface for reading audio samples that result from mixing the audio from one or more AVAssetTracks of an AVAssetReader's AVAsset.
 
 @discussion
	Clients can read the audio data mixed from one or more asset tracks by adding an instance of AVAssetReaderAudioMixOutput to an AVAssetReader using the -[AVAssetReader addOutput:] method.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetReaderAudioMixOutput : AVAssetReaderOutput
{
@private
	AVAssetReaderAudioMixOutputInternal	*_audioMixOutputInternal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderAudioMixOutputWithAudioTracks:audioSettings:
 @abstract
	Returns an instance of AVAssetReaderAudioMixOutput for reading mixed audio from the specified audio tracks, with optional audio settings.

 @param tracks
	An NSArray of AVAssetTrack objects from which the created object should read sample buffers to be mixed.
 @param audioSettings
	An NSDictionary of audio settings to be used for audio output.
 @result
	An instance of AVAssetReaderAudioMixOutput.

 @discussion
	Each track must be one of the tracks owned by the target AVAssetReader's asset and must be of media type AVMediaTypeAudio.
	
	The audio settings dictionary must contain values for keys in AVAudioSettings.h (linear PCM only). A value of nil configures the output to return samples in a convenient uncompressed format, with sample rate and other properties determined according to the properties of the specified audio tracks. Initialization will fail if the audio settings cannot be used with the specified tracks.  AVSampleRateConverterAudioQualityKey is not supported.
 */
+ (instancetype)assetReaderAudioMixOutputWithAudioTracks:(NSArray<AVAssetTrack *> *)audioTracks audioSettings:(nullable NSDictionary<NSString *, id> *)audioSettings;

/*!
 @method initWithAudioTracks:audioSettings:
 @abstract
	Creates an instance of AVAssetReaderAudioMixOutput for reading mixed audio from the specified audio tracks, with optional audio settings.

 @param tracks
	An NSArray of AVAssetTrack objects from which the created object should read sample buffers to be mixed.
 @param audioSettings
	An NSDictionary of audio settings to be used for audio output.
 @result
	An instance of AVAssetReaderAudioMixOutput.

 @discussion
	Each track must be one of the tracks owned by the target AVAssetReader's asset and must be of media type AVMediaTypeAudio.
	
	The audio settings dictionary must contain values for keys in AVAudioSettings.h (linear PCM only). A value of nil configures the output to return samples in a convenient uncompressed format, with sample rate and other properties determined according to the properties of the specified audio tracks. Initialization will fail if the audio settings cannot be used with the specified tracks.  AVSampleRateConverterAudioQualityKey is not supported.
 */
- (instancetype)initWithAudioTracks:(NSArray<AVAssetTrack *> *)audioTracks audioSettings:(nullable NSDictionary<NSString *, id> *)audioSettings NS_DESIGNATED_INITIALIZER;

/*!
 @property audioTracks
 @abstract
	The tracks from which the receiver reads mixed audio.

 @discussion
	The value of this property is an NSArray of AVAssetTracks owned by the target AVAssetReader's asset.
 */
@property (nonatomic, readonly) NSArray<AVAssetTrack *> *audioTracks;

/*!
 @property audioSettings
 @abstract
	The audio settings used by the receiver.

 @discussion
	The value of this property is an NSDictionary that contains values for keys from AVAudioSettings.h (linear PCM only).  A value of nil indicates that the receiver will return audio samples in a convenient uncompressed format, with sample rate and other properties determined according to the properties of the receiver's audio tracks.
 */ 
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *audioSettings;

/*!
 @property audioMix
 @abstract
	The audio mix used by the receiver.

 @discussion
	The value of this property is an AVAudioMix that can be used to specify how the volume of audio samples read from each source track will change over the timeline of the source asset.
 
	This property cannot be set after reading has started.
 */
@property (nonatomic, copy, nullable) AVAudioMix *audioMix;

/*!
 @property audioTimePitchAlgorithm
 @abstract
	Indicates the processing algorithm used to manage audio pitch for scaled audio edits.
 
 @discussion
	Constants for various time pitch algorithms, e.g. AVAudioTimePitchAlgorithmSpectral, are defined in AVAudioProcessingSettings.h.  An NSInvalidArgumentException will be raised if this property is set to a value other than the constants defined in that file.
 
	The default value is AVAudioTimePitchAlgorithmSpectral.
 */
@property (nonatomic, copy) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_9, 7_0);

@end


@class AVAssetReaderVideoCompositionOutputInternal;

/*!
 @class AVAssetReaderVideoCompositionOutput
 @abstract
	AVAssetReaderVideoCompositionOutput is a concrete subclass of AVAssetReaderOutput that defines an interface for reading video frames that have been composited together from the frames in one or more AVAssetTracks of an AVAssetReader's AVAsset.
 
 @discussion
	Clients can read the video frames composited from one or more asset tracks by adding an instance of AVAssetReaderVideoCompositionOutput to an AVAssetReader using the -[AVAssetReader addOutput:] method.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetReaderVideoCompositionOutput : AVAssetReaderOutput
{
@private
	AVAssetReaderVideoCompositionOutputInternal	*_videoCompositionOutputInternal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderVideoCompositionOutputWithVideoTracks:videoSettings:
 @abstract
	Creates an instance of AVAssetReaderVideoCompositionOutput for reading composited video from the specified video tracks and supplying media data according to the specified video settings.

 @param tracks
	An NSArray of AVAssetTrack objects from which the resulting AVAssetReaderVideoCompositionOutput should read video frames for compositing.
 @param videoSettings
	An NSDictionary of video settings to be used for video output.  See AVVideoSettings.h for more information about how to construct a video settings dictionary.
 @result
	An instance of AVAssetReaderVideoCompositionOutput.

 @discussion
	Each track must be one of the tracks owned by the target AVAssetReader's asset and must be of media type AVMediaTypeVideo.
 	
	A value of nil for videoSettings configures the output to return samples in a convenient uncompressed format, with properties determined according to the properties of the specified video tracks.  Initialization will fail if the video settings cannot be used with the specified tracks.
	
	AVAssetReaderVideoCompositionOutput can only produce uncompressed output.  This means that the video settings dictionary must follow the rules for uncompressed video output, as laid out in AVVideoSettings.h.  In addition, the following keys are not supported:

		AVVideoCleanApertureKey
		AVVideoPixelAspectRatioKey
		AVVideoScalingModeKey
 */
+ (instancetype)assetReaderVideoCompositionOutputWithVideoTracks:(NSArray<AVAssetTrack *> *)videoTracks videoSettings:(nullable NSDictionary<NSString *, id> *)videoSettings;

/*!
 @method initWithVideoTracks:videoSettings:
 @abstract
	Creates an instance of AVAssetReaderVideoCompositionOutput for reading composited video from the specified video tracks and supplying media data according to the specified video settings.

 @param tracks
	An NSArray of AVAssetTrack objects from which the resulting AVAssetReaderVideoCompositionOutput should read video frames for compositing.
 @param videoSettings
	An NSDictionary of video settings to be used for video output.  See AVVideoSettings.h for more information about how to construct a video settings dictionary.
 @result An instance of AVAssetReaderVideoCompositionOutput.

 @discussion
	Each track must be one of the tracks owned by the target AVAssetReader's asset and must be of media type AVMediaTypeVideo.
 	
	A value of nil for videoSettings configures the output to return samples in a convenient uncompressed format, with properties determined according to the properties of the specified video tracks.  Initialization will fail if the video settings cannot be used with the specified tracks.
	
	AVAssetReaderVideoCompositionOutput can only produce uncompressed output.  This means that the video settings dictionary must follow the rules for uncompressed video output, as laid out in AVVideoSettings.h.  In addition, the following keys are not supported:
 
		AVVideoCleanApertureKey
		AVVideoPixelAspectRatioKey
		AVVideoScalingModeKey
 */
- (instancetype)initWithVideoTracks:(NSArray<AVAssetTrack *> *)videoTracks videoSettings:(nullable NSDictionary<NSString *, id> *)videoSettings NS_DESIGNATED_INITIALIZER;

/*!
 @property videoTracks
 @abstract
	The tracks from which the receiver reads composited video.

 @discussion
	The value of this property is an NSArray of AVAssetTracks owned by the target AVAssetReader's asset.
 */
@property (nonatomic, readonly) NSArray<AVAssetTrack *> *videoTracks;

/*!
 @property videoSettings
 @abstract
	The video settings used by the receiver.

 @discussion
	The value of this property is an NSDictionary that contains values for keys as specified by AVVideoSettings.h.  A value of nil indicates that the receiver will return video frames in a convenient uncompressed format, with properties determined according to the properties of the receiver's video tracks.
 */ 
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *videoSettings;

/*!
 @property videoComposition
 @abstract
	The composition of video used by the receiver.

 @discussion
	The value of this property is an AVVideoComposition that can be used to specify the visual arrangement of video frames read from each source track over the timeline of the source asset.
 
	This property cannot be set after reading has started.
 */
@property (nonatomic, copy, nullable) AVVideoComposition *videoComposition;

/*!
 @property customVideoCompositor
 @abstract
 	Indicates the custom video compositor instance used by the receiver.

 @discussion
 	This property is nil if there is no video compositor, or if the internal video compositor is in use.
 */
@property (nonatomic, readonly, nullable) id <AVVideoCompositing> customVideoCompositor NS_AVAILABLE(10_9, 7_0);

@end


@class AVTimedMetadataGroup;
@class AVAssetReaderOutputMetadataAdaptorInternal;

/*!
 @class AVAssetReaderOutputMetadataAdaptor
 @abstract
	Defines an interface for reading metadata, packaged as instances of AVTimedMetadataGroup, from a single AVAssetReaderTrackOutput object.
 */

NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVAssetReaderOutputMetadataAdaptor : NSObject
{
@private
	AVAssetReaderOutputMetadataAdaptorInternal *_internal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderOutputMetadataAdaptorWithAssetReaderTrackOutput:
 @abstract
	Creates a new timed metadata group adaptor for retrieving timed metadata group objects from an asset reader output.

 @param	assetReaderOutput
	An instance of AVAssetReaderTrackOutput that vends sample buffers containing metadata, e.g. an AVAssetReaderTrackOutput object initialized with a track of media type AVMediaTypeMetadata and nil outputSettings.
 @result
	An instance of AVAssetReaderOutputMetadataAdaptor

 @discussion
	It is an error to create a timed metadata group adaptor with an asset reader output that does not vend metadata.  It is also an error to create a timed metadata group adaptor with an asset reader output whose asset reader has already started reading, or an asset reader output that already has been used to initialize another timed metadata group adaptor.
	
	Clients should not mix calls to -[AVAssetReaderTrackOutput copyNextSampleBuffer] and -[AVAssetReaderOutputMetadataAdaptor nextTimedMetadataGroup].  Once an AVAssetReaderTrackOutput instance has been used to initialize an AVAssetReaderOutputMetadataAdaptor, calling -copyNextSampleBuffer on that instance will result in an exception being thrown.
 */
+ (instancetype)assetReaderOutputMetadataAdaptorWithAssetReaderTrackOutput:(AVAssetReaderTrackOutput *)trackOutput;

/*!
 @method initWithAssetReaderTrackOutput:
 @abstract
	Creates a new timed metadata group adaptor for retrieving timed metadata group objects from an asset reader output.

 @param	assetReaderOutput
	An instance of AVAssetReaderTrackOutput that vends sample buffers containing metadata, e.g. an AVAssetReaderTrackOutput object initialized with a track of media type AVMediaTypeMetadata and nil outputSettings.
 @result
	An instance of AVAssetReaderOutputMetadataAdaptor

 @discussion
	It is an error to create a timed metadata group adaptor with an asset reader output that does not vend metadata.  It is also an error to create a timed metadata group adaptor with an asset reader output whose asset reader has already started reading, or an asset reader output that already has been used to initialize another timed metadata group adaptor.
	
	Clients should not mix calls to -[AVAssetReaderTrackOutput copyNextSampleBuffer] and -[AVAssetReaderOutputMetadataAdaptor nextTimedMetadataGroup].  Once an AVAssetReaderTrackOutput instance has been used to initialize an AVAssetReaderOutputMetadataAdaptor, calling -copyNextSampleBuffer on that instance will result in an exception being thrown.
 */
- (instancetype)initWithAssetReaderTrackOutput:(AVAssetReaderTrackOutput *)trackOutput NS_DESIGNATED_INITIALIZER;

/*!
 @property assetReaderTrackOutput
 @abstract
	The asset reader track output from which the receiver pulls timed metadata groups.
 */
@property (nonatomic, readonly) AVAssetReaderTrackOutput *assetReaderTrackOutput;

/*!
 @method nextTimedMetadataGroup
 @abstract
	Returns the next timed metadata group for the asset reader output, synchronously.
	
 @result
	An instance of AVTimedMetadataGroup, representing the next logical segment of metadata coming from the source asset reader output.
	
 @discussion
	This method will return nil when all timed metadata groups have been read from the asset reader output, or if there is an error that prevents the timed metadata group adaptor from reading more timed metadata groups.  When this method returns nil, clients should check the value of the associated AVAssetReader's status property to determine why no more samples could be read.
	
	Unlike -[AVAssetReaderTrackOutput copyNextSampleBuffer], this method returns an autoreleased object.
 
	Before calling this method, you must ensure that the output which underlies the receiver is attached to an AVAssetReader via a prior call to -addOutput: and that -startReading has been called on the asset reader.
 */
- (nullable AVTimedMetadataGroup *)nextTimedMetadataGroup;

@end

@class AVAssetReaderSampleReferenceOutputInternal;

/*!
 @class AVAssetReaderSampleReferenceOutput
 @abstract
	AVAssetReaderSampleReferenceOutput is a concrete subclass of AVAssetReaderOutput that defines an interface for reading sample references from a single AVAssetTrack of an AVAssetReader's AVAsset.
 @discussion
	Clients can extract information about the location (file URL and offset) of samples in a track by adding an instance of AVAssetReaderSampleReferenceOutput to an AVAssetReader using the -[AVAssetReader addOutput:] method. No actual sample data can be extracted using this class. The location of the sample data is described by the kCMSampleBufferAttachmentKey_SampleReferenceURL and kCMSampleBufferAttachmentKey_SampleReferenceByteOffset attachments on the extracted sample buffers. More information about sample buffers describing sample references can be found in the CMSampleBuffer documentation.
 
	Sample buffers extracted using this class can also be appended to an AVAssetWriterInput to create movie tracks that are not self-contained and reference data in the original file instead.  Currently, only instances of AVAssetWriter configured to write files of type AVFileTypeQuickTimeMovie can be used to write tracks that are not self-contained.
 
	Since no sample data is ever returned by instances of AVAssetReaderSampleReferenceOutput, the value of the alwaysCopiesSampleData property is ignored.
 */

NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVAssetReaderSampleReferenceOutput : AVAssetReaderOutput
{
@private
	AVAssetReaderSampleReferenceOutputInternal	*_sampleReferenceOutputInternal;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderSampleReferenceOutputWithTrack:
 @abstract
	Returns an instance of AVAssetReaderSampleReferenceOutput for supplying sample references.
 
 @param track
	The AVAssetTrack for which the resulting AVAssetReaderSampleReferenceOutput should provide sample references.
 @result
	An instance of AVAssetReaderSampleReferenceOutput.
 
 @discussion
	The track must be one of the tracks contained by the target AVAssetReader's asset.
 */
+ (instancetype)assetReaderSampleReferenceOutputWithTrack:(AVAssetTrack *)track;

/*!
 @method initWithTrack:
 @abstract
	Returns an instance of AVAssetReaderSampleReferenceOutput for supplying sample references.
 
 @param track
	The AVAssetTrack for which the resulting AVAssetReaderSampleReferenceOutput should provide sample references.
 @result
	An instance of AVAssetReaderTrackOutput.
 
 @discussion
	The track must be one of the tracks contained by the target AVAssetReader's asset.
  */
- (instancetype)initWithTrack:(AVAssetTrack *)track NS_DESIGNATED_INITIALIZER;

/*!
 @property track
 @abstract
	The track from which the receiver extracts sample references.
 
 @discussion
	The value of this property is an AVAssetTrack owned by the target AVAssetReader's asset.
 */
@property (nonatomic, readonly) AVAssetTrack *track;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioPlayer.h
/*
	File:           AVAudioPlayer.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioPlayer.h>

// ==========  AVFoundation.framework/Headers/AVCompositionTrackSegment.h
/*
	File:  AVCompositionTrackSegment.h

	Framework:  AVFoundation
 
	Copyright 2010-2015 Apple Inc. All rights reserved.

*/

/*!
	@class			AVCompositionTrackSegment

	@abstract		AVCompositionTrackSegment represents a segment of an AVCompositionTrack, comprising
					a URL, and track identifier, and a time mapping from the source track to the composition
					track.
	
	@discussion		This class is most useful for clients that want to save the low-level representation of
					a composition to storage formats of their choosing and reconstitute them from storage.	
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAssetTrackSegment.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

@class AVCompositionTrackSegmentInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVCompositionTrackSegment : AVAssetTrackSegment
{
@private
	AVCompositionTrackSegmentInternal	*_priv;
}

/*!
	@method			compositionTrackSegmentWithURL:trackID:sourceTimeRange:targetTimeRange:
	@abstract		Returns an instance of AVCompositionTrackSegment that presents a portion of a file referenced by URL.
	@param			URL
					An instance of NSURL that references the container file to be presented by the AVCompositionTrackSegment.
	@param			trackID
					The track identifier that specifies the track of the container file to be presented by the AVCompositionTrackSegment.
	@param			sourceTimeRange
					The timeRange of the track of the container file to be presented by the AVCompositionTrackSegment.
	@param			targetTimeRange
					The timeRange of the composition track during which the AVCompositionTrackSegment is to be presented.
	@result			An instance of AVCompositionTrackSegment.
	@discussion		To specify that the segment be played at the asset's normal rate, set source.duration == target.duration in the timeMapping.
					Otherwise, the segment will be played at a rate equal to the ratio source.duration / target.duration.
*/
+ (instancetype)compositionTrackSegmentWithURL:(NSURL *)URL trackID:(CMPersistentTrackID)trackID sourceTimeRange:(CMTimeRange)sourceTimeRange targetTimeRange:(CMTimeRange)targetTimeRange;

/*!
	@method			compositionTrackSegmentWithTimeRange:
	@abstract		Returns an instance of AVCompositionTrackSegment that presents an empty track segment.
	@param			timeRange
					The timeRange of the empty AVCompositionTrackSegment.
	@result			An instance of AVCompositionTrackSegment.
*/
+ (instancetype)compositionTrackSegmentWithTimeRange:(CMTimeRange)timeRange;

/*!
	@method			initWithURL:trackID:sourceTimeRange:targetTimeRange:
	@abstract		Initializes an instance of AVCompositionTrackSegment that presents a portion of a file referenced by URL.
	@param			URL
					An instance of NSURL that references the container file to be presented by the AVCompositionTrackSegment.
	@param			trackID
					The track identifier that specifies the track of the container file to be presented by the AVCompositionTrackSegment.
	@param			sourceTimeRange
					The timeRange of the track of the container file to be presented by the AVCompositionTrackSegment.
	@param			targetTimeRange
					The timeRange of the composition track during which the AVCompositionTrackSegment is to be presented.
	@result			An instance of AVCompositionTrackSegment.
	@discussion		To specify that the segment be played at the asset's normal rate, set source.duration == target.duration in the timeMapping.
					Otherwise, the segment will be played at a rate equal to the ratio source.duration / target.duration.
*/
- (instancetype)initWithURL:(NSURL *)URL trackID:(CMPersistentTrackID)trackID sourceTimeRange:(CMTimeRange)sourceTimeRange targetTimeRange:(CMTimeRange)targetTimeRange NS_DESIGNATED_INITIALIZER;

/*!
	@method			initWithTimeRange:
	@abstract		Initializes an instance of AVCompositionTrackSegment that presents an empty track segment.
	@param			timeRange
					The timeRange of the empty AVCompositionTrackSegment.
	@result			An instance of AVCompositionTrackSegment.
*/
- (instancetype)initWithTimeRange:(CMTimeRange)timeRange NS_DESIGNATED_INITIALIZER;

/* indicates whether the AVCompositionTrackSegment is an empty segment;
   an empty segment has a valid target time range but nil sourceURL and kCMTimeInvalid source start time; all other fields are undefined */
@property (nonatomic, readonly, getter=isEmpty) BOOL empty;

/* indicates the container file of the media presented by the AVCompositionTrackSegment */
@property (nonatomic, readonly, nullable) NSURL *sourceURL;

/* indicates the track of the container file of the media presented by the AVCompositionTrackSegment */
@property (nonatomic, readonly) CMPersistentTrackID sourceTrackID;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVFAudio.h
/*
	File:           AVFAudio.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVFAudio.h>

// ==========  AVFoundation.framework/Headers/AVContentKeySession.h
/*
    File: AVContentKeySession.h

    Copyright (c) 2015-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

/*!
  @class      AVContentKeySession

  @abstract
    An AVContentKeySession is used to create and track decryption keys for media data. Objects conforming to the AVContentKeyRecipient protocol, such as AVURLAssets, can be added to an AVContentKeySession to employ the services of the AVContentKeySession in handling new key requests and to obtain access to the session's already existing keys.

    Its secondary purpose is to provide a report of expired sessions to assist a controlling entity that wishes to track the set of sessions that are still active. If initialized with a location at which to store them, AVContentKeySession maintains a global collection of pending "expired session reports", each associated with an identifier for the app that created the session. The contents of this identifier are specified by the controlling entity that provides media data or that grants permission for its use.
    Expired sessions are tracked as follows: a stream processing session is considered to be started after an instance of AVContentKeySession is created and the first object conforming to the AVContentKeyRecipient protocol is added to it. If an instance of AVContentKeySession that has reached this state does not receive an expire message before it's deallocated or the process in which it's running is terminated, an "expired session report" will subsequently be added to the pending list of expired session reports that indicates that the session expired abnormally. In contrast, for AVContentKeySessions that reach the state of having at least one object conforming to the AVContentKeyRecipient protocol added to them and later receive an expire message, "expired session reports" will be generated that indicate that the session expired normally.
    To obtain the collection of pending expired session reports in order to provide them to the controlling entity associated with a specific app identifier, use +pendingExpiredSessionReportsWithAppIdentifier:.
    After pending expired session reports have been sent to the controlling entity and their receipt has been acknowledged, they can be removed from the collection of pending expired session reports maintained by AVContentKeySession by using +removePendingExpiredSessionReports:withAppIdentifier:.
*/

@class AVContentKeySessionInternal;
@class AVContentKeyRequest;
@class AVContentKeyResponse;
@class AVPersistableContentKeyRequest;
@protocol AVContentKeyRecipient;
@protocol AVContentKeySessionDelegate;

/*!
 @group         AVContentKeySystem string constants
 @brief         Used by AVContentKeySession to determine the method of key delivery
 */
typedef NSString *AVContentKeySystem NS_STRING_ENUM API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
 @constant      AVContentKeySystemFairPlayStreaming
 @discussion    Used to specify FairPlay Streaming (FPS) as the method of key delivery.
 */
AVF_EXPORT AVContentKeySystem const AVContentKeySystemFairPlayStreaming API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
 @constant      AVContentKeySystemClearKey
 @discussion    Used to specify clear key as the method of key delivery.
 */
AVF_EXPORT AVContentKeySystem const AVContentKeySystemClearKey API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED
@interface AVContentKeySession : NSObject {
@private
    AVContentKeySessionInternal *_session;
}
AV_INIT_UNAVAILABLE

/*!
 @method        contentKeySessionWithKeySystem:
 @abstract      Creates a new instance of AVContentKeySession to manage a collection of media content keys.
 @param         keySystem
                A valid key system for retrieving keys.
 @result        A new AVContentKeySession.
 @discussion    This method returns an AVContentKeySession instance that is capable of managing collection of media content keys corresponding to the input keySystem. An NSInvalidArgumentException will be raised if the value of keySystem is unsupported.
 */
+ (instancetype)contentKeySessionWithKeySystem:(AVContentKeySystem)keySystem API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

/*!
 @method        contentKeySessionWithKeySystem:storageDirectoryAtURL:
 @abstract      Creates a new instance of AVContentKeySession to manage a collection of media content keys.
 @param         keySystem
                A valid key system for retrieving keys.
 @param         storageURL
                URL to a writable directory that the session will use to facilitate expired session reports after abnormal session termination.
 @result        A new AVContentKeySession.
 @discussion    This method returns an AVContentKeySession instance that is capable of managing collection of media content keys corresponding to the input keySystem. An NSInvalidArgumentException will be raised if the value of keySystem is unsupported.
 */
+ (instancetype)contentKeySessionWithKeySystem:(AVContentKeySystem)keySystem storageDirectoryAtURL:(NSURL *)storageURL;

/*!
 @method        setDelegate:queue:
 @abstract      Sets the receiver's delegate. A delegate is required to handle content key initialization.
 @param         delegate
                An object conforming to the AVContentKeySessionDelegate protocol.
 @param         delegateQueue
                A dispatch queue on which delegate methods will be invoked whenever processes requiring content keys are executed asynchronously. Passing a value of nil for the delegateQueue parameter along with a non-nil value for the delegate parameter will result in an invalid argument exception.
*/
- (void)setDelegate:(nullable id <AVContentKeySessionDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
 @property      delegate
 @abstract      The receiver's delegate.
 @discussion    The value of this property is an object conforming to the AVContentKeySessionDelegate protocol. The delegate is set using the setDelegate:queue: method.
*/
@property (readonly, weak, nullable) id<AVContentKeySessionDelegate> delegate;

/*!
 @property      delegateQueue
 @abstract      The dispatch queue on which all delegate methods will be invoked whenever processes requiring content keys are executed asynchronously.
 @discussion    The value of this property is a dispatch_queue_t. The queue is set using the setDelegate:queue: method.
*/
@property (readonly, nullable) dispatch_queue_t delegateQueue;

/*!
 @property      storageURL
 @abstract      The storage URL provided when the AVContentKeySession was created. May be nil.
 @discussion    URL to a writable directory; may be nil. The session will use this to facilitate expired session reports after abnormal session termination.
*/
@property (readonly, nullable) NSURL *storageURL;

/*!
 @property      keySystem
 @abstract      The key system used for retrieving keys
*/
@property (readonly) AVContentKeySystem keySystem;

/*! 
 @method        expire
 @abstract      Tells the receiver to treat the session as having been intentionally and normally expired.
 @discussion    When an instance of AVContentKeySession receives an expire message, all of its associated objects conforming to the AVContentKeyRecipient protocol will become inoperable. Send this message only after you have finished operating on the media data.
*/
- (void)expire;

/*!
 @property      contentProtectionSessionIdentifier
 @abstract      An opaque identifier for the current content protection session. 
 @discussion    May be nil. Will call the delegate's contentKeySessionContentProtectionSessionIdentifierDidChange: when the identifier changes. The protection session ID is a unique string identifier generated by the AVContentKeySession that can be used by the application to identify content key session objects.
*/
@property (readonly, nullable) NSData *contentProtectionSessionIdentifier;

/*!
 @method        processContentKeyRequestWithIdentifier:initializationData:options:
 @abstract      Informs the receiver that it should attempt to instantiate a content decryption key using the specified initialization data.
 @param         identifier
                Container- and protocol-specific identifier to be used to obtain a key response. Either identifier or initializationData must be non-nil. Both can be non-nil, if the content protection protocol requires both.
 @param         initializationData
                Container- and protocol-specific data to be used to obtain a key response. Either identifier or initializationData must be non-nil. Both can be non-nil, if the content protection protocol requires both.
 @param         options
                No options are defined at this time, may be nil.
 @discussion    May be used to generate an AVContentKeyRequest from request initialization data already in hand, without awaiting such data during the processing of media data of an associated recipient.
*/
- (void)processContentKeyRequestWithIdentifier:(nullable id)identifier initializationData:(nullable NSData *)initializationData options:(nullable NSDictionary<NSString *, id> *)options;

/*!
 @method        renewExpiringResponseDataForContentKeyRequest:
 @abstract      Informs the receiver that the already provided response data for an earlier AVContentKeyRequest will imminently expire.
 @discussion    In response the receiver will invoke your delegate with a new content key request entreating it to renew the expiring response data, via -contentKeySession:didProvideRenewingContentKeyRequest:.
*/
- (void)renewExpiringResponseDataForContentKeyRequest:(AVContentKeyRequest *)contentKeyRequest;

/*!
 @method        makeSecureTokenForExpirationDateOfPersistableContentKey:
 @abstract      Creates a secure server playback context (SPC) that the client could send to the key server to obtain an expiration date for the provided persistable content key data.
 @param         persistableContentKeyData
                Persistable content key data that was previously created using -[AVContentKeyRequest persistableContentKeyFromKeyVendorResponse:options:error:] or obtained via AVContentKeySessionDelegate callback -contentKeySession:didUpdatePersistableContentKey:forContentKeyIdentifier:.
 @param         handler
                Once the secure token is ready, this block will be called with the token or an error describing the failure.
 */
- (void)makeSecureTokenForExpirationDateOfPersistableContentKey:(NSData *)persistableContentKeyData
											  completionHandler:(void (^)(NSData * _Nullable secureTokenData, NSError * _Nullable error))handler API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos);

@end

@interface AVContentKeySession (AVContentKeyRecipients)

/*! 
 @method        addContentKeyRecipient:
 @abstract      Informs the receiver that the specified recipient will be used for the session.
 @discussion    It is an error to add recipient to sessions that have received an expire message. It is also an error to add recipients after they have already begun to process media data (e.g. after an AVURLAsset has loaded the values of any of its keys). Such errors will result in NSInternalInconsistencyExceptions. Sending this message to an AVContentKeySession is atomic.
*/
- (void)addContentKeyRecipient:(id<AVContentKeyRecipient>)recipient;

/*! 
 @method        removeContentKeyRecipient:
 @abstract      Informs the receiver that the specified recipient will no longer be used.
 @discussion    After the specified recipient is removed from the receiver it will become inoperable. Remove the recipient only after you have finished operating on the media data associated with it. Sending this message to an AVContentKeySession is atomic.
*/
- (void)removeContentKeyRecipient:(id<AVContentKeyRecipient>)recipient;

/*!
 @property      contentKeyRecipients
 @abstract      The array of recipients of content keys currently associated with the AVContentKeySession.
*/
@property (readonly) NSArray<id<AVContentKeyRecipient>> *contentKeyRecipients;

@end

@interface AVContentKeySession (AVContentKeySessionPendingExpiredSessionReports)

/*! 
 @method        pendingExpiredSessionReportsWithAppIdentifier:storageDirectoryAtURL:
 @abstract      Provides "expired session reports" for prior AVContentKeySessions created with the specified app identifier that have expired either normally or abnormally.
 @param         appIdentifier
                An opaque identifier for the application. The contents of this identifier depend on the particular protocol in use by the entity that controls the use of the media data.
 @param         storageURL
                URL to a directory previously used with one or more instances of AVContentKeySession for the storage of expired session reports.
 @result        An NSArray containing instances of NSData, each containing a pending expired session report as a property-list serialization of an NSDictionary object. The contents of expired session reports depend on the particular protocol in use by the entity that controls the use of the media data.
 @discussion    Note that no reports for sessions still in progress will be included.
*/
+ (NSArray <NSData *> *)pendingExpiredSessionReportsWithAppIdentifier:(NSData *)appIdentifier storageDirectoryAtURL:(NSURL *)storageURL;

/*! 
 @method        removePendingExpiredSessionReports:withAppIdentifier:storageDirectoryAtURL:
 @abstract      Removes expired session reports for prior AVContentKeySessions from storage. Once they have been removed, they will no longer be available via subsequent invocations of +pendingExpiredSessionReportsWithAppIdentifier:.
 @param         expiredSessionReports
                An array of expired session reports to be discarded.
 @param         appIdentifier
                An opaque identifier for the application. The contents of this identifier depend on the particular protocol in use by the entity that controls the use of the media data.
 @param         storageURL
                URL to a writable folder.
 @discussion    This method is most suitable for use only after the specified expired session reports have been sent to the entity that controls the use of the media data and the entity has acknowledged their receipt.
*/
+ (void)removePendingExpiredSessionReports:(NSArray <NSData *> *)expiredSessionReports withAppIdentifier:(NSData *)appIdentifier storageDirectoryAtURL:(NSURL *)storageURL;

@end

/*!
 @group         AVContentKeyRequestRetryReason string constants
 @brief         Used to specify a reason for asking the client to retry a content key request.
 */
typedef NSString *AVContentKeyRequestRetryReason NS_STRING_ENUM API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
 @constant      AVContentKeyRequestRetryReasonTimedOut
 @discussion    Indicates that the content key request should be retried because the key response was not set soon enough either due the initial request/response was taking too long, or a lease was expiring in the meantime.
 */
AVF_EXPORT AVContentKeyRequestRetryReason const AVContentKeyRequestRetryReasonTimedOut API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
 @constant      AVContentKeyRequestRetryReasonReceivedResponseWithExpiredLease
 @discussion    Indicates that the content key request should be retried because a key response with expired lease was set on the previous content key request.
 */
AVF_EXPORT AVContentKeyRequestRetryReason const AVContentKeyRequestRetryReasonReceivedResponseWithExpiredLease API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
 @constant      AVContentKeyRequestRetryReasonReceivedObsoleteContentKey
 @discussion    Indicates that the content key request should be retried because an obsolete key response was set on the previous content key request.
 */
AVF_EXPORT AVContentKeyRequestRetryReason const AVContentKeyRequestRetryReasonReceivedObsoleteContentKey API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED
@protocol AVContentKeySessionDelegate <NSObject>

/*!
 @method        contentKeySession:didProvideContentKeyRequest:
 @abstract      Provides the receiver with a new content key request.
 @param         session
                An instance of AVContentKeySession that's providing a new content key request.
 @param         keyRequest
                An instance of AVContentKeyRequest.
 @discussion    Will be invoked by an AVContentKeySession as a result of a call to -processContentKeyRequestWithIdentifier:initializationData:options:. Can be invoked during playback or other processing of an AVURLAsset added to the AVContentKeySession.
*/
@required
- (void)contentKeySession:(AVContentKeySession *)session didProvideContentKeyRequest:(AVContentKeyRequest *)keyRequest;

/*!
 @method        contentKeySession:didProvideRenewingContentKeyRequest:
 @abstract      Provides the receiver with a new content key request representing a renewal of an existing content key.
 @param         session
                An instance of AVContentKeySession that's providing a new content key request.
 @param         keyRequest
                An instance of AVContentKeyRequest representing renewal of a previous content key.
 @discussion    Will be invoked by an AVContentKeySession as the result of a call to -renewExpiringResponseDataForContentKeyRequest:.
*/
@optional
- (void)contentKeySession:(AVContentKeySession *)session didProvideRenewingContentKeyRequest:(AVContentKeyRequest *)keyRequest;

/*!
 @method        contentKeySession:didProvidePersistableContentKeyRequest:
 @abstract      Provides the receiver with a new content key request that allows key persistence.
 @param         session
                An instance of AVContentKeySession that's providing a new content key request.
 @param         keyRequest
                An instance of AVPersistableContentKeyRequest.
 @discussion    Will be invoked by an AVContentKeyRequest as the result of a call to -respondByRequestingPersistableContentKeyRequest.
 */
@optional
- (void)contentKeySession:(AVContentKeySession *)session didProvidePersistableContentKeyRequest:(AVPersistableContentKeyRequest *)keyRequest;

/*!
 @method        contentKeySession:didUpdatePersistableContentKey:forContentKeyIdentifier:
 @abstract      Provides the receiver with an updated persistable content key for a particular key request.
 @param         session
                An instance of AVContentKeySession that is providing the updated persistable content key.
 @param         persistableContentKey
                Updated persistable content key data that may be stored offline and used to answer future requests to content keys with matching key identifier.
 @param         keyIdentifier
                Container- and protocol-specific identifier for the persistable content key that was updated.
 @discussion    If the content key session provides an updated persistable content key data, the previous key data is no longer valid and cannot be used to answer future loading requests.
 */
@optional
- (void)contentKeySession:(AVContentKeySession *)session didUpdatePersistableContentKey:(NSData *)persistableContentKey forContentKeyIdentifier:(id)keyIdentifier API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @method        contentKeySession:contentKeyRequest:didFailWithError:
 @abstract      Informs the receiver a content key request has failed.
 @param         session
                The instance of AVContentKeySession that initiated the content key request.
 @param         keyRequest
                The instance of AVContentKeyRequest that has failed.
 @param         error
                An instance of NSError that describes the failure that occurred.
*/
@optional
- (void)contentKeySession:(AVContentKeySession *)session contentKeyRequest:(AVContentKeyRequest *)keyRequest didFailWithError:(NSError *)err;

/*!
 @method        contentKeySession:shouldRetryContentKeyRequest:reason:
 @abstract      Provides the receiver a content key request that should be retried because a previous content key request failed.
 @param         session
                An instance of AVContentKeySession that's providing the content key request that should be retried.
 @param         keyRequest
                An instance of AVContentKeyRequest that should be retried.
 @param         retryReason
                An enum value to explain why the receiver could retry the new content key request.
 @result        A BOOL value indicating receiver's desire to retry the failed content key request.
 @discussion    Will be invoked by an AVContentKeySession when a content key request should be retried. The reason for failure of previous content key request is specified. The receiver can decide if it wants to request AVContentKeySession to retry this key request based on the reason. If the receiver returns YES, AVContentKeySession would restart the key request process. If the receiver returns NO or if it does not implement this delegate method, the content key request would fail and AVContentKeySession would let the receiver know through -contentKeySession:contentKeyRequest:didFailWithError:.
 */
@optional
- (BOOL)contentKeySession:(AVContentKeySession *)session shouldRetryContentKeyRequest:(AVContentKeyRequest *)keyRequest reason:(AVContentKeyRequestRetryReason)retryReason;

/*!
 @method        contentKeySession:contentKeyRequestDidSucceed:
 @abstract      Informs the receiver that the response to content key request was successfully processed.
 @param         session
                The instance of AVContentKeySession that initiated the content key request.
 @param         keyRequest
                The instance of AVContentKeyRequest whose response was successfully processed.
 @discussion    Will be invoked by an AVContentKeySession when it is certain that the response client provided via -[AVContentKeyRequest processContentKeyResponse:] was successfully processed by the system.
 */
@optional
- (void)contentKeySession:(AVContentKeySession *)session contentKeyRequestDidSucceed:(AVContentKeyRequest *)keyRequest API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0)) __WATCHOS_PROHIBITED;

/*!
 @method        contentKeySessionContentProtectionSessionIdentifierDidChange:
 @abstract      Informs the receiver that the value of -[AVContentKeySession contentProtectionSessionIdentifier] has changed.
*/
@optional
- (void)contentKeySessionContentProtectionSessionIdentifierDidChange:(AVContentKeySession *)session;


/*!
 @method        contentKeySessionDidGenerateExpiredSessionReport:
 @abstract      Notifies the sender that a expired session report has been generated
 @param         session
                An instance of AVContentKeySession that recorded the generation of an expired session report.
 @discussion    Will be invoked by an AVContentKeySession when an expired session report is added to the storageURL
*/
@optional
- (void)contentKeySessionDidGenerateExpiredSessionReport:(AVContentKeySession *)session API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0)) __WATCHOS_PROHIBITED;
@end


/*
 @constant    AVContentKeyRequestStatusRequestingResponse
    Indicates that the request has just been created.
 @constant    AVContentKeyRequestStatusReceivedResponse
    Indicates that a response to a key reequest was received and key is in use. This does not indicate that the key is valid.
 @constant    AVContentKeyRequestStatusRenewed
    Indicates that the key request was renewed. This does not indicate that the key is valid.
 @constant    AVContentKeyRequestStatusRetried
	Indicates that the key request was retried.
 @constant    AVContentKeyRequestStatusCancelled
    Indicates that the key request was cancelled.
 @constant    AVContentKeyRequestStatusFailed
    Indicates that the request has encountered an error. See also the error property.
*/
typedef NS_ENUM(NSInteger, AVContentKeyRequestStatus) {
    AVContentKeyRequestStatusRequestingResponse,
    AVContentKeyRequestStatusReceivedResponse,
    AVContentKeyRequestStatusRenewed,
	AVContentKeyRequestStatusRetried,
    AVContentKeyRequestStatusCancelled,
    AVContentKeyRequestStatusFailed
} API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

@class AVContentKeyRequestInternal;

API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED
@interface AVContentKeyRequest : NSObject
{
@private
    AVContentKeyRequestInternal *_contentKeyRequest;
}

/*
 @property      status
 @abstract      This describes the state of the AVContentKeyRequest, value is one of AVContentKeyRequestStatus.
*/
@property (readonly) AVContentKeyRequestStatus status;

/*!
 @property      error
 @abstract      If the receiver's status is AVContentKeyRequestStatusFailed, this describes the error that caused the failure.
 @discussion    The value of this property is an NSError that describes what caused the content key request to fail. If the receiver's status is not AVContentKeyRequestStatusFailed, the value of this property is nil.
 */
@property (readonly, nullable) NSError *error;

/*
 @property      identifier
 @abstract      Container- and protocol-specific identifier for the content key.
 @discussion    In order to use a key with an HTTP Live Streaming AVURLAsset, the identifier must be an NSURL that matches a key URI in the Media Playlist.
*/
@property (readonly, nullable) id identifier;

/*
 @property      initializationData
 @abstract      Container- and protocol-specific data to be used to obtain a key response.
*/
@property (nonatomic, readonly, nullable) NSData *initializationData;

/*
 @property      canProvidePersistableContentKey
 @abstract      When the value of this property is YES, you can use the method -persistableContentKeyFromKeyVendorResponse:options:error: to create a persistable content key from the content key response.
 @dicsussion    The value of this property will be YES only when the receiver is provided to your AVContentKeySession delegate via the method -contentKeySession:didProvidePersistableContentKeyRequest:. If you have an AVContentKeyRequest for which the value of canProvidePersistableContentKey is NO, but you wish to obtain a persistable content key, send the AVContentKeyRequest the message -respondByRequestingPersistableContentKeyRequest.
 */
@property (readonly) BOOL canProvidePersistableContentKey;

/*!
 @method        makeStreamingContentKeyRequestDataForApp:contentIdentifier:options:completionHandler:
 @abstract      Obtains a content key request data for a specific combination of application and content.
 @param         appIdentifier
                An opaque identifier for the application. The value of this identifier depends on the particular system used to provide the content key.
 @param         contentIdentifier
                An optional opaque identifier for the content. The value of this identifier depends on the particular system used to provide the content key.
 @param         options
                Additional information necessary to obtain the key, or nil if none. See AVContentKeyRequest*Key below.
 @param         handler
                Once the streaming content key request is prepared, this block will be called with the request data or an error describing the failure.
 @discussion    If option AVContentKeyRequestProtocolVersionsKey is not specified the default protocol version of 1 is assumed.
*/
- (void)makeStreamingContentKeyRequestDataForApp:(NSData *)appIdentifier
                               contentIdentifier:(nullable NSData *)contentIdentifier
                                         options:(nullable NSDictionary<NSString *, id> *)options
                               completionHandler:(void (^)(NSData * _Nullable contentKeyRequestData, NSError * _Nullable error))handler;

/*!
 @method        processContentKeyResponse:
 @abstract      Informs the receiver to process the specified content key response.
 @param         keyResponse
                An instance of AVContentKeyResponse carrying a response to a content key request.
 @discussion    After you receive an AVContentKeyRequest via -contentKeySession:didProvideContentKeyRequest: and after you invoke -[AVContentKeyRequest makeStreamingContentKeyRequestDataForApp:contentIdentifier:options:completionHandler:] on that request, you must obtain a response to the request in accordance with the protocol in use by the entity that controls the use of the media data. This is the method you use to provide the content key response to make protected content available for processing. If obtaining the content key response fails, use -processContentKeyResponseError:.
*/
- (void)processContentKeyResponse:(AVContentKeyResponse *)keyResponse;

/*!
 @method        processContentKeyResponseError:
 @abstract      Informs the receiver that obtaining a content key response has failed, resulting in failure handling.
 @param         error
                An instance of NSError that describes the specific failure that occurred.
*/
- (void)processContentKeyResponseError:(NSError *)error;

/*!
 @method        respondByRequestingPersistableContentKeyRequest
 @abstract      Informs the receiver to process a persistable content key request.
 @discussion    When you receive an AVContentKeyRequest via -contentKeySession:didProvideContentKeyRequest: and you want the resulting key response to produce a key that can persist across multiple playback sessions, you must invoke -respondByRequestingPersistableContentKeyRequest on that AVContentKeyRequest in order to signal that you want to process an AVPersistableContentKeyRequest instead. If the underlying protocol supports persistable content keys, in response your delegate will receive an AVPersistableContentKeyRequest via -contentKeySession:didProvidePersistableContentKeyRequest:. NSInternalInconsistencyException will be raised, if you are attempting to create and use a persistable key but your AVContentKeySession delegate does not respond to contentKeySession:didProvidePersistableContentKeyRequest:.
 */
- (void)respondByRequestingPersistableContentKeyRequest API_DEPRECATED("Use respondByRequestingPersistableContentKeyRequestAndReturnError: instead.", ios(10.3, 11.2)) API_UNAVAILABLE(macos, tvos, watchos);
/*
 @method		respondByRequestingPersistableContentKeyRequestAndReturnError:
 @abstract		Informs the receiver to process a persistable content key request.
 @param			outError
				The error returned if a persistable content key request cannot be requested.
 @result		YES if sucessful. If NO, this request should be responded to via processContentKeyResponse: or processContentKeyResponseError:.
 @discussion	When you receive an AVContentKeyRequest via -contentKeySession:didProvideContentKeyRequest: and you want the resulting key response to produce a key that can persist across multiple playback sessions, you must invoke -respondByRequestingPersistableContentKeyRequest on that AVContentKeyRequest in order to signal that you want to process an AVPersistableContentKeyRequest instead. If the underlying protocol supports persistable content keys, in response your delegate will receive an AVPersistableContentKeyRequest via -contentKeySession:didProvidePersistableContentKeyRequest:. NSInternalInconsistencyException will be raised, if you are attempting to create and use a persistable key but your AVContentKeySession delegate does not respond to contentKeySession:didProvidePersistableContentKeyRequest:.
*/
- (BOOL)respondByRequestingPersistableContentKeyRequestAndReturnError:(NSError **)outError API_AVAILABLE(ios(11.2)) API_UNAVAILABLE(macos, tvos, watchos);

@end

API_AVAILABLE(ios(10.3), tvos(10.3)) __WATCHOS_PROHIBITED
@interface AVPersistableContentKeyRequest : AVContentKeyRequest

/*!
 @method        persistableContentKeyFromKeyVendorResponse:options:error:
 @abstract      Obtains a persistable content key from a context.
 @param         keyVendorResponse
                The response returned from the key vendor as a result of a request generated from makeStreamingContentKeyRequestDataForApp:contentIdentifier:options:completionHandler:.
 @param         options
                Additional information necessary to obtain the persistable content key, or nil if none.
 @param         error
                If obtaining the persistable content key fails, will be set to an instance of NSError describing the failure.
 @result        The persistable content key data that may be stored offline to answer future loading requests of the same content key.
 @discussion    The data returned from this method may be used to immediately satisfy an AVPersistableContentKeyRequest, as well as any subsequent requests for the same key url using processContentKeyResponse: method. When you receive an AVContentKeyRequest via -contentKeySession:didProvideContentKeyRequest: and you want to use existing persistent content key from storage, you must invoke -respondByRequestingPersistableContentKeyRequest on that AVContentKeyRequest in order to signal that you want to process an AVPersistableContentKeyRequest instead. If the underlying protocol supports persistable content keys, in response your delegate will receive an AVPersistableContentKeyRequest via -contentKeySession:didProvidePersistableContentKeyRequest:. You can set the persistent key from storage on the AVPersistableContentKeyRequest using processContentKeyResponse:.
 */
- (nullable NSData *)persistableContentKeyFromKeyVendorResponse:(NSData *)keyVendorResponse
                                                        options:(nullable NSDictionary <NSString *, id> *)options
                                                          error:(NSError * _Nullable * _Nullable)outError;

@end

@interface AVContentKeyRequest (AVContentKeyRequestRenewal)

/*
 @property      renewsExpiringResponseData
 @abstract      Indicates whether the receiver represents a request to renew previously provided response data that is expiring or has expired.
*/
@property (readonly) BOOL renewsExpiringResponseData;

@end

@class AVContentKeyResponseInternal;

/*!
 @class         AVContentKeyResponse
 @abstract      AVContentKeyResponse is used to represent the data returned from the key server when requesting a key for decrypting content.
*/
API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED
@interface AVContentKeyResponse : NSObject
{
@private
    AVContentKeyResponseInternal * _keyResponse;
}

/*!
 @method        contentKeyResponseWithFairPlayStreamingKeyResponseData:
 @abstract      Create an AVContentKeyResponse from the server response to a key request made when using FairPlayStreaming (FPS) as the method of key delivery.
 @param         keyResponseData
                The response from the FairPlayStreaming key server
 @result        A new AVContentKeyResponse holding data from a FairPlayStreaming key server that is used to decrypt the content
 @discussion    The object created by this method is typically used with an AVContentKeyRequest created by an AVContentKeySession using keySystem AVContentKeySystemFairPlayStreaming. It is passed to AVContentKeyRequest -processContentKeyResponse: in order to supply the decryptor with key data
*/
+ (instancetype)contentKeyResponseWithFairPlayStreamingKeyResponseData:(NSData *)keyResponseData;

/*!
 @method		contentKeyResponseWithClearKeyData:initializationVector:
 @abstract		Create an AVContentKeyResponse from the key and IV when using AVContentKeySystemClearKey as the key system
 
 @param			keyData
				The key used for decrypting content.
 @param			initializationVector
				The initialization vector used for decrypting content, or nil if initialization vector is available in the media to be decrypted
 @result		A new AVContentKeyResponse holding Clear Key data.
 @discussion	The object created by this method is typically used with an AVContentKeyRequest created by an AVContentKeySession using keySystem AVContentKeySystemClearKey. It is passed to AVContentKeyRequest -processContentKeyResponse: in order to supply the decryptor with key data.
*/
+ (instancetype)contentKeyResponseWithClearKeyData:(NSData *)keyData initializationVector:(nullable NSData *)initializationVector API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

@end

// Options keys for use with -[AVContentKeyRequest makeStreamingContentKeyRequestDataForApp:contentIdentifier:options:completionHandler:]
/*!
 @constant      AVContentKeyRequestProtocolVersionsKey
 @abstract      Specifies the versions of the content protection protocol supported by the application as an NSArray of one or more NSNumber objects.
 */
AVF_EXPORT NSString *const AVContentKeyRequestProtocolVersionsKey API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2)) __WATCHOS_PROHIBITED;

/*!
  @protocol      AVContentKeyRecipient

  @abstract
    Classes of objects that may require decryption keys for media data in order to enable processing, such as parsing or playback, conform to this protocol.
*/
API_AVAILABLE(macos(10.12.4), ios(10.3), tvos(10.2), watchos(3.3))
@protocol AVContentKeyRecipient

@required

/*!
 @property      mayRequireContentKeysForMediaDataProcessing
 @abstract      Indicates whether the receiver may require decryption keys for media data in order to enable processing.
 @discussion    When the value of mayRequireContentKeysForMediaDataProcessing is YES, adding the receiver to an AVContentKeySession allows it to employ the session's already existing keys and also enables the handling of new key requests by the AVContentKeySession's delegate.
*/
@property (nonatomic, readonly) BOOL mayRequireContentKeysForMediaDataProcessing;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureVideoDataOutput.h
/*
    File:  AVCaptureVideoDataOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVVideoSettings.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureVideoDataOutput

@class AVCaptureVideoDataOutputInternal;
@protocol AVCaptureVideoDataOutputSampleBufferDelegate;

/*!
 @class AVCaptureVideoDataOutput
 @abstract
    AVCaptureVideoDataOutput is a concrete subclass of AVCaptureOutput that can be used to process uncompressed or compressed frames from the video being captured.
 
 @discussion
    Instances of AVCaptureVideoDataOutput produce video frames suitable for processing using other media APIs. Applications can access the frames with the captureOutput:didOutputSampleBuffer:fromConnection: delegate method.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureVideoDataOutput : AVCaptureOutput
{
@private
    AVCaptureVideoDataOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method setSampleBufferDelegate:queue:
 @abstract
    Sets the receiver's delegate that will accept captured buffers and dispatch queue on which the delegate will be called.
 
 @param sampleBufferDelegate
    An object conforming to the AVCaptureVideoDataOutputSampleBufferDelegate protocol that will receive sample buffers after they are captured.
 @param sampleBufferCallbackQueue
    A dispatch queue on which all sample buffer delegate methods will be called.
 
 @discussion
    When a new video sample buffer is captured it will be vended to the sample buffer delegate using the captureOutput:didOutputSampleBuffer:fromConnection: delegate method. All delegate methods will be called on the specified dispatch queue. If the queue is blocked when new frames are captured, those frames will be automatically dropped at a time determined by the value of the alwaysDiscardsLateVideoFrames property. This allows clients to process existing frames on the same queue without having to manage the potential memory usage increases that would otherwise occur when that processing is unable to keep up with the rate of incoming frames. If their frame processing is consistently unable to keep up with the rate of incoming frames, clients should consider using the minFrameDuration property, which will generally yield better performance characteristics and more consistent frame rates than frame dropping alone.
 
    Clients that need to minimize the chances of frames being dropped should specify a queue on which a sufficiently small amount of processing is being done outside of receiving sample buffers. However, if such clients migrate extra processing to another queue, they are responsible for ensuring that memory usage does not grow without bound from frames that have not been processed.
 
    A serial dispatch queue must be used to guarantee that video frames will be delivered in order. The sampleBufferCallbackQueue parameter may not be NULL, except when setting the sampleBufferDelegate to nil.
 */
- (void)setSampleBufferDelegate:(nullable id<AVCaptureVideoDataOutputSampleBufferDelegate>)sampleBufferDelegate queue:(nullable dispatch_queue_t)sampleBufferCallbackQueue;

/*!
 @property sampleBufferDelegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureVideoDataOutputSampleBufferDelegate protocol that will receive sample buffers after they are captured. The delegate is set using the setSampleBufferDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) id<AVCaptureVideoDataOutputSampleBufferDelegate> sampleBufferDelegate;

/*!
 @property sampleBufferCallbackQueue
 @abstract
    The dispatch queue on which all sample buffer delegate methods will be called.
 
 @discussion
    The value of this property is a dispatch_queue_t. The queue is set using the setSampleBufferDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) dispatch_queue_t sampleBufferCallbackQueue;

/*!
 @property videoSettings
 @abstract
    Specifies the settings used to decode or re-encode video before it is output by the receiver.
 
 @discussion
    See AVVideoSettings.h for more information on how to construct a video settings dictionary. To receive samples in their device native format, set this property to an empty dictionary (i.e. [NSDictionary dictionary]). To receive samples in a default uncompressed format, set this property to nil. Note that after this property is set to nil, subsequent querying of this property will yield a non-nil dictionary reflecting the settings used by the AVCaptureSession's current sessionPreset.
 
    On iOS, the only supported key is kCVPixelBufferPixelFormatTypeKey. Supported pixel formats are kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, kCVPixelFormatType_420YpCbCr8BiPlanarFullRange and kCVPixelFormatType_32BGRA.
 */
@property(nonatomic, copy, null_resettable) NSDictionary<NSString *, id> *videoSettings;

/*!
 @method recommendedVideoSettingsForAssetWriterWithOutputFileType:
 @abstract
    Specifies the recommended settings for use with an AVAssetWriterInput.
 
 @param outputFileType
    Specifies the UTI of the file type to be written (see AVMediaFormat.h for a list of file format UTIs).
 @result
    A fully populated dictionary of keys and values that are compatible with AVAssetWriter.
 
 @discussion
    The value of this property is an NSDictionary containing values for compression settings keys defined in AVVideoSettings.h. This dictionary is suitable for use as the "outputSettings" parameter when creating an AVAssetWriterInput, such as,
 
       [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:outputSettings sourceFormatHint:hint];
 
    The dictionary returned contains all necessary keys and values needed by AVAssetWriter (see AVAssetWriterInput.h, -initWithMediaType:outputSettings: for a more in depth discussion). For QuickTime movie and ISO file types, the recommended video settings will produce output comparable to that of AVCaptureMovieFileOutput.
 
    Note that the dictionary of settings is dependent on the current configuration of the receiver's AVCaptureSession and its inputs. The settings dictionary may change if the session's configuration changes. As such, you should configure your session first, then query the recommended video settings. As of iOS 8.3, movies produced with these settings successfully import into the iOS camera roll and sync to and from like devices via iTunes.
 */
- (nullable NSDictionary<NSString *, id> *)recommendedVideoSettingsForAssetWriterWithOutputFileType:(AVFileType)outputFileType API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method availableVideoCodecTypesForAssetWriterWithOutputFileType:
 @abstract
    Specifies the available video codecs for use with AVAssetWriter and a given file type.
 
 @param outputFileType
    Specifies the UTI of the file type to be written (see AVMediaFormat.h for a list of file format UTIs).
 @result
    An array of video codecs; see AVVideoSettings.h for a full list.
 
 @discussion
    This method allows you to query the available video codecs that may be used when specifying an AVVideoCodecKey in -recommendedVideoSettingsForVideoCodecType:assetWriterOutputFileType:. When specifying an outputFileType of AVFileTypeQuickTimeMovie, video codecs are ordered identically to -[AVCaptureMovieFileOutput availableVideoCodecTypes].
 */
- (NSArray<AVVideoCodecType> *)availableVideoCodecTypesForAssetWriterWithOutputFileType:(AVFileType)outputFileType API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @method recommendedVideoSettingsForVideoCodecType:assetWriterOutputFileType:
 @abstract
    Specifies the recommended settings for a particular video codec type, to be used with an AVAssetWriterInput.
 
 @param videoCodecType
    Specifies the desired AVVideoCodecKey to be used for compression (see AVVideoSettings.h).
 @param outputFileType
    Specifies the UTI of the file type to be written (see AVMediaFormat.h for a list of file format UTIs).
 @result
    A fully populated dictionary of keys and values that are compatible with AVAssetWriter.
 
 @discussion
    The value of this property is an NSDictionary containing values for compression settings keys defined in AVVideoSettings.h. This dictionary is suitable for use as the "outputSettings" parameter when creating an AVAssetWriterInput, such as,
        
       [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:outputSettings sourceFormatHint:hint];
 
    The dictionary returned contains all necessary keys and values needed by AVAssetWriter (see AVAssetWriterInput.h, -initWithMediaType:outputSettings: for a more in depth discussion). For QuickTime movie and ISO file types, the recommended video settings will produce output comparable to that of AVCaptureMovieFileOutput.
 
    The videoCodecType string provided must be present in the availableVideoCodecTypesForAssetWriterWithOutputFileType: array, or an NSInvalidArgumentException is thrown. 
 
    Note that the dictionary of settings is dependent on the current configuration of the receiver's AVCaptureSession and its inputs. The settings dictionary may change if the session's configuration changes. As such, you should configure your session first, then query the recommended video settings. As of iOS 8.3, movies produced with these settings successfully import into the iOS camera roll and sync to and from like devices via iTunes.
 */
- (nullable NSDictionary *)recommendedVideoSettingsForVideoCodecType:(AVVideoCodecType)videoCodecType assetWriterOutputFileType:(AVFileType)outputFileType API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property availableVideoCVPixelFormatTypes
 @abstract
    Indicates the supported video pixel formats that can be specified in videoSettings.
 
 @discussion
    The value of this property is an NSArray of NSNumbers that can be used as values for the kCVPixelBufferPixelFormatTypeKey in the receiver's videoSettings property. The first format in the returned list is the most efficient output format.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *availableVideoCVPixelFormatTypes API_AVAILABLE(ios(5.0));

/*!
 @property availableVideoCodecTypes
 @abstract
    Indicates the supported video codec formats that can be specified in videoSettings.
 
 @discussion
    The value of this property is an NSArray of AVVideoCodecTypes that can be used as values for the AVVideoCodecKey in the receiver's videoSettings property.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableVideoCodecTypes API_AVAILABLE(ios(5.0));

/*!
 @property minFrameDuration
 @abstract
    Specifies the minimum time interval between which the receiver should output consecutive video frames.
 
 @discussion
    The value of this property is a CMTime specifying the minimum duration of each video frame output by the receiver, placing a lower bound on the amount of time that should separate consecutive frames. This is equivalent to the inverse of the maximum frame rate. A value of kCMTimeZero or kCMTimeInvalid indicates an unlimited maximum frame rate. The default value is kCMTimeInvalid. As of iOS 5.0, minFrameDuration is deprecated. Use AVCaptureConnection's videoMinFrameDuration property instead.
 */
@property(nonatomic) CMTime minFrameDuration API_DEPRECATED("Use AVCaptureConnection's videoMinFrameDuration property instead.", ios(4.0, 5.0)) API_UNAVAILABLE(macos);

/*!
 @property alwaysDiscardsLateVideoFrames
 @abstract
    Specifies whether the receiver should always discard any video frame that is not processed before the next frame is captured.
 
 @discussion
    When the value of this property is YES, the receiver will immediately discard frames that are captured while the dispatch queue handling existing frames is blocked in the captureOutput:didOutputSampleBuffer:fromConnection: delegate method. When the value of this property is NO, delegates will be allowed more time to process old frames before new frames are discarded, but application memory usage may increase significantly as a result. The default value is YES.
 */
@property(nonatomic) BOOL alwaysDiscardsLateVideoFrames;

@end


/*!
 @protocol AVCaptureVideoDataOutputSampleBufferDelegate
 @abstract
    Defines an interface for delegates of AVCaptureVideoDataOutput to receive captured video sample buffers and be notified of late sample buffers that were dropped.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureVideoDataOutputSampleBufferDelegate <NSObject>

@optional

/*!
 @method captureOutput:didOutputSampleBuffer:fromConnection:
 @abstract
    Called whenever an AVCaptureVideoDataOutput instance outputs a new video frame.
 
 @param output
    The AVCaptureVideoDataOutput instance that output the frame.
 @param sampleBuffer
    A CMSampleBuffer object containing the video frame data and additional information about the frame, such as its format and presentation time.
 @param connection
    The AVCaptureConnection from which the video was received.
 
 @discussion
    Delegates receive this message whenever the output captures and outputs a new video frame, decoding or re-encoding it as specified by its videoSettings property. Delegates can use the provided video frame in conjunction with other APIs for further processing. This method will be called on the dispatch queue specified by the output's sampleBufferCallbackQueue property. This method is called periodically, so it must be efficient to prevent capture performance problems, including dropped frames.
 
    Clients that need to reference the CMSampleBuffer object outside of the scope of this method must CFRetain it and then CFRelease it when they are finished with it.
 
    Note that to maintain optimal performance, some sample buffers directly reference pools of memory that may need to be reused by the device system and other capture inputs. This is frequently the case for uncompressed device native capture where memory blocks are copied as little as possible. If multiple sample buffers reference such pools of memory for too long, inputs will no longer be able to copy new samples into memory and those samples will be dropped. If your application is causing samples to be dropped by retaining the provided CMSampleBuffer objects for too long, but it needs access to the sample data for a long period of time, consider copying the data into a new buffer and then calling CFRelease on the sample buffer if it was previously retained so that the memory it references can be reused.
 */
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection;

/*!
 @method captureOutput:didDropSampleBuffer:fromConnection:
 @abstract
    Called once for each frame that is discarded.
 
 @param output
    The AVCaptureVideoDataOutput instance that dropped the frame.
 @param sampleBuffer
    A CMSampleBuffer object containing information about the dropped frame, such as its format and presentation time. This sample buffer will contain none of the original video data.
 @param connection
    The AVCaptureConnection from which the dropped video frame was received.
 
 @discussion
    Delegates receive this message whenever a video frame is dropped. This method is called once for each dropped frame. The CMSampleBuffer object passed to this delegate method will contain metadata about the dropped video frame, such as its duration and presentation time stamp, but will contain no actual video data. On iOS, Included in the sample buffer attachments is the kCMSampleBufferAttachmentKey_DroppedFrameReason, which indicates why the frame was dropped. This method will be called on the dispatch queue specified by the output's sampleBufferCallbackQueue property. Because this method will be called on the same dispatch queue that is responsible for outputting video frames, it must be efficient to prevent further capture performance problems, such as additional dropped video frames.
 */
- (void)captureOutput:(AVCaptureOutput *)output didDropSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(6.0));

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVPlayer.h
/*
    File:  AVPlayer.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

/*!
	@class			AVPlayer
 
	@abstract
      AVPlayer offers a playback interface for single-item playback that's sufficient for
      the implementation of playback controllers and playback user interfaces.
 
	@discussion
      AVPlayer works equally well with local and remote media files, providing clients with appropriate
      information about readiness to play or about the need to await additional data before continuing.

      Visual content of items played by an instance of AVPlayer can be displayed in a CoreAnimation layer
      of class AVPlayerLayer.

	  To allow clients to add and remove their objects as key-value observers safely, AVPlayer serializes notifications of
	  changes that occur dynamically during playback on a dispatch queue. By default, this queue is the main queue. See dispatch_get_main_queue().
	  
	  To ensure safe access to AVPlayer's nonatomic properties while dynamic changes in playback state may be reported, clients must
	  serialize their access with the receiver's notification queue. In the common case, such serialization is naturally achieved
	  by invoking AVPlayer's various methods on the main thread or queue.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVAnimation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <CoreMedia/CMSync.h>
#import <Foundation/Foundation.h>

@class AVPlayerItem;
@class AVPlayerInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @enum AVPlayerStatus
 @abstract
	These constants are returned by the AVPlayer status property to indicate whether it can successfully play items.
 
 @constant	 AVPlayerStatusUnknown
	Indicates that the status of the player is not yet known because it has not tried to load new media resources for
	playback.
 @constant	 AVPlayerStatusReadyToPlay
	Indicates that the player is ready to play AVPlayerItem instances.
 @constant	 AVPlayerStatusFailed
	Indicates that the player can no longer play AVPlayerItem instances because of an error. The error is described by
	the value of the player's error property.
 */
typedef NS_ENUM(NSInteger, AVPlayerStatus) {
	AVPlayerStatusUnknown,
	AVPlayerStatusReadyToPlay,
	AVPlayerStatusFailed
};

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVPlayer : NSObject 
{
@private
	AVPlayerInternal     *_player;
}

/*!
	@method			playerWithURL:
	@abstract		Returns an instance of AVPlayer that plays a single audiovisual resource referenced by URL.
	@param			URL
	@result			An instance of AVPlayer
	@discussion		Implicitly creates an AVPlayerItem. Clients can obtain the AVPlayerItem as it becomes the player's currentItem.
*/
+ (instancetype)playerWithURL:(NSURL *)URL;

/*!
	@method			playerWithPlayerItem:
	@abstract		Create an AVPlayer that plays a single audiovisual item.
	@param			item
	@result			An instance of AVPlayer
	@discussion		Useful in order to play items for which an AVAsset has previously been created. See -[AVPlayerItem initWithAsset:].
*/
+ (instancetype)playerWithPlayerItem:(nullable AVPlayerItem *)item;

/*!
	@method			initWithURL:
	@abstract		Initializes an AVPlayer that plays a single audiovisual resource referenced by URL.
	@param			URL
	@result			An instance of AVPlayer
	@discussion		Implicitly creates an AVPlayerItem. Clients can obtain the AVPlayerItem as it becomes the player's currentItem.
*/
- (instancetype)initWithURL:(NSURL *)URL;

/*!
	@method			initWithPlayerItem:
	@abstract		Create an AVPlayer that plays a single audiovisual item.
	@param			item
	@result			An instance of AVPlayer
	@discussion		Useful in order to play items for which an AVAsset has previously been created. See -[AVPlayerItem initWithAsset:].
*/
- (instancetype)initWithPlayerItem:(nullable AVPlayerItem *)item;

/*!
 @property status
 @abstract
	The ability of the receiver to be used for playback.
 
 @discussion
	The value of this property is an AVPlayerStatus that indicates whether the receiver can be used for playback. When
	the value of this property is AVPlayerStatusFailed, the receiver can no longer be used for playback and a new
	instance needs to be created in its place. When this happens, clients can check the value of the error property to
	determine the nature of the failure. This property is key value observable.
 */
@property (nonatomic, readonly) AVPlayerStatus status;

/*!
 @property error
 @abstract
	If the receiver's status is AVPlayerStatusFailed, this describes the error that caused the failure.
 
 @discussion
	The value of this property is an NSError that describes what caused the receiver to no longer be able to play items.
	If the receiver's status is not AVPlayerStatusFailed, the value of this property is nil.
 */
@property (nonatomic, readonly, nullable) NSError *error;

@end


@interface AVPlayer (AVPlayerPlaybackControl)

/*!
 @property		rate
 @abstract		Indicates the desired rate of playback; 0.0 means "paused", 1.0 indicates a desire to play at the natural rate of the current item.
 @discussion
 Setting the value of rate to 0.0 pauses playback, causing the value of timeControlStatus to change to AVPlayerTimeControlStatusPaused.
 Setting the rate to a non-zero value causes the value of timeControlStatus to become either AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate or AVPlayerTimeControlStatusPlaying, depending on whether sufficient media data has been buffered for playback to occur and whether the player's default behavior of waiting in order to minimize stalling is permitted. See discussion of AVPlayerTimeControlStatus for more details.
 
 AVPlayer can reset the desired rate to 0.0 when a change in overall state requires playback to be halted, such as when an interruption occurs on iOS, as announced by AVAudioSession, or when the playback buffer becomes empty and playback stalls while automaticallyWaitsToMinimizeStalling is NO.

 The effective rate of playback may differ from the desired rate even while timeControlStatus is AVPlayerTimeControlStatusPlaying, if the processing algorithm in use for managing audio pitch requires quantization of playback rate. For information about quantization of rates for audio processing, see AVAudioProcessingSettings.h. You can always obtain the effective rate of playback from the currentItem's timebase; see the timebase property of AVPlayerItem.
 */
@property (nonatomic) float rate;

/*!
 @method		play
 @abstract		Signals the desire to begin playback at the current item's natural rate.
 @discussion	Equivalent to setting the value of rate to 1.0.
 */
- (void)play;

/*!
 @method		pause
 @abstract		Pauses playback.
 @discussion	Equivalent to setting the value of rate to 0.0.
 */
- (void)pause;

/*!
 @enum AVPlayerTimeControlStatus
 @abstract
	These constants are the allowable values of AVPlayer's timeControlStatus property. This discussion pertains when automaticallyWaitsToMinimizeStalling is YES, the default setting, and exceptions are discussed in connection with automaticallyWaitsToMinimizeStalling.
 
 @constant	 AVPlayerTimeControlStatusPaused
	This state is entered upon receipt of a -pause message, an invocation of -setRate: with a value of 0.0, when a change in overall state requires playback to be halted, such as when an interruption occurs on iOS, as announced by AVAudioSession.
    In this state, playback is paused indefinitely and will not resume until 1) a subsequent -play message is received or 2) a -setRate: or -playImmediatelyAtRate: message with a non-zero value for rate is received and sufficient media data has been buffered for playback to proceed.
 @constant	 AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate
    This state is entered when 1) the playback buffer becomes empty and playback stalls in AVPlayerTimeControlStatusPlaying, 2) when rate is set from zero to non-zero in AVPlayerTimeControlStatusPaused and insufficient media data has been buffered for playback to occur, or 3) when the player has no item to play, i.e. when the receiver's currentItem is nil.
    In this state, the value of the rate property is not currently effective but instead indicates the rate at which playback will start or resume. Refer to the value of reasonForWaitingToPlay for details about why the receiver is waiting and the conditions that allow waitStatus to change to AVPlayerWaitStatusPlaying.
	While waiting for buffering, you can attempt to start playback of any available media data via -playImmediatelyAtRate:.
 @constant	 AVPlayerTimeControlStatusPlaying
	In this state, playback is currently progressing and rate changes will take effect immediately. Should playback stall because of insufficient media data, timeControlStatus will change to AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate.
 */
typedef NS_ENUM(NSInteger, AVPlayerTimeControlStatus) {
	AVPlayerTimeControlStatusPaused,
	AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate,
	AVPlayerTimeControlStatusPlaying
} NS_ENUM_AVAILABLE(10_12, 10_0);


/*!
 @property		timeControlStatus
 @abstract		Indicates whether playback is currently paused indefinitely, suspended while waiting for appropriate conditions, or in progress.
 @discussion    For possible values and discussion, see AVPlayerTimeControlStatus.
 
When automaticallyWaitsToMinimizeStalling is YES, absent intervention in the form of invocations of -setRate: or -pause or, on iOS, an interruption that requires user intervention before playback can resume, the value of the property timeControlStatus automatically changes between AVPlayerTimeControlStatusPlaying and AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate depending on whether sufficient media data is available to continue playback. This property is key value observable.
*/
@property (nonatomic, readonly) AVPlayerTimeControlStatus timeControlStatus NS_AVAILABLE(10_12, 10_0);

/*!
 @typedef AVPlayerWaitingReason
 @abstract
    The type of reason that a player is waiting for playback.
*/
typedef NSString * AVPlayerWaitingReason NS_STRING_ENUM;

/*!
 @constant AVPlayerWaitingToMinimizeStallsReason
 @abstract Indicates that the player is waiting for appropriate playback buffer conditions before starting playback
 @discussion
	The player is waiting for playback because automaticallyWaitToMinimizeStalling is YES and playback at the specified rate would likely cause the playback buffer to become empty before playback completes. Playback will resume when 1) playback at the specified rate will likely complete without a stall or 2) the playback buffer becomes full, meaning no forther buffering of media data is possible.
	When the value of automaticallyWaitsToMinimizeStalling is NO, timeControlStatus cannot become AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate for this reason.
 */
AVF_EXPORT AVPlayerWaitingReason const AVPlayerWaitingToMinimizeStallsReason NS_AVAILABLE(10_12, 10_0);

/*!
 @constant AVPlayerWaitingWhileEvaluatingBufferingRateReason
 @abstract Indicates that the player is monitoring the playback buffer fill rate to determine if playback is likely to complete without interruptions.
 @discussion
	The player is waiting for playback because automaticallyWaitToMinimizeStalling is YES and it has not yet determined if starting playback at the specified rate would likely cause the buffer to become empty. When the brief initial monitoring period is over, either playback will begin or the value of reasonForWaitingToPlayAtSpecifiedRate will switch to AVPlayerWaitingToMinimizeStallsReason.
	Recommended practice is not to show UI indicating the waiting state to the user while the value of reasonForWaitingToPlayAtSpecifiedRate is AVPlayerWaitingWhileEvaluatingBufferingRateReason.
 */
AVF_EXPORT AVPlayerWaitingReason const AVPlayerWaitingWhileEvaluatingBufferingRateReason NS_AVAILABLE(10_12, 10_0);

/*!
 @constant AVPlayerWaitingWithNoItemToPlayReason
 @abstract Indicates that the AVPlayer is waiting because its currentItem is nil
 @discussion
	The player is waiting for playback because automaticallyWaitToMinimizeStalling is YES and the value of currentItem is nil. When an item becomes available, either because of a call to -replaceCurrentItemWithPlayerItem: or  -insertItem: afterItem:, playback will begin or the value of reasonForWaitingToPlay will change.
 */
AVF_EXPORT AVPlayerWaitingReason const AVPlayerWaitingWithNoItemToPlayReason NS_AVAILABLE(10_12, 10_0);


/*!
 @property		reasonForWaitingToPlay
 @abstract		Indicates the reason for waiting when the value of timeControlStatus is AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate
 @discussion
    When the value of timeControlStatus is AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate, this property describes why the player is currently waiting. It is nil otherwise.
    You can use the value of reasonForWaitingToPlay to show UI indicating the player's waiting state conditionally.
    This property is key value observable.
    Possible values are AVPlayerWaitingWithNoItemToPlayReason, AVPlayerWaitingWhileEvaluatingBufferingRateReason, and AVPlayerWaitingToMinimizeStallsReason.
*/

@property (nonatomic, readonly, nullable) AVPlayerWaitingReason reasonForWaitingToPlay NS_AVAILABLE(10_12, 10_0);


/*!
 @method		playImmediatelyAtRate:
 @abstract		Immediately plays the available media data at the specified rate.
 @discussion
 When the player's currentItem has a value of NO for playbackBufferEmpty, this method causes the value of rate to change to the specified rate, the value of timeControlStatus to change to AVPlayerTimeControlStatusPlaying, and the receiver to play the available media immediately, whether or not prior buffering of media data is sufficient to ensure smooth playback.
 If insufficient media data is buffered for playback to start (e.g. if the current item has a value of YES for playbackBufferEmpty), the receiver will act as if the buffer became empty during playback, except that no AVPlayerItemPlaybackStalledNotification will be posted.
 */
- (void)playImmediatelyAtRate:(float)rate NS_AVAILABLE(10_12, 10_0);

@end


@interface AVPlayer (AVPlayerItemControl)

/* indicates the current item of the player */
@property (nonatomic, readonly, nullable) AVPlayerItem *currentItem;

/*!
	@method			replaceCurrentItemWithPlayerItem:
	@abstract		Replaces the player's current item with the specified player item.
	@param			item
	  The AVPlayerItem that will become the player's current item.
	@discussion
	  In all releases of iOS 4, invoking replaceCurrentItemWithPlayerItem: with an AVPlayerItem that's already the receiver's currentItem results in an exception being raised. Starting with iOS 5, it's a no-op.
*/
- (void)replaceCurrentItemWithPlayerItem:(nullable AVPlayerItem *)item;

/*!
 @enum AVPlayerActionAtItemEnd
 @abstract
	These constants are the allowable values of AVPlayer's actionAtItemEnd property.
 
 @constant	 AVPlayerActionAtItemEndAdvance
	Indicates that when an AVPlayerItem reaches its end time the player will automatically advance to the next item in its queue.
	This value is supported only for players of class AVQueuePlayer. An AVPlayer that's not an AVQueuePlayer will raise an NSInvalidArgumentException if an attempt is made to set its actionAtItemEnd to AVPlayerActionAtItemEndAdvance.
 @constant	 AVPlayerActionAtItemEndPause
	Indicates that when an AVPlayerItem reaches its end time the player will automatically pause (which is to say, the player's
	rate will automatically be set to 0).
 @constant	 AVPlayerActionAtItemEndNone
	Indicates that when an AVPlayerItem reaches its end time the player will take no action (which is to say, the player's rate
	will not change, its currentItem will not change, and its currentTime will continue to be incremented or decremented as time
	elapses, according to its rate). After this, if the player's actionAtItemEnd is set to a value other than AVPlayerActionAtItemEndNone,
	the player will immediately take the action appropriate to that value.
*/
typedef NS_ENUM(NSInteger, AVPlayerActionAtItemEnd)
{
    AVPlayerActionAtItemEndAdvance	= 0,
	AVPlayerActionAtItemEndPause	= 1,
	AVPlayerActionAtItemEndNone		= 2,
};

/* indicates the action that the player should perform when playback of an item reaches its end time */
@property (nonatomic) AVPlayerActionAtItemEnd actionAtItemEnd;

@end


@interface AVPlayer (AVPlayerTimeControl)
/*!
 @method			currentTime
 @abstract			Returns the current time of the current item.
 @result			A CMTime
 @discussion		Returns the current time of the current item. Not key-value observable; use -addPeriodicTimeObserverForInterval:queue:usingBlock: instead.
 */
- (CMTime)currentTime;

/*!
 @method			seekToDate:
 @abstract			Moves the playback cursor.
 @param				date
 @discussion		Use this method to seek to a specified time for the current player item.
					The time seeked to may differ from the specified time for efficiency. For sample accurate seeking see seekToTime:toleranceBefore:toleranceAfter:.
 */
- (void)seekToDate:(NSDate *)date;

/*!
 @method			seekToDate:completionHandler:
 @abstract			Moves the playback cursor and invokes the specified block when the seek operation has either been completed or been interrupted.
 @param				date
 @param				completionHandler
 @discussion		Use this method to seek to a specified time for the current player item and to be notified when the seek operation is complete.
					The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter 
					set to NO. If the new request completes without being interrupted by another seek request or by any other operation the specified 
					completion handler will be invoked with the finished parameter set to YES. 
 */
- (void)seekToDate:(NSDate *)date completionHandler:(void (^)(BOOL finished))completionHandler NS_AVAILABLE(10_7, 5_0);

/*!
 @method			seekToTime:
 @abstract			Moves the playback cursor.
 @param				time
 @discussion		Use this method to seek to a specified time for the current player item.
					The time seeked to may differ from the specified time for efficiency. For sample accurate seeking see seekToTime:toleranceBefore:toleranceAfter:.
 */
- (void)seekToTime:(CMTime)time;

/*!
 @method			seekToTime:toleranceBefore:toleranceAfter:
 @abstract			Moves the playback cursor within a specified time bound.
 @param				time
 @param				toleranceBefore
 @param				toleranceAfter
 @discussion		Use this method to seek to a specified time for the current player item.
					The time seeked to will be within the range [time-toleranceBefore, time+toleranceAfter] and may differ from the specified time for efficiency.
					Pass kCMTimeZero for both toleranceBefore and toleranceAfter to request sample accurate seeking which may incur additional decoding delay. 
					Messaging this method with beforeTolerance:kCMTimePositiveInfinity and afterTolerance:kCMTimePositiveInfinity is the same as messaging seekToTime: directly.
 */
- (void)seekToTime:(CMTime)time toleranceBefore:(CMTime)toleranceBefore toleranceAfter:(CMTime)toleranceAfter;

/*!
 @method			seekToTime:completionHandler:
 @abstract			Moves the playback cursor and invokes the specified block when the seek operation has either been completed or been interrupted.
 @param				time
 @param				completionHandler
 @discussion		Use this method to seek to a specified time for the current player item and to be notified when the seek operation is complete.
					The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter 
					set to NO. If the new request completes without being interrupted by another seek request or by any other operation the specified 
					completion handler will be invoked with the finished parameter set to YES. 
 */
- (void)seekToTime:(CMTime)time completionHandler:(void (^)(BOOL finished))completionHandler NS_AVAILABLE(10_7, 5_0);

/*!
 @method			seekToTime:toleranceBefore:toleranceAfter:completionHandler:
 @abstract			Moves the playback cursor within a specified time bound and invokes the specified block when the seek operation has either been completed or been interrupted.
 @param				time
 @param				toleranceBefore
 @param				toleranceAfter
 @discussion		Use this method to seek to a specified time for the current player item and to be notified when the seek operation is complete.
					The time seeked to will be within the range [time-toleranceBefore, time+toleranceAfter] and may differ from the specified time for efficiency.
					Pass kCMTimeZero for both toleranceBefore and toleranceAfter to request sample accurate seeking which may incur additional decoding delay. 
					Messaging this method with beforeTolerance:kCMTimePositiveInfinity and afterTolerance:kCMTimePositiveInfinity is the same as messaging seekToTime: directly.
					The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter set to NO. If the new 
					request completes without being interrupted by another seek request or by any other operation the specified completion handler will be invoked with the 
					finished parameter set to YES.
 */
- (void)seekToTime:(CMTime)time toleranceBefore:(CMTime)toleranceBefore toleranceAfter:(CMTime)toleranceAfter completionHandler:(void (^)(BOOL finished))completionHandler NS_AVAILABLE(10_7, 5_0);

@end


@interface AVPlayer (AVPlayerAdvancedRateControl)


/*!
 @property		automaticallyWaitsToMinimizeStalling
 @abstract		Indicates that the player is allowed to delay playback at the specified rate in order to minimize stalling
 @discussion
 
 When this property is YES, whenever 1) the rate is set from zero to non-zero or 2) the playback buffer becomes empty and playback stalls, the player will attempt to determine if, at the specified rate, its currentItem will play to the end without interruptions. Should it determine that such interruptions would occur and these interruptions can be avoided by delaying the start or resumption of playback, the value of timeControlStatus will become AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate and playback will start automatically when the likelihood of stalling has been minimized.
 
 You may want to set this property to NO when you need precise control over playback start times, e.g., when synchronizing multiple instances of AVPlayer, and you should set it to NO if you use an AVAssetResourceLoader delegate to load media data (more on this below). If the value of this property is NO, reasonForWaitingToPlay cannot assume a value of AVPlayerWaitingToMinimizeStallsReason.
 This implies that setting rate to a non-zero value in AVPlayerTimeControlStatusPaused will cause playback to start immediately as long as the playback buffer is not empty. When the playback buffer becomes empty during AVPlayerTimeControlStatusPlaying and playback stalls, playback state will switch to AVPlayerTimeControlStatusPaused and the rate will become 0.0.
 
 Changing the value of this property to NO while the value of timeControlStatus is AVPlayerTimeControlStatusWaitingToPlayAtSpecifiedRate with a reasonForWaitingToPlay of AVPlayerWaitingToMinimizeStallsReason will cause the player to attempt playback at the specified rate immediately.
 
 For clients linked against iOS 10.0 and running on that version or later or linked against OS X 10.12 and running on that version or later, the default value of this property is YES.
 In versions of iOS prior to iOS 10.0 and versions of OS X prior to 10.12, this property is unavailable, and the behavior of the AVPlayer corresponds to the type of content being played. For streaming content, including HTTP Live Streaming, the AVPlayer acts as if automaticallyWaitsToMinimizeStalling is YES. For file-based content, including file-based content accessed via progressive http download, the AVPlayer acts as if automaticallyWaitsToMinimizeStalling is NO.

 If you employ an AVAssetResourceLoader delegate that loads media data for playback, you should set the value of your AVPlayer’s automaticallyWaitsToMinimizeStalling property to NO. Allowing the value of automaticallyWaitsToMinimizeStalling to remain YES when an AVAssetResourceLoader delegate is used for the loading of media data can result in poor start-up times for playback and poor recovery from stalls, because the behaviors provided by AVPlayer when automaticallyWaitsToMinimizeStalling has a value of YES depend on predictions of the future availability of media data that that do not function as expected when data is loaded via a client-controlled means, using the AVAssetResourceLoader delegate interface.

 You can allow the value of automaticallyWaitsToMinimizeStalling to remain YES if you use an AVAssetResourceLoader delegate to manage content keys for FairPlay Streaming, to provide dynamically-generated master playlists for HTTP Live Streaming, or to respond to authentication challenges, but not to load media data for playback.
*/

@property (nonatomic) BOOL automaticallyWaitsToMinimizeStalling NS_AVAILABLE(10_12, 10_0);



/*!
	@method			setRate:time:atHostTime:
	@abstract		Simultaneously sets the playback rate and the relationship between the current item's current time and host time.
	@discussion		You can use this function to synchronize playback with an external activity.
	
					The current item's timebase is adjusted so that its time will be (or was) itemTime when host time is (or was) hostClockTime.
					In other words: if hostClockTime is in the past, the timebase's time will be interpolated as though the timebase has been running at the requested rate since that time.  If hostClockTime is in the future, the timebase will immediately start running at the requested rate from an earlier time so that it will reach the requested itemTime at the requested hostClockTime.  (Note that the item's time will not jump backwards, but instead will sit at itemTime until the timebase reaches that time.)

					Note that setRate:time:atHostTime: is not currently supported for HTTP Live Streaming or when automaticallyWaitsToMinimizeStalling is YES. For clients linked against iOS 10.0 and later or OS X 12.0 and later, invoking setRate:time:atHostTime: when automaticallyWaitsToMinimizeStalling is YES will raise an NSInvalidArgument exception.
	@param itemTime	The time to start playback from, specified precisely (i.e., with zero tolerance).
					Pass kCMTimeInvalid to use the current item's current time.
	@param hostClockTime
					The host time at which to start playback.
					If hostClockTime is specified, the player will not ensure that media data is loaded before the timebase starts moving.
					If hostClockTime is kCMTimeInvalid, the rate and time will be set together, but without external synchronization;
					a host time in the near future will be used, allowing some time for media data loading.
*/
- (void)setRate:(float)rate time:(CMTime)itemTime atHostTime:(CMTime)hostClockTime NS_AVAILABLE(10_8, 6_0);

/*!
	@method			prerollAtRate:completionHandler:
	@abstract		Begins loading media data to prime the render pipelines for playback from the current time with the given rate.
	@discussion		Once the completion handler is called with YES, the player's rate can be set with minimal latency.
					The completion handler will be called with NO if the preroll is interrupted by a time change or incompatible rate change, or if preroll is not possible for some other reason.
					Call this method only when the rate is currently zero and only after the AVPlayer's status has become AVPlayerStatusReadyToPlay.

					Note that advanced rate control is not currently supported for HTTP Live Streaming.
	@param rate		The intended rate for subsequent playback.
	@param completionHandler
					The block that will be called when the preroll is either completed or is interrupted.
*/
- (void)prerollAtRate:(float)rate completionHandler:(nullable void (^)(BOOL finished))completionHandler NS_AVAILABLE(10_8, 6_0);

/*!
	@method			cancelPendingPrerolls
	@abstract		Cancel any pending preroll requests and invoke the corresponding completion handlers if present.
	@discussion		Use this method to cancel and release the completion handlers for pending prerolls. The finished parameter of the completion handlers will be set to NO.
*/
- (void)cancelPendingPrerolls NS_AVAILABLE(10_8, 6_0);

/* NULL by default.  if not NULL, overrides the automatic choice of master clock for item timebases. This is most useful for synchronizing video-only movies with audio played via other means. IMPORTANT: If you specify a master clock other than the appropriate audio device clock, audio may drift out of sync. */
@property (nonatomic, retain, nullable) __attribute__((NSObject)) CMClockRef masterClock NS_AVAILABLE(10_8, 6_0);

@end


@interface AVPlayer (AVPlayerTimeObservation)

/*!
	@method			addPeriodicTimeObserverForInterval:queue:usingBlock:
	@abstract		Requests invocation of a block during playback to report changing time.
	@param			interval
	  The interval of invocation of the block during normal playback, according to progress of the current time of the player.
	@param			queue
	  The serial queue onto which block should be enqueued.  If you pass NULL, the main queue (obtained using dispatch_get_main_queue()) will be used.  Passing a
	  concurrent queue to this method will result in undefined behavior.
	@param			block
	  The block to be invoked periodically.
	@result
	  An object conforming to the NSObject protocol.  You must retain this returned value as long as you want the time observer to be invoked by the player.
	  Pass this object to -removeTimeObserver: to cancel time observation.
	@discussion		The block is invoked periodically at the interval specified, interpreted according to the timeline of the current item.
					The block is also invoked whenever time jumps and whenever playback starts or stops.
					If the interval corresponds to a very short interval in real time, the player may invoke the block less frequently
					than requested. Even so, the player will invoke the block sufficiently often for the client to update indications
					of the current time appropriately in its end-user interface.
					Each call to -addPeriodicTimeObserverForInterval:queue:usingBlock: should be paired with a corresponding call to -removeTimeObserver:.
					Releasing the observer object without a call to -removeTimeObserver: will result in undefined behavior.
*/
- (id)addPeriodicTimeObserverForInterval:(CMTime)interval queue:(nullable dispatch_queue_t)queue usingBlock:(void (^)(CMTime time))block;

/*!
	@method			addBoundaryTimeObserverForTimes:queue:usingBlock:
	@abstract		Requests invocation of a block when specified times are traversed during normal playback.
	@param			times
	  The times for which the observer requests notification, supplied as an array of NSValues carrying CMTimes.
	@param			queue
	  The serial queue onto which block should be enqueued.  If you pass NULL, the main queue (obtained using dispatch_get_main_queue()) will be used.  Passing a
	  concurrent queue to this method will result in undefined behavior.
	@param			block
	  The block to be invoked when any of the specified times is crossed during normal playback.
	@result
	  An object conforming to the NSObject protocol.  You must retain this returned value as long as you want the time observer to be invoked by the player.
	  Pass this object to -removeTimeObserver: to cancel time observation.
	@discussion		Each call to -addPeriodicTimeObserverForInterval:queue:usingBlock: should be paired with a corresponding call to -removeTimeObserver:.
					Releasing the observer object without a call to -removeTimeObserver: will result in undefined behavior.
*/
- (id)addBoundaryTimeObserverForTimes:(NSArray<NSValue *> *)times queue:(nullable dispatch_queue_t)queue usingBlock:(void (^)(void))block;

/*!
	@method			removeTimeObserver:
	@abstract		Cancels a previously registered time observer.
	@param			observer
	  An object returned by a previous call to -addPeriodicTimeObserverForInterval:queue:usingBlock: or -addBoundaryTimeObserverForTimes:queue:usingBlock:.
	@discussion		Upon return, the caller is guaranteed that no new time observer blocks will begin executing.  Depending on the calling thread and the queue
					used to add the time observer, an in-flight block may continue to execute after this method returns.  You can guarantee synchronous time 
					observer removal by enqueuing the call to -removeTimeObserver: on that queue.  Alternatively, call dispatch_sync(queue, ^{}) after
					-removeTimeObserver: to wait for any in-flight blocks to finish executing.
					-removeTimeObserver: should be used to explicitly cancel each time observer added using -addPeriodicTimeObserverForInterval:queue:usingBlock:
					and -addBoundaryTimeObserverForTimes:queue:usingBlock:.
*/
- (void)removeTimeObserver:(id)observer;

@end


@interface AVPlayer (AVPlayerMediaControl)

/* Indicates the current audio volume of the player; 0.0 means "silence all audio", 1.0 means "play at the full volume of the current item".

   iOS note: Do not use this property to implement a volume slider for media playback. For that purpose, use MPVolumeView, which is customizable in appearance and provides standard media playback behaviors that users expect.
   This property is most useful on iOS to control the volume of the AVPlayer relative to other audio output, not for volume control by end users. */
@property (nonatomic) float volume NS_AVAILABLE(10_7, 7_0);

/* indicates whether or not audio output of the player is muted. Only affects audio muting for the player instance and not for the device. */
@property (nonatomic, getter=isMuted) BOOL muted NS_AVAILABLE(10_7, 7_0);

@end


@class AVPlayerMediaSelectionCriteria;

@interface AVPlayer (AVPlayerAutomaticMediaSelection)

/* Indicates whether the receiver should apply the current selection criteria automatically to AVPlayerItems.
 For clients linked against the iOS 7 SDK or later or against the OS X 10.9 SDK or later, the default is YES. For all others, the default is NO.

 By default, AVPlayer applies selection criteria based on system preferences. To override the default criteria for any media selection group, use -[AVPlayer setMediaSelectionCriteria:forMediaCharacteristic:].
*/
@property (nonatomic) BOOL appliesMediaSelectionCriteriaAutomatically NS_AVAILABLE(10_9, 7_0);

/*!
 @method     setMediaSelectionCriteria:forMediaCharacteristic:
 @abstract   Applies automatic selection criteria for media that has the specified media characteristic.
 @param      criteria
   An instance of AVPlayerMediaSelectionCriteria.
 @param      mediaCharacteristic
   The media characteristic for which the selection criteria are to be applied. Supported values include AVMediaCharacteristicAudible, AVMediaCharacteristicLegible, and AVMediaCharacteristicVisual.
 @discussion
	Criteria will be applied to an AVPlayerItem when:
		a) It is made ready to play
		b) Specific media selections are made by -[AVPlayerItem selectMediaOption:inMediaSelectionGroup:] in a different group. The automatic choice in one group may be influenced by a specific selection in another group.
		c) Underlying system preferences change, e.g. system language, accessibility captions.

   Specific selections made by -[AVPlayerItem selectMediaOption:inMediaSelectionGroup:] within any group will override automatic selection in that group until -[AVPlayerItem selectMediaOptionAutomaticallyInMediaSelectionGroup:] is received.
*/
- (void)setMediaSelectionCriteria:(nullable AVPlayerMediaSelectionCriteria *)criteria forMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic NS_AVAILABLE(10_9, 7_0);

/*!
 @method     mediaSelectionCriteriaForMediaCharacteristic:
 @abstract   Returns the automatic selection criteria for media that has the specified media characteristic.
 @param      mediaCharacteristic
  The media characteristic for which the selection criteria is to be returned. Supported values include AVMediaCharacteristicAudible, AVMediaCharacteristicLegible, and AVMediaCharacteristicVisual.
*/
- (nullable AVPlayerMediaSelectionCriteria *)mediaSelectionCriteriaForMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic NS_AVAILABLE(10_9, 7_0);

@end


@interface AVPlayer (AVPlayerAudioDeviceSupport)

/*!
 @property audioOutputDeviceUniqueID
 @abstract
	Specifies the unique ID of the Core Audio output device used to play audio.
 @discussion
	By default, the value of this property is nil, indicating that the default audio output device is used. Otherwise the value of this property is an NSString containing the unique ID of the Core Audio output device to be used for audio output.

	Core Audio's kAudioDevicePropertyDeviceUID is a suitable source of audio output device unique IDs.
*/
@property (nonatomic, copy, nullable) NSString *audioOutputDeviceUniqueID NS_AVAILABLE_MAC(10_9);

@end

/*
 @category		
	AVPlayer (AVPlayerExternalPlaybackSupport)
 
 @abstract
	Methods for supporting "external playback" of video 
 
 @discussion
	"External playback" is a mode where video data is sent to an external device for full screen playback at its original fidelity.
	AirPlay Video playback is considered as an "external playback" mode.
 
	In "external screen" mode (also known as mirroring and second display), video data is rendered on the host 
	device (e.g. Mac and iPhone), rendered video is recompressed and transferred to the external device, and the
	external device decompresses and displays the video.
 
	AVPlayerExternalPlaybackSupport properties affect AirPlay Video playback and are the replacement for the 
	deprecated AVPlayerAirPlaySupport properties.
 
	Additional note for iOS: AVPlayerExternalPlaybackSupport properties apply to the Lightning-based
	video adapters but do not apply to 30-pin-connector-based video output cables and adapters.
 */

@interface AVPlayer (AVPlayerExternalPlaybackSupport)

/* Indicates whether the player allows switching to "external playback" mode. The default value is YES. */
@property (nonatomic) BOOL allowsExternalPlayback NS_AVAILABLE(10_11, 6_0);

/* Indicates whether the player is currently playing video in "external playback" mode. */
@property (nonatomic, readonly, getter=isExternalPlaybackActive) BOOL externalPlaybackActive NS_AVAILABLE(10_11, 6_0);

/* Indicates whether the player should automatically switch to "external playback" mode while the "external 
	screen" mode is active in order to play video content and switching back to "external screen" mode as soon 
	as playback is done. Brief transition may be visible on the external display when automatically switching 
	between the two modes. The default value is NO. Has no effect if allowsExternalPlayback is NO. */
@property (nonatomic) BOOL usesExternalPlaybackWhileExternalScreenIsActive NS_AVAILABLE_IOS(6_0);

/* Video gravity strictly for "external playback" mode, one of AVLayerVideoGravity* defined in AVAnimation.h */
@property (nonatomic, copy) AVLayerVideoGravity externalPlaybackVideoGravity NS_AVAILABLE_IOS(6_0);

@end

#if TARGET_OS_IPHONE

@interface AVPlayer (AVPlayerAirPlaySupport)

/* Indicates whether the player allows AirPlay Video playback. The default value is YES. 
	This property is deprecated. Use AVPlayer's -allowsExternalPlayback instead. */
@property (nonatomic) BOOL allowsAirPlayVideo NS_DEPRECATED_IOS(5_0, 6_0);

/* Indicates whether the player is currently playing video via AirPlay. 
	This property is deprecated. Use AVPlayer's -externalPlaybackActive instead.*/
@property (nonatomic, readonly, getter=isAirPlayVideoActive) BOOL airPlayVideoActive NS_DEPRECATED_IOS(5_0, 6_0);

/* Indicates whether the player should automatically switch to AirPlay Video while AirPlay Screen is active in order to play video content, switching back to AirPlay Screen as soon as playback is done. 
	The default value is NO. Has no effect if allowsAirPlayVideo is NO.
	This property is deprecated. Use AVPlayer's -usesExternalPlaybackWhileExternalScreenIsActive instead. */
@property (nonatomic) BOOL usesAirPlayVideoWhileAirPlayScreenIsActive NS_DEPRECATED_IOS(5_0, 6_0);

@end

#endif // TARGET_OS_IPHONE

/*
	@category		AVPlayer (AVPlayerProtectedContent)
	@abstract		Methods supporting protected content.
*/

@interface AVPlayer (AVPlayerProtectedContent)

/*!
	@property outputObscuredDueToInsufficientExternalProtection
	@abstract
		Whether or not decoded output is being obscured due to insufficient external protection.
 
	@discussion
		The value of this property indicates whether the player is purposefully obscuring the visual output
		of the current item because the requirement for an external protection mechanism is not met by the
		current device configuration. It is highly recommended that clients whose content requires external
		protection observe this property and set the playback rate to zero and display an appropriate user
		interface when the value changes to YES. This property is key value observable.

		Note that the value of this property is dependent on the external protection requirements of the
		current item. These requirements are inherent to the content itself and cannot be externally specified.
		If the current item does not require external protection, the value of this property will be NO.
 */
@property (nonatomic, readonly) BOOL outputObscuredDueToInsufficientExternalProtection NS_AVAILABLE(10_12, 6_0);

@end

/*!
 @typedef AVPlayerHDRMode
 @abstract  A bitfield type that specifies an HDR mode.
 
 @constant	AVPlayerHDRModeHLG
 @abstract	Indicates that HLG (Hybrid Log-Gamma) HDR mode is available.
 @constant	AVPlayerHDRModeHDR10
 @abstract	Indicates that HDR10 HDR mode is available.
 @constant	AVPlayerHDRModeDolbyVision
 @abstract	Indicates that Dolby Vision HDR mode is available.
 */
typedef NS_OPTIONS(NSInteger, AVPlayerHDRMode) {
	AVPlayerHDRModeHLG				= 0x1,
	AVPlayerHDRModeHDR10				= 0x2,
	AVPlayerHDRModeDolbyVision		= 0x4,
} API_AVAILABLE(ios(11.2), tvos(11.2)) API_UNAVAILABLE(macos, watchos);

@interface AVPlayer (AVPlayerPlaybackCapabilities)

/*!
	 @property		availableHDRModes
	 @abstract		An AVPlayerHDRMode value that indicates the HDR modes the device can play to an appropriate display.   A value of 0 indicates that no HDR modes are supported.
 
	 @discussion
		 This property indicates all of the HDR modes that the device can play.  Each value indicates that an appropriate HDR display is available for the specified HDR mode.  Additionally, the device must be capable of playing the specified HDR type.  This property does not indicate whether video contains HDR content, whether HDR video is currently playing, or whether video is playing on an HDR display.
*/
@property (class, nonatomic, readonly) AVPlayerHDRMode availableHDRModes API_AVAILABLE(ios(11.2), tvos(11.2)) API_UNAVAILABLE(macos, watchos);

/*!
	 @constant		AVPlayerAvailableHDRModesDidChangeNotification
	 @abstract		A notification that fires whenever availableHDRModes changes.
 
	 @discussion
		 This notification fires when a value is added or removed from the list of availableHDRModes.  This can be caused by display connection/disconnection or resource changes.
*/
API_AVAILABLE(ios(11.2), tvos(11.2)) API_UNAVAILABLE(macos, watchos)
AVF_EXPORT NSNotificationName const AVPlayerAvailableHDRModesDidChangeNotification;

@end

@interface AVPlayer (AVPlayerVideoDecoderGPUSupport)

/*!
	@property		preferredVideoDecoderGPURegistryID
	@abstract		Specifies a registryID associated with a GPU that should be used for video decode.

	@discussion
		By default, whenever possible, video decode will be performed on the GPU associated with the display on which the presenting CALayer is located.  Decode will be transitioned to a new GPU if appropriate when the CALayer moves to a new display.  This property overrides this default behavior, forcing decode to prefer an affinity to the GPU specified regardless of which GPU is being used to display the associated CALayer.

		The GPU registryID can be obtained from the GPU MTLDevice using [MTLDevice registryID] or can be obtained from OpenGL or OpenCL.
*/
@property (nonatomic) uint64_t preferredVideoDecoderGPURegistryID API_AVAILABLE(macos(10.13)) API_UNAVAILABLE(ios, tvos, watchos);

@end

@interface AVPlayer (AVPlayerVideoDisplaySleepPrevention)

/*!
 @property   preventsDisplaySleepDuringVideoPlayback
 @abstract   Indicates whether video playback prevents display and device sleep.
 @discussion
	 Default is YES on iOS.  Default is NO on macOS.
	 Setting this property to NO does not force the display to sleep, it simply stops preventing display sleep.  Other apps or frameworks within your app may still be preventing display sleep for various reasons.
 */
@property (nonatomic) BOOL preventsDisplaySleepDuringVideoPlayback API_AVAILABLE(ios(12.0), tvos(12.0), macos(10.14)) __WATCHOS_PROHIBITED;

@end

@interface AVPlayer (AVPlayerDeprecated)

/*!
	@property closedCaptionDisplayEnabled
	@abstract
		Indicates whether display of closed captions is enabled.

	@discussion
		This property is deprecated.

		When the value of appliesMediaSelectionCriteriaAutomatically is YES, the receiver will enable closed captions automatically either according to user preferences or, if you provide them, according to AVPlayerMediaSelectionCriteria for the media characteristic AVMediaCharacteristicLegible.

		If you want to determine whether closed captions may be available for a given AVPlayerItem, you can examine the AVMediaSelectionOptions in the AVMediaSelectionGroup for the characteristic AVMediaCharacteristicLegible, as vended by -[AVAsset mediaSelectionGroupForMediaCharacteristic:]. See AVMediaCharacteristicTranscribesSpokenDialogForAccessibility and AVMediaCharacteristicDescribesMusicAndSoundForAccessibility as documented in AVMediaFormat.h for information about how to identify legible media selection options that offer the features of closed captions for accessibility purposes.

		You can select or deselect a specific AVMediaSelectionOption via -[AVPlayerItem selectMediaOption:inMediaSelectionGroup:].

		For further information about Media Accessibility preferences, see MediaAccessibility framework documentation.
 */
@property (nonatomic, getter=isClosedCaptionDisplayEnabled) BOOL closedCaptionDisplayEnabled NS_DEPRECATED(10_7, 10_13, 4_0, 11_0, "Allow AVPlayer to enable closed captions automatically according to user preferences by ensuring that the value of appliesMediaSelectionCriteriaAutomatically is YES.");

@end

/*!
	@class			AVQueuePlayer
 
	@abstract
      AVQueuePlayer is a subclass of AVPlayer that offers an interface for multiple-item playback.
 
	@discussion
      AVQueuePlayer extends AVPlayer with methods for managing a queue of items to be played in sequence.
      It plays these items as gaplessly as possible in the current runtime environment, depending on 
      the timely availability of media data for the enqueued items.
      
      For best performance clients should typically enqueue only as many AVPlayerItems as are necessary
      to ensure smooth playback. Note that once an item is enqueued it becomes eligible to be loaded and
      made ready for playback, with whatever I/O and processing overhead that entails.

*/

@class AVQueuePlayerInternal;

NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVQueuePlayer : AVPlayer 
{
@private
    AVQueuePlayerInternal   *_queuePlayer;
}

/*!
    @method     queuePlayerWithItems:
    @abstract   Creates an instance of AVQueuePlayer and enqueues the AVPlayerItems from the specified array.
    @param      items
      An NSArray of AVPlayerItems with which to populate the player's queue initially.
    @result
      An instance of AVQueuePlayer.
*/
+ (instancetype)queuePlayerWithItems:(NSArray<AVPlayerItem *> *)items;

/*!
    @method     initWithItems:
    @abstract   Initializes an instance of AVQueuePlayer by enqueueing the AVPlayerItems from the specified array.
    @param      items
      An NSArray of AVPlayerItems with which to populate the player's queue initially.
    @result
      An instance of AVQueuePlayer.
*/
- (AVQueuePlayer *)initWithItems:(NSArray<AVPlayerItem *> *)items;

/*!
    @method     items
    @abstract   Provides an array of the currently enqueued items.
    @result     An NSArray containing the enqueued AVPlayerItems.
*/
- (NSArray<AVPlayerItem *> *)items;

/*!
    @method     advanceToNextItem
    @abstract   Ends playback of the current item and initiates playback of the next item in the player's queue.
    @discussion Removes the current item from the play queue.
*/
- (void)advanceToNextItem;

/*!
    @method     canInsertItem:afterItem:
    @abstract   Tests whether an AVPlayerItem can be inserted into the player's queue.
    @param      item
      The AVPlayerItem to be tested.
    @param      afterItem
      The item that the item to be tested is to follow in the queue. Pass nil to test whether the item can be appended to the queue.
    @result
      An indication of whether the item can be inserted into the queue after the specified item.
    @discussion
      Note that adding the same AVPlayerItem to an AVQueuePlayer at more than one position in the queue is not supported.
*/
- (BOOL)canInsertItem:(AVPlayerItem *)item afterItem:(nullable AVPlayerItem *)afterItem;

/*!
    @method     insertItem:afterItem:
    @abstract   Places an AVPlayerItem after the specified item in the queue.
    @param      item
      The item to be inserted.
    @param      afterItem
      The item that the newly inserted item should follow in the queue. Pass nil to append the item to the queue.
*/
- (void)insertItem:(AVPlayerItem *)item afterItem:(nullable AVPlayerItem *)afterItem;

/*!
    @method     removeItem:
    @abstract   Removes an AVPlayerItem from the queue.
    @param      item
      The item to be removed.
    @discussion
      If the item to be removed is currently playing, has the same effect as -advanceToNextItem.
*/
- (void)removeItem:(AVPlayerItem *)item;

/*!
    @method     removeAllItems
    @abstract   Removes all items from the queue.
    @discussion Stops playback by the target.
*/
- (void)removeAllItems;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureSession.h
/*
    File:  AVCaptureSession.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVCaptureDevice.h>
#import <AVFoundation/AVCaptureSessionPreset.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMFormatDescription.h>
#import <CoreMedia/CMSync.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @constant AVCaptureSessionRuntimeErrorNotification
 @abstract
    Posted when an unexpected error occurs while an AVCaptureSession instance is running.
 
 @discussion
    The notification object is the AVCaptureSession instance that encountered a runtime error. The userInfo dictionary contains an NSError for the key AVCaptureSessionErrorKey.
 */
AVF_EXPORT NSString *const AVCaptureSessionRuntimeErrorNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionErrorKey
 @abstract
    The key used to provide an NSError describing the failure condition in an AVCaptureSessionRuntimeErrorNotification.
 
 @discussion
    AVCaptureSessionErrorKey may be found in the userInfo dictionary provided with an AVCaptureSessionRuntimeErrorNotification. The NSError associated with the notification gives greater detail on the nature of the error, and in some cases recovery suggestions.
 */
AVF_EXPORT NSString *const AVCaptureSessionErrorKey API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionDidStartRunningNotification
 @abstract
    Posted when an instance of AVCaptureSession successfully starts running.
 
 @discussion
    Clients may observe the AVCaptureSessionDidStartRunningNotification to know when an instance of AVCaptureSession starts running.
 */
AVF_EXPORT NSString *const AVCaptureSessionDidStartRunningNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionDidStopRunningNotification
 @abstract
    Posted when an instance of AVCaptureSession stops running.
 
 @discussion
    Clients may observe the AVCaptureSessionDidStopRunningNotification to know when an instance of AVCaptureSession stops running. An AVCaptureSession instance may stop running automatically due to external system conditions, such as the device going to sleep, or being locked by a user.
 */
AVF_EXPORT NSString *const AVCaptureSessionDidStopRunningNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionWasInterruptedNotification
 @abstract
    Posted when an instance of AVCaptureSession becomes interrupted.
 
 @discussion
    Clients may observe the AVCaptureSessionWasInterruptedNotification to know when an instance of AVCaptureSession has been interrupted, for example, by an incoming phone call, or alarm, or another application taking control of needed hardware resources. When appropriate, the AVCaptureSession instance will stop running automatically in response to an interruption.
 
    Beginning in iOS 9.0, the AVCaptureSessionWasInterruptedNotification userInfo dictionary contains an AVCaptureSessionInterruptionReasonKey indicating the reason for the interruption.
 */
AVF_EXPORT NSString *const AVCaptureSessionWasInterruptedNotification API_AVAILABLE(macos(10.14), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @enum AVCaptureSessionInterruptionReason
 @abstract
    Constants indicating interruption reason. One of these is returned with the AVCaptureSessionWasInterruptedNotification (see AVCaptureSessionInterruptionReasonKey).
 
 @constant AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableInBackground
    An interruption caused by the app being sent to the background while using a camera. Camera usage is prohibited while in the background. Beginning in iOS 9.0, AVCaptureSession no longer produces an AVCaptureSessionRuntimeErrorNotification if you attempt to start running a camera while in the background. Instead, it sends an AVCaptureSessionWasInterruptedNotification with AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableInBackground. Provided you don't explicitly call [session stopRunning], your -startRunning request is preserved, and when your app comes back to foreground, you receive AVCaptureSessionInterruptionEndedNotification and your session starts running.
 @constant AVCaptureSessionInterruptionReasonAudioDeviceInUseByAnotherClient
    An interruption caused by the audio hardware temporarily being made unavailable, for instance, for a phone call, or alarm.
 @constant AVCaptureSessionInterruptionReasonVideoDeviceInUseByAnotherClient
    An interruption caused by the video device temporarily being made unavailable, for instance, when stolen away by another AVCaptureSession.
 @constant AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableWithMultipleForegroundApps
    An interruption caused when the app is running in a multi-app layout, causing resource contention and degraded recording quality of service. Given your present AVCaptureSession configuration, the session may only be run if your app occupies the full screen.
 @constant AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableDueToSystemPressure
    An interruption caused by the video device temporarily being made unavailable due to system pressure, such as thermal duress. See AVCaptureDevice's AVCaptureSystemPressure category for more information.
 */
typedef NS_ENUM(NSInteger, AVCaptureSessionInterruptionReason) {
    AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableInBackground               = 1,
    AVCaptureSessionInterruptionReasonAudioDeviceInUseByAnotherClient                   = 2,
    AVCaptureSessionInterruptionReasonVideoDeviceInUseByAnotherClient                   = 3,
    AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableWithMultipleForegroundApps = 4,
    AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableDueToSystemPressure API_AVAILABLE(ios(11.1)) = 5,
} API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @constant AVCaptureSessionInterruptionReasonKey
 @abstract
    The key used to provide an NSNumber describing the interruption reason in an AVCaptureSessionWasInterruptedNotification.
 
 @discussion
    AVCaptureSessionInterruptionReasonKey may be found in the userInfo dictionary provided with an AVCaptureSessionWasInterruptedNotification. The NSNumber associated with the notification tells you why the interruption occurred.
 */
AVF_EXPORT NSString *const AVCaptureSessionInterruptionReasonKey API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionInterruptionSystemPressureStateKey
 @abstract
    The key used to provide an AVCaptureSystemPressureState indicating the system pressure level and contributing factors that caused the interruption.
 
 @discussion
    This key is only present when the AVCaptureSessionInterruptionReasonKey equals AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableDueToSystemPressure.
 */
AVF_EXPORT NSString *const AVCaptureSessionInterruptionSystemPressureStateKey API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureSessionInterruptionEndedNotification
 @abstract
    Posted when an instance of AVCaptureSession ceases to be interrupted.
 
 @discussion
    Clients may observe the AVCaptureSessionInterruptionEndedNotification to know when an instance of AVCaptureSession ceases to be interrupted, for example, when a phone call ends, and hardware resources needed to run the session are again available. When appropriate, the AVCaptureSession instance that was previously stopped in response to an interruption will automatically restart once the interruption ends.
 */
AVF_EXPORT NSString *const AVCaptureSessionInterruptionEndedNotification API_AVAILABLE(macos(10.14), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @enum AVCaptureVideoOrientation
 @abstract
    Constants indicating video orientation, for use with AVCaptureVideoPreviewLayer (see AVCaptureVideoPreviewLayer.h) and AVCaptureConnection (see below).
 
 @constant AVCaptureVideoOrientationPortrait
    Indicates that video should be oriented vertically, home button on the bottom.
 @constant AVCaptureVideoOrientationPortraitUpsideDown
    Indicates that video should be oriented vertically, home button on the top.
 @constant AVCaptureVideoOrientationLandscapeRight
    Indicates that video should be oriented horizontally, home button on the right.
 @constant AVCaptureVideoOrientationLandscapeLeft
    Indicates that video should be oriented horizontally, home button on the left.
 */
typedef NS_ENUM(NSInteger, AVCaptureVideoOrientation) {
    AVCaptureVideoOrientationPortrait           = 1,
    AVCaptureVideoOrientationPortraitUpsideDown = 2,
    AVCaptureVideoOrientationLandscapeRight     = 3,
    AVCaptureVideoOrientationLandscapeLeft      = 4,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVCaptureSession

@class AVCaptureInput;
@class AVCaptureOutput;
@class AVCaptureConnection;
@class AVCaptureSessionInternal;

/*!
 @class AVCaptureSession
 @abstract
    AVCaptureSession is the central hub of the AVFoundation capture classes.
 
 @discussion
    To perform a real-time capture, a client may instantiate AVCaptureSession and add appropriate AVCaptureInputs, such as AVCaptureDeviceInput, and outputs, such as AVCaptureMovieFileOutput. [AVCaptureSession startRunning] starts the flow of data from the inputs to the outputs, and [AVCaptureSession stopRunning] stops the flow. A client may set the sessionPreset property to customize the quality level or bitrate of the output.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureSession : NSObject
{
@private
    AVCaptureSessionInternal *_internal;
}

/*!
 @method canSetSessionPreset:
 @abstract
    Returns whether the receiver can be configured with the given preset.
 
 @param preset
    An AVCaptureSession preset.
 @result
    YES if the receiver can be set to the given preset, NO otherwise.
 
 @discussion
    An AVCaptureSession instance can be associated with a preset that configures its inputs and outputs to fulfill common use cases. This method can be used to determine if the receiver supports the desired preset given its current input and output configuration. The receiver's sessionPreset property may only be set to a certain preset if this method returns YES for that preset.
 */
- (BOOL)canSetSessionPreset:(AVCaptureSessionPreset)preset;

/*!
 @property sessionPreset
 @abstract
    Indicates the session preset currently in use by the receiver.
 
 @discussion
    The value of this property is an AVCaptureSessionPreset indicating the current session preset in use by the receiver. The sessionPreset property may be set while the receiver is running.
 */
@property(nonatomic, copy) AVCaptureSessionPreset sessionPreset;

/*!
 @property inputs
 @abstract
    An NSArray of AVCaptureInputs currently added to the receiver.
 
 @discussion
    The value of this property is an NSArray of AVCaptureInputs currently added to the receiver. Clients can add AVCaptureInputs to a session by calling -addInput:.
 */
@property(nonatomic, readonly) NSArray<__kindof AVCaptureInput *> *inputs;

/*!
 @method canAddInput:
 @abstract
    Returns whether the proposed input can be added to the receiver.
 
 @param input
    An AVCaptureInput instance.
 @result
    YES if the proposed input can be added to the receiver, NO otherwise.
 
 @discussion
    An AVCaptureInput instance can only be added to a session using -addInput: if -canAddInput: returns YES.
 */
- (BOOL)canAddInput:(AVCaptureInput *)input;

/*!
 @method addInput:
 @abstract
    Adds an AVCaptureInput to the session.
 
 @param input
    An AVCaptureInput instance.
 
 @discussion
    An AVCaptureInput instance can only be added to a session using -addInput: if -canAddInput: returns YES. -addInput: may be called while the session is running.
 */
- (void)addInput:(AVCaptureInput *)input;

/*!
 @method removeInput:
 @abstract
    Removes an AVCaptureInput from the session.
 
 @param input
    An AVCaptureInput instance.
 
 @discussion
    -removeInput: may be called while the session is running.
 */
- (void)removeInput:(AVCaptureInput *)input;

/*!
 @property outputs
 @abstract
    An NSArray of AVCaptureOutputs currently added to the receiver.
 
 @discussion
    The value of this property is an NSArray of AVCaptureOutputs currently added to the receiver. Clients can add AVCaptureOutputs to a session by calling -addOutput:.
 */
@property(nonatomic, readonly) NSArray<__kindof AVCaptureOutput *> *outputs;

/*!
 @method canAddOutput:
 @abstract
    Returns whether the proposed output can be added to the receiver.
 
 @param output
    An AVCaptureOutput instance.
 @result
    YES if the proposed output can be added to the receiver, NO otherwise.
 
 @discussion
    An AVCaptureOutput instance can only be added to a session using -addOutput: if -canAddOutput: returns YES.
 */
- (BOOL)canAddOutput:(AVCaptureOutput *)output;

/*!
 @method addOutput:
 @abstract
    Adds an AVCaptureOutput to the session.
 
 @param output
    An AVCaptureOutput instance.
 
 @discussion
    An AVCaptureOutput instance can only be added to a session using -addOutput: if -canAddOutput: returns YES. -addOutput: may be called while the session is running.
 */
- (void)addOutput:(AVCaptureOutput *)output;

/*!
 @method removeOutput:
 @abstract
    Removes an AVCaptureOutput from the session.
 
 @param output
    An AVCaptureOutput instance.
 
 @discussion
    -removeOutput: may be called while the session is running.
 */
- (void)removeOutput:(AVCaptureOutput *)output;

/*!
 @method addInputWithNoConnections:
 @abstract
    Adds an AVCaptureInput to the session without forming any connections.
 
 @param input
    An AVCaptureInput instance.
 
 @discussion
    -addInputWithNoConnections: may be called while the session is running. The -addInput: method is the preferred method for adding an input to an AVCaptureSession. -addInputWithNoConnections: may be called if you need fine-grained control over which inputs are connected to which outputs.
 */
- (void)addInputWithNoConnections:(AVCaptureInput *)input API_AVAILABLE(ios(8.0));

/*!
 @method addOutputWithNoConnections:
 @abstract
    Adds an AVCaptureOutput to the session without forming any connections.
 
 @param output
    An AVCaptureOutput instance.
 
 @discussion
    -addOutputWithNoConnections: may be called while the session is running. The -addOutput: method is the preferred method for adding an output to an AVCaptureSession. -addOutputWithNoConnections: may be called if you need fine-grained control over which inputs are connected to which outputs.
 */
- (void)addOutputWithNoConnections:(AVCaptureOutput *)output API_AVAILABLE(ios(8.0));

/*!
 @method canAddConnection:
 @abstract
    Returns whether the proposed connection can be added to the receiver.
 
 @param connection
    An AVCaptureConnection instance.
 
 @discussion
    An AVCaptureConnection instance can only be added to a session using -addConnection: if canAddConnection: returns YES. When using -addInput: or -addOutput:, connections are formed automatically between all compatible inputs and outputs. Manually adding connections is only necessary when adding an input or output with no connections.
 */
- (BOOL)canAddConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(8.0));

/*!
 @method addConnection:
 @abstract
    Adds an AVCaptureConnection to the session.
 
 @param connection
    An AVCaptureConnection instance.
 
 @discussion
    An AVCaptureConnection instance can only be added to a session using -addConnection: if canAddConnection: returns YES. When using -addInput: or -addOutput:, connections are formed automatically between all compatible inputs and outputs. Manually adding connections is only necessary when adding an input or output with no connections. -addConnection: may be called while the session is running.
 */
- (void)addConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(8.0));

/*!
 @method removeConnection:
 @abstract
    Removes an AVCaptureConnection from the session.
 
 @param connection
    An AVCaptureConnection instance.
 
 @discussion
    -removeConnection: may be called while the session is running.
 */
- (void)removeConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(8.0));

/*!
 @method beginConfiguration
 @abstract
    When paired with commitConfiguration, allows a client to batch multiple configuration operations on a running session into atomic updates.
 
 @discussion
    -beginConfiguration / -commitConfiguration are AVCaptureSession's mechanism for batching multiple configuration operations on a running session into atomic updates. After calling [session beginConfiguration], clients may add or remove outputs, alter the sessionPreset, or configure individual AVCaptureInput or Output properties. All changes will be pended until the client calls [session commitConfiguration], at which time they will be applied together. -beginConfiguration / -commitConfiguration pairs may be nested, and will only be applied when the outermost commit is invoked.
 */
- (void)beginConfiguration;

/*!
 @method commitConfiguration
 @abstract
    When preceded by beginConfiguration, allows a client to batch multiple configuration operations on a running session into atomic updates.
 
 @discussion
    -beginConfiguration / -commitConfiguration are AVCaptureSession's mechanism for batching multiple configuration operations on a running session into atomic updates. After calling [session beginConfiguration], clients may add or remove outputs, alter the sessionPreset, or configure individual AVCaptureInput or Output properties. All changes will be pended until the client calls [session commitConfiguration], at which time they will be applied together. -beginConfiguration / -commitConfiguration pairs may be nested, and will only be applied when the outermost commit is invoked.
 */
- (void)commitConfiguration;

/*!
 @property running
 @abstract
    Indicates whether the session is currently running.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is running. Clients can key value observe the value of this property to be notified when the session automatically starts or stops running.
 */
@property(nonatomic, readonly, getter=isRunning) BOOL running;

/*!
 @property interrupted
 @abstract
    Indicates whether the session is being interrupted.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is currently being interrupted, such as by a phone call or alarm. Clients can key value observe the value of this property to be notified when the session ceases to be interrupted and again has access to needed hardware resources.
 */
@property(nonatomic, readonly, getter=isInterrupted) BOOL interrupted API_AVAILABLE(ios(4.0)) API_UNAVAILABLE(macos);

/*!
 @property usesApplicationAudioSession
 @abstract
    Indicates whether the receiver will use the application's AVAudioSession for recording.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is currently using the application's AVAudioSession (see AVAudioSession.h). Prior to iOS 7, AVCaptureSession uses its own audio session, which can lead to unwanted interruptions when interacting with the application's audio session. In applications linked on or after iOS 7, AVCaptureSession shares the application's audio session, allowing for simultaneous play back and recording without unwanted interruptions. Clients desiring the pre-iOS 7 behavior may opt out by setting usesApplicationAudioSession to NO. The default value is YES.
 */
@property(nonatomic) BOOL usesApplicationAudioSession API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property automaticallyConfiguresApplicationAudioSession
 @abstract
    Indicates whether the receiver should configure the application's audio session for recording.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver should configure the application's audio session when needed for optimal recording. When set to YES, the receiver ensures the application's audio session is set to the PlayAndRecord category, and picks an appropriate microphone and polar pattern to match the video camera being used. When set to NO, and -usesApplicationAudioSession is set to YES, the receiver will use the application's audio session, but will not change any of its properties. If the session is not set up correctly for input, audio recording may fail. The default value is YES.
 */
@property(nonatomic) BOOL automaticallyConfiguresApplicationAudioSession API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property automaticallyConfiguresCaptureDeviceForWideColor
 @abstract
    Indicates whether the receiver automatically configures its video device's activeFormat and activeColorSpace properties, preferring wide color for photos.
 
 @discussion
    The default value is YES. By default, the receiver automatically adjusts its source video AVCaptureDevice's activeFormat and activeColorSpace properties based on the supportedColorSpaces of the device's formats and the current AVCaptureSession topology. Wide color spaces are preferred over sRGB if an AVCapturePhotoOutput is present in the session. If you wish to set AVCaptureDevice's activeColorSpace manually, and prevent the AVCaptureSession from undoing your work, you must set automaticallyConfiguresCaptureDeviceForWideColor to NO. If the receiver's sessionPreset is set to AVCaptureSessionPresetInputPriority, the session will not alter the capture device's activeFormat, but might still alter its activeColorSpace.
 */
@property(nonatomic) BOOL automaticallyConfiguresCaptureDeviceForWideColor API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @method startRunning
 @abstract
    Starts an AVCaptureSession instance running.
 
 @discussion
    Clients invoke -startRunning to start the flow of data from inputs to outputs connected to the AVCaptureSession instance. This call blocks until the session object has completely started up or failed. A failure to start running is reported through the AVCaptureSessionRuntimeErrorNotification mechanism.
 */
- (void)startRunning;

/*!
 @method stopRunning
 @abstract
    Stops an AVCaptureSession instance that is currently running.
 
 @discussion
    Clients invoke -stopRunning to stop the flow of data from inputs to outputs connected to the AVCaptureSession instance. This call blocks until the session object has completely stopped.
 */
- (void)stopRunning;

/*!
 @property masterClock
 @abstract
    Provides the master clock being used for output synchronization.
 @discussion
    The masterClock is readonly. Use masterClock to synchronize AVCaptureOutput data with external data sources (e.g motion samples). All capture output sample buffer timestamps are on the masterClock timebase.
 
    For example, if you want to reverse synchronize the output timestamps to the original timestamps, you can do the following: In captureOutput:didOutputSampleBuffer:fromConnection:
 
    AVCaptureInputPort *port = [[connection inputPorts] objectAtIndex:0];
    CMClockRef originalClock = [port clock];
 
    CMTime syncedPTS = CMSampleBufferGetPresentationTime( sampleBuffer );
    CMTime originalPTS = CMSyncConvertTime( syncedPTS, [session masterClock], originalClock );
 
    This property is key-value observable.
 */
@property(nonatomic, readonly, nullable) __attribute__((NSObject)) CMClockRef masterClock API_AVAILABLE(macos(10.9), ios(7.0));

@end


/*!
 @enum AVVideoFieldMode
 @abstract
    Constants indicating video field mode, for use with AVCaptureConnection's videoFieldMode property (see below).
 
 @constant AVVideoFieldModeBoth
    Indicates that both top and bottom video fields in interlaced content should be passed thru.
 @constant AVVideoFieldModeTopOnly
    Indicates that only the top video field in interlaced content should be passed thru.
 @constant AVVideoFieldModeBottomOnly
    Indicates that the bottom video field only in interlaced content should be passed thru.
 @constant AVVideoFieldModeDeinterlace
    Indicates that top and bottom video fields in interlaced content should be deinterlaced.
 */
typedef NS_ENUM(NSInteger, AVVideoFieldMode) {
    AVVideoFieldModeBoth        = 0,
    AVVideoFieldModeTopOnly     = 1,
    AVVideoFieldModeBottomOnly  = 2,
    AVVideoFieldModeDeinterlace = 3,
} API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos);


#pragma mark - AVCaptureConnection

@class AVCaptureAudioChannel;
@class AVCaptureVideoPreviewLayer;
@class AVCaptureInputPort;
@class AVCaptureConnectionInternal;

/*!
 @class AVCaptureConnection
 @abstract
    AVCaptureConnection represents a connection between an AVCaptureInputPort or ports, and an AVCaptureOutput or AVCaptureVideoPreviewLayer present in an AVCaptureSession.
 
 @discussion
    AVCaptureInputs have one or more AVCaptureInputPorts. AVCaptureOutputs can accept data from one or more sources (example - an AVCaptureMovieFileOutput accepts both video and audio data). AVCaptureVideoPreviewLayers can accept data from one AVCaptureInputPort whose mediaType is AVMediaTypeVideo. When an input or output is added to a session, or a video preview layer is associated with a session, the session greedily forms connections between all the compatible AVCaptureInputs' ports and AVCaptureOutputs or AVCaptureVideoPreviewLayers. Iterating through an output's connections or a video preview layer's sole connection, a client may enable or disable the flow of data from a given input to a given output or preview layer.
 
    Connections involving audio expose an array of AVCaptureAudioChannel objects, which can be used for monitoring levels.
 
    Connections involving video expose video specific properties, such as videoMirrored and videoOrientation.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureConnection : NSObject
{
@private
    AVCaptureConnectionInternal *_internal;
}

/*!
 @method connectionWithInputPorts:output:
 @abstract
    Returns an AVCaptureConnection instance describing a connection between the specified inputPorts and the specified output.
 
 @param ports
    An array of AVCaptureInputPort objects associated with AVCaptureInput objects.
 @param output
    An AVCaptureOutput object.
 @result
    An AVCaptureConnection instance joining the specified inputPorts to the specified output port.
 
 @discussion
    This method returns an instance of AVCaptureConnection that may be subsequently added to an AVCaptureSession instance using AVCaptureSession's -addConnection: method. When using -addInput: or -addOutput:, connections are formed between all compatible inputs and outputs automatically. You do not need to manually create and add connections to the session unless you use the primitive -addInputWithNoConnections: or -addOutputWithNoConnections: methods.
 */
+ (instancetype)connectionWithInputPorts:(NSArray<AVCaptureInputPort *> *)ports output:(AVCaptureOutput *)output API_AVAILABLE(ios(8.0));

/*!
 @method connectionWithInputPort:videoPreviewLayer:
 @abstract
    Returns an AVCaptureConnection instance describing a connection between the specified inputPort and the specified AVCaptureVideoPreviewLayer instance.
 
 @param port
    An AVCaptureInputPort object associated with an AVCaptureInput object.
 @param layer
    An AVCaptureVideoPreviewLayer object.
 @result
    An AVCaptureConnection instance joining the specified inputPort to the specified video preview layer.
 
 @discussion
    This method returns an instance of AVCaptureConnection that may be subsequently added to an AVCaptureSession instance using AVCaptureSession's -addConnection: method. When using AVCaptureVideoPreviewLayer's -initWithSession: or -setSession:, a connection is formed between the first compatible input port and the video preview layer automatically. You do not need to manually create and add connections to the session unless you use AVCaptureVideoPreviewLayer's primitive -initWithSessionWithNoConnection: or -setSessionWithNoConnection: methods.
 */
+ (instancetype)connectionWithInputPort:(AVCaptureInputPort *)port videoPreviewLayer:(AVCaptureVideoPreviewLayer *)layer API_AVAILABLE(ios(8.0));

/*!
 @method initWithInputPorts:output:
 @abstract
    Returns an AVCaptureConnection instance describing a connection between the specified inputPorts and the specified output.
 
 @param ports
    An array of AVCaptureInputPort objects associated with AVCaptureInput objects.
 @param output
    An AVCaptureOutput object.
 @result
    An AVCaptureConnection instance joining the specified inputPorts to the specified output port.
 
 @discussion
    This method returns an instance of AVCaptureConnection that may be subsequently added to an AVCaptureSession instance using AVCaptureSession's -addConnection: method. When using -addInput: or -addOutput:, connections are formed between all compatible inputs and outputs automatically. You do not need to manually create and add connections to the session unless you use the primitive -addInputWithNoConnections: or -addOutputWithNoConnections: methods.
 */
- (instancetype)initWithInputPorts:(NSArray<AVCaptureInputPort *> *)ports output:(AVCaptureOutput *)output API_AVAILABLE(ios(8.0));

/*!
 @method initWithInputPort:videoPreviewLayer:
 @abstract
    Returns an AVCaptureConnection instance describing a connection between the specified inputPort 
    and the specified AVCaptureVideoPreviewLayer instance.
 
 @param port
    An AVCaptureInputPort object associated with an AVCaptureInput object.
 @param layer
    An AVCaptureVideoPreviewLayer object.
 @result
    An AVCaptureConnection instance joining the specified inputPort to the specified video preview layer.
 
 @discussion
    This method returns an instance of AVCaptureConnection that may be subsequently added to an AVCaptureSession instance using AVCaptureSession's -addConnection: method. When using AVCaptureVideoPreviewLayer's -initWithSession: or -setSession:, a connection is formed between the first compatible input port and the video preview layer automatically. You do not need to manually create and add connections to the session unless you use AVCaptureVideoPreviewLayer's primitive -initWithSessionWithNoConnection: or -setSessionWithNoConnection: methods.
 */
- (instancetype)initWithInputPort:(AVCaptureInputPort *)port videoPreviewLayer:(AVCaptureVideoPreviewLayer *)layer API_AVAILABLE(ios(8.0));

/*!
 @property inputPorts
 @abstract
    An array of AVCaptureInputPort instances providing data through this connection.
 
 @discussion
    An AVCaptureConnection may involve one or more AVCaptureInputPorts producing data to the connection's AVCaptureOutput. This property is read-only. An AVCaptureConnection's inputPorts remain static for the life of the object.
 */
@property(nonatomic, readonly) NSArray<AVCaptureInputPort *> *inputPorts;

/*!
 @property output
 @abstract
    The AVCaptureOutput instance consuming data from this connection's inputPorts.

 @discussion
    An AVCaptureConnection may involve one or more AVCaptureInputPorts producing data to the connection's AVCaptureOutput. This property is read-only. An AVCaptureConnection's output remains static for the life of the object. Note that a connection can either be to an output or a video preview layer, but never to both.
 */
@property(nonatomic, readonly) AVCaptureOutput *output;

/*!
 @property videoPreviewLayer
 @abstract
    The AVCaptureVideoPreviewLayer instance consuming data from this connection's inputPort.
 
 @discussion
    An AVCaptureConnection may involve one AVCaptureInputPort producing data to an AVCaptureVideoPreviewLayer object. This property is read-only. An AVCaptureConnection's videoPreviewLayer remains static for the life of the object. Note that a connection can either be to an output or a video preview layer, but never to both.
 */
@property(nonatomic, readonly) AVCaptureVideoPreviewLayer *videoPreviewLayer API_AVAILABLE(ios(6.0));

/*!
 @property enabled
 @abstract
    Indicates whether the connection's output should consume data.

 @discussion
    The value of this property is a BOOL that determines whether the receiver's output should consume data from its connected inputPorts when a session is running. Clients can set this property to stop the flow of data to a given output during capture. The default value is YES.
 */
@property(nonatomic, getter=isEnabled) BOOL enabled;

/*!
 @property active
 @abstract
    Indicates whether the receiver's output is currently capable of consuming data through this connection.

 @discussion
    The value of this property is a BOOL that determines whether the receiver's output can consume data provided through this connection. This property is read-only. Clients may key-value observe this property to know when a session's configuration forces a connection to become inactive. The default value is YES.
 
    Prior to iOS 11, the audio connection feeding an AVCaptureAudioDataOutput is made inactive when using AVCaptureSessionPresetPhoto or the equivalent photo format using -[AVCaptureDevice activeFormat]. On iOS 11 and later, the audio connection feeding AVCaptureAudioDataOutput is active for all presets and device formats.
 */
@property(nonatomic, readonly, getter=isActive) BOOL active;

/*!
 @property audioChannels
 @abstract
    An array of AVCaptureAudioChannel objects representing individual channels of audio data flowing through the connection.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving audio. In such connections, the audioChannels array contains one AVCaptureAudioChannel object for each channel of audio data flowing through this connection.
 */
@property(nonatomic, readonly) NSArray<AVCaptureAudioChannel *> *audioChannels;

/*!
 @property supportsVideoMirroring
 @abstract
    Indicates whether the connection supports setting the videoMirrored property.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoMirrored property may only be set if
    -isVideoMirroringSupported returns YES.
 */
@property(nonatomic, readonly, getter=isVideoMirroringSupported) BOOL supportsVideoMirroring;

/*!
 @property videoMirrored
 @abstract
    Indicates whether the video flowing through the connection should be mirrored about its vertical axis.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. if -isVideoMirroringSupported returns YES, videoMirrored may be set to flip the video about its vertical axis and produce a mirror-image effect.
 */
@property(nonatomic, getter=isVideoMirrored) BOOL videoMirrored;

/*!
 @property automaticallyAdjustsVideoMirroring
 @abstract
    Specifies whether or not the value of @"videoMirrored" can change based on configuration of the session.
 
 @discussion
    For some session configurations, video data flowing through the connection will be mirrored by default. When the value of this property is YES, the value of @"videoMirrored" may change depending on the configuration of the session, for example after switching to a different AVCaptureDeviceInput. The default value is YES.
 */
@property(nonatomic) BOOL automaticallyAdjustsVideoMirroring API_AVAILABLE(ios(6.0));

/*!
 @property supportsVideoOrientation
 @abstract
    Indicates whether the connection supports setting the videoOrientation property.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoOrientation property may only be set if -isVideoOrientationSupported returns YES.
 */
@property(nonatomic, readonly, getter=isVideoOrientationSupported) BOOL supportsVideoOrientation;

/*!
 @property videoOrientation
 @abstract
    Indicates whether the video flowing through the connection should be rotated to a given orientation.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. If -isVideoOrientationSupported returns YES, videoOrientation may be set to rotate the video buffers being consumed by the connection's output. Note that setting videoOrientation does not necessarily result in a physical rotation of video buffers. For instance, a video connection to an AVCaptureMovieFileOutput handles orientation using a Quicktime track matrix. In the AVCaptureStillImageOutput, orientation is handled using Exif tags.
 */
@property(nonatomic) AVCaptureVideoOrientation videoOrientation;

/*!
 @property supportsVideoFieldMode
 @abstract
    Indicates whether the connection supports setting the videoFieldMode property.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoFieldMode property may only be set if -isVideoFieldModeSupported returns YES.
 */
@property(nonatomic, readonly, getter=isVideoFieldModeSupported) BOOL supportsVideoFieldMode API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property videoFieldMode
 @abstract
    Indicates how interlaced video flowing through the connection should be treated.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. If -isVideoFieldModeSupported returns YES, videoFieldMode may be set to affect interlaced video content flowing through the connection.
 */
@property(nonatomic) AVVideoFieldMode videoFieldMode API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property supportsVideoMinFrameDuration
 @abstract
    Indicates whether the connection supports setting the videoMinFrameDuration property.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoMinFrameDuration property may only be set if -isVideoMinFrameDurationSupported returns YES.
 
    This property is deprecated on iOS, where min and max frame rate adjustments are applied exclusively at the AVCaptureDevice using the activeVideoMinFrameDuration and activeVideoMaxFrameDuration properties. On Mac OS X, frame rate adjustments are supported both at the AVCaptureDevice and at AVCaptureConnection, enabling connections to output different frame rates.
 */
@property(nonatomic, readonly, getter=isVideoMinFrameDurationSupported) BOOL supportsVideoMinFrameDuration API_DEPRECATED("Use AVCaptureDevice's activeFormat.videoSupportedFrameRateRanges instead.", ios(5.0, 7.0));

/*!
 @property videoMinFrameDuration
 @abstract
    Indicates the minimum time interval at which the receiver should output consecutive video frames.
 
 @discussion
    The value of this property is a CMTime specifying the minimum duration of each video frame output by the receiver, placing a lower bound on the amount of time that should separate consecutive frames. This is equivalent to the reciprocal of the maximum frame rate. A value of kCMTimeZero or kCMTimeInvalid indicates an unlimited maximum frame rate. The default value is kCMTimeInvalid.
 
    This property is deprecated on iOS, where min and max frame rate adjustments are applied exclusively at the AVCaptureDevice using the activeVideoMinFrameDuration and activeVideoMaxFrameDuration properties. On Mac OS X, frame rate adjustments are supported both at the AVCaptureDevice and at AVCaptureConnection, enabling connections to output different frame rates.
 */
@property(nonatomic) CMTime videoMinFrameDuration API_DEPRECATED("Use AVCaptureDevice's activeVideoMinFrameDuration instead.", ios(5.0, 7.0));

/*!
 @property supportsVideoMaxFrameDuration
 @abstract
    Indicates whether the connection supports setting the videoMaxFrameDuration property.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoMaxFrameDuration property may only be set if -isVideoMaxFrameDurationSupported returns YES.
 
    This property is deprecated on iOS, where min and max frame rate adjustments are applied exclusively at the AVCaptureDevice using the activeVideoMinFrameDuration and activeVideoMaxFrameDuration properties. On Mac OS X, frame rate adjustments are supported both at the AVCaptureDevice and at AVCaptureConnection, enabling connections to output different frame rates.
 */
@property(nonatomic, readonly, getter=isVideoMaxFrameDurationSupported) BOOL supportsVideoMaxFrameDuration API_AVAILABLE(macos(10.9)) API_DEPRECATED("Use AVCaptureDevice's activeFormat.videoSupportedFrameRateRanges instead.", ios(5.0, 7.0));

/*!
 @property videoMaxFrameDuration
 @abstract
    Indicates the maximum time interval at which the receiver should output consecutive video frames.
 
 @discussion
    The value of this property is a CMTime specifying the maximum duration of each video frame output by the receiver, placing an upper bound on the amount of time that should separate consecutive frames. This is equivalent to the reciprocal of the minimum frame rate. A value of kCMTimeZero or kCMTimeInvalid indicates an unlimited minimum frame rate. The default value is kCMTimeInvalid.
 
    This property is deprecated on iOS, where min and max frame rate adjustments are applied exclusively at the AVCaptureDevice using the activeVideoMinFrameDuration and activeVideoMaxFrameDuration properties. On Mac OS X, frame rate adjustments are supported both at the AVCaptureDevice and at AVCaptureConnection, enabling connections to output different frame rates.
 */
@property(nonatomic) CMTime videoMaxFrameDuration API_AVAILABLE(macos(10.9)) API_DEPRECATED("Use AVCaptureDevice's activeVideoMaxFrameDuration instead.", ios(5.0, 7.0));

/*!
 @property videoMaxScaleAndCropFactor
 @abstract
    Indicates the maximum video scale and crop factor supported by the receiver.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the videoMaxScaleAndCropFactor property specifies the maximum CGFloat value that may be used when setting the videoScaleAndCropFactor property.
 */
@property(nonatomic, readonly) CGFloat videoMaxScaleAndCropFactor API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

/*!
 @property videoScaleAndCropFactor
 @abstract
    Indicates the current video scale and crop factor in use by the receiver.
 
 @discussion
    This property only applies to AVCaptureStillImageOutput connections. In such connections, the videoScaleAndCropFactor property may be set to a value in the range of 1.0 to videoMaxScaleAndCropFactor. At a factor of 1.0, the image is its original size. At a factor greater than 1.0, the image is scaled by the factor and center-cropped to its original dimensions. This factor is applied in addition to any magnification from AVCaptureDevice's videoZoomFactor property.
 
 @see -[AVCaptureDevice videoZoomFactor]
 */
@property(nonatomic) CGFloat videoScaleAndCropFactor API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

/*!
 @property preferredVideoStabilizationMode
 @abstract
    Indicates the stabilization mode to apply to video flowing through the receiver when it is supported.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. On devices where the video stabilization feature is supported, only a subset of available source formats may be available for stabilization. By setting the preferredVideoStabilizationMode property to a value other than AVCaptureVideoStabilizationModeOff, video flowing through the receiver is stabilized when the mode is available. Enabling video stabilization introduces additional latency into the video capture pipeline and may consume more system memory depending on the stabilization mode and format. If the preferred stabilization mode isn't available, the activeVideoStabilizationMode will be set to AVCaptureVideoStabilizationModeOff. Clients may key-value observe the activeVideoStabilizationMode property to know which stabilization mode is in use or when it is off. The default value is AVCaptureVideoStabilizationModeOff. When setting this property to AVCaptureVideoStabilizationModeAuto, an appropriate stabilization mode will be chosen based on the format and frame rate. For apps linked before iOS 6.0, the default value is AVCaptureVideoStabilizationModeStandard for a video connection attached to an AVCaptureMovieFileOutput instance. For apps linked on or after iOS 6.0, the default value is always AVCaptureVideoStabilizationModeOff. Setting a video stabilization mode using this property may change the value of enablesVideoStabilizationWhenAvailable.
 */
@property(nonatomic) AVCaptureVideoStabilizationMode preferredVideoStabilizationMode API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property activeVideoStabilizationMode
 @abstract
    Indicates the stabilization mode currently being applied to video flowing through the receiver.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. On devices where the video stabilization feature is supported, only a subset of available source formats may be stabilized. The activeVideoStabilizationMode property returns a value other than AVCaptureVideoStabilizationModeOff if video stabilization is currently in use. This property never returns AVCaptureVideoStabilizationModeAuto. This property is key-value observable.
 */
@property(nonatomic, readonly) AVCaptureVideoStabilizationMode activeVideoStabilizationMode API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property supportsVideoStabilization
 @abstract
    Indicates whether the connection supports video stabilization.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the -enablesVideoStabilizationWhenAvailable property may only be set if -supportsVideoStabilization returns YES. This property returns YES if the connection's input device has one or more formats that support video stabilization and the connection's output supports video stabilization. See [AVCaptureDeviceFormat isVideoStabilizationModeSupported:] to check which video stabilization modes are supported by the active device format.
 */
@property(nonatomic, readonly, getter=isVideoStabilizationSupported) BOOL supportsVideoStabilization API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @property videoStabilizationEnabled
 @abstract
    Indicates whether stabilization is currently being applied to video flowing through the receiver.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. On devices where the video stabilization feature is supported, only a subset of available source formats and resolutions may be available for stabilization. The videoStabilizationEnabled property returns YES if video stabilization is currently in use. This property is key-value observable. This property is deprecated. Use activeVideoStabilizationMode instead.
 */
@property(nonatomic, readonly, getter=isVideoStabilizationEnabled) BOOL videoStabilizationEnabled API_DEPRECATED("Use activeVideoStabilizationMode instead.", ios(6.0, 8.0)) API_UNAVAILABLE(macos);

/*!
 @property enablesVideoStabilizationWhenAvailable
 @abstract
    Indicates whether stabilization should be applied to video flowing through the receiver when the feature is available.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. On devices where the video stabilization feature is supported, only a subset of available source formats and resolutions may be available for stabilization. By setting the enablesVideoStabilizationWhenAvailable property to YES, video flowing through the receiver is stabilized when available. Enabling video stabilization may introduce additional latency into the video capture pipeline. Clients may key-value observe the videoStabilizationEnabled property to know when stabilization is in use or not. The default value is NO. For apps linked before iOS 6.0, the default value is YES for a video connection attached to an AVCaptureMovieFileOutput instance. For apps linked on or after iOS 6.0, the default value is always NO. This property is deprecated. Use preferredVideoStabilizationMode instead.
 */
@property(nonatomic) BOOL enablesVideoStabilizationWhenAvailable API_DEPRECATED("Use preferredVideoStabilizationMode instead.", ios(6.0, 8.0)) API_UNAVAILABLE(macos);

/*!
 @property cameraIntrinsicMatrixDeliverySupported
 @abstract
    Indicates whether the connection supports camera intrinsic matrix delivery.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. In such connections, the cameraIntrinsicMatrixDeliveryEnabled property may only be set to YES if - isCameraIntrinsicMatrixDeliverySupported returns YES. This property returns YES if both the connection's input device format and the connection's output support camera intrinsic matrix delivery. In iOS 11, only the AVCaptureVideoDataOutput's connection supports this property.
 */
@property(nonatomic, readonly, getter=isCameraIntrinsicMatrixDeliverySupported) BOOL cameraIntrinsicMatrixDeliverySupported API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property cameraIntrinsicMatrixDeliveryEnabled
 @abstract
    Indicates whether camera intrinsic matrix delivery should be enabled.
 
 @discussion
    This property is only applicable to AVCaptureConnection instances involving video. Refer to property cameraIntrinsicMatrixDeliverySupported before setting this property. When this property is set to YES, the receiver's output will add the kCMSampleBufferAttachmentKey_CameraIntrinsicMatrix sample buffer attachment to all vended sample buffers. This property must be set before the session starts running.
 */
@property(nonatomic, getter=isCameraIntrinsicMatrixDeliveryEnabled) BOOL cameraIntrinsicMatrixDeliveryEnabled API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

@end


#pragma mark - AVCaptureAudioChannel

@class AVCaptureAudioChannelInternal;

/*!
 @class AVCaptureAudioChannel
 @abstract
    AVCaptureAudioChannel represents a single channel of audio flowing through an AVCaptureSession.
 
 @discussion
    An AVCaptureConnection from an input producing audio to an output receiving audio exposes an array of AVCaptureAudioChannel objects, one for each channel of audio available. Iterating through these audio channel objects, a client may poll for audio levels. Instances of AVCaptureAudioChannel cannot be created directly.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureAudioChannel : NSObject
{
@private
    AVCaptureAudioChannelInternal *_internal;
}

/*!
 @property averagePowerLevel
 @abstract
    A measurement of the instantaneous average power level of the audio flowing through the receiver.
 
 @discussion
    A client may poll an AVCaptureAudioChannel object for its current averagePowerLevel to get its instantaneous average power level in decibels. This property is not key-value observable.
 */
@property(nonatomic, readonly) float averagePowerLevel;

/*!
 @property peakHoldLevel
 @abstract
    A measurement of the peak/hold level of the audio flowing through the receiver.
 
 @discussion
    A client may poll an AVCaptureAudioChannel object for its current peakHoldLevel to get its most recent peak hold level in decibels. This property is not key-value observable.
 */
@property(nonatomic, readonly) float peakHoldLevel;

/*!
 @property volume
 @abstract
    A property indicating the current volume (gain) of the receiver.
 
 @discussion
    The volume property indicates the current volume or gain of the receiver as a floating point value between 0.0 -> 1.0. If you desire to boost the gain in software, you may specify a a value greater than 1.0.
 */
@property(nonatomic) float volume API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property enabled
 @abstract
    A property indicating whether the receiver is currently enabled for data capture.
 
 @discussion
    By default, all AVCaptureAudioChannel objects exposed by a connection are enabled. You may set enabled to NO to stop the flow of data for a particular AVCaptureAudioChannel.
 */
@property(nonatomic, getter=isEnabled) BOOL enabled API_UNAVAILABLE(ios, watchos, tvos);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureAudioPreviewOutput.h
/*
    File:  AVCaptureAudioPreviewOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureAudioPreviewOutput

@class AVCaptureAudioPreviewOutputInternal;

/*!
 @class AVCaptureAudioPreviewOutput
 @abstract
    AVCaptureAudioPreviewOutput is a concrete subclass of AVCaptureOutput that can be used to preview the audio being captured.
 
 @discussion
    Instances of AVCaptureAudioPreviewOutput have an associated Core Audio output device that can be used to play audio being captured by the capture session. The unique ID of a Core Audio device can be obtained from its kAudioDevicePropertyDeviceUID property.
 */
API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos)
@interface AVCaptureAudioPreviewOutput : AVCaptureOutput
{
@private
    AVCaptureAudioPreviewOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @property outputDeviceUniqueID
 @abstract
    Specifies the unique ID of the Core Audio output device being used to play preview audio.
 
 @discussion
    The value of this property is an NSString containing the unique ID of the Core Audio device to be used for output, or nil if the default system output should be used.
 */
@property(nonatomic, copy, nullable) NSString *outputDeviceUniqueID;

/*!
 @property volume
 @abstract
    Specifies the preview volume of the output.
 
 @discussion
    The value of this property is the preview volume of the receiver, where 1.0 is the maximum volume and 0.0 is muted. 
 */
@property(nonatomic) float volume;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioConnectionPoint.h
/*
	File:           AVAudioConnectionPoint.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioConnectionPoint.h>

// ==========  AVFoundation.framework/Headers/AVBase.h
/*
	File:  AVBase.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

 */

#import <Availability.h>
#import <Foundation/NSObjCRuntime.h>

#if defined(__cplusplus)
	#define AVF_EXPORT extern "C"
#else
	#define AVF_EXPORT extern
#endif

// Annotation for classes that inherit -init and +new from NSObject but cannot be usefully initialized using -init or +new
#define AV_INIT_UNAVAILABLE - (instancetype)init NS_UNAVAILABLE; \
                            + (instancetype)new  NS_UNAVAILABLE;

#ifndef __has_feature
	#define __has_feature(FEATURE) 0
#endif

#ifndef NS_STRING_ENUM
	#define NS_STRING_ENUM
#endif

#ifndef NS_EXTENSIBLE_STRING_ENUM
	#define NS_EXTENSIBLE_STRING_ENUM
#endif

// Pre-10.12
#ifndef __NSi_10_12
	#define __NSi_10_12 introduced=10.12
#endif

#ifndef __NSd_10_12
	#define __NSd_10_12 ,deprecated=10.12
#endif

// Pre-10.11
#ifndef __NSi_10_11_3
	#define __NSi_10_11_3 introduced=10.11.3
#endif

#ifndef __NSi_10_11
	#define __NSi_10_11 introduced=10.11
#endif

#ifndef __NSd_10_11
	#define __NSd_10_11 ,deprecated=10.11
#endif

// Pre-10.10
#ifndef __NSi_10_10
	#define __NSi_10_10 introduced=10.10
#endif

// Pre-10.9, weak import
#ifndef __AVAILABILITY_INTERNAL__MAC_10_9
	#define __AVAILABILITY_INTERNAL__MAC_10_9 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-10.8, weak import
#ifndef __AVAILABILITY_INTERNAL__MAC_10_8
	#define __AVAILABILITY_INTERNAL__MAC_10_8 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

#ifndef AVAILABLE_MAC_OS_X_VERSION_10_8_AND_LATER
   #define AVAILABLE_MAC_OS_X_VERSION_10_8_AND_LATER WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef AVAILABLE_MAC_OS_X_VERSION_10_9_AND_LATER
   #define AVAILABLE_MAC_OS_X_VERSION_10_9_AND_LATER WEAK_IMPORT_ATTRIBUTE
#endif

// Pre-10.7, weak import
#ifndef __AVAILABILITY_INTERNAL__MAC_10_7
	#define __AVAILABILITY_INTERNAL__MAC_10_7 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-9.0
#ifndef __NSi_9_0
	#define __NSi_9_0 introduced=9.0
#endif

// Pre-8.3
#ifndef __NSi_8_3
	#define __NSi_8_3 introduced=8.3
#endif

// Pre-5.1, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_5_1
	#define __AVAILABILITY_INTERNAL__IPHONE_5_1 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-5.0, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_5_0
	#define __AVAILABILITY_INTERNAL__IPHONE_5_0 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-6.0, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_6_0
       #define __AVAILABILITY_INTERNAL__IPHONE_6_0 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-6.1, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_6_1
       #define __AVAILABILITY_INTERNAL__IPHONE_6_1 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-7.0, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_7_0
       #define __AVAILABILITY_INTERNAL__IPHONE_7_0 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Pre-7.1, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_7_1
       #define __AVAILABILITY_INTERNAL__IPHONE_7_1 __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

// Deprecations
#ifndef AVAILABLE_MAC_OS_X_VERSION_10_7_AND_LATER_BUT_DEPRECATED_IN_MAC_OS_X_VERSION_10_8
	#define AVAILABLE_MAC_OS_X_VERSION_10_7_AND_LATER_BUT_DEPRECATED_IN_MAC_OS_X_VERSION_10_8 WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef AVAILABLE_MAC_OS_X_VERSION_10_7_AND_LATER_BUT_DEPRECATED_IN_MAC_OS_X_VERSION_10_9
	#define AVAILABLE_MAC_OS_X_VERSION_10_7_AND_LATER_BUT_DEPRECATED_IN_MAC_OS_X_VERSION_10_9 WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef __NSd_9_0
	#define __NSd_9_0 ,deprecated=9.0
#endif

#ifndef __NSd_8_3
	#define __NSd_8_3 ,deprecated=8.3
#endif

#ifndef __NSd_11_0
	#define __NSd_11_0 ,deprecated=11.0
#endif

#ifndef __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_0
#define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_0 WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_0
#define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_0 WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef __AVAILABILITY_INTERNAL__IPHONE_4_3_DEP__IPHONE_7_0
#define __AVAILABILITY_INTERNAL__IPHONE_4_3_DEP__IPHONE_7_0 WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_0
#define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_0 WEAK_IMPORT_ATTRIBUTE
#endif

// To be determined, weak import
#ifndef __AVAILABILITY_INTERNAL__IPHONE_TBD
	#define __AVAILABILITY_INTERNAL__IPHONE_TBD __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

#ifndef __AVAILABILITY_INTERNAL__MAC_TBD
	#define __AVAILABILITY_INTERNAL__MAC_TBD __AVAILABILITY_INTERNAL_WEAK_IMPORT
#endif

#ifndef AVAILABLE_MAC_OS_X_VERSION_TBD_AND_LATER
   #define AVAILABLE_MAC_OS_X_VERSION_TBD_AND_LATER WEAK_IMPORT_ATTRIBUTE
#endif

#ifndef API_AVAILABLE
	#define API_AVAILABLE(...)
#endif

#ifndef NS_AVAILABLE
    #define NS_AVAILABLE(a, b)
#endif

#ifndef NS_AVAILABLE_IOS
	#define NS_AVAILABLE_IOS(a)
#endif

#ifndef NS_CLASS_AVAILABLE
    #define NS_CLASS_AVAILABLE(a, b)
#endif

#ifndef NS_DEPRECATED
    #define NS_DEPRECATED(a, b, c, d)
#endif

#ifndef SDK_HIDE_TIDE
#ifndef __TVOS_PROHIBITED
	#define __TVOS_PROHIBITED
#endif
#endif


// ==========  AVFoundation.framework/Headers/AVAudioNode.h
/*
	File:           AVAudioNode.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioNode.h>

// ==========  AVFoundation.framework/Headers/AVPlayerItemMediaDataCollector.h
/*
 File:  AVPlayerItemMediaDataCollector.h

	Framework:  AVFoundation

	Copyright 2015-2016 Apple Inc. All rights reserved.

 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVPlayerItemMediaDataCollectorInternal;

/*!
	@class			AVPlayerItemMediaDataCollector
	@abstract		AVPlayerItemMediaDataCollector is an abstract class encapsulating the common API for all AVPlayerItemMediaDataCollector subclasses.
	@discussion
		Instances of AVPlayerItemMediaDataCollector permit the collection of media data from an AVAsset during playback by an AVPlayer. As opposed to AVPlayerItemOutputs, AVPlayerItemMediaDataCollectors collect all media data across an AVPlayerItem's timebase, relevant to the specific collector being used. Attaching an AVPlayerItemMediaDataCollector may incur additional I/O accordingly.

		You manage an association of an AVPlayerItemMediaDataCollector instance with an AVPlayerItem as the source input using the AVPlayerItem methods:

		• addMediaDataCollector:
		• removeMediaDataCollector:
*/
NS_CLASS_AVAILABLE(10_11_3, 9_3)
@interface AVPlayerItemMediaDataCollector : NSObject
{
@private
	AVPlayerItemMediaDataCollectorInternal *_collectorInternal;
}
@end

@protocol AVPlayerItemMetadataCollectorPushDelegate;
@class AVPlayerItemMetadataCollectorInternal;
@class AVDateRangeMetadataGroup;

/*!
	@class			AVPlayerItemMetadataCollector
	@abstract		A subclass of AVPlayerItemMediaDataCollector that provides AVMetadataGroups for an AVPlayerItem.
	@discussion
		This class can be used to inform clients of the current set of AVMetadataGroups on an AVPlayerItem, and when new AVMetadataGroups become available - e.g. in a Live HLS stream.
*/
NS_CLASS_AVAILABLE(10_11_3, 9_3)
@interface AVPlayerItemMetadataCollector : AVPlayerItemMediaDataCollector
{
@private
	AVPlayerItemMetadataCollectorInternal *_metadataCollectorInternal;
}

/*!
	@method			initWithIdentifiers:classifyingLabels:
	@abstract		Returns an instance of AVPlayerItemMetadataCollector that can provide all available AVMetadataGroups matching a set of criteria.
	@param			identifiers
					A array of metadata identifiers indicating the metadata items that the output should provide. See AVMetadataIdentifiers.h for publicly defined metadata identifiers. Pass nil to include metadata with any identifier.
	@param			classifyingLabels
					If the metadata format supports labeling each metadata group with a string, supplying an array of group labels indicates that the output should provide metadata groups that match one of the supplied labels. Pass nil to include metadata with any (or no) classifying label.
	@result			An instance of AVPlayerItemMetadataCollector.
	@discussion
		Some metadata available in some formats - such as timed metadata embedded in HLS segments - is not available for collector output.
		The default init method can be used as an alternative to setting both identifiers and classifyingLabels to nil.
*/
- (instancetype)initWithIdentifiers:(nullable NSArray<NSString *> *)identifiers classifyingLabels:(nullable NSArray<NSString *> *)classifyingLabels;

/*!
	@method			setDelegate:queue:
	@abstract		Sets the receiver's delegate and a dispatch queue on which the delegate will be called.
	@param			delegate
					An object conforming to AVPlayerItemMetadataCollectorPushDelegate protocol.
	@param			delegateQueue
					A dispatch queue on which all delegate methods will be called.
*/
- (void)setDelegate:(nullable id <AVPlayerItemMetadataCollectorPushDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
	@property		delegate
	@abstract		The receiver's delegate.
	@discussion
		The delegate is held using a zeroing-weak reference, so this property will have a value of nil after a delegate that was previously set has been deallocated.  This property is not key-value observable.
*/
@property (nonatomic, readonly, weak, nullable) id <AVPlayerItemMetadataCollectorPushDelegate> delegate;

/*!
	@property		delegateQueue
	@abstract		The dispatch queue on which messages are sent to the delegate.
	@discussion
		This property is not key-value observable.
*/
@property (nonatomic, readonly, nullable) dispatch_queue_t delegateQueue;

@end

@protocol AVPlayerItemMetadataCollectorPushDelegate <NSObject>

/*!
	@method			metadataCollector:didCollectDateRangeMetadataGroups:indexesOfNewGroup:indexesOfModifiedGroups:
	@abstract		A delegate callback that delivers the total set of AVDateRangeMetadataGroups for this collector.
	@param			metadataCollector
					The AVPlayerItemMetadataCollector source.
	@param			metadataGroups
					The set of all metadata groups meeting the criteria of the output.
	@param			indexesOfNewGroups
					Indexes of metadataGroups added since the last delegate invocation of this method.
	@param			indexesOfModifiedGroups
					Indexes of metadataGroups modified since the last delegate invocation of this method.
	@discussion
		This method will be invoked whenever new AVDateRangeMetadataGroups are added to metadataGroups or whenever any AVDateRangeMetadataGroups in metadataGroups have been modified since previous invocations. The initial invocation will have indexesOfNewGroup referring to every index in metadataGroups. Subsequent invocations may not contain all previously collected metadata groups if they no longer refer to a region in the AVPlayerItem's seekableTimeRanges.
*/
- (void)metadataCollector:(AVPlayerItemMetadataCollector *)metadataCollector didCollectDateRangeMetadataGroups:(NSArray<AVDateRangeMetadataGroup *> *)metadataGroups indexesOfNewGroups:(NSIndexSet *)indexesOfNewGroups indexesOfModifiedGroups:(NSIndexSet *)indexesOfModifiedGroups;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnit.h
/*
	File:           AVAudioUnit.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnit.h>

// ==========  AVFoundation.framework/Headers/AVCaptureStillImageOutput.h
/*
    File:  AVCaptureStillImageOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVVideoSettings.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureStillImageOutput

@class AVCaptureStillImageOutputInternal;

/*!
 @class AVCaptureStillImageOutput
 @abstract
    AVCaptureStillImageOutput is a concrete subclass of AVCaptureOutput that can be used to capture high-quality still images with accompanying metadata.
 
 @discussion
    Instances of AVCaptureStillImageOutput can be used to capture, on demand, high quality snapshots from a realtime capture source. Clients can request a still image for the current time using the captureStillImageAsynchronouslyFromConnection:completionHandler: method. Clients can also configure still image outputs to produce still images in specific image formats.
 */
API_AVAILABLE(macos(10.7)) API_DEPRECATED("Use AVCapturePhotoOutput instead.", ios(4.0, 10.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureStillImageOutput : AVCaptureOutput
{
@private
    AVCaptureStillImageOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @property outputSettings
 @abstract
    Specifies the options the receiver uses to encode still images before they are delivered.
 
 @discussion
    See AVVideoSettings.h for more information on how to construct an output settings dictionary.
 
    On iOS, the only currently supported keys are AVVideoCodecKey and kCVPixelBufferPixelFormatTypeKey. Use -availableImageDataCVPixelFormatTypes and -availableImageDataCodecTypes to determine what codec keys and pixel formats are supported. AVVideoQualityKey is supported on iOS 6.0 and later and may only be used when AVVideoCodecKey is set to AVVideoCodecTypeJPEG.
 */
@property(nonatomic, copy) NSDictionary<NSString *, id> *outputSettings;

/*!
 @property availableImageDataCVPixelFormatTypes
 @abstract
    Indicates the supported image pixel formats that can be specified in outputSettings.
 
 @discussion
    The value of this property is an NSArray of NSNumbers that can be used as values for the kCVPixelBufferPixelFormatTypeKey in the receiver's outputSettings property. The first format in the returned list is the most efficient output format.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *availableImageDataCVPixelFormatTypes;

/*!
 @property availableImageDataCodecTypes
 @abstract
    Indicates the supported image codec formats that can be specified in outputSettings.
 
 @discussion
    The value of this property is an NSArray of AVVideoCodecTypes that can be used as values for the AVVideoCodecKey in the receiver's outputSettings property.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableImageDataCodecTypes;

/*!
 @property stillImageStabilizationSupported
 @abstract
    Indicates whether the receiver supports still image stabilization.
 
 @discussion
    The receiver's automaticallyEnablesStillImageStabilizationWhenAvailable property can only be set if this property returns YES. Its value may change as the session's -sessionPreset or input device's -activeFormat changes.
 */
@property(nonatomic, readonly, getter=isStillImageStabilizationSupported) BOOL stillImageStabilizationSupported API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property automaticallyEnablesStillImageStabilizationWhenAvailable
 @abstract
    Indicates whether the receiver should automatically use still image stabilization when necessary.
 
 @discussion
    On a receiver where -isStillImageStabilizationSupported returns YES, image stabilization may be applied to reduce blur commonly found in low light photos. When stabilization is enabled, still image captures incur additional latency. The default value is YES when supported, NO otherwise. Setting this property throws an NSInvalidArgumentException if -isStillImageStabilizationSupported returns NO.
 */
@property(nonatomic) BOOL automaticallyEnablesStillImageStabilizationWhenAvailable API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property stillImageStabilizationActive
 @abstract
    Indicates whether still image stabilization is in use for the current capture.
 
 @discussion
    On a receiver where -isStillImageStabilizationSupported returns YES, and automaticallyEnablesStillImageStabilizationWhenAvailable is set to YES, this property may be key-value observed, or queried from inside your key-value observation callback for the @"capturingStillImage" property, to find out if still image stabilization is being applied to the current capture.
 */
@property(nonatomic, readonly, getter=isStillImageStabilizationActive) BOOL stillImageStabilizationActive API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property highResolutionStillImageOutputEnabled
 @abstract
    Indicates whether the receiver should emit still images at the highest resolution supported by its source AVCaptureDevice's activeFormat.
 
 @discussion
    By default, AVCaptureStillImageOutput emits images with the same dimensions as its source AVCaptureDevice's activeFormat.formatDescription. However, if you set this property to YES, the receiver emits still images at its source AVCaptureDevice's activeFormat.highResolutionStillImageDimensions. Note that if you enable video stabilization (see AVCaptureConnection's preferredVideoStabilizationMode) for any output, the high resolution still images emitted by AVCaptureStillImageOutput may be smaller by 10 or more percent.
 */
@property(nonatomic, getter=isHighResolutionStillImageOutputEnabled) BOOL highResolutionStillImageOutputEnabled API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property capturingStillImage
 @abstract
    A boolean value that becomes true when a still image is being captured.
 
 @discussion
    The value of this property is a BOOL that becomes true when a still image is being captured, and false when no still image capture is underway. This property is key-value observable.
 */
@property(readonly, getter=isCapturingStillImage) BOOL capturingStillImage API_AVAILABLE(macos(10.8), ios(5.0));

/*!
 @method captureStillImageAsynchronouslyFromConnection:completionHandler:
 @abstract
    Initiates an asynchronous still image capture, returning the result to a completion handler.
 
 @param connection
    The AVCaptureConnection object from which to capture the still image.
 @param handler
    A block that will be called when the still image capture is complete. The block will be passed a CMSampleBuffer object containing the image data or an NSError object if an image could not be captured.
 
 @discussion
    This method will return immediately after it is invoked, later calling the provided completion handler block when image data is ready. If the request could not be completed, the error parameter will contain an NSError object describing the failure.
 
    Attachments to the image data sample buffer may contain metadata appropriate to the image data format. For instance, a sample buffer containing JPEG data may carry a kCGImagePropertyExifDictionary as an attachment. See <ImageIO/CGImageProperties.h> for a list of keys and value types.
 
    Clients should not assume that the completion handler will be called on a specific thread.
 
    Calls to captureStillImageAsynchronouslyFromConnection:completionHandler: are not synchronized with AVCaptureDevice manual control completion handlers. Setting a device manual control, waiting for its completion, then calling captureStillImageAsynchronouslyFromConnection:completionHandler: DOES NOT ensure that the still image returned reflects your manual control change. It may be from an earlier time. You can compare your manual control completion handler sync time to the returned still image's presentation time. You can retrieve the sample buffer's pts using CMSampleBufferGetPresentationTimestamp(). If the still image has an earlier timestamp, your manual control command does not apply to it.
 */
- (void)captureStillImageAsynchronouslyFromConnection:(AVCaptureConnection *)connection completionHandler:(void (^)(CMSampleBufferRef _Nullable imageDataSampleBuffer, NSError * _Nullable error))handler;

/*!
 @method jpegStillImageNSDataRepresentation:
 @abstract
    Converts the still image data and metadata attachments in a JPEG sample buffer to an NSData representation.
 
 @param jpegSampleBuffer
    The sample buffer carrying JPEG image data, optionally with Exif metadata sample buffer attachments. This method throws an NSInvalidArgumentException if jpegSampleBuffer is NULL or not in the JPEG format.
 
 @discussion
    This method returns an NSData representation of a JPEG still image sample buffer, merging the image data and Exif metadata sample buffer attachments without recompressing the image. The returned NSData is suitable for writing to disk.
 */
+ (nullable NSData *)jpegStillImageNSDataRepresentation:(CMSampleBufferRef)jpegSampleBuffer;

@end


#pragma mark - AVCaptureBracketedStillImageSettings

/*!
 @class AVCaptureBracketedStillImageSettings
 @abstract
    AVCaptureBracketedStillImageSettings is an abstract base class that defines an interface for settings pertaining to a bracketed capture.
 
 @discussion
    AVCaptureBracketedStillImageSettings may not be instantiated directly.
 */
API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureBracketedStillImageSettings : NSObject

AV_INIT_UNAVAILABLE

@end


#pragma mark - AVCaptureManualExposureBracketedStillImageSettings

/*!
 @class AVCaptureManualExposureBracketedStillImageSettings
 @abstract
    AVCaptureManualExposureBracketedStillImageSettings is a concrete subclass of AVCaptureBracketedStillImageSettings to be used when bracketing exposure duration and ISO.
 
 @discussion
    An AVCaptureManualExposureBracketedStillImageSettings instance defines the exposure duration and ISO settings that should be applied to one image in a bracket. An array of settings objects is passed to -[AVCaptureStillImageOutput captureStillImageBracketAsynchronouslyFromConnection:withSettingsArray:completionHandler:]. Min and max duration and ISO values are queryable properties of the AVCaptureDevice supplying data to an AVCaptureStillImageOutput instance. If you wish to leave exposureDuration unchanged for this bracketed still image, you may pass the special value AVCaptureExposureDurationCurrent. To keep ISO unchanged, you may pass AVCaptureISOCurrent (see AVCaptureDevice.h).
 */
API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureManualExposureBracketedStillImageSettings : AVCaptureBracketedStillImageSettings

/*!
 @method manualExposureSettingsWithExposureDuration:ISO:
 @abstract
    Creates an AVCaptureManualExposureBracketedStillImageSettings using the specified exposure duration and ISO.
 
 @param duration
    The exposure duration in seconds. Pass AVCaptureExposureDurationCurrent to leave the duration unchanged for this bracketed image.
 @param ISO
    The ISO. Pass AVCaptureISOCurrent to leave the ISO unchanged for this bracketed image.
 @result
    An initialized AVCaptureManualExposureBracketedStillImageSettings instance.
 */
+ (instancetype)manualExposureSettingsWithExposureDuration:(CMTime)duration ISO:(float)ISO;

/*!
 @property exposureDuration
 @abstract
    The exposure duration for the still image.
 */
@property(readonly) CMTime exposureDuration;

/*!
 @property ISO
 @abstract
    The ISO for the still image.
 */
@property(readonly) float ISO;

@end


#pragma mark - AVCaptureAutoExposureBracketedStillImageSettings

/*!
 @class AVCaptureAutoExposureBracketedStillImageSettings
 @abstract
    AVCaptureAutoExposureBracketedStillImageSettings is a concrete subclass of AVCaptureBracketedStillImageSettings to be used when bracketing exposure target bias.
 
 @discussion
    An AVCaptureAutoExposureBracketedStillImageSettings instance defines the exposure target bias setting that should be applied to one image in a bracket. An array of settings objects is passed to -[AVCaptureStillImageOutput captureStillImageBracketAsynchronouslyFromConnection:withSettingsArray:completionHandler:]. Min and max exposure target bias are queryable properties of the AVCaptureDevice supplying data to an AVCaptureStillImageOutput instance. If you wish to leave exposureTargetBias unchanged for this bracketed still image, you may pass the special value AVCaptureExposureTargetBiasCurrent (see AVCaptureDevice.h).
 */
API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureAutoExposureBracketedStillImageSettings : AVCaptureBracketedStillImageSettings

/*!
 @method autoExposureSettingsWithExposureTargetBias
 @abstract
     Creates an AVCaptureAutoExposureBracketedStillImageSettings using the specified exposure target bias.
 
 @param exposureTargetBias
     The exposure target bias. Pass AVCaptureExposureTargetBiasCurrent to leave the exposureTargetBias unchanged for this image.
 @result
     An initialized AVCaptureAutoExposureBracketedStillImageSettings instance.
 */
+ (instancetype)autoExposureSettingsWithExposureTargetBias:(float)exposureTargetBias;

/*!
 @property exposureTargetBias
 @abstract
     The exposure bias for the auto exposure bracketed settings
 */
@property(readonly) float exposureTargetBias;

@end


/*!
 @category AVCaptureStillImageOutput (AVCaptureStillImageOutputBracketedCapture)
 @abstract
    A category of methods for bracketed still image capture.
 
 @discussion
    A "still image bracket" is a batch of images taken as quickly as possible in succession, optionally with different settings from picture to picture.
 
    In a bracketed capture, AVCaptureDevice flashMode property is ignored (flash is forced off), as is AVCaptureStillImageOutput's automaticallyEnablesStillImageStabilizationWhenAvailable property (stabilization is forced off).
 */
@interface AVCaptureStillImageOutput (AVCaptureStillImageOutputBracketedCapture)

/*!
 @property maxBracketedCaptureStillImageCount
 @abstract
    Specifies the maximum number of still images that may be taken in a single bracket.
 
 @discussion
    AVCaptureStillImageOutput can only satisfy a limited number of image requests in a single bracket without exhausting system resources. The maximum number of still images that may be taken in a single bracket depends on the size of the images being captured, and consequently may vary with AVCaptureSession -sessionPreset and AVCaptureDevice -activeFormat. Some formats do not support bracketed capture and return a maxBracketedCaptureStillImageCount of 0. This read-only property is key-value observable. If you exceed -maxBracketedCaptureStillImageCount, then -captureStillImageBracketAsynchronouslyFromConnection:withSettingsArray:completionHandler: fails and the completionHandler is called [settings count] times with a NULL sample buffer and AVErrorMaximumStillImageCaptureRequestsExceeded.
 */
@property(nonatomic, readonly) NSUInteger maxBracketedCaptureStillImageCount API_DEPRECATED("Use AVCapturePhotoOutput maxBracketedCapturePhotoCount instead.", ios(8.0, 10.0)) API_UNAVAILABLE(macos);

/*!
 @property lensStabilizationDuringBracketedCaptureSupported
 @abstract
    Indicates whether the receiver supports lens stabilization during bracketed captures.
 
 @discussion
    The receiver's lensStabilizationDuringBracketedCaptureEnabled property can only be set if this property returns YES. Its value may change as the session's -sessionPreset or input device's -activeFormat changes. This read-only property is key-value observable.
 */
@property(nonatomic, readonly, getter=isLensStabilizationDuringBracketedCaptureSupported) BOOL lensStabilizationDuringBracketedCaptureSupported API_DEPRECATED("Use AVCapturePhotoOutput lensStabilizationDuringBracketedCaptureSupported instead.", ios(9.0, 10.0)) API_UNAVAILABLE(macos);

/*!
 @property lensStabilizationDuringBracketedCaptureEnabled
 @abstract
    Indicates whether the receiver should use lens stabilization during bracketed captures.
 
 @discussion
    On a receiver where -isLensStabilizationDuringBracketedCaptureSupported returns YES, lens stabilization may be applied to the bracket to reduce blur commonly found in low light photos. When lens stabilization is enabled, bracketed still image captures incur additional latency. Lens stabilization is more effective with longer-exposure captures, and offers limited or no benefit for exposure durations shorter than 1/30 of a second. It is possible that during the bracket, the lens stabilization module may run out of correction range and therefore will not be active for every frame in the bracket. Each emitted CMSampleBuffer from the bracket will have an attachment of kCMSampleBufferAttachmentKey_StillImageLensStabilizationInfo indicating additional information about stabilization was applied to the buffer, if any. The default value of -isLensStabilizationDuringBracketedCaptureEnabled is NO. This value will be set to NO when -isLensStabilizationDuringBracketedCaptureSupported changes to NO. Setting this property throws an NSInvalidArgumentException if -isLensStabilizationDuringBracketedCaptureSupported returns NO. This property is key-value observable.
 */
@property(nonatomic, getter=isLensStabilizationDuringBracketedCaptureEnabled) BOOL lensStabilizationDuringBracketedCaptureEnabled API_DEPRECATED("Use AVCapturePhotoOutput with AVCapturePhotoBracketSettings instead.", ios(9.0, 10.0)) API_UNAVAILABLE(macos);

/*!
 @method prepareToCaptureStillImageBracketFromConnection:withSettingsArray:completionHandler:
 @abstract
    Allows the receiver to prepare resources in advance of capturing a still image bracket.
 
 @param connection
    The connection through which the still image bracket should be captured.
 @param settings
    An array of AVCaptureBracketedStillImageSettings objects. All must be of the same kind of AVCaptureBracketedStillImageSettings subclass, or an NSInvalidArgumentException is thrown.
 @param handler
    A user provided block that will be called asynchronously once resources have successfully been allocated for the specified bracketed capture operation. If sufficient resources could not be allocated, the "prepared" parameter contains NO, and "error" parameter contains a non-nil error value. If [settings count] exceeds -maxBracketedCaptureStillImageCount, then AVErrorMaximumStillImageCaptureRequestsExceeded is returned. You should not assume that the completion handler will be called on a specific thread.
 
 @discussion
    -maxBracketedCaptureStillImageCount tells you the maximum number of images that may be taken in a single bracket given the current AVCaptureDevice/AVCaptureSession/AVCaptureStillImageOutput configuration. But before taking a still image bracket, additional resources may need to be allocated. By calling -prepareToCaptureStillImageBracketFromConnection:withSettingsArray:completionHandler: first, you are able to deterministically know when the receiver is ready to capture the bracket with the specified settings array.
 */
- (void)prepareToCaptureStillImageBracketFromConnection:(AVCaptureConnection *)connection withSettingsArray:(NSArray<__kindof AVCaptureBracketedStillImageSettings *> *)settings completionHandler:(void (^)(BOOL prepared, NSError * _Nullable error))handler API_DEPRECATED("Use AVCapturePhotoOutput setPreparedPhotoSettingsArray:completionHandler: instead.", ios(8.0, 10.0)) API_UNAVAILABLE(macos);

/*!
 @method captureStillImageBracketAsynchronouslyFromConnection:withSettingsArray:completionHandler:
 @abstract
    Captures a still image bracket.
 
 @param connection
    The connection through which the still image bracket should be captured.
 @param settings
    An array of AVCaptureBracketedStillImageSettings objects. All must be of the same kind of AVCaptureBracketedStillImageSettings subclass, or an NSInvalidArgumentException is thrown.
 @param handler
    A user provided block that will be called asynchronously as each still image in the bracket is captured. If the capture request is successful, the "sampleBuffer" parameter contains a valid CMSampleBuffer, the "stillImageSettings" parameter contains the settings object corresponding to this still image, and a nil "error" parameter. If the bracketed capture fails, sample buffer is NULL and error is non-nil. If [settings count] exceeds -maxBracketedCaptureStillImageCount, then AVErrorMaximumStillImageCaptureRequestsExceeded is returned. You should not assume that the completion handler will be called on a specific thread.
 
 @discussion
    If you have not called -prepareToCaptureStillImageBracketFromConnection:withSettingsArray:completionHandler: for this still image bracket request, the bracket may not be taken immediately, as the receiver may internally need to prepare resources.
 */
- (void)captureStillImageBracketAsynchronouslyFromConnection:(AVCaptureConnection *)connection withSettingsArray:(NSArray<__kindof AVCaptureBracketedStillImageSettings *> *)settings completionHandler:(void (^)(CMSampleBufferRef _Nullable sampleBuffer, AVCaptureBracketedStillImageSettings * _Nullable stillImageSettings, NSError * _Nullable error))handler API_DEPRECATED("Use AVCapturePhotoOutput capturePhotoWithSettings:delegate: instead.", ios(8.0, 10.0)) API_UNAVAILABLE(macos);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMediaFormat.h
/*
    File:  AVMediaFormat.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

// Media types
typedef NSString * AVMediaType NS_EXTENSIBLE_STRING_ENUM;

AVF_EXPORT AVMediaType const AVMediaTypeVideo                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeAudio                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeText                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeClosedCaption         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeSubtitle              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeTimecode              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMediaType const AVMediaTypeMetadata              NS_AVAILABLE(10_8, 6_0);
AVF_EXPORT AVMediaType const AVMediaTypeMuxed                 NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVMediaTypeMetadataObject
 @abstract mediaType of AVCaptureInputPorts that provide AVMetadataObjects.
 @discussion
 Prior to iOS 9.0, camera AVCaptureDeviceInputs provide metadata (detected faces and barcodes) to an
 AVCaptureMetadataOutput through an AVCaptureInputPort whose mediaType is AVMediaTypeMetadata.  The
 AVCaptureMetadataOutput presents metadata to the client as an array of AVMetadataObjects, which are
 defined by Apple and not externally subclassable.  Starting in iOS 9.0, clients may record arbitrary
 metadata to a movie file using the AVCaptureMovieFileOutput.  The movie file output consumes metadata
 in a different format than the AVCaptureMetadataOutput, namely it accepts CMSampleBuffers of type
 'meta'.  Starting in iOS 9.0, two types of AVCaptureInput can produce suitable metadata for the
 movie file output.
 
 <ul>
 <li>The camera AVCaptureDeviceInput now presents an additional AVCaptureInputPort for recording detected
 faces to a movie file. When linked on or after iOS 9, ports that deliver AVCaptureMetadataObjects have a
 mediaType of AVMediaTypeMetadataObject rather than AVMediaTypeMetadata.  Input ports that deliver CMSampleBuffer
 metadata have a mediaType of AVMediaTypeMetadata.</li>
 
 <li>New to iOS 9 is the AVCaptureMetadataInput, which allows clients to record arbitrary metadata to a movie
 file.  Clients package metadata as an AVTimedMetadataGroup, the AVCaptureMetadataInput presents a port of mediaType
 AVMediaTypeMetadata, and when connected to a movie file output, transforms the timed metadata group's AVMetadataItems
 into CMSampleBuffers which can be written to the movie file.</li>
 </ul>
 
 When linked on or after iOS 9, AVCaptureInputPorts with a mediaType of AVMediaTypeMetadata are handled
 specially by the AVCaptureSession. When inputs and outputs are added to the session, the session does
 not form connections implicitly between eligible AVCaptureOutputs and input ports of type AVMediaTypeMetadata.
 If clients want to record a particular kind of metadata to a movie, they must manually form connections
 between a AVMediaTypeMetadata port and the movie file output using AVCaptureSession's -addConnection API.
*/
AVF_EXPORT AVMediaType const AVMediaTypeMetadataObject API_AVAILABLE(ios(9.0), tvos(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED;

AVF_EXPORT AVMediaType const AVMediaTypeDepthData NS_AVAILABLE(10_13, 11_0);


// Media characteristics
typedef NSString * AVMediaCharacteristic NS_EXTENSIBLE_STRING_ENUM;

/*!
 @constant AVMediaCharacteristicVisual
 @abstract A media characteristic that indicates that a track or media selection option includes visual content.
 @discussion
 AVMediaTypeVideo, AVMediaTypeSubtitle, AVMediaTypeClosedCaption are examples of media types with the characteristic AVMediaCharacteristicVisual.
 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicVisual      NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVMediaCharacteristicAudible
 @abstract A media characteristic that indicates that a track or media selection option includes audible content.
 @discussion
 AVMediaTypeAudio is a media type with the characteristic AVMediaCharacteristicAudible.
 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicAudible     NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVMediaCharacteristicLegible
 @abstract A media characteristic that indicates that a track or media selection option includes legible content.
 @discussion
 AVMediaTypeSubtitle and AVMediaTypeClosedCaption are examples of media types with the characteristic AVMediaCharacteristicLegible.
 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicLegible     NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVMediaCharacteristicFrameBased
 @abstract A media characteristic that indicates that a track or media selection option includes content that's frame-based.
 @discussion
 Frame-based content typically comprises discrete media samples that, once rendered, can remain current for indefinite periods of time without additional processing in support of "time-stretching". Further, any dependencies between samples are always explicitly signalled, so that the operations required to render any single sample can readily be performed on demand. AVMediaTypeVideo is the most common type of frame-based media. AVMediaTypeAudio is the most common counterexample. 
 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicFrameBased  NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVMediaCharacteristicUsesWideGamutColorSpace
 @abstract A media characteristic that indicates that a track uses a wide gamut color space and therefore may make use of colors that cannot be accurately represented otherwise.
 @discussion
 A wide color space such as AVVideo*_P3_D65 contains additional dynamic range that may benefit from special treatment when compositing. Care should be taken to avoid clamping. Non-wide spaces include AVVideo*_ITU_R_709_2 and AVVideo*_SMPTE_C.
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicUsesWideGamutColorSpace NS_AVAILABLE(10_12, 10_0);

/*!
 @constant AVMediaCharacteristicIsMainProgramContent
 @abstract A media characteristic that indicates that a track or media selection option includes content that's marked by the content author as intrinsic to the presentation of the asset.
 @discussion
 Example: an option that presents the main program audio for the presentation, regardless of locale, would typically have this characteristic.
 The value of this characteristic is @"public.main-program-content".
 Note for content authors: the presence of this characteristic for a media option is inferred; any option that does not have the characteristic AVMediaCharacteristicIsAuxiliaryContent is considered to have the characteristic AVMediaCharacteristicIsMainProgramContent.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicIsMainProgramContent NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicIsAuxiliaryContent
 @abstract A media characteristic that indicates that a track or media selection option includes content that's marked by the content author as auxiliary to the presentation of the asset.
 @discussion
 The value of this characteristic is @"public.auxiliary-content".
 Example: an option that presents audio media containing commentary on the presentation would typically have this characteristic.
 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicIsAuxiliaryContent if it's explicitly tagged with that characteristic or if, as a member of an alternate track group, its associated track is excluded from autoselection.
 See the discussion of the tagging of tracks with media characteristics below.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicIsAuxiliaryContent NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicContainsOnlyForcedSubtitles
 @abstract A media characteristic that indicates that a track or media selection option presents only forced subtitles.
 @discussion
 Media options with forced-only subtitles are typically selected when 1) the user has not selected a legible option with an accessibility characteristic or an auxiliary purpose and 2) its locale matches the locale of the selected audible media selection option.
 The value of this characteristic is @"public.subtitles.forced-only".
 Note for content authors: the presence of this characteristic for a legible media option may be inferred from the format description of the associated track that presents the subtitle media, if the format description carries sufficient information to indicate the presence or absence of forced and non-forced subtitles. If the format description does not carry this information, the legible media option can be explicitly tagged with the characteristic.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicContainsOnlyForcedSubtitles NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicTranscribesSpokenDialogForAccessibility
 @abstract A media characteristic that indicates that a track or media selection option includes legible content in the language of its specified locale that:
 	- transcribes spoken dialog and
 	- identifies speakers whenever other visual cues are insufficient for a viewer to determine who is speaking.
 @discussion
 Legible tracks provided for accessibility purposes are typically tagged both with this characteristic as well as with AVMediaCharacteristicDescribesMusicAndSoundForAccessibility.

 A legible track provided for accessibility purposes that's associated with an audio track that has no spoken dialog can be tagged with this characteristic, because it trivially meets these requirements.

 The value of this characteristic is @"public.accessibility.transcribes-spoken-dialog".

 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicTranscribesSpokenDialogForAccessibility only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicTranscribesSpokenDialogForAccessibility NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicDescribesMusicAndSoundForAccessibility
 @abstract A media characteristic that indicates that a track or media selection option includes legible content in the language of its specified locale that:
 	- describes music and
 	- describes sound other than spoken dialog, such as sound effects and significant silences, occurring in program audio.
 @discussion
 Legible tracks provided for accessibility purposes are typically tagged both with this characteristic as well as with AVMediaCharacteristicTranscribesSpokenDialogForAccessibility.

 A legible track provided for accessibility purposes that's associated with an audio track without music and without sound other than spoken dialog -- lacking even significant silences -- can be tagged with this characteristic, because it trivially meets these requirements.

 The value of this characteristic is @"public.accessibility.describes-music-and-sound".

 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicDescribesMusicAndSoundForAccessibility only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicDescribesMusicAndSoundForAccessibility NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicEasyToRead
 @abstract A media characteristic that indicates that a track or media selection option provides legible content in the language of its specified locale that has been edited for ease of reading.
 @discussion
 The value of this characteristic is @"public.easy-to-read".
 
 Closed caption tracks that carry "easy reader" captions (per the CEA-608 specification) should be tagged with this characteristic. Subtitle tracks can also be tagged with this characteristic, where appropriate.

 Note for content authors: for QuickTime movie and .m4v files a track is considered to have the characteristic AVMediaCharacteristicEasyToRead only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicEasyToRead NS_AVAILABLE(10_8, 6_0);

/*!
 @constant AVMediaCharacteristicDescribesVideoForAccessibility
 @abstract A media characteristic that indicates that a track or media selection option provides descriptions of the visual portion of the presentation that are sufficient to comprehend essential information that it depicts, such as action and setting.
 @discussion
 See -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
 The value of this characteristic is @"public.accessibility.describes-video".
 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicDescribesVideoForAccessibility only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.

 Also see -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicDescribesVideoForAccessibility NS_AVAILABLE(10_8, 5_0);

/*!
 @constant AVMediaCharacteristicLanguageTranslation
 @abstract A media characteristic that indicates that a track or media selection option contains a language or dialect translation of originally or previously produced content, intended to be used as a substitute for that content by users who prefer its designated language.
 @discussion
 See -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
 The value of this characteristic is @"public.translation".
 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicLanguageTranslation only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicLanguageTranslation NS_AVAILABLE(10_11, 9_0);

/*!
 @constant AVMediaCharacteristicDubbedTranslation
 @abstract A media characteristic that indicates that a track or media selection option contains a language or dialect translation of originally or previously produced content, created by substituting most or all of the dialog in a previous mix of audio content with dialog spoken in its designated language.
 @discussion
 Tracks to which this characteristic is assigned should typically also be assigned the characteristic AVMediaCharacteristicLanguageTranslation.
 See -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
 The value of this characteristic is @"public.translation.dubbed".
 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicDubbedTranslation only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicDubbedTranslation NS_AVAILABLE(10_11, 9_0);

/*!
 @constant AVMediaCharacteristicVoiceOverTranslation NS_AVAILABLE(10_11, 9_0);
 @abstract A media characteristic that indicates that a track or media selection option contains a language translation of originally or previously produced content, created by adding, in its designated language, a verbal interpretation of dialog and translations of other important information to a new mix of the audio content.
 @discussion
 Tracks to which this characteristic is assigned should typically also be assigned the characteristic AVMediaCharacteristicLanguageTranslation.
 See -[AVAssetTrack hasMediaCharacteristic:] and -[AVMediaSelectionOption hasMediaCharacteristic:].
 The value of this characteristic is @"public.translation.voice-over".
 Note for content authors: for QuickTime movie and .m4v files a media option is considered to have the characteristic AVMediaCharacteristicVoiceOverTranslation only if it's explicitly tagged with that characteristic.
 See the discussion of the tagging of tracks with media characteristics below.
*/
AVF_EXPORT AVMediaCharacteristic const AVMediaCharacteristicVoiceOverTranslation NS_AVAILABLE(10_11, 9_0);

/*
	Tagging of tracks of .mov and .m4v files with media characteristics

	Each track of .mov files and .m4v files (that is, files of type AVFileTypeQuickTimeMovie and AVFileTypeAppleM4V) can optionally carry one or more tagged media characteristics, each of which declares a purpose, a trait, or some other disinguishing property of the track's media.

	For example, a track containing audio that mixes original program content with additional narrative descriptions of visual action may be tagged with the media characteristic "public.accessibility.describes-video" in order to distinguish it from other audio tracks stored in the same file that do not contain additional narrative. 

	Each tagged media characteristic in .mov and .m4v files is stored in track userdata as a userdata item of type 'tagc' (represented as a FourCharCode) that consists of a standard atom header (size and type) followed by an array of US-ASCII characters (8-bit, high bit clear) comprising the value of the tag. The character array is not a C string; there is no terminating zero. The userdata item atom size is sum of the standard atom header size (8) and the size of the US-ASCII character array.

	AVFoundation clients can inspect the tagged media characteristics of a track as follows:

		NSArray *trackUserDataItems = [myAVAssetTrack metadataForFormat:AVMetadataFormatQuickTimeUserData];
		NSArray *trackTaggedMediaCharacteristics = [AVMetadataItem metadataItemsFromArray:trackUserDataItems withKey:AVMetadataQuickTimeUserDataKeyTaggedCharacteristic keySpace:AVMetadataKeySpaceQuickTimeUserData];

		for (AVMetadataItem *metadataItem in trackTaggedMediaCharacteristics) {
			NSString *thisTrackMediaCharacteristic = [metadataItem stringValue];
		}

	-[AVAssetTrack hasMediaCharacteristic:] can be used to determine whether a track has a particular media characteristic, whether the characteristic is inferred from its media type or format descriptions (e.g. AVMediaCharacteristicAudible, AVMediaCharacteristicContainsOnlyForcedSubtitles) or requires explicit tagging (e.g. AVMediaCharacteristicTranscribesSpokenDialogForAccessibility, AVMediaCharacteristicDescribesVideoForAccessibility). Note that explicit tagging can't be used to override inferences from tracks' media types or format descriptions; for example, -[AVAssetTrack hasMediaCharacteristic:AVMediaCharacteristicVisual] will return NO for any audio track, even if the track has been perversely tagged with the visual characteristic.

	Tagged media characteristics can be written to the QuickTime userdata of an output track associated with an AVAssetWriterInput as follows, provided that the outputFileType of the AVAssetWriter is either AVFileTypeQuickTimeMovie or AVFileTypeAppleM4V:

		AVMutableMetadataItem *myTaggedMediaCharacteristic = [[[AVMutableMetadataItem alloc] init] autorelease];

		[myTaggedMediaCharacteristic setKey:AVMetadataQuickTimeUserDataKeyTaggedCharacteristic];
		[myTaggedMediaCharacteristic setKeySpace:AVMetadataKeySpaceQuickTimeUserData];

		[myTaggedMediaCharacteristic setValue:aMeaningfulCharacteristicAsNSString];

		[myMutableArrayOfMetadata addObject:myTaggedMediaCharacteristic];

		[myAssetWriterInput setMetadata:myMutableArrayOfMetadata];

*/

// File format UTIs
typedef NSString * AVFileType NS_EXTENSIBLE_STRING_ENUM;

/*!
 @constant AVFileTypeQuickTimeMovie
 @abstract A UTI for the QuickTime movie file format.
 @discussion
 The value of this UTI is @"com.apple.quicktime-movie".
 Files are identified with the .mov and .qt extensions.
 */
AVF_EXPORT AVFileType const AVFileTypeQuickTimeMovie NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeMPEG4
 @abstract A UTI for the MPEG-4 file format.
 @discussion
 The value of this UTI is @"public.mpeg-4".
 Files are identified with the .mp4 extension.
 */
AVF_EXPORT AVFileType const AVFileTypeMPEG4 NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeAppleM4V
 @discussion
 The value of this UTI is @"com.apple.m4v-video".
 Files are identified with the .m4v extension.
 */
AVF_EXPORT AVFileType const AVFileTypeAppleM4V NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeAppleM4A
 @discussion
 The value of this UTI is @"com.apple.m4a-audio".
 Files are identified with the .m4a extension.
 */
AVF_EXPORT AVFileType const AVFileTypeAppleM4A NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileType3GPP
 @abstract A UTI for the 3GPP file format.
 @discussion
 The value of this UTI is @"public.3gpp".
 Files are identified with the .3gp, .3gpp, and .sdv extensions.
 */
AVF_EXPORT AVFileType const AVFileType3GPP NS_AVAILABLE(10_11, 4_0);

/*!
 @constant AVFileType3GPP2
 @abstract A UTI for the 3GPP file format.
 @discussion
 The value of this UTI is @"public.3gpp2".
 Files are identified with the .3g2, .3gp2 extensions.
 */
AVF_EXPORT AVFileType const AVFileType3GPP2 NS_AVAILABLE(10_11, 4_0);

/*!
 @constant AVFileTypeCoreAudioFormat
 @abstract A UTI for the CoreAudio file format.
 @discussion
 The value of this UTI is @"com.apple.coreaudio-format".
 Files are identified with the .caf extension.
 */
AVF_EXPORT AVFileType const AVFileTypeCoreAudioFormat NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeWAVE
 @abstract A UTI for the WAVE audio file format.
 @discussion
 The value of this UTI is @"com.microsoft.waveform-audio".
 Files are identified with the .wav, .wave, and .bwf extensions.
 */
AVF_EXPORT AVFileType const AVFileTypeWAVE NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeAIFF
 @abstract A UTI for the AIFF audio file format.
 @discussion
 The value of this UTI is @"public.aiff-audio".
 Files are identified with the .aif and .aiff extensions.
 */
AVF_EXPORT AVFileType const AVFileTypeAIFF NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeAIFC
 @abstract A UTI for the AIFC audio file format.
 @discussion
 The value of this UTI is @"public.aifc-audio".
 Files are identified with the .aifc and .cdda extensions.
 */
AVF_EXPORT AVFileType const AVFileTypeAIFC NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeAMR
 @abstract A UTI for the adaptive multi-rate audio file format.
 @discussion
 The value of this UTI is @"org.3gpp.adaptive-multi-rate-audio".
 Files are identified with the .amr extension.
 */
AVF_EXPORT AVFileType const AVFileTypeAMR NS_AVAILABLE(10_7, 4_0);

/*!
 @constant AVFileTypeMPEGLayer3
 @abstract A UTI for the MPEG layer 3 audio file format.
 @discussion
 The value of this UTI is @"public.mp3".
 Files are identified with the .mp3 extension.
 */
AVF_EXPORT AVFileType const AVFileTypeMPEGLayer3 NS_AVAILABLE(10_9, 7_0);

/*!
 @constant AVFileTypeSunAU
 @abstract A UTI for the Sun/NeXT audio file format.
 @discussion
 The value of this UTI is @"public.au-audio".
 Files are identified with the .au and .snd extensions.
 */
AVF_EXPORT AVFileType const AVFileTypeSunAU NS_AVAILABLE(10_9, 7_0);

/*!
 @constant AVFileTypeAC3
 @abstract A UTI for the AC-3 audio file format.
 @discussion
 The value of this UTI is @"public.ac3-audio".
 Files are identified with the .ac3 extension.
 */
AVF_EXPORT AVFileType const AVFileTypeAC3 NS_AVAILABLE(10_9, 7_0);

/*!
 @constant AVFileTypeEnhancedAC3
 @abstract A UTI for the enhanced AC-3 audio file format.
 @discussion
 The value of this UTI is @"public.enhanced-ac3-audio".
 Files are identified with the .eac3 extension.
 */
AVF_EXPORT AVFileType const AVFileTypeEnhancedAC3 NS_AVAILABLE(10_11, 9_0);

/*!
 @constant AVFileTypeJPEG
 @abstract A UTI for the JPEG (JFIF) format.
 @discussion
 The value of this UTI is @"public.jpeg".
 Files are identified with the .jpg or .jpeg extension.
 */
AVF_EXPORT AVFileType const AVFileTypeJPEG NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVFileTypeDNG
 @abstract A UTI for the Adobe digital negative file format.
 @discussion
 The value of this UTI is @"com.adobe.raw-image".
 Files are identified with the .dng extension.
 */
AVF_EXPORT AVFileType const AVFileTypeDNG NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVFileTypeHEIC
 @abstract A UTI for the high efficiency image file format containing HEVC compressed images.
 @discussion
 The value of this UTI is @"public.heic".
 Files are identified with the .heic extension.
 */
AVF_EXPORT AVFileType const AVFileTypeHEIC NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVFileTypeAVCI
 @abstract A UTI for the high efficiency image file format containing H.264 compressed images.
 @discussion
 The value of this UTI is @"public.avci".
 Files are identified with the .avci extension.
 */
AVF_EXPORT AVFileType const AVFileTypeAVCI NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVFileTypeHEIF
 @abstract A UTI for the high efficiency image file format containing images compressed with any codec.
 @discussion
 The value of this UTI is @"public.heif".
 Files are identified with the .heif extension.
 */
AVF_EXPORT AVFileType const AVFileTypeHEIF NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVFileTypeTIFF
 @abstract A UTI for the tagged image file format.
 @discussion
 The value of this UTI is @"public.tiff".
 Files are identified with the .tiff or .tif extension.
 */
AVF_EXPORT AVFileType const AVFileTypeTIFF NS_AVAILABLE(10_13, 11_0);

/*!
 @constant AVStreamingKeyDeliveryContentKeyType
 @abstract A UTI for streaming key delivery content keys
 @discussion
 The value of this UTI is @"com.apple.streamingkeydelivery.contentkey".
 */
AVF_EXPORT NSString * const AVStreamingKeyDeliveryContentKeyType NS_AVAILABLE(10_11, 9_0);

/*!
 @constant AVStreamingKeyDeliveryPersistentContentKeyType
 @abstract A UTI for persistent streaming key delivery content keys
 @discussion
 The value of this UTI is @"com.apple.streamingkeydelivery.persistentcontentkey".
 */
AVF_EXPORT NSString * const AVStreamingKeyDeliveryPersistentContentKeyType NS_AVAILABLE(10_11, 9_0);


// ==========  AVFoundation.framework/Headers/AVVideoSettings.h
/*
	File:  AVVideoSettings.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

/*!
 @header AVVideoSettings
 @abstract
	NSDictionary keys for configuring output video format
	
 @discussion
	A video settings dictionary may take one of two forms:
	
	1. For compressed video output, use only the keys in this header, AVVideoSettings.h.
	2. For uncompressed video output, start with kCVPixelBuffer* keys in <CoreVideo/CVPixelBuffer.h>.
	
	In addition to the keys in CVPixelBuffer.h, uncompressed video settings dictionaries may also contain the following keys:
 
		AVVideoPixelAspectRatioKey
		AVVideoCleanApertureKey
		AVVideoScalingModeKey
		AVVideoColorPropertiesKey
		AVVideoAllowWideColorKey
 
	It is an error to add any other AVVideoSettings.h keys to an uncompressed video settings dictionary.
*/

AVF_EXPORT NSString *const AVVideoCodecKey /* NSString (CMVideoCodecType) */				NS_AVAILABLE(10_7, 4_0);

/*!
  @typedef      AVVideoCodecType
  @abstract     The type of the strings used to specify a video codec type (for instance, as values for the AVVideoCodecKey key in a video settings dictionary).
 */
typedef NSString * AVVideoCodecType NS_STRING_ENUM;

	AVF_EXPORT AVVideoCodecType const AVVideoCodecTypeHEVC /* @"hvc1" */                    API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT AVVideoCodecType const AVVideoCodecTypeH264 /* @"avc1" */                    API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT AVVideoCodecType const AVVideoCodecTypeJPEG /* @"jpeg" */                    API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT AVVideoCodecType const AVVideoCodecTypeAppleProRes4444 /* @"ap4h" */         API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT AVVideoCodecType const AVVideoCodecTypeAppleProRes422 /* @"apcn" */          API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

	AVF_EXPORT NSString *const AVVideoCodecHEVC /* @"hvc1" */                               API_DEPRECATED_WITH_REPLACEMENT("AVVideoCodecTypeHEVC", macos(10.13, 10.13), ios(11.0, 11.0), tvos(11.0, 11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT NSString *const AVVideoCodecH264 /* @"avc1" */                               API_DEPRECATED_WITH_REPLACEMENT("AVVideoCodecTypeH264", macos(10.7, 10.13), ios(4.0, 11.0), tvos(9.0, 11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT NSString *const AVVideoCodecJPEG /* @"jpeg" */                               API_DEPRECATED_WITH_REPLACEMENT("AVVideoCodecTypeJPEG", macos(10.7, 10.13), ios(4.0, 11.0), tvos(9.0, 11.0)) __WATCHOS_PROHIBITED;
	AVF_EXPORT NSString *const AVVideoCodecAppleProRes4444 /* @"ap4h" */                    API_DEPRECATED_WITH_REPLACEMENT("AVVideoCodecTypeAppleProRes4444", macos(10.7, 10.13)) API_UNAVAILABLE(ios, tvos, watchos);
	AVF_EXPORT NSString *const AVVideoCodecAppleProRes422 /* @"apcn" */                     API_DEPRECATED_WITH_REPLACEMENT("AVVideoCodecTypeAppleProRes422", macos(10.7, 10.13)) API_UNAVAILABLE(ios, tvos, watchos);

// For best results, always use even number values for AVVideoWidthKey and AVVideoHeightKey when encoding to AVVideoCodecTypeH264 or any other format that uses 4:2:0 downsampling
AVF_EXPORT NSString *const AVVideoWidthKey /* NSNumber (encoded pixels) */					NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVVideoHeightKey /* NSNumber (encoded pixels) */					NS_AVAILABLE(10_7, 4_0);

/*!
 @constant	AVVideoPixelAspectRatioKey
 @abstract	The aspect ratio of the pixels in the video frame
 @discussion
	The value for this key is an NSDictionary containing AVVideoPixelAspectRatio*Key keys.  If no value is specified for this key, the default value for the codec is used.  Usually this is 1:1, meaning square pixels.
 
	Note that prior to OS X 10.9 and iOS 7.0, this key could only be specified as part of the dictionary given for AVVideoCompressionPropertiesKey.  As of OS X 10.9 and iOS 7.0, the top level of an AVVideoSettings dictionary is the preferred place to specify this key.
*/
AVF_EXPORT NSString *const AVVideoPixelAspectRatioKey										NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoPixelAspectRatioHorizontalSpacingKey /* NSNumber */	NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoPixelAspectRatioVerticalSpacingKey /* NSNumber */		NS_AVAILABLE(10_7, 4_0);

/*!
 @constant	AVVideoCleanApertureKey
 @abstract	Defines the region within the video dimensions that will be displayed during playback
 @discussion
	The value for this key is an NSDictionary containing AVVideoCleanAperture*Key keys.  AVVideoCleanApertureWidthKey and AVVideoCleanApertureHeightKey define a clean rectangle which is centered on the video frame.  To offset this rectangle from center, use AVVideoCleanApertureHorizontalOffsetKey and AVVideoCleanApertureVerticalOffsetKey.  A positive value for AVVideoCleanApertureHorizontalOffsetKey moves the clean aperture region to the right, and a positive value for AVVideoCleanApertureVerticalOffsetKey moves the clean aperture region down.
 
	If no clean aperture region is specified, the entire frame will be displayed during playback.
 
	Note that prior to OS X 10.9 and iOS 7.0, this key could only be specified as part of the dictionary given for AVVideoCompressionPropertiesKey.  As of OS X 10.9 and iOS 7.0, the top level of an AVVideoSettings dictionary is the preferred place to specify this key.
*/
AVF_EXPORT NSString *const AVVideoCleanApertureKey											NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoCleanApertureWidthKey /* NSNumber */					NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoCleanApertureHeightKey /* NSNumber */					NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoCleanApertureHorizontalOffsetKey /* NSNumber */		NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoCleanApertureVerticalOffsetKey /* NSNumber */			NS_AVAILABLE(10_7, 4_0);

AVF_EXPORT NSString *const AVVideoScalingModeKey /* NSString */								NS_AVAILABLE(10_7, 5_0);
	/* AVVideoScalingModeFit - Crop to remove edge processing region; preserve aspect ratio of cropped source by reducing specified width or height if necessary.  Will not scale a small source up to larger dimensions. */
	AVF_EXPORT NSString *const AVVideoScalingModeFit										NS_AVAILABLE(10_7, 5_0);
	/* AVVideoScalingModeResize - Crop to remove edge processing region; scale remainder to destination area.  Does not preserve aspect ratio. */
	AVF_EXPORT NSString *const AVVideoScalingModeResize										NS_AVAILABLE(10_7, 5_0);
	/* AVVideoScalingModeResizeAspect - Preserve aspect ratio of the source, and fill remaining areas with black to fit destination dimensions. */
	AVF_EXPORT NSString *const AVVideoScalingModeResizeAspect								NS_AVAILABLE(10_7, 5_0);
	/* AVVideoScalingModeResizeAspectFill - Preserve aspect ratio of the source, and crop picture to fit destination dimensions. */
	AVF_EXPORT NSString *const AVVideoScalingModeResizeAspectFill							NS_AVAILABLE(10_7, 5_0);

/*
	Clients who specify AVVideoColorPropertiesKey must specify a color primary, transfer function, and Y'CbCr matrix.
	Most clients will want to specify HD, which consists of:
 
		AVVideoColorPrimaries_ITU_R_709_2
		AVVideoTransferFunction_ITU_R_709_2
		AVVideoYCbCrMatrix_ITU_R_709_2
 
	If you require SD colorimetry use:
 
		AVVideoColorPrimaries_SMPTE_C
		AVVideoTransferFunction_ITU_R_709_2
		AVVideoYCbCrMatrix_ITU_R_601_4
 
	If you require wide gamut HD colorimetry, you can use:
 
		 AVVideoColorPrimaries_P3_D65
		 AVVideoTransferFunction_ITU_R_709_2
		 AVVideoYCbCrMatrix_ITU_R_709_2
 
	AVFoundation will color match if the source and destination color properties differ according to the following rules:
 
	If you want to override the tagging of color properties in the video that you will be processing, set a value for AVVideoColorPropertiesKey:
 
	If AVVideoColorPropertiesKey is set in the output settings and the source buffers are not tagged with color properties, then the output will be tagged according to the AVVideoColorPropertiesKey color properties.
	If AVVideoColorPropertiesKey is set in the output settings and the source buffers are also tagged with color properties, then if necessary the source buffers will be color converted to match the color properties specified by AVVideoColorPropertiesKey, and the output will be tagged according to the AVVideoColorPropertiesKey color properties.
 
	If you do not want to override the tagging of color properties in the video that you will be processing, do not set a value for AVVideoColorPropertiesKey:
 
	If AVVideoColorPropertiesKey is not set in the output settings and the source buffers are tagged with color properties, then the output will be tagged according to the source buffer color properties.
	If AVVideoColorPropertiesKey is not set in the output settings and the source buffers are not tagged with color properties, then the output will not be tagged with any color properties.
 
	 It is important that the source be tagged.
*/
AVF_EXPORT NSString *const AVVideoColorPropertiesKey /* NSDictionary, all 3 below keys required */           NS_AVAILABLE(10_7, 10_0);
	AVF_EXPORT NSString *const AVVideoColorPrimariesKey /* NSString */                                       NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoColorPrimaries_ITU_R_709_2                                         NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoColorPrimaries_EBU_3213                                            NS_AVAILABLE(10_7, NA);
		AVF_EXPORT NSString *const AVVideoColorPrimaries_SMPTE_C                                             NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoColorPrimaries_P3_D65                                              NS_AVAILABLE(10_12, 10_0);
		AVF_EXPORT NSString *const AVVideoColorPrimaries_ITU_R_2020                                          NS_AVAILABLE(10_13, 11_0);
	AVF_EXPORT NSString *const AVVideoTransferFunctionKey /* NSString */                                     NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoTransferFunction_ITU_R_709_2                                       NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoTransferFunction_SMPTE_240M_1995                                   NS_AVAILABLE(10_7, NA);
		AVF_EXPORT NSString *const AVVideoTransferFunction_SMPTE_ST_2084_PQ                                  NS_AVAILABLE(10_13, 11_0);
		AVF_EXPORT NSString *const AVVideoTransferFunction_ITU_R_2100_HLG                                    NS_AVAILABLE(10_13, 11_0);
	AVF_EXPORT NSString *const AVVideoYCbCrMatrixKey /* NSString */                                          NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoYCbCrMatrix_ITU_R_709_2                                            NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoYCbCrMatrix_ITU_R_601_4                                            NS_AVAILABLE(10_7, 10_0);
		AVF_EXPORT NSString *const AVVideoYCbCrMatrix_SMPTE_240M_1995                                        NS_AVAILABLE(10_7, NA);
		AVF_EXPORT NSString *const AVVideoYCbCrMatrix_ITU_R_2020                                             NS_AVAILABLE(10_13, 11_0);

/*!
 @constant	AVVideoAllowWideColorKey
 @abstract	Indicates whether the client can process wide color
 @discussion
	Clients who wish to process wide color content should set the value of this key to @YES, or specify AVVideoColorPropertiesKey.
 
	The default value, @NO, permits implicit color conversions to occur to a non-wide gamut color space.
 */
AVF_EXPORT NSString *const AVVideoAllowWideColorKey /* NSNumber(BOOL)	*/					NS_AVAILABLE(10_12, 10_0);

/*!
 @constant	AVVideoCompressionPropertiesKey
 @abstract
	The value for this key is an instance of NSDictionary, containing properties to be passed down to the video encoder.
 @discussion
	Package the below keys in an instance of NSDictionary and use it as the value for AVVideoCompressionPropertiesKey in the top-level AVVideoSettings dictionary.  In addition to the keys listed below, you can also include keys from VideoToolbox/VTCompressionProperties.h.
 
	Most keys can only be used for certain encoders.  Look at individual keys for details.
 */
AVF_EXPORT NSString *const AVVideoCompressionPropertiesKey /* NSDictionary */                                NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoAverageBitRateKey /* NSNumber (bits per second, H.264 only) */         NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoQualityKey /* NSNumber (0.0-1.0, JPEG and HEIC only. With HEIC, 1.0 indicates lossless compression) */ NS_AVAILABLE(10_7, 5_0);
	AVF_EXPORT NSString *const AVVideoMaxKeyFrameIntervalKey /* NSNumber (frames, 1 means key frames only, H.264 only) */ NS_AVAILABLE(10_7, 4_0);
	AVF_EXPORT NSString *const AVVideoMaxKeyFrameIntervalDurationKey /* NSNumber (seconds, 0.0 means no limit, H.264 only) */ NS_AVAILABLE(10_9, 7_0);

	/*!
	 @constant	AVVideoAllowFrameReorderingKey
	 @abstract
		 Enables or disables frame reordering.
	 @discussion
		 In order to achieve the best compression while maintaining image quality, some video encoders can reorder frames.  This means that the order in which the frames will be emitted and stored (the decode order) will be different from the order in which they are presented to the video encoder (the display order).
		
		Encoding using frame reordering requires more system resources than encoding without frame reordering, so encoding performance should be taken into account when deciding whether to enable frame reordering.  This is especially important when encoding video data from a real-time source, such as AVCaptureVideoDataOutput.  In this situation, using a value of @NO for AVVideoAllowFrameReorderingKey may yield the best results.
	 
		The default is @YES, which means that the encoder decides whether to enable frame reordering.
	 */
	AVF_EXPORT NSString *const AVVideoAllowFrameReorderingKey /* NSNumber (BOOL) */							 NS_AVAILABLE(10_10, 7_0);

	AVF_EXPORT NSString *const AVVideoProfileLevelKey /* NSString, profile/level constants are specific to a particular encoder. See VideoToolbox/VTCompressionProperties.h for additional profiles/levels that can used as the value of this key. */               NS_AVAILABLE(10_8, 4_0);

		/* HEVC profiles/levels are defined in VideoToolbox/VTCompressionProperties.h, e.g. kVTProfileLevel_HEVC_Main_AutoLevel. The constants defined there can be used as the value for the key AVVideoProfileLevelKey. */

		AVF_EXPORT NSString *const AVVideoProfileLevelH264Baseline30 /* Baseline Profile Level 3.0 */        NS_AVAILABLE(10_8, 4_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264Baseline31 /* Baseline Profile Level 3.1 */        NS_AVAILABLE(10_8, 4_0);
        AVF_EXPORT NSString *const AVVideoProfileLevelH264Baseline41 /* Baseline Profile Level 4.1 */        NS_AVAILABLE(10_8, 5_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264BaselineAutoLevel /* Baseline Profile Auto Level */ NS_AVAILABLE(10_9, 7_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264Main30 /* Main Profile Level 3.0 */                NS_AVAILABLE(10_8, 4_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264Main31 /* Main Profile Level 3.1 */                NS_AVAILABLE(10_8, 4_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264Main32 /* Main Profile Level 3.2 */                NS_AVAILABLE(10_8, 5_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264Main41 /* Main Profile Level 4.1 */                NS_AVAILABLE(10_8, 5_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264MainAutoLevel /* Main Profile Auto Level */        NS_AVAILABLE(10_9, 7_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264High40 /* High Profile Level 4.0 */                NS_AVAILABLE(10_9, 6_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264High41 /* High Profile Level 4.1 */                NS_AVAILABLE(10_9, 6_0);
		AVF_EXPORT NSString *const AVVideoProfileLevelH264HighAutoLevel /* High Profile Auto Level */        NS_AVAILABLE(10_9, 7_0);

	/*!
	 @constant	AVVideoH264EntropyModeKey
	 @abstract
		The entropy encoding mode for H.264 compression.
	 @discussion
		If supported by an H.264 encoder, this property controls whether the encoder should use Context-based Adaptive Variable Length Coding (CAVLC) or Context-based Adaptive Binary Arithmetic Coding (CABAC).  CABAC generally gives better compression at the expense of higher computational overhead.  The default value is encoder-specific and may change depending on other encoder settings.  Care should be taken when using this property -- changes may result in a configuration which is not compatible with a requested Profile and Level.  Results in this case are undefined, and could include encode errors or a non-compliant output stream.
	*/
	AVF_EXPORT NSString *const AVVideoH264EntropyModeKey     /* NSString, H.264 only, one of: */			 NS_AVAILABLE(10_10, 7_0);
		AVF_EXPORT NSString *const AVVideoH264EntropyModeCAVLC /* Context-based Adaptive Variable Length Coding */   NS_AVAILABLE(10_10, 7_0);
		AVF_EXPORT NSString *const AVVideoH264EntropyModeCABAC /* Context-based Adaptive Binary Arithmetic Coding */ NS_AVAILABLE(10_10, 7_0);

	/*!
	 @constant	AVVideoExpectedSourceFrameRateKey
	 @abstract
		Indicates the expected source frame rate, if known.
	 @discussion
		The frame rate is measured in frames per second. This is not used to control the frame rate; it is provided as a hint to the video encoder so that it can set up internal configuration before compression begins. The actual frame rate will depend on frame durations and may vary. This should be set if an AutoLevel AVVideoProfileLevelKey is used, or if the source content has a high frame rate (higher than 30 fps). The encoder might have to drop frames to satisfy bit stream requirements if this key is not specified.
	 */
	AVF_EXPORT NSString *const AVVideoExpectedSourceFrameRateKey /* NSNumber (frames per second) */				NS_AVAILABLE(10_10, 7_0);

	/*!
	 @constant	AVVideoAverageNonDroppableFrameRateKey
	 @abstract
		The desired average number of non-droppable frames to be encoded for each second of video.
	 @discussion
		Some video encoders can produce a flexible mixture of non-droppable frames and droppable frames.  The difference between these types is that it is necessary for a video decoder to decode a non-droppable frame in order to successfully decode subsequent frames, whereas droppable frames are optional and can be skipped without impact on decode of subsequent frames.  Having a proportion of droppable frames in a sequence has advantages for temporal scalability: at playback time more or fewer frames may be decoded depending on the play rate.  This property requests that the encoder emit an overall proportion of non-droppable and droppable frames so that there are the specified number of non-droppable frames per second.
 
		For example, to specify that the encoder should include an average of 30 non-droppable frames for each second of video:
 
		[myVideoSettings setObject:@30 forKey:AVVideoAverageNonDroppableFrameRateKey];
	 */
	AVF_EXPORT NSString *const AVVideoAverageNonDroppableFrameRateKey /* NSNumber (frames per second) */		NS_AVAILABLE(10_10, 7_0);

/*!
 @constant	AVVideoDecompressionPropertiesKey
 @abstract
 The value for this key is an instance of NSDictionary, containing properties to be passed down to the video decoder.
 @discussion
 Package the below keys in an instance of NSDictionary and use it as the value for AVVideoDecompressionPropertiesKey in the top-level AVVideoSettings dictionary.  In addition to the keys listed below, you can also include keys from VideoToolbox/VTDecompressionProperties.h.
 
 Most keys can only be used for certain decoders.  Look at individual keys for details.
 */
AVF_EXPORT NSString *const AVVideoDecompressionPropertiesKey /* NSDictionary */   API_AVAILABLE(macos(10.13)) __IOS_PROHIBITED __TVOS_PROHIBITED __WATCHOS_PROHIBITED;

/*!
	@constant AVVideoEncoderSpecificationKey
	@abstract
		The video encoder specification includes options for choosing a specific video encoder.
		
	@discussion
		The value for this key is a dictionary containing kVTVideoEncoderSpecification_* keys specified in the VideoToolbox framework.  This key should be specified at the top level of an AVVideoSettings dictionary.
 */
AVF_EXPORT NSString *const AVVideoEncoderSpecificationKey /* NSDictionary */ NS_AVAILABLE(10_10, NA);

typedef NSString * AVVideoApertureMode NS_STRING_ENUM;

/*!
 @constant	AVVideoApertureModeCleanAperture
 @abstract	Both pixel aspect ratio and clean aperture will be applied.
 @discussion
	An image's clean aperture is a region of video free from transition artifacts caused by the encoding of the signal.
 */
AVF_EXPORT AVVideoApertureMode const AVVideoApertureModeCleanAperture API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

/*!
 @constant	AVVideoApertureModeProductionAperture
 @abstract	Only pixel aspect ratio will be applied.
 @discussion
	The image is not cropped to the clean aperture region, but it is scaled according to the pixel aspect ratio. Use this option when you want to see all the pixels in your video, including the edges.
 */
AVF_EXPORT AVVideoApertureMode const AVVideoApertureModeProductionAperture API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

/*!
 @constant	AVVideoApertureModeEncodedPixels
 @abstract	Neither pixel aspect ratio nor clean aperture will be applied.
 @discussion
	The image is not cropped to the clean aperture region and is not scaled according to the pixel aspect ratio. The encoded dimensions of the image description are displayed.
 */
AVF_EXPORT AVVideoApertureMode const AVVideoApertureModeEncodedPixels API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;
// ==========  AVFoundation.framework/Headers/AVAudioBuffer.h
/*
	File:           AVAudioBuffer.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioBuffer.h>

// ==========  AVFoundation.framework/Headers/AVError.h
/*
    File:  AVError.h
 
	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

AVF_EXPORT NSErrorDomain const AVFoundationErrorDomain              NS_AVAILABLE(10_7, 4_0);

AVF_EXPORT NSString *const AVErrorDeviceKey                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVErrorTimeKey                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVErrorFileSizeKey                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVErrorPIDKey                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVErrorRecordingSuccessfullyFinishedKey  NS_AVAILABLE(10_7, 4_0); // an NSNumber carrying a BOOL indicating whether the recording is playable
AVF_EXPORT NSString *const AVErrorMediaTypeKey                      NS_AVAILABLE(10_7, 4_3); // an NSString, as defined in AVMediaFormat.h
AVF_EXPORT NSString *const AVErrorMediaSubTypeKey                   NS_AVAILABLE(10_7, 4_3); // an NSArray of NSNumbers carrying four character codes (4ccs) as defined in CoreAudioTypes.h for audio media and in CMFormatDescription.h for video media.
AVF_EXPORT NSString *const AVErrorPresentationTimeStampKey          NS_AVAILABLE(10_10, 8_0); // an NSValue carrying a CMTime
AVF_EXPORT NSString *const AVErrorPersistentTrackIDKey				NS_AVAILABLE(10_10, 8_0); // an NSNumber carrying a CMPersistentTrackID
AVF_EXPORT NSString *const AVErrorFileTypeKey						NS_AVAILABLE(10_10, 8_0); // an NSString, as defined in AVMediaFormat.h

#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))

AVF_EXPORT NSString *const AVErrorDiscontinuityFlagsKey             NS_AVAILABLE(10_7, NA);

#endif // (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))

typedef NS_ERROR_ENUM(AVFoundationErrorDomain, AVError) {
    AVErrorUnknown                                      = -11800,
    AVErrorOutOfMemory                                  = -11801,
    AVErrorSessionNotRunning                            = -11803,
    AVErrorDeviceAlreadyUsedByAnotherSession            = -11804,
    AVErrorNoDataCaptured                               = -11805,
    AVErrorSessionConfigurationChanged                  = -11806,
    AVErrorDiskFull                                     = -11807,
    AVErrorDeviceWasDisconnected                        = -11808,
    AVErrorMediaChanged                                 = -11809,
    AVErrorMaximumDurationReached                       = -11810,
    AVErrorMaximumFileSizeReached                       = -11811,
    AVErrorMediaDiscontinuity                           = -11812,
    AVErrorMaximumNumberOfSamplesForFileFormatReached   = -11813,
    AVErrorDeviceNotConnected                           = -11814,
    AVErrorDeviceInUseByAnotherApplication              = -11815,
    AVErrorDeviceLockedForConfigurationByAnotherProcess = -11817,
#if TARGET_OS_IPHONE
    AVErrorSessionWasInterrupted                        = -11818,
    AVErrorMediaServicesWereReset                       = -11819,
#endif
    AVErrorExportFailed                                 = -11820,
    AVErrorDecodeFailed                                 = -11821,  // userInfo may contain AVErrorMediaTypeKey, AVErrorMediaSubTypeKey & AVErrorPresentationTimeStampKey, if available
    AVErrorInvalidSourceMedia                           = -11822,
    AVErrorFileAlreadyExists                            = -11823,
    AVErrorCompositionTrackSegmentsNotContiguous        = -11824,
    AVErrorInvalidCompositionTrackSegmentDuration       = -11825,
    AVErrorInvalidCompositionTrackSegmentSourceStartTime= -11826,
    AVErrorInvalidCompositionTrackSegmentSourceDuration = -11827,
    AVErrorFileFormatNotRecognized                      = -11828,
    AVErrorFileFailedToParse                            = -11829,
    AVErrorMaximumStillImageCaptureRequestsExceeded     = -11830,
    AVErrorContentIsProtected                           = -11831,
    AVErrorNoImageAtTime                                = -11832,
    AVErrorDecoderNotFound                              = -11833,  // userInfo may contain AVErrorMediaTypeKey & AVErrorMediaSubTypeKey, if available
    AVErrorEncoderNotFound                              = -11834,  // userInfo may contain AVErrorMediaTypeKey & AVErrorMediaSubTypeKey, if available
    AVErrorContentIsNotAuthorized                       = -11835,
    AVErrorApplicationIsNotAuthorized                   = -11836,
#if TARGET_OS_IPHONE
    AVErrorDeviceIsNotAvailableInBackground NS_DEPRECATED_IOS(4_3, 9_0, "AVCaptureSession no longer produces an AVCaptureSessionRuntimeErrorNotification with this error. See AVCaptureSessionInterruptionReasonVideoDeviceNotAvailableInBackground.") = -11837,
#endif
    AVErrorOperationNotSupportedForAsset                = -11838,
    
    AVErrorDecoderTemporarilyUnavailable                = -11839,  // userInfo may contain AVErrorMediaTypeKey & AVErrorMediaSubTypeKey, if available
    AVErrorEncoderTemporarilyUnavailable                = -11840,  // userInfo may contain AVErrorMediaTypeKey & AVErrorMediaSubTypeKey, if available
    AVErrorInvalidVideoComposition                      = -11841,
    AVErrorReferenceForbiddenByReferencePolicy          = -11842,
    AVErrorInvalidOutputURLPathExtension                = -11843,
    AVErrorScreenCaptureFailed                          = -11844,
    AVErrorDisplayWasDisabled                           = -11845,
    AVErrorTorchLevelUnavailable                        = -11846,
#if TARGET_OS_IPHONE
    AVErrorOperationInterrupted                         = -11847,
#endif
    AVErrorIncompatibleAsset                            = -11848,
    AVErrorFailedToLoadMediaData                        = -11849,
    AVErrorServerIncorrectlyConfigured                  = -11850,
	AVErrorApplicationIsNotAuthorizedToUseDevice		= -11852,
    AVErrorFailedToParse	NS_AVAILABLE(10_10, 8_0)		= -11853,
	AVErrorFileTypeDoesNotSupportSampleReferences NS_AVAILABLE(10_10, 8_0)	= -11854,  // userInfo contains AVErrorFileTypeKey
    AVErrorUndecodableMediaData NS_AVAILABLE(10_10, 8_0)  = -11855,
    AVErrorAirPlayControllerRequiresInternet NS_AVAILABLE(10_10, 8_3) = -11856,
    AVErrorAirPlayReceiverRequiresInternet NS_AVAILABLE(10_10, 8_3) = -11857,
    AVErrorVideoCompositorFailed NS_AVAILABLE(10_11, 9_0) = -11858,
#if TARGET_OS_IPHONE
    AVErrorRecordingAlreadyInProgress NS_AVAILABLE_IOS(9_0) = -11859, // on iOS, AVCaptureMovieFileOutput only supports one recording at a time
#endif
#if !TARGET_OS_IPHONE
    AVErrorCreateContentKeyRequestFailed NS_AVAILABLE(10_11, NA) = -11860,
#endif
    AVErrorUnsupportedOutputSettings NS_AVAILABLE(10_12, 10_0) = -11861,
	AVErrorOperationNotAllowed NS_AVAILABLE(10_12, 10_0) = -11862,
	AVErrorContentIsUnavailable NS_AVAILABLE(10_13, 11_0) = -11863,
	AVErrorFormatUnsupported NS_AVAILABLE(10_13, 11_0)  = -11864,
	AVErrorMalformedDepth NS_AVAILABLE(10_13, 11_0)     = -11865,
	AVErrorContentNotUpdated NS_AVAILABLE(10_13, 11_0)	= -11866,
	AVErrorNoLongerPlayable	 NS_AVAILABLE(10_13, 11_0)	= -11867,
	AVErrorNoCompatibleAlternatesForExternalDisplay API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED = -11868,
	AVErrorNoSourceTrack API_AVAILABLE(macos(10.13.2), ios(11.2), tvos(11.2)) __WATCHOS_PROHIBITED = -11869,
	AVErrorExternalPlaybackNotSupportedForAsset API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0)) __WATCHOS_PROHIBITED = -11870,
};
// ==========  AVFoundation.framework/Headers/AVTime.h
/*
	File:  AVTime.h

	Framework:  AVFoundation
 
	Copyright 2010-2015 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

/*
	Utilities for carriage of CoreMedia time structures in NSValues
	
	Notes for archiving NSValues created with CMTime / CMTimeRange / CMTimeMapping.
	We recommend that on macOS Sierra, iOS 10, and later you use NSKeyedArchiver to serialize NSValues created with a CMTime, CMTimeRange, or CMTimeMapping. This will ensure that before and after archiving, use of such values with -isEqual:, with -[NSDictionary objectForKey:], and with other facilities that depend on hashing or equality, will have correct results.
	Should it be necessary to write a value in a way that is readable pre-Sierra or pre-iOS 10, you can re-encode the values using [NSValue valueWithBytes:objCType:] before handing it to the NSArchiver. If you are reading serialized values produced with an NSArchiver, it will still successfully produce NSValues. If in this scenario you require the use of such values with -isEqual:, with -[NSDictionary objectForKey:], and with other facilities that depend on hashing or equality to have correct results, we recommend that you immediately re-create a new NSValue via the NSValue utilities provided here and use the new NSValue in place of the unarchived one.
*/

@interface NSValue (NSValueAVFoundationExtensions)

+ (NSValue *)valueWithCMTime:(CMTime)time NS_AVAILABLE(10_7, 4_0);
@property (readonly) CMTime CMTimeValue NS_AVAILABLE(10_7, 4_0);

+ (NSValue *)valueWithCMTimeRange:(CMTimeRange)timeRange NS_AVAILABLE(10_7, 4_0);
@property (readonly) CMTimeRange CMTimeRangeValue NS_AVAILABLE(10_7, 4_0);

+ (NSValue *)valueWithCMTimeMapping:(CMTimeMapping)timeMapping NS_AVAILABLE(10_7, 4_0);
@property (readonly) CMTimeMapping CMTimeMappingValue NS_AVAILABLE(10_7, 4_0);

@end

// utilities for encoding and decoding CoreMedia time structures for NSCoding

@interface NSCoder (AVTimeCoding)

- (void)encodeCMTime:(CMTime)time forKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);
- (CMTime)decodeCMTimeForKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);

- (void)encodeCMTimeRange:(CMTimeRange)timeRange forKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);
- (CMTimeRange)decodeCMTimeRangeForKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);

- (void)encodeCMTimeMapping:(CMTimeMapping)timeMapping forKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);
- (CMTimeMapping)decodeCMTimeMappingForKey:(NSString *)key NS_AVAILABLE(10_7, 4_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureOutputBase.h
/*
    File:  AVCaptureOutputBase.h
 
    Framework:  AVFoundation
 
    Copyright 2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>
#import <AVFoundation/AVCaptureSession.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureOutput

@class AVMetadataObject;
@class AVCaptureOutputInternal;

/*!
 @class AVCaptureOutput
 @abstract
    AVCaptureOutput is an abstract class that defines an interface for an output destination of an AVCaptureSession.
 
 @discussion
    AVCaptureOutput provides an abstract interface for connecting capture output destinations, such as files and video previews, to an AVCaptureSession.

    An AVCaptureOutput can have multiple connections represented by AVCaptureConnection objects, one for each stream of media that it receives from an AVCaptureInput. An AVCaptureOutput does not have any connections when it is first created. When an output is added to an AVCaptureSession, connections are created that map media data from that session's inputs to its outputs.

    Concrete AVCaptureOutput instances can be added to an AVCaptureSession using the -[AVCaptureSession addOutput:] and -[AVCaptureSession addOutputWithNoConnections:] methods.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureOutput : NSObject
{
@private
    AVCaptureOutputInternal *_outputInternal;
}

AV_INIT_UNAVAILABLE

/*!
 @property connections
 @abstract
    The connections that describe the flow of media data to the receiver from AVCaptureInputs.

 @discussion
    The value of this property is an NSArray of AVCaptureConnection objects, each describing the mapping between the receiver and the AVCaptureInputPorts of one or more AVCaptureInputs.
 */
@property(nonatomic, readonly) NSArray<AVCaptureConnection *> *connections;

/*!
 @method connectionWithMediaType:
 @abstract
    Returns the first connection in the connections array with an inputPort of the specified mediaType.

 @param mediaType
    An AVMediaType constant from AVMediaFormat.h, e.g. AVMediaTypeVideo.

 @discussion
    This convenience method returns the first AVCaptureConnection in the receiver's connections array that has an AVCaptureInputPort of the specified mediaType. If no connection with the specified mediaType is found, nil is returned.
 */
- (nullable AVCaptureConnection *)connectionWithMediaType:(AVMediaType)mediaType API_AVAILABLE(ios(5.0));

/*!
 @method transformedMetadataObjectForMetadataObject:connection:
 @abstract
    Converts an AVMetadataObject's visual properties to the receiver's coordinates.

 @param metadataObject
    An AVMetadataObject originating from the same AVCaptureInput as the receiver.
 @param connection
    The receiver's connection whose AVCaptureInput matches that of the metadata object to be converted.
 @result
    An AVMetadataObject whose properties are in output coordinates.

 @discussion
    AVMetadataObject bounds may be expressed as a rect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. Face metadata objects likewise express yaw and roll angles with respect to an unrotated picture. -transformedMetadataObjectForMetadataObject:connection: converts the visual properties in the coordinate space of the supplied AVMetadataObject to the coordinate space of the receiver. The conversion takes orientation, mirroring, and scaling into consideration. If the provided metadata object originates from an input source other than the preview layer's, nil will be returned.
 
    If an AVCaptureVideoDataOutput instance's connection's videoOrientation or videoMirrored properties are set to non-default values, the output applies the desired mirroring and orientation by physically rotating and or flipping sample buffers as they pass through it. AVCaptureStillImageOutput, on the other hand, does not physically rotate its buffers. It attaches an appropriate kCGImagePropertyOrientation number to captured still image buffers (see ImageIO/CGImageProperties.h) indicating how the image should be displayed on playback. Likewise, AVCaptureMovieFileOutput does not physically apply orientation/mirroring to its sample buffers -- it uses a QuickTime track matrix to indicate how the buffers should be rotated and/or flipped on playback.
 
    transformedMetadataObjectForMetadataObject:connection: alters the visual properties of the provided metadata object to match the physical rotation / mirroring of the sample buffers provided by the receiver through the indicated connection. I.e., for video data output, adjusted metadata object coordinates are rotated/mirrored. For still image and movie file output, they are not.
 */
- (nullable AVMetadataObject *)transformedMetadataObjectForMetadataObject:(AVMetadataObject *)metadataObject connection:(AVCaptureConnection *)connection API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @method metadataOutputRectOfInterestForRect:
 @abstract
    Converts a rectangle in the receiver's coordinate space to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the receiver.
 
 @param rectInOutputCoordinates
    A CGRect in the receiver's coordinates.
 @result
    A CGRect in the coordinate space of the metadata output whose capture device is providing input to the receiver.
 
 @discussion
    AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of the receiver to a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the receiver. The conversion takes orientation, mirroring, and scaling into consideration. See -transformedMetadataObjectForMetadataObject:connection: for a full discussion of how orientation and mirroring are applied to sample buffers passing through the output.
 */
- (CGRect)metadataOutputRectOfInterestForRect:(CGRect)rectInOutputCoordinates API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method rectForMetadataOutputRectOfInterest:
 @abstract
    Converts a rectangle of interest in the coordinate space of an AVCaptureMetadataOutput whose capture device is providing input to the receiver to a rectangle in the receiver's coordinates.
 
 @param rectInMetadataOutputCoordinates
    A CGRect in the coordinate space of the metadata output whose capture device is providing input to the receiver.
 @result
    A CGRect in the receiver's coordinates.
 
 @discussion
    AVCaptureMetadataOutput rectOfInterest is expressed as a CGRect where {0,0} represents the top left of the picture area, and {1,1} represents the bottom right on an unrotated picture. This convenience method converts a rectangle in the coordinate space of an AVCaptureMetadataOutput whose AVCaptureDevice is providing input to the coordinate space of the receiver. The conversion takes orientation, mirroring, and scaling into consideration. See -transformedMetadataObjectForMetadataObject:connection: for a full discussion of how orientation and mirroring are applied to sample buffers passing through the output.
 */
- (CGRect)rectForMetadataOutputRectOfInterest:(CGRect)rectInMetadataOutputCoordinates API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVCaptureOutputDataDroppedReason
 @abstract
    Constants indicating the reason a capture data output dropped data.

 @constant AVCaptureOutputDataDroppedReasonNone
    No data was dropped.
 @constant AVCaptureOutputDataDroppedReasonLateData
    Data was dropped because alwaysDiscardsLate{VideoFrames | DepthData} is YES and the client was still processing previous data when the current data needed to be delivered.
 @constant AVCaptureOutputDataDroppedReasonOutOfBuffers
    Data was dropped because its pool of buffers ran dry. This is usually indicative that the client is holding onto data objects too long.
 @constant AVCaptureOutputDataDroppedReasonDiscontinuity
    Data was dropped because the device providing the data experienced a discontinuity, and an unknown number of data objects have been lost. This condition is typically caused by the system being too busy.
 */
typedef NS_ENUM(NSInteger, AVCaptureOutputDataDroppedReason) {
    AVCaptureOutputDataDroppedReasonNone          = 0,
    AVCaptureOutputDataDroppedReasonLateData      = 1,
    AVCaptureOutputDataDroppedReasonOutOfBuffers  = 2,
    AVCaptureOutputDataDroppedReasonDiscontinuity = 3,
} API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVPlayerLooper.h
/*
 File:  AVPlayerLooper.h

	Framework:  AVFoundation

	Copyright 2016 Apple Inc. All rights reserved.

 */

/*!
 @class AVPlayerLooper

 @abstract
    AVPlayerLooper is a helper object that repeatedly plays an AVPlayerItem with an AVQueuePlayer

 @discussion
    The same result can be accomplished with AVQueuePlayer directly, but AVPlayerLooper provides a simpler interface to loop a single AVPlayerItem with an option to specify a time range. AVPlayerLooper only supports looping for forward playback (positive player rate). Behavior is undefined for negative player rate.

    Sample usage code:
    // Create player and configure
    AVQueuePlayer *player = [[AVQueuePlayer alloc] init];
    AVPlayerLayer *playerLayer = [AVPlayerLayer playerLayerWithPlayer:player];
    [player pause];
    // Create looping item
    AVPlayerItem* itemToLoop = [AVPlayerItem playerItemWithURL:@“loop.mov”];
    // Create looping helper object. Loop item segment from 5sec to 7sec
    AVPlayerLooper* looper = [AVPlayerLooper playerLooperWithPlayer:player templateItem:itemToLoop timeRange:CMTimeRangeMake(CMTimeMake(5000,1000), CMTimeMake(2000,1000))];
    // Perform any other set up operations like setting AVPlayerItemDataOutputs on the looping item replicas
    // Start playback
    [player play];
    // itemToLoop between 5s and 7s plays repeatedly
    ....
    // To end the looping
    [looper disableLooping];
    // Player will play through the end of the current looping item
 */


#import <AVFoundation/AVBase.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <Foundation/Foundation.h>
#import <AVFoundation/AVPlayer.h>
#import <AVFoundation/AVPlayerItem.h>

@class AVPlayerLooperInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @enum AVPlayerLooperStatus
 @abstract
    These constants are returned by the AVPlayerLooper status property to indicate whether it can successfully accomplish looping playback.
 @constant AVPlayerLooperStatusUnknown
    Indicates that the status of the looper is not yet known.
 @constant AVPlayerLooperStatusReady
    Indicates that the looper is ready for looping playback.
 @constant AVPlayerLooperStatusFailed
    Indicates that the looper is not able to perform looping playback because of an error. The error is described by the value of the error property.
 @constant AVPlayerLooperStatusCancelled
    Indicates that the looper is no longer looping because -disableLooping was invoked.
 */
typedef NS_ENUM(NSInteger, AVPlayerLooperStatus) {
    AVPlayerLooperStatusUnknown,
    AVPlayerLooperStatusReady,
    AVPlayerLooperStatusFailed,
    AVPlayerLooperStatusCancelled
};

NS_CLASS_AVAILABLE(10_12, 10_0)
@interface AVPlayerLooper : NSObject
{
@private
    AVPlayerLooperInternal *_looper;
}
AV_INIT_UNAVAILABLE

/*!
 @method playerLooperWithPlayer:templateItem:timeRange:
 @abstract
    Returns an instance of AVPlayerLooper to loop specified AVPlayerItem within the specified time range with specified AVQueuePlayer.
 @param player
    Must not be nil
 @param itemToLoop
    Must not be nil
 @param loopRange
    Playback time range in [0, itemToLoop's duration]. kCMTimeRangeInvalid means [0, itemToLoop's duration].
 @result
    An instance of AVPlayerLooper.
 @discussion
    The specified AVPlayerItem will be used as a template to generate at least 3 AVPlayerItem replicas and the replicas will be inserted into specified AVQueuePlayer's play queue to accomplish the looping playback. The specified AVPlayerItem should have its asset's duration property loaded beforehand so looping setup work would not be blocked until the duration value is known. Otherwise, AVPlayerLooper's status property is  AVPlayerLooperStatusUnknown until the duration property is loaded. The specified AVPlayerItem will not be used in the actual looping playback. Furthermore, AVPlayerItem replicas will be generated at initialization time so any changes made to the specified AVPlayerItem's property afterwards will not be reflected in the replicas used for looping playback. Specified CMTimeRange will limit each item loop iteration to playing within the specified time range. To play from beginning and the whole duration of the item, specify kCMTimeRangeInvalid for the range parameter. Time range will be accomplished by seeking to range start time and setting AVPlayerItem's forwardPlaybackEndTime property on the looping item replicas. Client should not modify AVQueuePlayer's play queue while AVPlayerLooper is performing the looping. AVPlayerLooper will insert the replica items before any existing items in the specified AVQueuePlayer's play queue and change the actionAtItemEnd to AVPlayerActionAtItemEndAdvance if required. AVQueuePlayer's play queue and actionAtItemEnd will be restored when -disableLooping method is called and then current looping item replicas completes playback or when AVPlayerLooper is destroyed. While AVPlayerLooper is being initialized, the specified AVQueuePlayer will be paused (rate of 0.0) if necessary and the original player rate will be restored after initialization completes. The client shall set the specified AVQueuePlayer's rate to 0 beforehand if additional set-up work needs to be performed after AVPlayerLooper initialization and before starting looping playback. An NSInvalidArgumentException will be raised if the player and template item are not specified or the template item has a 0 duration. An NSInvalidArgumentException will be raised if a valid time range has a duration of 0 or is not contained within time 0 and duration of the templateItem.
*/
+ (instancetype)playerLooperWithPlayer:(AVQueuePlayer *)player templateItem:(AVPlayerItem *)itemToLoop timeRange:(CMTimeRange)loopRange;

/*!
 @method playerLooperWithPlayer:templateItem:
 @abstract
    Returns an instance of AVPlayerLooper to loop specified AVPlayerItem with specified AVQueuePlayer.
 @param player
    Must not be nil
 @param itemToLoop
    Must not be nil
 @result
    An instance of AVPlayerLooper.
 @discussion
    Equivalent to +playerLooperWithPlayer:templateItem:timeRange: and passing in kCMTimeRangeInvalid for timeRange parameter.
*/
+ (instancetype)playerLooperWithPlayer:(AVQueuePlayer *)player templateItem:(AVPlayerItem *)itemToLoop;

/*!
 @method initWithPlayer:templateItem:timeRange:
 @abstract
    Initializes an instance of AVPlayerLooper to loop specified AVPlayerItem within the specified time range with specified AVQueuePlayer.
 @param player
    Must not be nil
 @param itemToLoop
    Must not be nil
 @param loopRange
    Playback time range in [0, itemToLoop's duration]. kCMTimeRangeInvalid means [0, itemToLoop's duration].
 @result
    An initialized AVPlayerLooper.
 @discussion
    The specified AVPlayerItem will be used as a template to generate at least 3 AVPlayerItem replicas and the replicas will be inserted into specified AVQueuePlayer's play queue to accomplish the looping playback. The specified AVPlayerItem should have its asset's duration property loaded beforehand so looping setup work would not be blocked until the duration value is known. Otherwise, AVPlayerLooper's status property is  AVPlayerLooperStatusUnknown until the duration property is loaded. The specified AVPlayerItem will not be used in the actual looping playback. Furthermore, AVPlayerItem replicas will be generated at initialization time so any changes made to the specified AVPlayerItem's property afterwards will not be reflected in the replicas used for looping playback. Specified CMTimeRange will limit each item loop iteration to playing within the specified time range. To play from beginning and the whole duration of the item, specify kCMTimeRangeInvalid for the range parameter. Time range will be accomplished by seeking to range start time and setting AVPlayerItem's forwardPlaybackEndTime property on the looping item replicas. Client should not modify AVQueuePlayer's play queue while AVPlayerLooper is performing the looping. AVPlayerLooper will insert the replica items before any existing items in the specified AVQueuePlayer's play queue and change the actionAtItemEnd to AVPlayerActionAtItemEndAdvance if required. AVQueuePlayer's play queue and actionAtItemEnd will be restored when -disableLooping method is called and then current looping item replicas completes playback or when AVPlayerLooper is destroyed. While AVPlayerLooper is being initialized, the specified AVQueuePlayer will be paused (rate of 0.0) if necessary and the original player rate will be restored after initialization completes. The client shall set the specified AVQueuePlayer's rate to 0 beforehand if additional set-up work needs to be performed after AVPlayerLooper initialization and before starting looping playback. An NSInvalidArgumentException will be raised if the player and template item are not specified or the template item has a 0 duration. An NSInvalidArgumentException will be raised if a valid time range has a duration of 0 or is not contained within time 0 and duration of the templateItem.
 */
- (instancetype)initWithPlayer:(AVQueuePlayer *)player templateItem:(AVPlayerItem *)itemToLoop timeRange:(CMTimeRange)loopRange NS_DESIGNATED_INITIALIZER;

/*!
 @property status
 @abstract
    The ability of the receiver to be used for looping playback.
 @discussion
    The value of this property is an AVPlayerLooperStatus that indicates whether the receiver is ready for looping playback. When the value of this property is AVPlayerStatusFailed, the receiver can no longer be used for playback and a new instance needs to be created in its place. When this happens, clients can check the value of the error property to determine the nature of the failure. This property is key value observable.
 */
@property (readonly) AVPlayerLooperStatus status;

/*!
 @property error
 @abstract
    If the receiver's status is AVPlayerLooperStatusFailed, this describes the error that caused the failure.
 @discussion
    The value of this property is a NSError that describes what caused the receiver to not be able to perform looping playback. If the receiver's status is not AVPlayerLooperStatusFailed, the value of this property is nil.
 */
@property (readonly, nullable) NSError *error;

/*!
 @method disableLooping
 @abstract
    Disables the item looping
 @discussion
    AVPlayerLooper will stop performing player queue operations for looping and let the current looping item replica play to the end. The player's original actionAtItemEnd property will be restored afterwards. After this method is called, the value of the receiver's status property will be AVPlayerLooperStatusCancelled.
 */
- (void)disableLooping;

/*!
 @property loopCount
 @abstract
    Number of times the specified AVPlayerItem has been played
 @discussion
    Starts at 0 and increments when the player starts playback of the AVPlayerItem again. This property is key value observable.
 */
@property (readonly) NSInteger loopCount;

/*!
 @property loopingPlayerItems
 @abstract
    Returns an array containing replicas of specified AVPlayerItem used to accomplish the looping
 @discussion
    AVPlayerLooper creates replicas of the template AVPlayerItem using -copyWithZone: and inserts the replicas in the specified AVQueuePlayer to accomplish the looping. The AVPlayerItem replicas are for informational purposes and to allow the client to apply properties that are not transferred from the template AVPlayerItem to the replicas. The client can determine the number of replicas created and can listen for notifications and property changes from the replicas if desired. AVPlayerItemOutputs and AVPlayerItemMediaDataCollectors are not transferred to the replicas so the client should add them to each replica if desired. The client shall not modify the properties on the replicas that would disrupt looping playback. Examples of such properties are playhead time/date, selected media option, and forward playback end time. This property is key value observable.
 @result
    Array containing replicas of specified AVPlayerItem
 */
@property (nonatomic, readonly) NSArray<AVPlayerItem *> *loopingPlayerItems;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Headers/AVCameraCalibrationData.h
/*
    File:  AVCameraCalibrationData.h
 
    Framework:  AVFoundation
 
    Copyright 2016-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <simd/matrix_types.h>
#import <CoreGraphics/CGGeometry.h>

NS_ASSUME_NONNULL_BEGIN

@class AVCameraCalibrationDataInternal;

/*!
 @class AVCameraCalibrationData
 @abstract
    AVCameraCalibrationData is a model object describing a camera's calibration information.
 
 @discussion
    When rendering effects to images produced by cameras, or performing computer vision tasks such as correcting images for geometric distortions, it is necessary to characterize the camera's calibration information, such as its pixel focal length, principal point, lens distortion characteristics, etc. AVCameraCalibrationData provides this information.
 */
API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED
@interface AVCameraCalibrationData : NSObject
{
@private
    AVCameraCalibrationDataInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property intrinsicMatrix
 @abstract
    A camera's intrinsic (K) matrix describes its geometric properties.
 
 @discussion
    The intrinsic matrix allows one to transform 3D coordinates to 2D coordinates on an image plane using the pinhole camera model. All values are expressed in pixels. The elements in the matrix are:
       /           \
       | fx 0   ox |
       | 0  fy  oy |
       | 0  0   1  |
       \           /
    where fx and fy describe the focal length. For square pixels, their values are identical.
    ox and oy are the offset of the principal point. The origin is the upper left of the frame.
 */
@property(nonatomic, readonly) matrix_float3x3 intrinsicMatrix;

/*!
 @property intrinsicMatrixReferenceDimensions
 @abstract
    The reference frame dimensions used in calculating a camera's principal point.
 
 @discussion
    A camera's intrinsic matrix expresses values in pixels with respect to a frame of this width and height.
 */
@property(nonatomic, readonly) CGSize intrinsicMatrixReferenceDimensions;

/*!
 @property extrinsicMatrix
 @abstract
    A camera's extrinsic matrix describes its pose (position and direction) in world coordinates.
 
 @discussion
    The extrinsic matrix consists of a unitless 3x3 rotation matrix (R) on the left and a translation (t) 3x1 column vector on the right. The translation vector's units are millimeters. The camera's pose is expressed with respect to a reference camera (camera-to-world view). If the rotation matrix is an identity matrix, then this camera is the reference camera. Note that a matrix_float4x3 matrix is column major with 3 rows and 4 columns.
               /                       \
       /   \   | r1,1  r1,2  r1,3 | t1 |
       |R|t| = | r2,1  r2,2  r2,3 | t2 |
       \   /   | r3,1  r3,2  r3,3 | t3 |
               \                       /
 */
@property(nonatomic, readonly) matrix_float4x3 extrinsicMatrix;

/*!
 @property pixelSize
 @abstract
    The size of one pixel in millimeters
 */
@property(nonatomic, readonly) float pixelSize;

/*!
 @property lensDistortionLookupTable
 @abstract
    An NSData of floats describing the camera lens' radial distortions.
 
 @discussion
    Images captured by a camera are geometrically warped by radial distortions in the lens. In order to project from the 2D image plane back into the 3D world, the images must be distortion corrected, or made rectilinear. Lens distortion is modeled using a one-dimensional lookup table of 32-bit float values evenly distributed along a radius from the center of the distortion to the farthest corner, with each value representing an elongation or compression of the radius (0.0 for any given point indicates no elongation). This model assumes radially symmetric lens distortion. When dealing with AVDepthData, the disparity / depth map representations are geometrically distorted to align with images produced by the camera. For more information, see the reference implementation below.
 
    If the camera lacks the calibration data needed to accurately characterize lens distortions, this property's value is nil.
 */
@property(nullable, nonatomic, readonly) NSData *lensDistortionLookupTable;

/*!
 @property inverseLensDistortionLookupTable
 @abstract
    An NSData of floats describing the inverse lookup table required to reapply the camera lens' radial distortions to a rectified image.
 
 @discussion
    See lensDistortionLookupTable. If you've rectified an image by removing the distortions characterized by the lensDistortionLookupTable, and now wish to go back to geometrically distorted, you may use the inverseLensDistortionLookupTable. For more information, see the reference implementation below.
 
    If the camera lacks the calibration data needed to accurately characterize lens distortions, this property's value is nil.
 */
@property(nullable, nonatomic, readonly) NSData *inverseLensDistortionLookupTable;

/*!
 @property lensDistortionCenter
 @abstract
    A CGPoint describing the offset of the lens' distortion center from the top left.
 
 @discussion
    Due to geometric distortions in the image, the center of the distortion may not be equal to the optical center (principal point) of the lens. When making an image rectilinear, the distortion center should be used rather than the optical center of the image. For more information, see the reference implementation below.
 
    If the camera lacks the calibration data needed to accurately characterize lens distortions, this property's value is set to CGPointZero and should not be used.
 */
@property(nonatomic, readonly) CGPoint lensDistortionCenter;

/*
    The following reference implementation illustrates how to use the lensDistortionLookupTable, inverseLensDistortionLookupTable, and lensDistortionCenter properties to find points in the lens-distorted or undistorted (rectilinear, corrected) space. If you have a distorted image (such as a photo taken by a camera) and want to find a particular point in a corresponding undistorted image, you would call the sample method below using the inverseLensDistortionLookupTable. If you have an undistorted (aka distortion-corrected) image and want to find a point in the distorted image's space, you would call the sample method below using the lensDistortionLookupTable.
 
    To apply distortion correction to an image, you'd begin with an empty destination buffer and iterate through it row by row, calling the sample implementation below for each point in the output image, passing the lensDistortionLookupTable to find the corresponding value in the distorted image, and write it to your output buffer. Please note that the "point", "opticalCenter", and "imageSize" parameters below must be in the same coordinate system, i.e. both at full resolution, or both scaled to a different resolution but with the same aspect ratio.
 
    The reference function below returns floating-point x and y values. If you wish to match the results with actual pixels in a bitmap, you should either round to the nearest integer value or interpolate from surrounding integer positions (i.e. bilinear interpolation from the 4 surrounding pixels).
 
- (CGPoint)lensDistortionPointForPoint:(CGPoint)point
                           lookupTable:(NSData *)lookupTable
               distortionOpticalCenter:(CGPoint)opticalCenter
                             imageSize:(CGSize)imageSize
{
    // The lookup table holds the relative radial magnification for n linearly spaced radii.
    // The first position corresponds to radius = 0
    // The last position corresponds to the largest radius found in the image.
 
    // Determine the maximum radius.
    float delta_ocx_max = MAX( opticalCenter.x, imageSize.width  - opticalCenter.x );
    float delta_ocy_max = MAX( opticalCenter.y, imageSize.height - opticalCenter.y );
    float r_max = sqrtf( delta_ocx_max * delta_ocx_max + delta_ocy_max * delta_ocy_max );
 
    // Determine the vector from the optical center to the given point.
    float v_point_x = point.x - opticalCenter.x;
    float v_point_y = point.y - opticalCenter.y;
 
    // Determine the radius of the given point.
    float r_point = sqrtf( v_point_x * v_point_x + v_point_y * v_point_y );
 
    // Look up the relative radial magnification to apply in the provided lookup table
    float magnification;
    const float *lookupTableValues = lookupTable.bytes;
    NSUInteger lookupTableCount = lookupTable.length / sizeof(float);
 
    if ( r_point < r_max ) {
        // Linear interpolation
        float val   = r_point * ( lookupTableCount - 1 ) / r_max;
        int   idx   = (int)val;
        float frac  = val - idx;
 
        float mag_1 = lookupTableValues[idx];
        float mag_2 = lookupTableValues[idx + 1];
 
        magnification = ( 1.0f - frac ) * mag_1 + frac * mag_2;
    }
    else {
        magnification = lookupTableValues[lookupTableCount - 1];
    }
 
    // Apply radial magnification
    float new_v_point_x = v_point_x + magnification * v_point_x;
    float new_v_point_y = v_point_y + magnification * v_point_y;
 
    // Construct output
    return CGPointMake( opticalCenter.x + new_v_point_x, opticalCenter.y + new_v_point_y );
}
 */

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVPlayerLayer.h
/*
	File:  AVPlayerLayer.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/


/*!
    @class			AVPlayerLayer

    @abstract		AVPlayerLayer is a subclass of CALayer to which an AVPlayer can direct its visual output.

	@discussion		To create an AVPlayerLayer instance:
					
					AVPlayer *player = ...;
					// ... set up an AVPlayer
					
					CALayer *superlayer = ...;
					AVPlayerLayer *playerLayer = [AVPlayerLayer playerLayerWithPlayer:player];
 
					// ... set up the AVPlayerLayer's geometry. For example: set the AVPlayerLayer frame according to the presentationSize of the AVPlayer's currentItem.
					
					[superlayer addSublayer:playerLayer];
					
					AVPlayerLayer provides a property 'videoGravity' that defines how the video content is displayed within the AVPlayerLayer property 'bounds' rect. 
					The value for the @"contents" key of an AVPlayerLayer is opaque and effectively read-only.

					Note that during playback AVPlayer may compensate for temporal drift between its visual output
					and its audible output to one or more independently-clocked audio output devices by adjusting the timing of its
					associated AVPlayerLayers. The effects of these adjustments are usually very minute; however, clients that
					wish to remain entirely unaffected by such adjustments may wish to place other layers for which timing is
					important into indepedently timed subtrees of their layer trees.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAnimation.h>
#import <QuartzCore/CoreAnimation.h>

@class AVPlayer;
@class AVPlayerLayerInternal;

NS_ASSUME_NONNULL_BEGIN

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVPlayerLayer : CALayer
{
@private
	AVPlayerLayerInternal		*_playerLayer;
}

/*!
	@method		layerWithPlayer:
	@abstract		Returns an instance of AVPlayerLayer to display the visual output of the specified AVPlayer.
	@result		An instance of AVPlayerLayer.
*/
+ (AVPlayerLayer *)playerLayerWithPlayer:(nullable AVPlayer *)player;

/*! 
	@property		player
	@abstract		Indicates the instance of AVPlayer for which the AVPlayerLayer displays visual output
*/
@property (nonatomic, retain, nullable) AVPlayer *player;

/*!
	@property		videoGravity
	@abstract		A string defining how the video is displayed within an AVPlayerLayer bounds rect.
	@discusssion	Options are AVLayerVideoGravityResizeAspect, AVLayerVideoGravityResizeAspectFill 
 					and AVLayerVideoGravityResize. AVLayerVideoGravityResizeAspect is default. 
					See <AVFoundation/AVAnimation.h> for a description of these options.
 */
@property(copy) AVLayerVideoGravity videoGravity;

/*!
	 @property		readyForDisplay
	 @abstract		Boolean indicating that the first video frame has been made ready for display for the current item of the associated AVPlayer.
	 @discusssion	Use this property as an indicator of when best to show or animate-in an AVPlayerLayer into view. 
					An AVPlayerLayer may be displayed, or made visible, while this propoerty is NO, however the layer will not have any user-visible content until the value becomes YES. Note that if an animation is added to an AVPlayerLayer before it becomes readyForDisplay the video image displayed inside might not animate with the receiver.
					This property remains NO for an AVPlayer currentItem whose AVAsset contains no enabled video tracks.
 */
@property(nonatomic, readonly, getter=isReadyForDisplay) BOOL readyForDisplay;

/*!
	@property		videoRect
	@abstract		The current size and position of the video image as displayed within the receiver's bounds.
 */
@property (nonatomic, readonly) CGRect videoRect NS_AVAILABLE(10_9, 7_0);

/*!
	@property		pixelBufferAttributes
	@abstract		The client requirements for the visual output displayed in AVPlayerLayer during playback.  	
	@discussion		Pixel buffer attribute keys are defined in <CoreVideo/CVPixelBuffer.h>
 */
@property (nonatomic, copy, nullable) NSDictionary<NSString *, id> *pixelBufferAttributes NS_AVAILABLE(10_11, 9_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVSampleBufferAudioRenderer.h
/*
	File:  AVSampleBufferAudioRenderer.h

	Framework:  AVFoundation
 
	Copyright 2016-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVQueuedSampleBufferRendering.h>
#import <AVFoundation/AVAudioProcessingSettings.h>

NS_ASSUME_NONNULL_BEGIN

@class AVSampleBufferAudioRendererInternal;

/*
	@class			AVSampleBufferAudioRenderer
	@abstract		AVSampleBufferAudioRenderer can decompress and play compressed or uncompressed audio.
	@discussion
		An instance of AVSampleBufferAudioRenderer must be added to an AVSampleBufferRenderSynchronizer before the first sample buffer is enqueued.
*/
API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED
@interface AVSampleBufferAudioRenderer : NSObject <AVQueuedSampleBufferRendering>
{
@private
	AVSampleBufferAudioRendererInternal *_audioRendererInternal;
}

/*
	@property		status
	@abstract		Indicates the status of the audio renderer.
	@discussion
		A renderer begins with status AVQueuedSampleBufferRenderingStatusUnknown.

		As sample buffers are enqueued for rendering using -enqueueSampleBuffer:, the renderer will transition to either AVQueuedSampleBufferRenderingStatusRendering or AVQueuedSampleBufferRenderingStatusFailed.
					
		If the status is AVQueuedSampleBufferRenderingStatusFailed, check the value of the renderer's error property for information on the error encountered.  This is terminal status from which recovery is not always possible.
		
		This property is key value observable.
*/
@property (nonatomic, readonly) AVQueuedSampleBufferRenderingStatus status;

/*
	@property		error
	@abstract		If the renderer's status is AVQueuedSampleBufferRenderingStatusFailed, this describes the error that caused the failure.
	@discussion
		The value of this property is an NSError that describes what caused the renderer to no longer be able to render sample buffers.  The value of this property is nil unless the value of status is AVQueuedSampleBufferRenderingStatusFailed.
*/
@property (nonatomic, readonly, nullable) NSError *error;

/*!
	@property		audioOutputDeviceUniqueID
	@abstract		Specifies the unique ID of the Core Audio output device used to play audio.
	@discussion
		By default, the value of this property is nil, indicating that the default audio output device is used. Otherwise the value of this property is an NSString containing the unique ID of the Core Audio output device to be used for audio output.

		Core Audio's kAudioDevicePropertyDeviceUID is a suitable source of audio output device unique IDs.
	
		Modifying this property while the timebase's rate is not 0.0 may cause the rate to briefly change to 0.0.
					
		On macOS, the audio device clock may be used as the AVSampleBufferRenderSynchronizer's and all attached AVQueuedSampleBufferRendering's timebase's clocks.  If the audioOutputDeviceUniqueID is modified, the clocks of all these timebases may also change.
 
		If multiple AVSampleBufferAudioRenderers with different values for audioOutputDeviceUniqueID are attached to the same AVSampleBufferRenderSynchronizer, audio may not stay in sync during playback.  To avoid this, ensure that all synchronized AVSampleBufferAudioRenderers are using the same audio output device.
*/
@property (nonatomic, copy, nullable) NSString *audioOutputDeviceUniqueID API_AVAILABLE(macos(10.13)) API_UNAVAILABLE(ios, tvos, watchos);

/*!
	@property		audioTimePitchAlgorithm
	@abstract		Indicates the processing algorithm used to manage audio pitch at varying rates.
	@discussion
		Constants for various time pitch algorithms, e.g. AVAudioTimePitchSpectral, are defined in AVAudioProcessingSettings.h.
	
		The default value on iOS is AVAudioTimePitchAlgorithmLowQualityZeroLatency and on macOS is AVAudioTimePitchAlgorithmTimeDomain.

		If the timebase's rate is not supported by the audioTimePitchAlgorithm, audio will be muted.

		Modifying this property while the timebase's rate is not 0.0 may cause the rate to briefly change to 0.0.
*/
@property (nonatomic, copy) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm;

@end

@interface AVSampleBufferAudioRenderer (AVSampleBufferAudioRendererVolumeControl)

/* 
	@property		volume
	@abstract		Indicates the current audio volume of the AVSampleBufferAudioRenderer.
	@discussion
		A value of 0.0 means "silence all audio", while 1.0 means "play at the full volume of the audio media".
	
		This property should be used for frequent volume changes, for example via a volume knob or fader.
		
		This property is most useful on iOS to control the volume of the AVSampleBufferAudioRenderer relative to other audio output, not for setting absolute volume.
*/
@property (nonatomic) float volume;

/*
	@property		muted
	@abstract		Indicates whether or not audio output of the AVSampleBufferAudioRenderer is muted.
	@discussion
		Setting this property only affects audio muting for the renderer instance and not for the device.
 */
@property (nonatomic, getter=isMuted) BOOL muted;

@end

@interface AVSampleBufferAudioRenderer (AVSampleBufferAudioRendererQueueManagement)

/*!
	@method			flushFromSourceTime:completionHandler:
	@abstract		Flushes enqueued sample buffers with presentation time stamps later than or equal to the specified time.
	@param			completionHandler
		A block that is invoked, possibly asynchronously, after the flush operation completes or fails.
	@discussion
		This method can be used to replace media data scheduled to be rendered in the future, without interrupting playback.  One example of this is when the data that has already been enqueued is from a sequence of two songs and the second song is swapped for a new song.  In this case, this method would be called with the time stamp of the first sample buffer from the second song.  After the completion handler is executed with a YES parameter, media data may again be enqueued with timestamps at the specified time.
 
		If NO is provided to the completion handler, the flush did not succeed and the set of enqueued sample buffers remains unchanged.  A flush can fail becuse the source time was too close to (or earlier than) the current time or because the current configuration of the receiver does not support flushing at a particular time.  In these cases, the caller can choose to flush all enqueued media data by invoking the -flush method.
 */
- (void)flushFromSourceTime:(CMTime)time completionHandler:(void (^)(BOOL flushSucceeded))completionHandler;

/*!
	@constant		AVSampleBufferAudioRendererWasFlushedAutomaticallyNotification
	@abstract		A notification that fires whenever the receiver's enqueued media data has been flushed for a reason other than a call to the -flush method.
	@discussion
		The renderer may flush enqueued media data when the user routes playback to a new destination.  The renderer may also flush enqueued media data when the playback rate of the attached AVSampleBufferRenderSynchronizer is changed (e.g. 1.0 -> 2.0 or 1.0 -> 0.0 -> 2.0), however no flush will occur for normal pauses (non-zero -> 0.0) and resumes (0.0 -> same non-zero rate as before).
 
		When an automatic flush occurs, the attached render synchronizer's timebase will remain running at its current rate.  It is typically best to respond to this notification by enqueueing media data with timestamps starting at the timebase's current time.  To the listener, this will sound similar to muting the audio for a short period of time.  If it is more desirable to ensure that all audio is played than to keep the timeline moving, you may also stop the synchronizer, set the synchronizer's current time to the value of AVSampleBufferAudioRendererFlushTimeKey, start reenqueueing sample buffers with timestamps starting at that time, and restart the synchronizer.  To the listener, this will sound similar to pausing the audio for a short period of time.
 
		This notification is delivered on an arbitrary thread.  If sample buffers are being enqueued with the renderer concurrently with the receipt of this notification, it is possible that one or more sample buffers will remain enqueued in the renderer.  This is generally undesirable, because the sample buffers that remain will likely have timestamps far ahead of the timebase's current time and so won't be rendered for some time.  The best practice is to invoke the -flush method, in a manner that is serialized with enqueueing sample buffers, after receiving this notification and before resuming the enqueueing of sample buffers.
 */
AVF_EXPORT NSNotificationName const AVSampleBufferAudioRendererWasFlushedAutomaticallyNotification API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

	/*!
		@constant		AVSampleBufferAudioRendererFlushTimeKey
		@abstract		The presentation timestamp of the first enqueued sample that was flushed.
		@discussion
			The value of this key is an NSValue wrapping a CMTime.
	 */
	AVF_EXPORT NSString * const AVSampleBufferAudioRendererFlushTimeKey API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioIONode.h
/*
	File:           AVAudioIONode.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioIONode.h>

// ==========  AVFoundation.framework/Headers/AVAudioPlayerNode.h
/*
	File:           AVAudioPlayerNode.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioPlayerNode.h>

// ==========  AVFoundation.framework/Headers/AVOutputSettingsAssistant.h
/*
	File:  AVOutputSettingsAssistant.h

	Framework:  AVFoundation
 
	Copyright 2012-2017 Apple Inc. All rights reserved.

*/

#import <Foundation/Foundation.h>
#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <CoreMedia/CMFormatDescription.h>

NS_ASSUME_NONNULL_BEGIN

typedef NSString * AVOutputSettingsPreset NS_STRING_ENUM;

/*
 Use these identifiers with +[AVOutputSettingsAssistant outputSettingsAssistantWithPreset:].
 
 When source format information is supplied with these presets, the resulting video settings will not scale up the video from a smaller size.
 */
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPreset640x480		NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPreset960x540   	NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPreset1280x720  	NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPreset1920x1080		NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPreset3840x2160		NS_AVAILABLE(10_10, 9_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPresetHEVC1920x1080	NS_AVAILABLE(10_13, 11_0);
AVF_EXPORT AVOutputSettingsPreset const AVOutputSettingsPresetHEVC3840x2160	NS_AVAILABLE(10_13, 11_0);

@class AVOutputSettingsAssistantInternal;

/*!
	@class AVOutputSettingsAssistant
	@abstract
		A class, each instance of which specifies a set of parameters for configuring objects that use output settings dictionaries, for example AVAssetWriter & AVAssetWriterInput, so that the resulting media file conforms to some specific criteria
	@discussion
		Instances of AVOutputSettingsAssistant are typically created using a string constant representing a specific preset configuration, such as AVOutputSettingsPreset1280x720.  Once you have an instance, its properties can be used as a guide for creating and configuring an AVAssetWriter object and one or more AVAssetWriterInput objects.  If all the suggested properties are respected, the resulting media file will conform to the criteria implied by the preset.  Alternatively, the properties of an instance can be used as a "base" configuration which can be customized to suit your individual needs.
 
		The recommendations made by an instance get better as you tell it more about the format of your source data.  For example, if you set the sourceVideoFormat property, the recommendation made by the videoSettings property will ensure that your video frames are not scaled up from a smaller size.
 */
NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVOutputSettingsAssistant : NSObject
{
@private
	AVOutputSettingsAssistantInternal *_internal;
}
AV_INIT_UNAVAILABLE

/*!
	@method availableOutputSettingsPresets
	@abstract
		Returns the list of presets that can be used to create an instance of AVOutputSettingsAssistant
	@result
		An NSArray of NSString objects, each of which is a preset identifier
	@discussion
		Each preset in the returned list can be passed in to +outputSettingsAssistantWithPreset: to create a new instance of AVOutputSettingsAssistant.
 
		On iOS, the returned array may be different between different device models.
 */
+ (NSArray<AVOutputSettingsPreset> *)availableOutputSettingsPresets NS_AVAILABLE(10_10, 7_0);

/*!
	@method outputSettingsAssistantWithPreset:
	@abstract
		Returns an instance of AVOutputSettingsAssistant corresponding to the given preset
	@param presetIdentifier
		The string identifier, for example AVOutputSettingsPreset1280x720, for the desired preset
	@result
		An instance of AVOutputSettingsAssistant with properties corresponding to the given preset, or nil if there is no such available preset.
	@discussion
		The properties of the returned object can be used as a guide for creating and configuring an AVAssetWriter object and one or more AVAssetWriterInput objects.  If all the suggested properties are respected in creating the AVAssetWriter, the resulting media file will conform to the criteria implied by the preset.
 
		Use +availableOutputSettingsPresets to get a list of presets identifiers that can be used with this method.
 */
+ (nullable instancetype)outputSettingsAssistantWithPreset:(AVOutputSettingsPreset)presetIdentifier;

/*!
	@property audioSettings
	@abstract
		A dictionary of key/value pairs, as specified in AVAudioSettings.h, to be used when e.g. creating an instance of AVAssetWriterInput
	@discussion
		The value of this property may change as a result of setting a new value for the sourceAudioFormat property.
 */
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *audioSettings;

/*!
	@property videoSettings
	@abstract
		A dictionary of key/value pairs, as specified in AVVideoSettings.h, to be used when e.g. creating an instance of AVAssetWriterInput
	@discussion
		The value of this property may change as a result of setting a new value for the sourceVideoFormat property.
 */
@property (nonatomic, readonly, nullable) NSDictionary<NSString *, id> *videoSettings;

/*!
	@property outputFileType
	@abstract
		A UTI indicating the type of file to be written, to be used when e.g. creating an instance of AVAssetWriter
	@discussion
		Use UTTypeCopyPreferredTagWithClass / kUTTagClassFilenameExtension to get a suitable file extension for a given file type.
 */
@property (nonatomic, readonly) AVFileType outputFileType;

@end


// Use these properties to give more information about the attributes of your source data, in order to get more informed recommendations
@interface AVOutputSettingsAssistant (AVOutputSettingsAssistant_SourceInformation)

/*!
	@property sourceAudioFormat
	@abstract
		A CMAudioFormatDescription object describing the format of you audio data
	@discussion
		Setting this property will allow the receiver to make a more informed recommendation for the audio settings that should be used.  After setting this property, you should re-query the audioSettings property to get the new recommendation.  The default value is NULL, which means that the receiver does not know anything about the format of your audio data.

		If you set a non-NULL value for this property, and are using the receiver to initialize an AVAssetWriterInput, the same format description should be used to initialize the AVAssetWriterInput, along with the dictionary from the audioSettings property.
 */
@property (nonatomic, retain, nullable) __attribute__((NSObject)) CMAudioFormatDescriptionRef sourceAudioFormat;

/*!
	@property sourceVideoFormat
	@abstract
		A CMVideoFormatDescription object describing the format of your video data
	@discussion
		Setting this property will allow the receiver to make a more informed recommendation for the video settings that should be used.  After setting this property, you should re-query the videoSettings property to get the new recommendation.  The default value is NULL, which means that the receiver does not know anything about the format of your video data.

		If you set a non-NULL value for this property, and are using the receiver to initialize an AVAssetWriterInput, the same format description should be used to initialize the AVAssetWriterInput, along with the dictionary from the videoSettings property.
 */
@property (nonatomic, retain, nullable) __attribute__((NSObject)) CMVideoFormatDescriptionRef sourceVideoFormat;

/*!
	@property sourceVideoAverageFrameDuration
	@abstract
		A CMTime describing the average frame duration (reciprocal of average frame rate) of your video data
	@discussion
		Setting this property will allow the receiver to make a more informed recommendation for the video settings that should be used.  After setting this property, you should re-query the videoSettings property to get the new recommendation.
 
		The default value is 1/30, which means that the receiver is assuming that your source video has an average frame rate of 30fps.
 
		It is an error to set this property to a value that is not positive or not numeric.  See CMTIME_IS_NUMERIC.
 */
@property (nonatomic) CMTime sourceVideoAverageFrameDuration;

/*!
	@property sourceVideoMinFrameDuration
	@abstract
		A CMTime describing the minimum frame duration (reciprocal of the maximum frame rate) of your video data
	@discussion
		Setting this property will allow the receiver to make a more informed recommendation for the video settings that should be used.  After setting this property, you should re-query the videoSettings property to get the new recommendation.
 
		If your source of video data is an instance of AVAssetReaderOutput, you can discover the minimum frame duration of your source asset using the AVAssetTrack.minFrameDuration property.
 
		The default value is 1/30, which means that the receiver is assuming that your source video has a maximum frame rate of 30fps.
 
		It is an error to set this property to a value that is not positive or not numeric.  See CMTIME_IS_NUMERIC.
 */
@property (nonatomic) CMTime sourceVideoMinFrameDuration NS_AVAILABLE(10_10, 7_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioRecorder.h
/*
	File:           AVAudioRecorder.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioRecorder.h>

// ==========  AVFoundation.framework/Headers/AVAudioTypes.h
/*
	File:           AVAudioTypes.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

// ==========  AVFoundation.framework/Headers/AVAudioConverter.h
/*
	File:           AVAudioConverter.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioConverter.h>

// ==========  AVFoundation.framework/Headers/AVCaptureSystemPressure.h
/*
    File:  AVCaptureSystemPressure.h
 
    Framework:  AVFoundation
 
    Copyright 2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @group AVCaptureSystemPressureLevel string constants
 
 @discussion
    The AVCaptureSystemPressureLevel string constants describe varying levels of system pressure that affect capture hardware availability and/or quality.
 */
typedef NSString *AVCaptureSystemPressureLevel NS_STRING_ENUM API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @constant AVCaptureSystemPressureLevelNominal
    System pressure level is normal (not pressured).
 */
AVF_EXPORT AVCaptureSystemPressureLevel const AVCaptureSystemPressureLevelNominal API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @constant AVCaptureSystemPressureLevelFair
    System pressure is slightly elevated.
 */
AVF_EXPORT AVCaptureSystemPressureLevel const AVCaptureSystemPressureLevelFair API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @constant AVCaptureSystemPressureLevelSerious
    System pressure is highly elevated. Capture performance may be impacted. Frame rate throttling is advised.
 */
AVF_EXPORT AVCaptureSystemPressureLevel const AVCaptureSystemPressureLevelSerious API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @constant AVCaptureSystemPressureLevelCritical
    System pressure is critically elevated. Capture quality and performance are significantly impacted. Frame rate throttling is highly advised.
 */
AVF_EXPORT AVCaptureSystemPressureLevel const AVCaptureSystemPressureLevelCritical API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @constant AVCaptureSystemPressureLevelShutdown
    System pressure is beyond critical. Capture must immediately stop.
 */
AVF_EXPORT AVCaptureSystemPressureLevel const AVCaptureSystemPressureLevelShutdown API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

/*!
 @enum AVCaptureSystemPressureFactors
 @abstract
    Constants indicating factors contributing to the current system pressured state.
 
 @constant AVCaptureSystemPressureFactorNone
    Indicates that the system is not currently pressured.
 @constant AVCaptureSystemPressureFactorSystemTemperature
    Indicates that the entire system is currently experiencing an elevated thermal level.
 @constant AVCaptureSystemPressureFactorPeakPower
    Indicates that the system's peak power requirements exceed the battery's current capacity and may result in a system power off.
 @constant AVCaptureSystemPressureFactorDepthModuleTemperature
    Indicates that the module capturing depth information is operating at an elevated temperature. As system pressure increases, depth quality may become degraded.
 */
typedef NS_OPTIONS(NSUInteger, AVCaptureSystemPressureFactors) {
    AVCaptureSystemPressureFactorNone                   = 0UL,
    AVCaptureSystemPressureFactorSystemTemperature      = (1UL << 0),
    AVCaptureSystemPressureFactorPeakPower              = (1UL << 1),
    AVCaptureSystemPressureFactorDepthModuleTemperature = (1UL << 2),
} API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

@class AVCaptureSystemPressureStateInternal;

/*!
 @class AVCaptureSystemPressureState
 @abstract
    A model object describing a system pressure level and contributing factors to the pressured state.
 
 @discussion
    Beginning in iOS 11.1, AVCaptureDevice can report its current system pressure state. System pressure refers to a state in which capture quality is degraded or capture hardware availability is limited due to factors such as overall system temperature, insufficient battery charge for current peak power requirements, or camera module temperature.
 */
API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) 
@interface AVCaptureSystemPressureState : NSObject
{
@private
    AVCaptureSystemPressureStateInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property level
 @discussion
    An enumerated string value characterizing the pressure level to which the system is currently elevated.
 */
@property(atomic, readonly) AVCaptureSystemPressureLevel level;

/*!
 @property factors
 @discussion
    A bitmask of values indicating the factors contributing to the current system pressure level.
 */
@property(atomic, readonly) AVCaptureSystemPressureFactors factors;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitDistortion.h
/*
	File:           AVAudioUnitDistortion.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitDistortion.h>

// ==========  AVFoundation.framework/Headers/AVCaptureDepthDataOutput.h
/*
    File:  AVCaptureDepthDataOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2016-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVCaptureOutputBase.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureDepthDataOutput

@class AVDepthData;

@class AVCaptureDepthDataOutputInternal;
@protocol AVCaptureDepthDataOutputDelegate;

/*!
 @class AVCaptureDepthDataOutput
 @abstract
    AVCaptureDepthDataOutput is a concrete subclass of AVCaptureOutput that can be used to process depth data in a streaming fashion.
 
 @discussion
    Instances of AVCaptureDepthDataOutput capture AVDepthData objects expressing disparity/depth. Applications can access the frames with the captureOutput:didOutputDepthData:fromConnection: delegate method.
 
    AVCaptureDepthDataOutput always provides depth data in the format expressed by its source's -[AVCaptureDevice activeDepthDataFormat] property. If you wish to receive depth data in another format, you may choose from the -[AVCaptureDevice activeFormat]'s -[AVCaptureDeviceFormat supportedDepthDataFormats], and set it using -[AVCaptureDevice setActiveDepthDataFormat:].
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDepthDataOutput : AVCaptureOutput
{
@private
    AVCaptureDepthDataOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method setDelegate:queue:
 @abstract
    Sets the receiver's delegate that receives captured depth data and the dispatch queue on which the delegate is called.
 
 @param delegate
    An object conforming to the AVCaptureDepthDataOutputDelegate protocol that receives depth data in a streaming fashion.
 @param callbackQueue
    A dispatch queue on which all delegate methods are called.
 
 @discussion
    The depth data output vends captured depth data to its delegate using the methods specified in the AVCaptureDepthOutputDelegate protocol. All delegate methods are called on the specified dispatch queue. If the callback queue is blocked when new depth data is captured, that depth data is automatically dropped at a time determined by the value of the alwaysDiscardsLateDepthData property. This allows clients to process existing depth data on the same queue without having to manage the potential memory usage increases that would otherwise occur when that processing is unable to keep up with the rate of incoming depth data.
 
    Clients who need to minimize the chances of depth data being dropped should provide a dedicated queue and not share it with other data outputs. Processing of depth data may be deferred to another queue, but beware that the depth data pixel buffer maps may come from a finite buffer pool, which may be starved if your deferred processing fails to keep up.
 
    A serial dispatch queue must be used to guarantee that depth data will be delivered in order. The callbackQueue parameter may not be NULL, except when setting the delegate to nil.
 */
- (void)setDelegate:(nullable id<AVCaptureDepthDataOutputDelegate>)delegate callbackQueue:(nullable dispatch_queue_t)callbackQueue;

/*!
 @property delegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureDepthDataOutputDelegate protocol that receives depth data as it is captured. The delegate is set using the setDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) id<AVCaptureDepthDataOutputDelegate> delegate;

/*!
 @property delegateCallbackQueue
 @abstract
    The dispatch queue on which all delegate methods are called.
 
 @discussion
    The value of this property is a dispatch_queue_t. The queue is set using the setDelegate:queue: method.
 */
@property(nonatomic, readonly, nullable) dispatch_queue_t delegateCallbackQueue;

/*!
 @property alwaysDiscardsLateDepthData
 @abstract
    Specifies whether the receiver should always discard any depth data that is not processed before the next depth data is captured.
 
 @discussion
    When the value of this property is YES, the receiver will immediately discard depth data that are captured while the depthDataCallbackQueue is blocked. When the value of this property is NO, delegates will be allowed more time to process old depth data before new depth data are discarded, but application memory usage may increase as a result. The default value is YES.
 */
@property(nonatomic) BOOL alwaysDiscardsLateDepthData;

/*!
 @property filteringEnabled
 @abstract
    Specifies whether the depth data output should filter depth data to smooth out noise and fill invalid values.

 @discussion
    When the value of this property is YES, the receiver temporally filters the stream of AVDepthData objects to reduce noise, as well as fill invalid values. Invalid values (NaN) may be present in AVDepthData pixel buffer maps due to factors such as low light or lens occlusion. When filtering is enabled, the depth data output interpolates missing depth data values. Filtering should be disabled if you desire the raw depth data values. The default value is YES.
 */
@property(nonatomic, getter=isFilteringEnabled) BOOL filteringEnabled;

@end


/*!
 @protocol AVCaptureDepthDataOutputDelegate
 @abstract
    Defines an interface for delegates of AVCaptureDepthDataOutput to receive captured depth data and be notified of late depth data that were dropped.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureDepthDataOutputDelegate <NSObject>

@optional

/*!
 @method depthDataOutput:didOutputDepthData:timestamp:connection:
 @abstract
    Called whenever an AVCaptureDepthDataOutput instance outputs a new depth data object.
 
 @param output
    The AVCaptureDepthDataOutput instance vending the depth data.
 @param depthData
    An AVDepthData object containing the depth/disparity data.
 @param timestamp
    A CMTime indicating when the depth data was captured.
 @param connection
    The AVCaptureConnection through which the depth data is received.
 
 @discussion
    The delegate receives this message whenever the depth data output captures and outputs a new depth data object. This method is called on the dispatch queue specified by the output's delegateCallbackQueue property. This method is called frequently. Care must be taken to process the depth data quickly in order to prevent dropped depth data.
 
    Clients that need to reference the AVDepthData object outside of the scope of this method must retain it and then release it when they are finished with it (in a MRR app).
 
    Note that to maintain optimal performance, AVDepthData pixel buffer maps may be backed by a finite memory pool. If AVDepthData objects are held onto for too long, capture inputs will no longer be able to copy new depth data into memory, resulting in droppage. If your application is causing depth data drops by holding on to provided depth data objects for too long, consider copying the pixel buffer map data into a new pixel buffer so that the AVDepthData backing memory can be reused more quickly.
 */
- (void)depthDataOutput:(AVCaptureDepthDataOutput *)output didOutputDepthData:(AVDepthData *)depthData timestamp:(CMTime)timestamp connection:(AVCaptureConnection *)connection;

/*!
 @method depthDataOutput:didDropDepthData:timestamp:connection:reason:
 @abstract
    Called once for each depth data that is discarded.
 
 @param output
    The AVCaptureDepthDataOutput instance that dropped the depth data.
 @param depthData
    A depth data object containing information about the dropped depth, such as its native depth type. This depth data object produces nil CVPixelBuffers for depth / disparity as it has no backing depth map.
 @param timestamp
    A CMTime indicating when the depth data was captured.
 @param connection
    The AVCaptureConnection from which the dropped depth data object was received.
 @param reason
    The reason the depth data object was dropped.
 
 @discussion
    Delegates receive this message whenever a depth data object is dropped. This method is called once for each dropped depth data. The object passed to this delegate method will contain a shell of an AVDepthData that contains no actual depth data backing pixel buffer, as well as a presentation time stamp and a reason for the drop. This method will be called on the dispatch queue specified by the output's delegateCallbackQueue property. Because this method is called on the same dispatch queue that outputs depth data, it must be efficient to prevent further capture performance problems, such as additional drops.
  */
- (void)depthDataOutput:(AVCaptureDepthDataOutput *)output didDropDepthData:(AVDepthData *)depthData timestamp:(CMTime)timestamp connection:(AVCaptureConnection *)connection reason:(AVCaptureOutputDataDroppedReason)reason;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVDisplayCriteria.h
/*
	File:  AVDisplayCriteria.h

	Framework:  AVFoundation

	Copyright 2017-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVDisplayCriteriaInternal;

/*
 @class			AVDisplayCriteria
 @abstract		AVDisplayCriteria is an abstract set of properties that is used to guide the selection of a display mode.
 @discussion	Certain display modes are more suitable for certain types of content.
 */

API_AVAILABLE(tvos(11.2)) API_UNAVAILABLE(ios) API_UNAVAILABLE(macos, watchos)
@interface AVDisplayCriteria : NSObject <NSCopying>
{
@private
	AVDisplayCriteriaInternal *_displayCriteria;
}

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCapturePhotoOutput.h
/*
    File:  AVCapturePhotoOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2016-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVVideoSettings.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCapturePhotoOutput

@class AVCapturePhotoSettings;
@class AVCapturePhotoBracketSettings;
@class AVCaptureResolvedPhotoSettings;
@class AVCaptureBracketedStillImageSettings;
@class AVMetadataItem;
@class AVPortraitEffectsMatte;
@class AVCapturePhotoOutputInternal;

@protocol AVCapturePhotoCaptureDelegate;

/*!
 @class AVCapturePhotoOutput
 @abstract
    AVCapturePhotoOutput is a concrete subclass of AVCaptureOutput that supersedes AVCaptureStillImageOutput as the preferred interface for capturing photos. In addition to capturing all flavors of still image supported by AVCaptureStillImageOutput, it supports Live Photo capture, preview-sized image delivery, wide color, RAW, RAW+JPG and RAW+DNG formats.
 
 @discussion
    Taking a photo is multi-step process. Clients wishing to build a responsive UI need to know about the progress of a photo capture request as it advances from capture to processing to finished delivery. AVCapturePhotoOutput informs clients of photo capture progress through a delegate protocol. To take a picture, a client instantiates and configures an AVCapturePhotoSettings object, then calls AVCapturePhotoOutput's -capturePhotoWithSettings:delegate:, passing a delegate to be informed when events relating to the photo capture occur (e.g., the photo is about to be captured, the photo has been captured but not processed yet, the Live Photo movie is ready, etc.).
 
    Some AVCapturePhotoSettings properties are "Auto", such as autoStillImageStabilizationEnabled. When set to YES, the photo output decides at capture time whether the current scene and lighting conditions require still image stabilization. Thus the client doesn't know with certainty which features are enabled when making the capture request. With the first and each subsequent delegate callback, the client is provided an AVCaptureResolvedPhotoSettings instance that indicates the settings that were applied to the capture. All "Auto" features have now been resolved to on or off. The AVCaptureResolvedPhotoSettings object passed in the client's delegate callbacks has a uniqueID identical to the AVCapturePhotoSettings request. This uniqueID allows clients to pair unresolved and resolved settings objects. See AVCapturePhotoCaptureDelegate below for a detailed discussion of the delegate callbacks.
 
    Enabling certain photo features (Live Photo capture and high resolution capture) requires a reconfiguration of the capture render pipeline. Clients wishing to opt in for these features should call -setLivePhotoCaptureEnabled: and/or -setHighResolutionCaptureEnabled: before calling -startRunning on the AVCaptureSession. Changing any of these properties while the session is running requires a disruptive reconfiguration of the capture render pipeline. Live Photo captures in progress will be ended immediately; unfulfilled photo requests will be aborted; video preview will temporarily freeze. If you wish to capture Live Photos containing sound, you must add an audio AVCaptureDeviceInput to your AVCaptureSession.

    Simultaneous Live Photo capture and MovieFileOutput capture is not supported. If an AVCaptureMovieFileOutput is added to your session, AVCapturePhotoOutput's livePhotoCaptureSupported property returns NO. Note that simultaneous Live Photo capture and AVCaptureVideoDataOutput is supported.
 
    AVCaptureStillImageOutput and AVCapturePhotoOutput may not both be added to a capture session. You must use one or the other. If you add both to a session, a NSInvalidArgumentException is thrown.
 
    AVCapturePhotoOutput implicitly supports wide color photo capture, following the activeColorSpace of the source AVCaptureDevice. If the source device's activeColorSpace is AVCaptureColorSpace_P3_D65, photos are encoded with wide color information, unless you've specified an output format of '420v', which does not support wide color.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCapturePhotoOutput : AVCaptureOutput
{
@private
    AVCapturePhotoOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method capturePhotoWithSettings:delegate:
 @abstract
    Method for initiating a photo capture request with progress monitoring through the supplied delegate.
 
 @param settings
    An AVCapturePhotoSettings object you have configured. May not be nil.
 @param delegate
    An object conforming to the AVCapturePhotoCaptureDelegate protocol. This object's delegate methods are called back as the photo advances from capture to processing to finished delivery. May not be nil.
 
 @discussion
    This method initiates a photo capture. The receiver copies your provided settings to prevent unintentional mutation. It is illegal to re-use settings. The receiver throws a NSInvalidArgumentException if your settings.uniqueID matches that of any previously used settings. This method is used to initiate all flavors of photo capture: single photo, RAW capture with or without a processed image (such as a JPEG), bracketed capture, and Live Photo.
 
    Clients need not wait for a capture photo request to complete before issuing another request. This is true for single photo captures as well as Live Photos, where movie complements of adjacent photo captures are allowed to overlap.
 
    This method validates your settings and enforces the following rules in order to ensure deterministic behavior. If any of these rules are violated, a NSInvalidArgumentException is thrown.
    RAW rules:
        - If rawPhotoPixelFormatType is non-zero, it must be present in the receiver's -availableRawPhotoPixelFormatTypes array.
        - If rawPhotoPixelFormatType is non-zero, autoStillImageStabilizationEnabled must be set to NO.
        - If rawPhotoPixelFormatType is non-zero, your delegate must respond to -captureOutput:didFinishProcessingRawPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:.
        - If rawPhotoPixelFormatType is non-zero, highResolutionPhotoEnabled may be YES or NO, but the setting only applies to the processed image, if you've specified one.
        - If rawPhotoPixelFormatType is non-zero, the videoZoomFactor of the source device and the videoScaleAndCropFactor of the photo output's video connection must both be 1.0. Ensure no zoom is applied before requesting a RAW capture, and don't change the zoom during RAW capture.
        - If rawFileType is specified, it must be present in -availableRawPhotoFileTypes and must support the rawPhotoPixelFormatType specified using -supportedRawPhotoPixelFormatTypesForFileType:.
    Processed Format rules:
        - If format is non-nil, a kCVPixelBufferPixelFormatTypeKey or AVVideoCodecKey must be present, and both may not be present.
        - If format has a kCVPixelBufferPixelFormatTypeKey, its value must be present in the receiver's -availablePhotoPixelFormatTypes array.
        - If format has a AVVideoCodecKey, its value must be present in the receiver's -availablePhotoCodecTypes array.
        - If format is non-nil, your delegate must respond to -captureOutput:didFinishProcessingPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:.
        - If processedFileType is specified, it must be present in -availablePhotoFileTypes and must support the format's specified kCVPixelBufferPixelFormatTypeKey (using -supportedPhotoPixelFormatTypesForFileType:) or AVVideoCodecKey (using -supportedPhotoCodecTypesForFileType:).
    Flash rules:
        - The specified flashMode must be present in the receiver's -supportedFlashModes array.
    Live Photo rules:
        - The receiver's livePhotoCaptureEnabled must be YES if settings.livePhotoMovieURL is non-nil.
        - If settings.livePhotoMovieURL is non-nil, the receiver's livePhotoCaptureSuspended property must be set to NO.
        - If settings.livePhotoMovieURL is non-nil, it must be a file URL that's accessible to your app's sandbox.
        - If settings.livePhotoMovieURL is non-nil, your delegate must respond to -captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error:.
    Bracketed capture rules:
        - bracketedSettings.count must be <= the receiver's maxBracketedCapturePhotoCount property.
        - For manual exposure brackets, ISO value must be within the source device activeFormat's minISO and maxISO values.
        - For manual exposure brackets, exposureDuration value must be within the source device activeFormat's minExposureDuration and maxExposureDuration values.
        - For auto exposure brackets, exposureTargetBias value must be within the source device's minExposureTargetBias and maxExposureTargetBias values.
 */
- (void)capturePhotoWithSettings:(AVCapturePhotoSettings *)settings delegate:(id<AVCapturePhotoCaptureDelegate>)delegate;

/*!
 @property preparedPhotoSettingsArray
 @abstract
    An array of AVCapturePhotoSettings instances for which the receiver is prepared to capture.

 @discussion
    @seealso setPreparedPhotoSettingsArray:completionHandler:
    Some types of photo capture, such as bracketed captures and RAW captures, require the receiver to allocate additional buffers or prepare other resources. To prevent photo capture requests from executing slowly due to lazy resource allocation, you may call -setPreparedPhotoSettingsArray:completionHandler: with an array of settings objects representative of the types of capture you will be performing (e.g., settings for a bracketed capture, RAW capture, and/or still image stabilization capture). By default, the receiver prepares sufficient resources to capture photos with default settings, +[AVCapturePhotoSettings photoSettings].
 */
@property(nonatomic, readonly) NSArray<AVCapturePhotoSettings *> *preparedPhotoSettingsArray;

/*!
 @method setPreparedPhotoSettingsArray:completionHandler:
 @abstract
    Method allowing the receiver to prepare resources in advance for future -capturePhotoWithSettings:delegate: requests.
 
 @param preparedPhotoSettingsArray
    An array of AVCapturePhotoSettings instances indicating the types of capture for which the receiver should prepare resources.
 @param completionHandler
    A completion block to be fired on a serial dispatch queue once the receiver has finished preparing. You may pass nil to indicate you do not wish to be called back when preparation is complete.
 
 @discussion
    Some types of photo capture, such as bracketed captures and RAW captures, require the receiver to allocate additional buffers or prepare other resources. To prevent photo capture requests from executing slowly due to lazy resource allocation, you may call this method with an array of settings objects representative of the types of capture you will be performing (e.g., settings for a bracketed capture, RAW capture, and/or still image stabilization capture). You may call this method even before calling -[AVCaptureSession startRunning] in order to hint the receiver up front which features you'll be utilizing. Each time you call this method with an array of settings, the receiver evaluates what additional resources it needs to allocate, as well as existing resources that can be reclaimed, and calls back your completionHandler when it has finished preparing (and possibly reclaiming) needed resources. By default, the receiver prepares sufficient resources to capture photos with default settings, +[AVCapturePhotoSettings photoSettings]. If you wish to reclaim all possible resources, you may call this method with an empty array.
 
    Preparation for photo capture is always optional. You may call -capturePhotoWithSettings:delegate: without first calling -setPreparedPhotoSettingsArray:completionHandler:, but be advised that some of your photo captures may execute slowly as additional resources are allocated just-in-time.
 
    If you call this method while your AVCaptureSession is not running, your completionHandler does not fire immediately. It only fires once you've called -[AVCaptureSession startRunning], and the needed resources have actually been prepared. If you call -setPreparedPhotoSettingsArray:completionHandler: with an array of settings, and then call it a second time, your first prepare call's completionHandler fires immediately with prepared == NO.
 
    Prepared settings persist across session starts/stops and committed configuration changes. This property participates in -[AVCaptureSession beginConfiguration] / -[AVCaptureSession commitConfiguration] deferred work behavior. That is, if you call -[AVCaptureSession beginConfiguration], change your session's input/output topology, and call this method, preparation is deferred until you call -[AVCaptureSession commitConfiguration], enabling you to atomically commit a new configuration as well as prepare to take photos in that new configuration.
 */
- (void)setPreparedPhotoSettingsArray:(NSArray<AVCapturePhotoSettings *> *)preparedPhotoSettingsArray completionHandler:(nullable void (^)(BOOL prepared, NSError * _Nullable error))completionHandler;

/*!
 @property availablePhotoPixelFormatTypes
 @abstract
    An array of kCVPixelBufferPixelFormatTypeKey values that are currently supported by the receiver.

 @discussion
    If you wish to capture a photo in an uncompressed format, such as 420f, 420v, or BGRA, you must ensure that the format you want is present in the receiver's availablePhotoPixelFormatTypes array. If you've not yet added your receiver to an AVCaptureSession with a video source, no pixel format types are available. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *availablePhotoPixelFormatTypes;

/*!
 @property availablePhotoCodecTypes
 @abstract
    An array of AVVideoCodecKey values that are currently supported by the receiver.

 @discussion
    If you wish to capture a photo in a compressed format, such as JPEG, you must ensure that the format you want is present in the receiver's availablePhotoCodecTypes array. If you've not yet added your receiver to an AVCaptureSession with a video source, no codec types are available. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availablePhotoCodecTypes;

/*!
 @property availableRawPhotoPixelFormatTypes
 @abstract
    An array of Bayer RAW CVPixelBufferPixelFormatTypeKey values that are currently supported by the receiver.

 @discussion
    If you wish to capture a RAW photo, you must ensure that the Bayer RAW format you want is present in the receiver's availableRawPhotoPixelFormatTypes array. If you've not yet added your receiver to an AVCaptureSession with a video source, no RAW formats are available. This property is key-value observable. RAW capture is not supported on all platforms.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *availableRawPhotoPixelFormatTypes;

/*!
 @property availablePhotoFileTypes
 @abstract
    An array of AVFileType values that are currently supported by the receiver.

 @discussion
    If you wish to capture a photo that is formatted for a particular file container, such as HEIF, you must ensure that the fileType you desire is present in the receiver's availablePhotoFileTypes array. If you've not yet added your receiver to an AVCaptureSession with a video source, no file types are available. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<AVFileType> *availablePhotoFileTypes API_AVAILABLE(ios(11.0));

/*!
 @property availableRawPhotoFileTypes
 @abstract
    An array of AVFileType values that are currently supported by the receiver for RAW capture.

 @discussion
    If you wish to capture a RAW photo that is formatted for a particular file container, such as DNG, you must ensure that the fileType you desire is present in the receiver's availableRawPhotoFileTypes array. If you've not yet added your receiver to an AVCaptureSession with a video source, no file types are available. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<AVFileType> *availableRawPhotoFileTypes API_AVAILABLE(ios(11.0));

/*!
 @method supportedPhotoPixelFormatTypesForFileType:
 @abstract
    An array of pixel format type values that are currently supported by the receiver for a particular file container.
 
 @param fileType
    The AVFileType container type intended for storage of a photo.
 @result
    An array of CVPixelBufferPixelFormatTypeKey values supported by the receiver for the file type in question.
 
 @discussion
    If you wish to capture a photo for storage in a particular file container, such as TIFF, you must ensure that the photo pixel format type you request is valid for that file type. If no pixel format types are supported for a given fileType, an empty array is returned. If you've not yet added your receiver to an AVCaptureSession with a video source, no pixel format types are supported.
 */
- (NSArray<NSNumber *> *)supportedPhotoPixelFormatTypesForFileType:(AVFileType)fileType API_AVAILABLE(ios(11.0));

/*!
 @method supportedPhotoCodecTypesForFileType:
 @abstract
    An array of AVVideoCodecKey values that are currently supported by the receiver for a particular file container.
 
 @param fileType
    The AVFileType container type intended for storage of a photo.
 @result
    An array of AVVideoCodecKey values supported by the receiver for the file type in question.
 
 @discussion
    If you wish to capture a photo for storage in a particular file container, such as HEIF, you must ensure that the photo codec type you request is valid for that file type. If no codec types are supported for a given fileType, an empty array is returned. If you've not yet added your receiver to an AVCaptureSession with a video source, no codec types are supported.
 */
- (NSArray<AVVideoCodecType> *)supportedPhotoCodecTypesForFileType:(AVFileType)fileType API_AVAILABLE(ios(11.0));

/*!
 @method supportedRawPhotoPixelFormatTypesForFileType:
 @abstract
    An array of CVPixelBufferPixelFormatType values that are currently supported by the receiver for a particular file container.
 
 @param fileType
    The AVFileType container type intended for storage of a photo.
 @result
    An array of CVPixelBufferPixelFormatType values supported by the receiver for the file type in question.
 
 @discussion
    If you wish to capture a photo for storage in a particular file container, such as DNG, you must ensure that the RAW pixel format type you request is valid for that file type. If no RAW pixel format types are supported for a given fileType, an empty array is returned. If you've not yet added your receiver to an AVCaptureSession with a video source, no pixel format types are supported.
 */
- (NSArray<NSNumber *> *)supportedRawPhotoPixelFormatTypesForFileType:(AVFileType)fileType API_AVAILABLE(ios(11.0));

/*!
 @property stillImageStabilizationSupported
 @abstract
    Indicates whether the still image stabilization feature is supported by the receiver.

 @discussion
    This property may change as the session's -sessionPreset or source device's -activeFormat change. When still image stabilization is not supported, your capture requests always resolve stillImageStabilizationEnabled to NO. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isStillImageStabilizationSupported) BOOL stillImageStabilizationSupported;

/*!
 @property isStillImageStabilizationScene
 @abstract
    Indicates whether the current scene is dark enough to warrant use of still image stabilization.

 @discussion
    This property reports whether the current scene being previewed by the camera is dark enough to benefit from still image stabilization. You can influence this property's answers by setting the photoSettingsForSceneMonitoring property, indicating whether autoStillImageStabilization monitoring should be on or off. If you set autoStillImageStabilization to NO, isStillImageStabilizationScene always reports NO. If you set it to YES, this property returns YES or NO depending on the current scene's lighting conditions. Note that some very dark scenes do not benefit from still image stabilization, but do benefit from flash. By default, this property always returns NO unless you set photoSettingsForSceneMonitoring to a non-nil value. This property may be key-value observed.
 */
@property(nonatomic, readonly) BOOL isStillImageStabilizationScene;

/*!
 @property dualCameraFusionSupported
 @abstract
    Indicates whether the DualCamera image fusion feature is supported by the receiver.

 @discussion
    This property may change as the session's -sessionPreset or source device's -activeFormat change. When using the AVCaptureDevice with deviceType AVCaptureDeviceTypeBuiltInDualCamera, the wide-angle and telephoto camera images can be fused together to improve image quality in some configurations. When DualCamera image fusion is not supported by the current configuration, your capture requests always resolve dualCameraFusionEnabled to NO. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isDualCameraFusionSupported) BOOL dualCameraFusionSupported API_AVAILABLE(ios(10.2));

/*!
 @property dualCameraDualPhotoDeliverySupported
 @abstract
    Specifies whether the photo output's current configuration supports delivery of both telephoto and wide images from the DualCamera.

 @discussion
    DualCamera dual photo delivery is only supported for certain AVCaptureSession sessionPresets and AVCaptureDevice activeFormats. When switching cameras or formats this property may change. When this property changes from YES to NO, dualCameraDualPhotoDeliveryEnabled also reverts to NO. If you've previously opted in for DualCamera dual photo delivery and then change configurations, you may need to set dualCameraDualPhotoDeliveryEnabled = YES again. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isDualCameraDualPhotoDeliverySupported) BOOL dualCameraDualPhotoDeliverySupported API_AVAILABLE(ios(11.0));

/*!
 @property dualCameraDualPhotoDeliveryEnabled
 @abstract
    Indicates whether the photo output is configured for delivery of both the telephoto and wide images from the DualCamera.

 @discussion
    Default value is NO. This property may only be set to YES if dualCameraDualPhotoDeliverySupported is YES. DualCamera dual photo delivery requires a lengthy reconfiguration of the capture render pipeline, so if you intend to do any dual photo delivery captures, you should set this property to YES before calling -[AVCaptureSession startRunning]. See also -[AVCapturePhotoSettings dualCameraDualPhotoDeliveryEnabled].
 */
@property(nonatomic, getter=isDualCameraDualPhotoDeliveryEnabled) BOOL dualCameraDualPhotoDeliveryEnabled API_AVAILABLE(ios(11.0));

/*!
 @property cameraCalibrationDataDeliverySupported
 @abstract
    Specifies whether the photo output's current configuration supports delivery of AVCameraCalibrationData in the resultant AVCapturePhoto.

 @discussion
    Camera calibration data delivery (intrinsics, extrinsics, lens distortion characteristics, etc.) is only supported in certain configurations. In iOS 11, its value is only YES if dualCameraDualPhotoDeliveryEnabled is YES. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isCameraCalibrationDataDeliverySupported) BOOL cameraCalibrationDataDeliverySupported API_AVAILABLE(ios(11.0));

/*!
 @property supportedFlashModes
 @abstract
    An array of AVCaptureFlashMode constants for the current capture session configuration.

 @discussion
    This property supersedes AVCaptureDevice's isFlashModeSupported: It returns an array of AVCaptureFlashMode constants. To test whether a particular flash mode is supported, use NSArray's containsObject API: [photoOutput.supportedFlashModes containsObject:@(AVCaptureFlashModeAuto)]. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *supportedFlashModes;


/*!
 @property autoRedEyeReductionSupported
 @abstract
    Indicates whether the receiver supports automatic red-eye reduction for flash captures.

 @discussion
    Flash images may cause subjects' eyes to appear red, golden, or white. Automatic red-eye reduction detects and corrects for reflected light in eyes, at the cost of additional processing time per image. This property may change as the session's -sessionPreset or source device's -activeFormat change. When red-eye reduction is not supported, your capture requests always resolve redEyeReductionEnabled to NO. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isAutoRedEyeReductionSupported) BOOL autoRedEyeReductionSupported API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, watchos, tvos);

/*!
 @property isFlashScene
 @abstract
    Indicates whether the current scene is dark enough to warrant use of the flash.

 @discussion
    This property reports whether the current scene being previewed by the camera is dark enough to need the flash. If -supportedFlashModes only contains AVCaptureFlashModeOff, isFlashScene always reports NO. You can influence this property's answers by setting the photoSettingsForSceneMonitoring property, indicating the flashMode you wish to monitor. If you set flashMode to AVCaptureFlashModeOff, isFlashScene always reports NO. If you set it to AVCaptureFlashModeAuto or AVCaptureFlashModeOn, isFlashScene answers YES or NO based on the current scene's lighting conditions. By default, this property always returns NO unless you set photoSettingsForSceneMonitoring to a non-nil value. Note that there is some overlap in the light level ranges that benefit from still image stabilization and flash. If your photoSettingsForSceneMonitoring indicate that both still image stabilization and flash scenes should be monitored, still image stabilization takes precedence, and isFlashScene becomes YES at lower overall light levels. This property may be key-value observed.
 */
@property(nonatomic, readonly) BOOL isFlashScene;

/*!
 @property photoSettingsForSceneMonitoring
 @abstract
    Settings that govern the behavior of isFlashScene and isStillImageStabilizationScene.

 @discussion
    You can influence the return values of isFlashScene and isStillImageStabilizationScene by setting this property, indicating the flashMode and autoStillImageStabilizationEnabled values that should be considered for scene monitoring. For instance, if you set flashMode to AVCaptureFlashModeOff, isFlashScene always reports NO. If you set it to AVCaptureFlashModeAuto or AVCaptureFlashModeOn, isFlashScene answers YES or NO based on the current scene's lighting conditions. Note that there is some overlap in the light level ranges that benefit from still image stabilization and flash. If your photoSettingsForSceneMonitoring indicate that both still image stabilization and flash scenes should be monitored, still image stabilization takes precedence, and isFlashScene becomes YES at lower overall light levels. The default value for this property is nil. See isStillImageStabilizationScene and isFlashScene for further discussion.
 */
@property(nonatomic, copy, nullable) AVCapturePhotoSettings *photoSettingsForSceneMonitoring;

/*!
 @property highResolutionCaptureEnabled
 @abstract
    Indicates whether the photo render pipeline should be configured to deliver high resolution still images.

 @discussion
    Some AVCaptureDeviceFormats support outputting higher resolution stills than their streaming resolution (See AVCaptureDeviceFormat.highResolutionStillImageDimensions). Under some conditions, AVCaptureSession needs to set up the photo render pipeline differently to support high resolution still image capture. If you intend to take high resolution still images at all, you should set this property to YES before calling -[AVCaptureSession startRunning]. Once you've opted in for high resolution capture, you are free to issue photo capture requests with or without highResolutionCaptureEnabled in the AVCapturePhotoSettings. If you have not set this property to YES and call capturePhotoWithSettings:delegate: with settings.highResolutionCaptureEnabled set to YES, an NSInvalidArgumentException will be thrown.
 */
@property(nonatomic, getter=isHighResolutionCaptureEnabled) BOOL highResolutionCaptureEnabled;

/*!
 @property maxBracketedCapturePhotoCount
 @abstract
    Specifies the maximum number of photos that may be taken in a single bracket.

 @discussion
     AVCapturePhotoOutput can only satisfy a limited number of image requests in a single bracket without exhausting system resources. The maximum number of photos that may be taken in a single bracket depends on the size and format of the images being captured, and consequently may vary with AVCaptureSession -sessionPreset and AVCaptureDevice -activeFormat. Some formats do not support bracketed capture at all, and thus this property may return a value of 0. This read-only property is key-value observable. If you call -capturePhotoWithSettings:delegate: with a bracketedSettings whose count exceeds -maxBracketedCapturePhotoCount, an NSInvalidArgumentException is thrown.
 */
@property(nonatomic, readonly) NSUInteger maxBracketedCapturePhotoCount;

/*!
 @property lensStabilizationDuringBracketedCaptureSupported
 @abstract
    Indicates whether the receiver supports lens stabilization during bracketed captures.

 @discussion
    The AVCapturePhotoBracketSettings lensStabilizationEnabled property may only be set if this property returns YES. Its value may change as the session's -sessionPreset or input device's -activeFormat changes. This read-only property is key-value observable.
 */
@property(nonatomic, readonly, getter=isLensStabilizationDuringBracketedCaptureSupported) BOOL lensStabilizationDuringBracketedCaptureSupported;

/*!
 @property livePhotoCaptureSupported
 @abstract
    Indicates whether the receiver supports Live Photo capture.

 @discussion
    Live Photo capture is only supported for certain AVCaptureSession sessionPresets and AVCaptureDevice activeFormats. When switching cameras or formats this property may change. When this property changes from YES to NO, livePhotoCaptureEnabled also reverts to NO. If you've previously opted in for Live Photo capture and then change configurations, you may need to set livePhotoCaptureEnabled = YES again. 
 */
@property(nonatomic, readonly, getter=isLivePhotoCaptureSupported) BOOL livePhotoCaptureSupported;

/*!
 @property livePhotoCaptureEnabled
 @abstract
    Indicates whether the receiver is configured for Live Photo capture

 @discussion
    Default value is NO. This property may only be set to YES if livePhotoCaptureSupported is YES. Live Photo capture requires a lengthy reconfiguration of the capture render pipeline, so if you intend to do any Live Photo captures at all, you should set livePhotoCaptureEnabled to YES before calling -[AVCaptureSession startRunning].
 */
@property(nonatomic, getter=isLivePhotoCaptureEnabled) BOOL livePhotoCaptureEnabled;

/*!
 @property livePhotoCaptureSuspended
 @abstract
    Indicates whether Live Photo capture is enabled, but currently suspended.

 @discussion
    This property allows you to cut current Live Photo movie captures short (for instance, if you suddenly need to do something that you don't want to show up in the Live Photo movie, such as take a non Live Photo capture that makes a shutter sound). By default, livePhotoCaptureSuspended is NO. When you set livePhotoCaptureSuspended = YES, any Live Photo movie captures in progress are trimmed to the current time. Likewise, when you toggle livePhotoCaptureSuspended from YES to NO, subsequent Live Photo movie captures will not contain any samples earlier than the time you un-suspended Live Photo capture. Setting this property to YES throws an NSInvalidArgumentException if livePhotoCaptureEnabled is NO. This property may only be set while the session is running. Setting this property to YES when the session is not running will fail resulting in livePhotoCaptureSuspended being reverted to NO.
 */
@property(nonatomic, getter=isLivePhotoCaptureSuspended) BOOL livePhotoCaptureSuspended;

/*!
 @property livePhotoAutoTrimmingEnabled
 @abstract
    Indicates whether Live Photo movies are trimmed in real time to avoid excessive movement.

 @discussion
    This property defaults to YES when livePhotoCaptureSupported is YES. Changing this property's value while your session is running will cause a lengthy reconfiguration of the session. You should set livePhotoAutoTrimmingEnabled to YES or NO before calling -[AVCaptureSession startRunning]. When set to YES, Live Photo movies are analyzed in real time and trimmed if there's excessive movement before or after the photo is taken. Nominally, Live Photos are approximately 3 seconds long. With trimming enabled, they may be shorter, depending on movement. This feature prevents common problems such as Live Photo movies containing shoe or pocket shots.
 */
@property(nonatomic, getter=isLivePhotoAutoTrimmingEnabled) BOOL livePhotoAutoTrimmingEnabled;

/*!
 @property availableLivePhotoVideoCodecTypes
 @abstract
    An array of AVVideoCodecKey values that are currently supported by the receiver for use in the movie complement of a Live Photo.

 @discussion
    Prior to iOS 11, all Live Photo movie video tracks are compressed using H.264. Beginning in iOS 11, you can select the Live Photo movie video compression format using one of the AVVideoCodecKey strings presented in this property. The system's default (preferred) video codec is always presented first in the list. If you've not yet added your receiver to an AVCaptureSession with a video source, no codecs are available. This property is key-value observable.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableLivePhotoVideoCodecTypes API_AVAILABLE(ios(11.0));

/*!
 @method JPEGPhotoDataRepresentationForJPEGSampleBuffer:previewPhotoSampleBuffer:
 @abstract
    A class method that writes a JPEG sample buffer to an NSData in the JPEG file format.
 
 @param JPEGSampleBuffer
    A CMSampleBuffer containing JPEG compressed data.
 @param previewPhotoSampleBuffer
    An optional CMSampleBuffer containing pixel buffer image data to be written as a thumbnail image.
 @result
    An NSData containing bits in the JPEG file format. May return nil if the re-packaging process fails.

 @discussion
    AVCapturePhotoOutput's depecrated -captureOutput:didFinishProcessingPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error: callback delivers JPEG photos to clients as CMSampleBuffers. To re-package these buffers in a data format suitable for writing to a JPEG file, you may call this class method, optionally inserting your own metadata into the JPEG CMSampleBuffer first, and optionally passing a preview image to be written to the JPEG file format as a thumbnail image.
 */
+ (nullable NSData *)JPEGPhotoDataRepresentationForJPEGSampleBuffer:(CMSampleBufferRef)JPEGSampleBuffer previewPhotoSampleBuffer:(nullable CMSampleBufferRef)previewPhotoSampleBuffer API_DEPRECATED("Use -[AVCapturePhoto fileDataRepresentation] instead.", ios(10.0, 11.0));

/*!
 @method DNGPhotoDataRepresentationForRawSampleBuffer:previewPhotoSampleBuffer:
 @abstract
    A class method that writes a RAW sample buffer to an NSData containing bits in the DNG file format.
 
 @param rawSampleBuffer
    A CMSampleBuffer containing Bayer RAW data.
 @param previewPhotoSampleBuffer
    An optional CMSampleBuffer containing pixel buffer image data to be written as a thumbnail image.
 @result
    An NSData containing bits in the DNG file format. May return nil if the re-packaging process fails.

 @discussion
    AVCapturePhotoOutput's deprecated -captureOutput:didFinishProcessingRawPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error: callback delivers RAW photos to clients as CMSampleBuffers. To re-package these buffers in a data format suitable for writing to a DNG file, you may call this class method, optionally inserting your own metadata into the RAW CMSampleBuffer first, and optionally passing a preview image to be written to the DNG file format as a thumbnail image. Only RAW images from Apple built-in cameras are supported.
 */
+ (nullable NSData *)DNGPhotoDataRepresentationForRawSampleBuffer:(CMSampleBufferRef)rawSampleBuffer previewPhotoSampleBuffer:(nullable CMSampleBufferRef)previewPhotoSampleBuffer API_DEPRECATED("Use -[AVCapturePhoto fileDataRepresentation] instead.", ios(10.0, 11.0));

@end


@class AVCapturePhoto;

@interface AVCapturePhotoOutput (AVCapturePhotoOutputDepthDataDeliverySupport)

/*!
 @property depthDataDeliverySupported
 @abstract
    A BOOL value specifying whether depth data delivery is supported.

 @discussion
    Some cameras and configurations support the delivery of depth data (e.g. disparity maps) along with the photo. This property returns YES if the session's current configuration allows photos to be captured with depth data, from which depth-related filters may be applied. When switching cameras or formats this property may change. When this property changes from YES to NO, depthDataDeliveryEnabled also reverts to NO. If you've previously opted in for depth data delivery and then change configurations, you may need to set depthDataDeliveryEnabled = YES again.This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isDepthDataDeliverySupported) BOOL depthDataDeliverySupported API_AVAILABLE(ios(11.0));

/*!
 @property depthDataDeliveryEnabled
 @abstract
    A BOOL specifying whether the photo render pipeline is prepared for depth data delivery.

 @discussion
    Default is NO. Set to YES if you wish depth data to be delivered with your AVCapturePhotos. This property may only be set to YES if depthDataDeliverySupported is YES. Enabling depth data delivery requires a lengthy reconfiguration of the capture render pipeline, so if you intend to capture depth data, you should set this property to YES before calling -[AVCaptureSession startRunning].
 */
@property(nonatomic, getter=isDepthDataDeliveryEnabled) BOOL depthDataDeliveryEnabled API_AVAILABLE(ios(11.0));

/*!
 @property portraitEffectsMatteDeliverySupported
 @abstract
    A BOOL value specifying whether portrait effects matte delivery is supported.

 @discussion
    Some cameras and configurations support the delivery of a matting image to augment depth data and aid in high quality portrait effect rendering (see AVPortraitEffectsMatte.h). This property returns YES if the session's current configuration allows photos to be captured with a portrait effects matte. When switching cameras or formats this property may change. When this property changes from YES to NO, portraitEffectsMatteDeliveryEnabled also reverts to NO. If you've previously opted in for portrait effects matte delivery and then change configurations, you may need to set portraitEffectsMatteDeliveryEnabled = YES again.This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isPortraitEffectsMatteDeliverySupported) BOOL portraitEffectsMatteDeliverySupported API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property portraitEffectsMatteDeliveryEnabled
 @abstract
    A BOOL specifying whether the photo render pipeline is prepared for portrait effects matte delivery.

 @discussion
    Default is NO. Set to YES if you wish portrait effects mattes to be delivered with your AVCapturePhotos. This property may only be set to YES if portraitEffectsMatteDeliverySupported is YES. Portrait effects matte generation requires depth to be present, so when enabling portrait effects matte delivery, you must also set depthDataDeliveryEnabled to YES. Enabling portrait effects matte delivery requires a lengthy reconfiguration of the capture render pipeline, so if you intend to capture portrait effects mattes, you should set this property to YES before calling -[AVCaptureSession startRunning].
 */
@property(nonatomic, getter=isPortraitEffectsMatteDeliveryEnabled) BOOL portraitEffectsMatteDeliveryEnabled API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

@end


/*!
 @protocol AVCapturePhotoCaptureDelegate
 @abstract
    A set of delegate callbacks to be implemented by a client who calls AVCapturePhotoOutput's -capturePhotoWithSettings:delegate.
 
 @discussion
    AVCapturePhotoOutput invokes the AVCapturePhotoCaptureDelegate callbacks on a common dispatch queue — not necessarily the main queue. While the -captureOutput:willBeginCaptureForResolvedSettings: callback always comes first and the -captureOutput:didFinishCaptureForResolvedSettings: callback always comes last, none of the other callbacks can be assumed to come in any particular order. The AVCaptureResolvedPhotoSettings instance passed to the client with each callback has the same uniqueID as the AVCapturePhotoSettings instance passed in -capturePhotoWithSettings:delegate:. All callbacks are marked optional, but depending on the features you've specified in your AVCapturePhotoSettings, some callbacks become mandatory and are validated in -capturePhotoWithSettings:delegate:. If your delegate does not implement the mandatory callbacks, an NSInvalidArgumentException is thrown.

    - If you initialize your photo settings with a format dictionary, or use one of the default constructors (that is, if you're not requesting a RAW-only capture), your delegate must respond to either - captureOutput:didFinishProcessingPhoto:error: or the deprecated -captureOutput:didFinishProcessingPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:. If your delegate responds to both of these callbacks, only the undeprecated variant will be called.
    - If you initialize your photo settings with a rawPhotoPixelFormatType, your delegate must respond to either -captureOutput:didFinishProcessingPhoto:error: or the deprecated -captureOutput:didFinishProcessingRawPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:. If your delegate responds to both of these callbacks, only the undeprecated variant will be called.
    - If you set livePhotoMovieFileURL to non-nil, your delegate must respond to -captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error:.
 
    In the event of an error, all expected callbacks are fired with an appropriate error.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCapturePhotoCaptureDelegate <NSObject>

@optional
/*!
 @method captureOutput:willBeginCaptureForResolvedSettings:
 @abstract
    A callback fired as soon as the capture settings have been resolved.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.

 @discussion
    This callback is always delivered first for a particular capture request. It is delivered as soon as possible after you call -capturePhotoWithSettings:delegate:, so you can know what to expect in the remainder of your callbacks.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output willBeginCaptureForResolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings;

/*!
 @method captureOutput:willCapturePhotoForResolvedSettings:
 @abstract
    A callback fired just as the photo is being taken.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 
 @discussion
    The timing of this callback is analogous to AVCaptureStillImageOutput's capturingStillImage property changing from NO to YES. The callback is delivered right after the shutter sound is heard (note that shutter sounds are suppressed when Live Photos are being captured).
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output willCapturePhotoForResolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings;

/*!
 @method captureOutput:didCapturePhotoForResolvedSettings:
 @abstract
    A callback fired just after the photo is taken.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 
 @discussion
    The timing of this callback is analogous to AVCaptureStillImageOutput's capturingStillImage property changing from YES to NO.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didCapturePhotoForResolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings;

/*!
 @method captureOutput:didFinishProcessingPhoto:error:
 @abstract
    A callback fired when photos are ready to be delivered to you (RAW or processed).
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param photo
    An instance of AVCapturePhoto.
 @param error
    An error indicating what went wrong. If the photo was processed successfully, nil is returned.
 
 @discussion
    This callback fires resolvedSettings.expectedPhotoCount number of times for a given capture request. Note that the photo parameter is always non nil, even if an error is returned. The delivered AVCapturePhoto's rawPhoto property can be queried to know if it's a RAW image or processed image.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishProcessingPhoto:(AVCapturePhoto *)photo error:(nullable NSError *)error API_AVAILABLE(ios(11.0));

/*!
 @method captureOutput:didFinishProcessingPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:
 @abstract
    A callback fired when the primary processed photo or photos are done.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param photoSampleBuffer
    A CMSampleBuffer containing an uncompressed pixel buffer or compressed data, along with timing information and metadata. May be nil if there was an error.
 @param previewPhotoSampleBuffer
    An optional CMSampleBuffer containing an uncompressed, down-scaled preview pixel buffer. Note that the preview sample buffer contains no metadata. Refer to the photoSampleBuffer for metadata (e.g., the orientation). May be nil.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 @param bracketSettings
    If this image is being delivered as part of a bracketed capture, the bracketSettings corresponding to this image. Otherwise nil.
 @param error
    An error indicating what went wrong if photoSampleBuffer is nil.
 
 @discussion
    If you've requested a single processed image (uncompressed or compressed) capture, the photo is delivered here. If you've requested a bracketed capture, this callback is fired bracketedSettings.count times (once for each photo in the bracket).
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishProcessingPhotoSampleBuffer:(nullable CMSampleBufferRef)photoSampleBuffer previewPhotoSampleBuffer:(nullable CMSampleBufferRef)previewPhotoSampleBuffer resolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings bracketSettings:(nullable AVCaptureBracketedStillImageSettings *)bracketSettings error:(nullable NSError *)error API_DEPRECATED("Use -captureOutput:didFinishProcessingPhoto:error: instead.", ios(10.0, 11.0));

/*!
 @method captureOutput:didFinishProcessingRawPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error:
 @abstract
    A callback fired when the RAW photo or photos are done.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param rawSampleBuffer
    A CMSampleBuffer containing Bayer RAW pixel data, along with timing information and metadata. May be nil if there was an error.
 @param previewPhotoSampleBuffer
    An optional CMSampleBuffer containing an uncompressed, down-scaled preview pixel buffer. Note that the preview sample buffer contains no metadata. Refer to the rawSampleBuffer for metadata (e.g., the orientation). May be nil.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 @param bracketSettings
    If this image is being delivered as part of a bracketed capture, the bracketSettings corresponding to this image. Otherwise nil.
 @param error
    An error indicating what went wrong if rawSampleBuffer is nil.
 
 @discussion
    Single RAW image and bracketed RAW photos are delivered here. If you've requested a RAW bracketed capture, this callback is fired bracketedSettings.count times (once for each photo in the bracket).
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishProcessingRawPhotoSampleBuffer:(nullable CMSampleBufferRef)rawSampleBuffer previewPhotoSampleBuffer:(nullable CMSampleBufferRef)previewPhotoSampleBuffer resolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings bracketSettings:(nullable AVCaptureBracketedStillImageSettings *)bracketSettings error:(nullable NSError *)error API_DEPRECATED("Use -captureOutput:didFinishProcessingPhoto:error: instead.", ios(10.0, 11.0));

/*!
 @method captureOutput:didFinishRecordingLivePhotoMovieForEventualFileAtURL:resolvedSettings:
 @abstract
    A callback fired when the Live Photo movie has captured all its media data, though all media has not yet been written to file.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param outputFileURL
    The URL to which the movie file will be written. This URL is equal to your AVCapturePhotoSettings.livePhotoMovieURL.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 
 @discussion
    When this callback fires, no new media is being written to the file. If you are displaying a "Live" badge, this is an appropriate time to dismiss it. The movie file itself is not done being written until the -captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error: callback fires.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishRecordingLivePhotoMovieForEventualFileAtURL:(NSURL *)outputFileURL resolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings;

/*!
 @method captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error:
 @abstract
    A callback fired when the Live Photo movie is finished being written to disk.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param outputFileURL
    The URL where the movie file resides. This URL is equal to your AVCapturePhotoSettings.livePhotoMovieURL.
 @param duration
    A CMTime indicating the duration of the movie file.
 @param photoDisplayTime
    A CMTime indicating the time in the movie at which the still photo should be displayed.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features have been selected.
 @param error
    An error indicating what went wrong if the outputFileURL is damaged.
 
 @discussion
    When this callback fires, the movie on disk is fully finished and ready for consumption.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishProcessingLivePhotoToMovieFileAtURL:(NSURL *)outputFileURL duration:(CMTime)duration photoDisplayTime:(CMTime)photoDisplayTime resolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings error:(nullable NSError *)error;

/*!
 @method captureOutput:didFinishCaptureForResolvedSettings:error:
 @abstract
    A callback fired when the photo capture is completed and no more callbacks will be fired.
 
 @param output
    The calling instance of AVCapturePhotoOutput.
 @param resolvedSettings
    An instance of AVCaptureResolvedPhotoSettings indicating which capture features were selected.
 @param error
    An error indicating whether the capture was unsuccessful. Nil if there were no problems.
 
 @discussion
    This callback always fires last and when it does, you may clean up any state relating to this photo capture.
 */
- (void)captureOutput:(AVCapturePhotoOutput *)output didFinishCaptureForResolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings error:(nullable NSError *)error;

@end


#pragma mark - AVCapturePhotoSettings

@class AVCapturePhotoSettingsInternal;

/*!
 @class AVCapturePhotoSettings
 @abstract
    A mutable settings object encapsulating all the desired properties of a photo capture.
 
 @discussion
    To take a picture, a client instantiates and configures an AVCapturePhotoSettings object, then calls AVCapturePhotoOutput's -capturePhotoWithSettings:delegate:, passing the settings and a delegate to be informed when events relating to the photo capture occur. Since AVCapturePhotoSettings has no reference to the AVCapturePhotoOutput instance with which it will be used, minimal validation occurs while you configure an AVCapturePhotoSettings instance. The bulk of the validation is executed when you call AVCapturePhotoOutput's -capturePhotoWithSettings:delegate:.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCapturePhotoSettings : NSObject <NSCopying>
{
@private
    AVCapturePhotoSettingsInternal *_internal;
}

/*!
 @method photoSettings
 @abstract
    Creates a default instance of AVCapturePhotoSettings.
 
 @result
    An instance of AVCapturePhotoSettings.
 
 @discussion
    A default AVCapturePhotoSettings object has a format of AVVideoCodecTypeJPEG, a fileType of AVFileTypeJPEG, and autoStillImageStabilizationEnabled set to YES.
 */
+ (instancetype)photoSettings;

/*!
 @method photoSettingsWithFormat:
 @abstract
    Creates an instance of AVCapturePhotoSettings with a user-specified output format.
 
 @param format
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property.
 @result
    An instance of AVCapturePhotoSettings.
 
 @discussion
    If you wish an uncompressed format, your dictionary must contain kCVPixelBufferPixelFormatTypeKey, and the format specified must be present in AVCapturePhotoOutput's -availablePhotoPixelFormatTypes array. kCVPixelBufferPixelFormatTypeKey is the only supported key when expressing uncompressed output. If you wish a compressed format, your dictionary must contain AVVideoCodecKey and the codec specified must be present in AVCapturePhotoOutput's -availablePhotoCodecTypes array. If you are specifying a compressed format, the AVVideoCompressionPropertiesKey is also supported, with a payload dictionary containing a single AVVideoQualityKey. Passing a nil format dictionary is analogous to calling +photoSettings.
 */
+ (instancetype)photoSettingsWithFormat:(nullable NSDictionary<NSString *, id> *)format;

/*!
 @method photoSettingsWithRawPixelFormatType:
 @abstract
    Creates an instance of AVCapturePhotoSettings specifying RAW only output.
 
 @param rawPixelFormatType
    A Bayer RAW pixel format OSType (defined in CVPixelBuffer.h).
 @result
    An instance of AVCapturePhotoSettings.

 @discussion
    rawPixelFormatType must be one of the OSTypes contained in AVCapturePhotoOutput's -availableRawPhotoPixelFormatTypes array. See AVCapturePhotoOutput's -capturePhotoWithSettings:delegate: inline documentation for a discussion of restrictions on AVCapturePhotoSettings when requesting RAW capture.
 */
+ (instancetype)photoSettingsWithRawPixelFormatType:(OSType)rawPixelFormatType;

/*!
 @method photoSettingsWithRawPixelFormatType:processedFormat:
 @abstract
    Creates an instance of AVCapturePhotoSettings specifying RAW + a processed format (such as JPEG).
 
 @param rawPixelFormatType
    A Bayer RAW pixel format OSType (defined in CVPixelBuffer.h).
 @param processedFormat
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property.
 @result
    An instance of AVCapturePhotoSettings.
 
 @discussion
    rawPixelFormatType must be one of the OSTypes contained in AVCapturePhotoOutput's -availableRawPhotoPixelFormatTypes array. If you wish an uncompressed processedFormat, your dictionary must contain kCVPixelBufferPixelFormatTypeKey, and the processedFormat specified must be present in AVCapturePhotoOutput's -availablePhotoPixelFormatTypes array. kCVPixelBufferPixelFormatTypeKey is the only supported key when expressing uncompressed processedFormat. If you wish a compressed format, your dictionary must contain AVVideoCodecKey and the codec specified must be present in AVCapturePhotoOutput's -availablePhotoCodecTypes array. If you are specifying a compressed format, the AVVideoCompressionPropertiesKey is also supported, with a payload dictionary containing a single AVVideoQualityKey. Passing a nil processedFormat dictionary is analogous to calling +photoSettingsWithRawPixelFormatType:. See AVCapturePhotoOutput's -capturePhotoWithSettings:delegate: inline documentation for a discussion of restrictions on AVCapturePhotoSettings when requesting RAW capture.
 */
+ (instancetype)photoSettingsWithRawPixelFormatType:(OSType)rawPixelFormatType processedFormat:(nullable NSDictionary<NSString *, id> *)processedFormat;

/*!
 @method photoSettingsWithRawPixelFormatType:processedFormat:fileType:
 @abstract
    Creates an instance of AVCapturePhotoSettings specifying RAW + a processed format (such as JPEG) and a file container to which it will be written.
 
 @param rawPixelFormatType
    A Bayer RAW pixel format OSType (defined in CVPixelBuffer.h). Pass 0 if you do not desire a RAW photo callback.
 @param rawFileType
    The file container for which the RAW image should be formatted to be written. Pass nil if you have no preferred file container. A default container will be chosen for you.
 @param processedFormat
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property. Pass nil if you do not desire a processed photo callback.
 @param processedFileType
    The file container for which the processed image should be formatted to be written. Pass nil if you have no preferred file container. A default container will be chosen for you.
 @result
    An instance of AVCapturePhotoSettings.
 
 @discussion
    rawPixelFormatType must be one of the OSTypes contained in AVCapturePhotoOutput's -availableRawPhotoPixelFormatTypes array. Set rawPixelFormatType to 0 if you do not desire a RAW photo callback. If you are specifying a rawFileType, it must be present in AVCapturePhotoOutput's -availableRawPhotoFileTypes array. If you wish an uncompressed processedFormat, your dictionary must contain kCVPixelBufferPixelFormatTypeKey, and the processedFormat specified must be present in AVCapturePhotoOutput's -availablePhotoPixelFormatTypes array. kCVPixelBufferPixelFormatTypeKey is the only supported key when expressing uncompressed processedFormat. If you wish a compressed format, your dictionary must contain AVVideoCodecKey and the codec specified must be present in AVCapturePhotoOutput's -availablePhotoCodecTypes array. If you are specifying a compressed format, the AVVideoCompressionPropertiesKey is also supported, with a payload dictionary containing a single AVVideoQualityKey. If you are specifying a processedFileType, it must be present in AVCapturePhotoOutput's -availablePhotoFileTypes array. Pass a nil processedFormat dictionary if you only desire a RAW photo capture. See AVCapturePhotoOutput's -capturePhotoWithSettings:delegate: inline documentation for a discussion of restrictions on AVCapturePhotoSettings when requesting RAW capture.
 */
+ (instancetype)photoSettingsWithRawPixelFormatType:(OSType)rawPixelFormatType rawFileType:(nullable AVFileType)rawFileType processedFormat:(nullable NSDictionary<NSString *, id> *)processedFormat processedFileType:(nullable AVFileType)processedFileType API_AVAILABLE(ios(11.0));

/*!
 @method photoSettingsFromPhotoSettings:
 @abstract
    Creates an instance of AVCapturePhotoSettings with a new uniqueID from an existing instance of AVCapturePhotoSettings.
 
 @param photoSettings
     An existing AVCapturePhotoSettings instance.
 @result
    An new instance of AVCapturePhotoSettings with new uniqueID.
 
 @discussion
    Use this factory method to create a clone of an existing photo settings instance, but with a new uniqueID that can safely be passed to AVCapturePhotoOutput -capturePhotoWithSettings:delegate:.
 */
+ (instancetype)photoSettingsFromPhotoSettings:(AVCapturePhotoSettings *)photoSettings;

/*!
 @property uniqueID
 @abstract
    A 64-bit number that uniquely identifies this instance.

 @discussion
    When you create an instance of AVCapturePhotoSettings, a uniqueID is generated automatically. This uniqueID is guaranteed to be unique for the life time of your process.
 */
@property(readonly) int64_t uniqueID;

/*!
 @property format
 @abstract
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property.

 @discussion
    The format dictionary you passed to one of the creation methods. May be nil if you've specified RAW-only capture.
 */
@property(readonly, copy, nullable) NSDictionary<NSString *, id> *format;

/*!
 @property processedFileType
 @abstract
    The file container for which the processed photo is formatted to be stored.

 @discussion
    The formatting of data within a photo buffer is often dependent on the file format intended for storage. For instance, a JPEG encoded photo buffer intended for storage in a JPEG (JPEG File Interchange Format) file differs from JPEG to be stored in HEIF. The HEIF-containerized JPEG buffer is tiled for readback efficiency and partitioned into the box structure dictated by the HEIF file format. Some codecs are only supported by AVCapturePhotoOutput if containerized. For instance, the AVVideoCodecTypeHEVC is only supported with AVFileTypeHEIF formatting. To discover which photo pixel format types and video codecs are supported for a given file type, you may query AVCapturePhotoOutput's -supportedPhotoPixelFormatTypesForFileType:, or -supportedPhotoCodecTypesForFileType: respectively.
 */
@property(nullable, readonly) AVFileType processedFileType API_AVAILABLE(ios(11.0));

/*!
 @property rawPhotoPixelFormatType
 @abstract
    A Bayer RAW pixel format OSType (defined in CVPixelBuffer.h).

 @discussion
    The rawPixelFormatType you specified in one of the creation methods. Returns 0 if you did not specify RAW capture. See AVCapturePhotoOutput's -capturePhotoWithSettings:delegate: inline documentation for a discussion of restrictions on AVCapturePhotoSettings when requesting RAW capture.
 */
@property(readonly) OSType rawPhotoPixelFormatType;

/*!
 @property rawFileType
 @abstract
    The file container for which the RAW photo is formatted to be stored.

 @discussion
    The formatting of data within a RAW photo buffer may be dependent on the file format intended for storage. To discover which RAW photo pixel format types are supported for a given file type, you may query AVCapturePhotoOutput's -supportedRawPhotoPixelFormatTypesForFileType:.
 */
@property(nullable, readonly) AVFileType rawFileType API_AVAILABLE(ios(11.0));

/*!
 @property flashMode
 @abstract
    Specifies whether the flash should be on, off, or chosen automatically by AVCapturePhotoOutput.

 @discussion
    flashMode takes the place of the deprecated AVCaptureDevice -flashMode API. Setting AVCaptureDevice.flashMode has no effect on AVCapturePhotoOutput, which only pays attention to the flashMode specified in your AVCapturePhotoSettings. The default value is AVCaptureFlashModeOff. Flash modes are defined in AVCaptureDevice.h. If you specify a flashMode of AVCaptureFlashModeOn, it wins over autoStillImageStabilizationEnabled=YES. When the device becomes very hot, the flash becomes temporarily unavailable until the device cools down (see AVCaptureDevice's -flashAvailable). While the flash is unavailable, AVCapturePhotoOutput's -supportedFlashModes property still reports AVCaptureFlashModeOn and AVCaptureFlashModeAuto as being available, thus allowing you to specify a flashMode of AVCaptureModeOn. You should always check the AVCaptureResolvedPhotoSettings provided to you in the AVCapturePhotoCaptureDelegate callbacks, as the resolved flashEnabled property will tell you definitively if the flash is being used.
 */
@property(nonatomic) AVCaptureFlashMode flashMode;


/*!
 @property autoRedEyeReductionEnabled
 @abstract
    Specifies whether red-eye reduction should be applied automatically on flash captures.
 
 @discussion
    Default is YES on platforms that support automatic red-eye reduction unless you are capturing a bracket using AVCapturePhotoBracketSettings or a RAW photo without a processed photo.  For RAW photos with a processed photo the red-eye reduction will be applied to the processed photo only (RAW photos by definition are not processed). When set to YES, red-eye reduction is applied as needed for flash captures if the photo output's autoRedEyeReductionSupported property returns YES.
 */
@property(nonatomic, getter=isAutoRedEyeReductionEnabled) BOOL autoRedEyeReductionEnabled API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, watchos, tvos);

/*!
 @property autoStillImageStabilizationEnabled
 @abstract
    Specifies whether still image stabilization should be used automatically.

 @discussion
    Default is YES unless you are capturing a RAW photo (RAW photos may not be processed by definition) or a bracket using AVCapturePhotoBracketSettings. When set to YES, still image stabilization is applied automatically in low light to counteract hand shake. If the device has optical image stabilization, autoStillImageStabilizationEnabled makes use of lens stabilization as well.
 */
@property(nonatomic, getter=isAutoStillImageStabilizationEnabled) BOOL autoStillImageStabilizationEnabled;

/*!
 @property autoDualCameraFusionEnabled
 @abstract
    Specifies whether DualCamera image fusion should be used automatically.

 @discussion
    Default is YES unless you are capturing a RAW photo (RAW photos may not be processed by definition) or a bracket using AVCapturePhotoBracketSettings. When set to YES, and -[AVCapturePhotoOutput isDualCameraFusionSupported] is also YES, wide-angle and telephoto images may be fused to improve still image quality, depending on the current zoom factor, light levels, and focus position. You may determine whether DualCamera fusion is enabled for a particular capture request by inspecting the dualCameraFusionEnabled property of the AVCaptureResolvedPhotoSettings. Note that when using the deprecated AVCaptureStillImageOutput interface with the DualCamera, auto DualCamera fusion is always enabled and may not be turned off.
 */
@property(nonatomic, getter=isAutoDualCameraFusionEnabled) BOOL autoDualCameraFusionEnabled API_AVAILABLE(ios(10.2));

/*!
 @property dualCameraDualPhotoDeliveryEnabled
 @abstract
    Specifies whether the DualCamera should return both the telephoto and wide image.

 @discussion
    Default is NO. When set to YES, your captureOutput:didFinishProcessingPhoto:error: callback will receive twice the number of callbacks, as both the telephoto image(s) and wide-angle image(s) are delivered. You may only set this property to YES if you've set your AVCapturePhotoOutput's dualCameraDualPhotoDeliveryEnabled property to YES, and your delegate responds to the captureOutput:didFinishProcessingPhoto:error: selector.
 */
@property(nonatomic, getter=isDualCameraDualPhotoDeliveryEnabled) BOOL dualCameraDualPhotoDeliveryEnabled API_AVAILABLE(ios(11.0));

/*!
 @property highResolutionPhotoEnabled
 @abstract
    Specifies whether photos should be captured at the highest resolution supported by the source AVCaptureDevice's activeFormat.

 @discussion
    Default is NO. By default, AVCapturePhotoOutput emits images with the same dimensions as its source AVCaptureDevice's activeFormat.formatDescription. However, if you set this property to YES, the AVCapturePhotoOutput emits images at its source AVCaptureDevice's activeFormat.highResolutionStillImageDimensions. Note that if you enable video stabilization (see AVCaptureConnection's preferredVideoStabilizationMode) for any output, the high resolution photos emitted by AVCapturePhotoOutput may be smaller by 10 or more percent. You may inspect your AVCaptureResolvedPhotoSettings in the delegate callbacks to discover the exact dimensions of the capture photo(s).
 */
@property(nonatomic, getter=isHighResolutionPhotoEnabled) BOOL highResolutionPhotoEnabled;

/*!
 @property depthDataDeliveryEnabled
 @abstract
    Specifies whether AVDepthData should be captured along with the photo.

 @discussion
    Default is NO. Set to YES if you wish to receive depth data with your photo. Throws an exception if -[AVCapturePhotoOutput depthDataDeliveryEnabled] is not set to YES or your delegate does not respond to the captureOutput:didFinishProcessingPhoto:error: selector. Note that setting this property to YES may add significant processing time to the delivery of your didFinishProcessingPhoto: callback.
 
    For best rendering results in Apple's Photos.app, portrait photos should be captured with both embedded depth data and a portrait effects matte (see portraitEffectsMatteDeliveryEnabled). When supported, it is recommended to opt in for both of these auxiliary images in your photo captures involving depth.
 */
@property(nonatomic, getter=isDepthDataDeliveryEnabled) BOOL depthDataDeliveryEnabled API_AVAILABLE(ios(11.0));

/*!
 @property embedsDepthDataInPhoto
 @abstract
    Specifies whether depth data included with this photo should be written to the photo's file structure.

 @discussion
    Default is YES. When depthDataDeliveryEnabled is set to YES, this property specifies whether the included depth data should be written to the resulting photo's internal file structure. Depth data is currently only supported in HEIF and JPEG. This property is ignored if depthDataDeliveryEnabled is set to NO.
 */
@property(nonatomic) BOOL embedsDepthDataInPhoto API_AVAILABLE(ios(11.0));

/*!
 @property depthDataFiltered
 @abstract
    Specifies whether the depth data delivered with the photo should be filtered to fill invalid values.

 @discussion
    Default is YES. This property is ignored unless depthDataDeliveryEnabled is set to YES. Depth data maps may contain invalid pixel values due to a variety of factors including occlusions and low light. When depthDataFiltered is set to YES, the photo output interpolates missing data, filling in all holes.
 */
@property(nonatomic, getter=isDepthDataFiltered) BOOL depthDataFiltered API_AVAILABLE(ios(11.0));

/*!
 @property cameraCalibrationDataDeliveryEnabled
 @abstract
    Specifies whether AVCameraCalibrationData should be captured and delivered along with this photo.

 @discussion
    Default is NO. Set to YES if you wish to receive camera calibration data with your photo. Camera calibration data is delivered as a property of an AVCapturePhoto, so if you are using the CMSampleBuffer delegate callbacks rather than -captureOutput:didFinishProcessingPhoto:error:, an exception is thrown. Also, you may only set this property to YES if your AVCapturePhotoOutput's cameraCalibrationDataDeliverySupported property is YES. When requesting dual camera dual photo delivery plus camera calibration data, the wide and tele photos each contain camera calibration data for their respective camera. Note that AVCameraCalibrationData can be delivered as a property of an AVCapturePhoto or an AVDepthData, thus your delegate must respond to the captureOutput:didFinishProcessingPhoto:error: selector.
 */
@property(nonatomic, getter=isCameraCalibrationDataDeliveryEnabled) BOOL cameraCalibrationDataDeliveryEnabled API_AVAILABLE(ios(11.0));

/*!
 @property portraitEffectsMatteDeliveryEnabled
 @abstract
    Specifies whether an AVPortraitEffectsMatte should be captured along with the photo.

 @discussion
    Default is NO. Set to YES if you wish to receive a portrait effects matte with your photo. Throws an exception if -[AVCapturePhotoOutput portraitEffectsMatteDeliveryEnabled] is not set to YES or your delegate does not respond to the captureOutput:didFinishProcessingPhoto:error: selector. Portrait effects matte generation requires depth to be present, so if you wish to enable portrait effects matte delivery, you must set depthDataDeliveryEnabled to YES. Setting this property to YES does not guarantee that a portrait effects matte will be present in the resulting AVCapturePhoto. As the property name implies, the matte is primarily used to improve the rendering quality of portrait effects on the image. If the photo's content lacks a clear foreground subject, no portrait effects matte is generated, and the -[AVCapturePhoto portraitEffectsMatte] property returns nil. Note that setting this property to YES may add significant processing time to the delivery of your didFinishProcessingPhoto: callback.
 
    For best rendering results in Apple's Photos.app, portrait photos should be captured with both embedded depth data (see depthDataDeliveryEnabled) and a portrait effects matte. When supported, it is recommended to opt in for both of these auxiliary images in your photo captures involving depth.
 */
@property(nonatomic, getter=isPortraitEffectsMatteDeliveryEnabled) BOOL portraitEffectsMatteDeliveryEnabled API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property embedsPortraitEffectsMatteInPhoto
 @abstract
    Specifies whether the portrait effects matte captured with this photo should be written to the photo's file structure.

 @discussion
    Default is YES. When portraitEffectsMatteDeliveryEnabled is set to YES, this property specifies whether the included portrait effects matte should be written to the resulting photo's internal file structure. Portrait effects mattes are currently only supported in HEIF and JPEG. This property is ignored if portraitEffectsMatteDeliveryEnabled is set to NO.
 */
@property(nonatomic) BOOL embedsPortraitEffectsMatteInPhoto API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property metadata
 @abstract
    A dictionary of metadata key/value pairs you'd like to have written to each photo in the capture request.

 @discussion
    Valid metadata keys are found in <ImageIO/CGImageProperties.h>. AVCapturePhotoOutput inserts a base set of metadata into each photo it captures, such as kCGImagePropertyOrientation, kCGImagePropertyExifDictionary, and kCGImagePropertyMakerAppleDictionary. You may specify metadata keys and values that should be written to each photo in the capture request. If you've specified metadata that also appears in AVCapturePhotoOutput's base set, your value replaces the base value. An NSInvalidArgumentException is thrown if you specify keys other than those found in <ImageIO/CGImageProperties.h>.
 */
@property(nonatomic, copy) NSDictionary<NSString *, id> *metadata API_AVAILABLE(ios(11.0));

/*!
 @property livePhotoMovieFileURL
 @abstract
    Specifies that a Live Photo movie be captured to complement the still photo.

 @discussion
    A Live Photo movie is a short movie (with audio, if you've added an audio input to your session) containing the moments right before and after the still photo. A QuickTime movie file will be written to disk at the URL specified if it is a valid file URL accessible to your app's sandbox. You may only set this property if AVCapturePhotoOutput's livePhotoCaptureSupported property is YES. When you specify a Live Photo, your AVCapturePhotoCaptureDelegate object must implement -captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error:.
 */
@property(nonatomic, copy, nullable) NSURL *livePhotoMovieFileURL;

/*!
 @property livePhotoVideoCodecType
 @abstract
    Specifies the video codec type to use when compressing video for the Live Photo movie complement.

 @discussion
    Prior to iOS 11, all Live Photo movie video tracks are compressed using H.264. Beginning in iOS 11, you can select the Live Photo movie video compression format by specifying one of the strings present in AVCapturePhotoOutput's availableLivePhotoVideoCodecTypes array.
 */
@property(nonatomic, copy) AVVideoCodecType livePhotoVideoCodecType API_AVAILABLE(ios(11.0));

/*!
 @property livePhotoMovieMetadata
 @abstract
    Movie-level metadata to be written to the Live Photo movie.

 @discussion
    An array of AVMetadataItems to be inserted into the top level of the Live Photo movie. The receiver makes immutable copies of the AVMetadataItems in the array. Live Photo movies always contain a AVMetadataQuickTimeMetadataKeyContentIdentifier which allow them to be paired with a similar identifier in the MakerNote of the photo complement. AVCapturePhotoSettings generates a unique content identifier for you. If you provide a metadata array containing an AVMetadataItem with keyspace = AVMetadataKeySpaceQuickTimeMetadata and key = AVMetadataQuickTimeMetadataKeyContentIdentifier, an NSInvalidArgumentException is thrown.
 */
@property(nonatomic, copy, null_resettable) NSArray<AVMetadataItem *> *livePhotoMovieMetadata;

/*!
 @property availablePreviewPhotoPixelFormatTypes
 @abstract
    An array of available kCVPixelBufferPixelFormatTypeKeys that may be used when specifying a previewPhotoFormat.
 
 @discussion
    The array is sorted such that the preview format requiring the fewest conversions is presented first.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *availablePreviewPhotoPixelFormatTypes;

/*!
 @property previewPhotoFormat
 @abstract
    A dictionary of Core Video pixel buffer attributes specifying the preview photo format to be delivered along with the RAW or processed photo.

 @discussion
    A dictionary of pixel buffer attributes specifying a smaller version of the RAW or processed photo for preview purposes. The kCVPixelBufferPixelFormatTypeKey is required and must be present in the receiver's -availablePreviewPhotoPixelFormatTypes array. Optional keys are { kCVPixelBufferWidthKey | kCVPixelBufferHeightKey }. If you wish to specify dimensions, you must add both width and height. Width and height are only honored up to the display dimensions. If you specify a width and height whose aspect ratio differs from the RAW or processed photo, the larger of the two dimensions is honored and aspect ratio of the RAW or processed photo is always preserved.
 */
@property(nonatomic, copy, nullable) NSDictionary<NSString *, id> *previewPhotoFormat;

/*!
 @property availableEmbeddedThumbnailPhotoCodecTypes
 @abstract
    An array of available AVVideoCodecKeys that may be used when specifying an embeddedThumbnailPhotoFormat.
 
 @discussion
    The array is sorted such that the thumbnail codec type that is most backward compatible is listed first.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableEmbeddedThumbnailPhotoCodecTypes API_AVAILABLE(ios(11.0));

/*!
 @property embeddedThumbnailPhotoFormat
 @abstract
    A dictionary of AVVideoSettings keys specifying the thumbnail format to be written to the processed or RAW photo.

 @discussion
    A dictionary of AVVideoSettings keys specifying a thumbnail (usually smaller) version of the processed photo to be embedded in that image before calling the AVCapturePhotoCaptureDelegate. This image is sometimes referred to as a "thumbnail image". The AVVideoCodecKey is required and must be present in the receiver's -availableEmbeddedThumbnailPhotoCodecTypes array. Optional keys are { AVVideoWidthKey | AVVideoHeightKey }. If you wish to specify dimensions, you must specify both width and height. If you specify a width and height whose aspect ratio differs from the processed photo, the larger of the two dimensions is honored and aspect ratio of the RAW or processed photo is always preserved. For RAW captures, use -rawEmbeddedThumbnailPhotoFormat to specify the thumbnail format you'd like to capture in the RAW image. For apps linked on or after iOS 12, the raw thumbnail format must be specified using the -rawEmbeddedThumbnailPhotoFormat API rather than -embeddedThumbnailPhotoFormat. Beginning in iOS 12, HEIC files may contain thumbnails up to the full resolution of the main image.
 */
@property(nonatomic, copy, nullable) NSDictionary<NSString *, id> *embeddedThumbnailPhotoFormat API_AVAILABLE(ios(11.0));

/*!
 @property availableRawEmbeddedThumbnailPhotoCodecTypes
 @abstract
    An array of available AVVideoCodecKeys that may be used when specifying a rawEmbeddedThumbnailPhotoFormat.
 
 @discussion
    The array is sorted such that the thumbnail codec type that is most backward compatible is listed first.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableRawEmbeddedThumbnailPhotoCodecTypes API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property rawEmbeddedThumbnailPhotoFormat
 @abstract
    A dictionary of AVVideoSettings keys specifying the thumbnail format to be written to the RAW photo in a RAW photo request.

 @discussion
    A dictionary of AVVideoSettings keys specifying a thumbnail (usually smaller) version of the RAW photo to be embedded in that image's DNG before calling back the AVCapturePhotoCaptureDelegate. The AVVideoCodecKey is required and must be present in the receiver's -availableRawEmbeddedThumbnailPhotoCodecTypes array. Optional keys are { AVVideoWidthKey | AVVideoHeightKey }. If you wish to specify dimensions, you must specify both width and height. If you specify a width and height whose aspect ratio differs from the RAW or processed photo, the larger of the two dimensions is honored and aspect ratio of the RAW or processed photo is always preserved. For apps linked on or after iOS 12, the raw thumbnail format must be specified using the -rawEmbeddedThumbnailPhotoFormat API rather than -embeddedThumbnailPhotoFormat. Beginning in iOS 12, DNG files may contain thumbnails up to the full resolution of the RAW image.
 */
@property(nonatomic, copy, nullable) NSDictionary<NSString *, id> *rawEmbeddedThumbnailPhotoFormat API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

@end


#pragma mark - AVCapturePhotoBracketSettings

@class AVCapturePhotoBracketSettingsInternal;

/*!
 @class AVCapturePhotoBracketSettings
 @abstract
    A concrete subclass of AVCapturePhotoSettings that describes a bracketed capture.
 
 @discussion
    In addition to the properties expressed in the base class, an AVCapturePhotoBracketSettings contains an array of AVCaptureBracketedStillImageSettings objects, where each describes one individual photo in the bracket. bracketedSettings.count must be <= AVCapturePhotoOutput's -maxBracketedCapturePhotoCount. Capturing a photo bracket may require the allocation of additional resources.

    When you request a bracketed capture, your AVCapturePhotoCaptureDelegate's -captureOutput:didFinishProcessing{Photo | RawPhoto}... callbacks are called back bracketSettings.count times and provided with the corresponding AVCaptureBracketedStillImageSettings object from your request.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCapturePhotoBracketSettings : AVCapturePhotoSettings
{
@private
    AVCapturePhotoBracketSettingsInternal *_bracketSettingsInternal;
}

/*!
 @method photoBracketSettingsWithRawPixelFormatType:processedFormat:bracketedSettings:
 @abstract
    Creates an instance of AVCapturePhotoBracketSettings.
 
 @param rawPixelFormatType
    One of the OSTypes contained in AVCapturePhotoOutput's -availableRawPhotoPixelFormatTypes array. May be set to 0 if you do not desire RAW capture.
 @param processedFormat
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property. If you wish an uncompressed format, your dictionary must contain kCVPixelBufferPixelFormatTypeKey, and the format specified must be present in AVCapturePhotoOutput's -availablePhotoPixelFormatTypes array. kCVPixelBufferPixelFormatTypeKey is the only supported key when expressing uncompressed output. If you wish a compressed format, your dictionary must contain AVVideoCodecKey and the codec specified must be present in AVCapturePhotoOutput's -availablePhotoCodecTypes array. If you are specifying a compressed format, the AVVideoCompressionPropertiesKey is also supported, with a payload dictionary containing a single AVVideoQualityKey. If you only wish to capture RAW, you may pass a non-zero rawPixelFormatType and a nil processedFormat dictionary. If you pass a rawPixelFormatType of 0 AND a nil processedFormat dictionary, the default output of AVVideoCodecTypeJPEG will be delivered.
 @param bracketedSettings
    An array of AVCaptureBracketedStillImageSettings objects (defined in AVCaptureStillImageOutput.h). All must be of the same type, either AVCaptureManualExposureBracketedStillImageSettings or AVCaptureAutoExposureBracketedStillImageSettings. bracketedSettings.count must be <= AVCapturePhotoOutput's -maxBracketedCapturePhotoCount.
 @result
    An instance of AVCapturePhotoBracketSettings.

 @discussion
    An NSInvalidArgumentException is thrown if bracketedSettings is nil, contains zero elements, or mixes and matches different subclasses of AVCaptureBracketedStillImageSettings.
 
    AVCapturePhotoBracketSettings do not support flashMode, autoStillImageStabilizationEnabled, livePhotoMovieFileURL or livePhotoMovieMetadata.
 */
+ (instancetype)photoBracketSettingsWithRawPixelFormatType:(OSType)rawPixelFormatType processedFormat:(nullable NSDictionary<NSString *, id> *)processedFormat bracketedSettings:(NSArray<__kindof AVCaptureBracketedStillImageSettings *> *)bracketedSettings;

/*!
 @method photoBracketSettingsWithRawPixelFormatType:rawFileType:processedFormat:processedFileType:bracketedSettings:
 @abstract
    Creates an instance of AVCapturePhotoBracketSettings.
 
 @param rawPixelFormatType
    One of the OSTypes contained in AVCapturePhotoOutput's -availableRawPhotoPixelFormatTypes array. May be set to 0 if you do not desire RAW capture.
 @param rawFileType
    The file container for which the RAW image should be formatted to be written. Pass nil if you have no preferred file container. A default container will be chosen for you.
 @param processedFormat
    A dictionary of Core Video pixel buffer attributes or AVVideoSettings, analogous to AVCaptureStillImageOutput's outputSettings property. If you wish an uncompressed format, your dictionary must contain kCVPixelBufferPixelFormatTypeKey, and the format specified must be present in AVCapturePhotoOutput's -availablePhotoPixelFormatTypes array. kCVPixelBufferPixelFormatTypeKey is the only supported key when expressing uncompressed output. If you wish a compressed format, your dictionary must contain AVVideoCodecKey and the codec specified must be present in AVCapturePhotoOutput's -availablePhotoCodecTypes array. If you are specifying a compressed format, the AVVideoCompressionPropertiesKey is also supported, with a payload dictionary containing a single AVVideoQualityKey. If you only wish to capture RAW, you may pass a non-zero rawPixelFormatType and a nil processedFormat dictionary. If you pass a rawPixelFormatType of 0 AND a nil processedFormat dictionary, the default output of AVVideoCodecTypeJPEG will be delivered.
 @param processedFileType
    The file container for which the processed image should be formatted to be written. Pass nil if you have no preferred file container. A default container will be chosen for you.
 @param bracketedSettings
    An array of AVCaptureBracketedStillImageSettings objects (defined in AVCaptureStillImageOutput.h). All must be of the same type, either AVCaptureManualExposureBracketedStillImageSettings or AVCaptureAutoExposureBracketedStillImageSettings. bracketedSettings.count must be <= AVCapturePhotoOutput's -maxBracketedCapturePhotoCount.
 @result
    An instance of AVCapturePhotoBracketSettings.
 
 @discussion
    An NSInvalidArgumentException is thrown if bracketedSettings is nil, contains zero elements, or mixes and matches different subclasses of AVCaptureBracketedStillImageSettings.
 
    AVCapturePhotoBracketSettings do not support flashMode, autoStillImageStabilizationEnabled, livePhotoMovieFileURL or livePhotoMovieMetadata.
 */
+ (instancetype)photoBracketSettingsWithRawPixelFormatType:(OSType)rawPixelFormatType rawFileType:(nullable AVFileType)rawFileType processedFormat:(nullable NSDictionary<NSString *, id> *)processedFormat processedFileType:(nullable AVFileType)processedFileType bracketedSettings:(NSArray<AVCaptureBracketedStillImageSettings *> *)bracketedSettings API_AVAILABLE(ios(11.0));

/*!
 @property bracketedSettings
 @abstract
    An array of AVCaptureBracketedStillImageSettings objects you passed in -initWithFormat:rawPixelFormatType:bracketedSettings:

 @discussion
    This read-only property never returns nil.
 */
@property(nonatomic, readonly) NSArray<__kindof AVCaptureBracketedStillImageSettings *> *bracketedSettings;

/*!
 @property lensStabilizationEnabled
 @abstract
    Specifies whether lens (optical) stabilization should be employed during the bracketed capture.

 @discussion
    Default value is NO. This property may only be set to YES if AVCapturePhotoOutput's isLensStabilizationDuringBracketedCaptureSupported is YES. When set to YES, AVCapturePhotoOutput holds the lens steady for the duration of the bracket to counter hand shake and produce a sharper bracket of images.
 */
@property(nonatomic, getter=isLensStabilizationEnabled) BOOL lensStabilizationEnabled;

@end


#pragma mark - AVCaptureResolvedPhotoSettings

@class AVCaptureResolvedPhotoSettingsInternal;

/*!
 @class AVCaptureResolvedPhotoSettings
 @abstract
    An immutable object produced by callbacks in each and every AVCapturePhotoCaptureDelegate protocol method.
 
 @discussion
    When you initiate a photo capture request using -capturePhotoWithSettings:delegate:, some of your settings are not yet certain. For instance, auto flash and auto still image stabilization allow the AVCapturePhotoOutput to decide just in time whether to employ flash or still image stabilization, depending on the current scene. Once the request is issued, AVCapturePhotoOutput begins the capture, resolves the uncertain settings, and in its first callback informs you of its choices through an AVCaptureResolvedPhotoSettings object. This same object is presented to all the callbacks fired for a particular photo capture request. Its uniqueID property matches that of the AVCapturePhotoSettings instance you used to initiate the photo request.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureResolvedPhotoSettings : NSObject
{
@private
    AVCaptureResolvedPhotoSettingsInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property uniqueID
 @abstract
    uniqueID matches that of the AVCapturePhotoSettings instance you passed to -capturePhotoWithSettings:delegate:.
 */
@property(readonly) int64_t uniqueID;

/*!
 @property photoDimensions
 @abstract
    The resolved dimensions of the photo buffer that will be delivered to the -captureOutput:didFinishProcessingPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error: callback.
 
 @discussion
    If you request a RAW capture with no processed companion image, photoDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions photoDimensions;

/*!
 @property rawPhotoDimensions
 @abstract
    The resolved dimensions of the RAW photo buffer that will be delivered to the -captureOutput:didFinishProcessingRawPhotoSampleBuffer:previewPhotoSampleBuffer:resolvedSettings:bracketSettings:error: callback.

 @discussion
    If you request a non-RAW capture, rawPhotoDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions rawPhotoDimensions;

/*!
 @property previewDimensions
 @abstract
    The resolved dimensions of the preview photo buffer that will be delivered to the -captureOutput:didFinishProcessing{Photo | RawPhoto}... AVCapturePhotoCaptureDelegate callbacks.

 @discussion
    If you don't request a preview image, previewDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions previewDimensions;

/*!
 @property embeddedThumbnailDimensions
 @abstract
    The resolved dimensions of the embedded thumbnail that will be written to the processed photo delivered to the -captureOutput:didFinishProcessingPhoto:error: AVCapturePhotoCaptureDelegate callback.

 @discussion
    If you don't request an embedded thumbnail image, embeddedThumbnailDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions embeddedThumbnailDimensions API_AVAILABLE(ios(11.0));

/*!
 @property rawEmbeddedThumbnailDimensions
 @abstract
    The resolved dimensions of the embedded thumbnail that will be written to the RAW photo delivered to the -captureOutput:didFinishProcessingPhoto:error: AVCapturePhotoCaptureDelegate callback.

 @discussion
    If you don't request a raw embedded thumbnail image, rawEmbeddedThumbnailDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions rawEmbeddedThumbnailDimensions API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property portraitEffectsMatteDimensions
 @abstract
    The resolved dimensions of the portrait effects matte that will be delivered to the AVCapturePhoto in the -captureOutput:didFinishProcessingPhoto:error: AVCapturePhotoCaptureDelegate callback.

 @discussion
    If you request a portrait effects matte by calling -[AVCapturePhotoSettings setPortraitEffectsMatteDeliveryEnabled:YES], portraitEffectsMatteDimensions resolve to the expected dimensions of the portrait effects matte, assuming one is generated (see -[AVCapturePhotoSettings portraitEffectsMatteDeliveryEnabled] for a discussion of why a portrait effects matte might not be delivered). If you don't request a portrait effects matte, portraitEffectsMatteDimensions always resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions portraitEffectsMatteDimensions API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, watchos, tvos);

/*!
 @property livePhotoMovieDimensions
 @abstract
    The resolved dimensions of the video track in the movie that will be delivered to the -captureOutput:didFinishProcessingLivePhotoToMovieFileAtURL:duration:photoDisplayTime:resolvedSettings:error: callback.
 
 @discussion
    If you don't request Live Photo capture, livePhotoMovieDimensions resolve to { 0, 0 }.
 */
@property(readonly) CMVideoDimensions livePhotoMovieDimensions;

/*!
 @property flashEnabled
 @abstract
    Indicates whether the flash will fire when capturing the photo.

 @discussion
    When you specify AVCaptureFlashModeAuto as you AVCapturePhotoSettings.flashMode, you don't know if flash capture will be chosen until you inspect the AVCaptureResolvedPhotoSettings flashEnabled property. If the device becomes too hot, the flash becomes temporarily unavailable. You can key-value observe AVCaptureDevice's flashAvailable property to know when this occurs. If the flash is unavailable due to thermal issues, and you specify a flashMode of AVCaptureFlashModeOn, flashEnabled still resolves to NO until the device has sufficiently cooled off.
 */
@property(readonly, getter=isFlashEnabled) BOOL flashEnabled;


/*!
 @property redEyeReductionEnabled
 @abstract
    Indicates whether red-eye reduction will be applied as necessary when capturing the photo if flashEnabled is YES.
 */
@property(readonly, getter=isRedEyeReductionEnabled) BOOL redEyeReductionEnabled API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, watchos, tvos);

/*!
 @property stillImageStabilizationEnabled
 @abstract
    Indicates whether still image stabilization will be employed when capturing the photo.
 */
@property(readonly, getter=isStillImageStabilizationEnabled) BOOL stillImageStabilizationEnabled;

/*!
 @property dualCameraFusionEnabled
 @abstract
    Indicates whether DualCamera wide-angle and telephoto image fusion will be employed when capturing the photo.
 */
@property(readonly, getter=isDualCameraFusionEnabled) BOOL dualCameraFusionEnabled API_AVAILABLE(ios(10.2));

/*!
 @property expectedPhotoCount
 @abstract
    Indicates the number of times your -captureOutput:didFinishProcessingPhoto:error: callback will be called. For instance, if you've requested an auto exposure bracket of 3 with JPEG and RAW, the expectedPhotoCount is 6.
 */
@property(readonly) NSUInteger expectedPhotoCount API_AVAILABLE(ios(11.0));

@end


#pragma mark - AVCapturePhoto

@class AVCapturePhotoInternal;
@class AVDepthData;
@class AVCameraCalibrationData;

/*!
 @class AVCapturePhoto
 @abstract
    An object representing a photo in memory, produced by the -captureOutput:didFinishingProcessingPhoto:error: in the AVCapturePhotoCaptureDelegate protocol method.
 
 @discussion
    Beginning in iOS 11, AVCapturePhotoOutput's AVCapturePhotoCaptureDelegate supports a simplified callback for delivering image data, namely -captureOutput:didFinishingProcessingPhoto:error:. This callback presents each image result for your capture request as an AVCapturePhoto object, an immutable wrapper from which various properties of the photo capture may be queried, such as the photo's preview pixel buffer, metadata, depth data, camera calibration data, and image bracket specific properties. AVCapturePhoto can wrap file-containerized photo results, such as HEVC encoded image data, containerized in the HEIC file format. CMSampleBufferRef, on the other hand, may only be used to express non file format containerized photo data. For this reason, the AVCapturePhotoCaptureDelegate protocol methods that return CMSampleBuffers have been deprecated in favor of -captureOutput:didFinishingProcessingPhoto:error:. A AVCapturePhoto wraps a single image result. For instance, if you've requested a bracketed capture of 3 images, your callback is called 3 times, each time delivering an AVCapturePhoto.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCapturePhoto : NSObject
{
@private
    AVCapturePhotoInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property timestamp
 @abstract
    The time at which this image was captured, synchronized to the masterClock of the AVCaptureSession

 @discussion
    The timestamp property indicates the time the image was captured, and is analogous to CMSampleBufferGetPresentationTimeStamp(). If an error was provided in the -captureOutput:didFinishingProcessingPhoto:error: callback, timestamp returns kCMTimeInvalid.
 */
@property(readonly) CMTime timestamp;

/*!
 @property rawPhoto
 @abstract
    This property returns YES if this photo is a RAW image.

 @discussion
    Your AVCapturePhotoCaptureDelegate's -captureOutput:didFinishingProcessingPhoto:error: method may be called one or more times with image results, including RAW or non-RAW images. This property distinguishes RAW from non-RAW image results, for instance, if you've requested a RAW + JPEG capture.
 */
@property(readonly, getter=isRawPhoto) BOOL rawPhoto;

/*!
 @property pixelBuffer
 @abstract
    For uncompressed or RAW captures, this property offers access to the pixel data.

 @discussion
    Uncompressed captures, such as '420f' or 'BGRA', or RAW captures, such as 'bgg4', present pixel data as a CVPixelBuffer. This property is analogous to CMSampleBufferGetImageBuffer(). The pixel buffer contains only the minimal attachments required for correct display. Compressed captures, such as 'jpeg', return nil.
 */
@property(nullable, readonly) CVPixelBufferRef pixelBuffer NS_RETURNS_INNER_POINTER;

/*!
 @property previewPixelBuffer
 @abstract
    This property offers access to the preview image pixel data if you've requested it.

 @discussion
    If you requested a preview image by calling -[AVCapturePhotoSettings setPreviewPhotoFormat:] with a non-nil value, this property offers access to the resulting preview image pixel data, and is analogous to CMSampleBufferGetImageBuffer(). The pixel buffer contains only the minimal attachments required for correct display. Nil is returned if you did not request a preview image.
 */
@property(nullable, readonly) CVPixelBufferRef previewPixelBuffer NS_RETURNS_INNER_POINTER;

/*!
 @property embeddedThumbnailPhotoFormat
 @abstract
    The format of the embedded thumbnail contained in this AVCapturePhoto.

 @discussion
    If you requested an embedded thumbnail image by calling -[AVCapturePhotoSettings setEmbeddedThumbnailPhotoFormat:] with a non-nil value, this property offers access to the resolved embedded thumbnail AVVideoSettings dictionary. Nil is returned if you did not request an embedded thumbnail image.
 */
@property(nullable, readonly) NSDictionary<NSString *, id> *embeddedThumbnailPhotoFormat;

/*!
 @property depthData
 @abstract
    An AVDepthData object wrapping a disparity/depth map associated with this photo.

 @discussion
    If you requested depth data delivery by calling -[AVCapturePhotoSettings setDepthDataDeliveryEnabled:YES], this property offers access to the resulting AVDepthData object. Nil is returned if you did not request depth data delivery. Note that the depth data is only embedded in the photo's internal file format container if you set -[AVCapturePhotoSettings setEmbedsDepthDataInPhoto:YES].
 */
@property(nullable, readonly) AVDepthData *depthData;

/*!
 @property portraitEffectsMatte
 @abstract
    An AVPortraitEffectsMatte object wrapping a matte associated with this photo.

 @discussion
    If you requested portrait effects matte delivery by calling -[AVCapturePhotoSettings setPortraitEffectsMatteDeliveryEnabled:YES], this property offers access to the resulting AVPortraitEffectsMatte object. Nil is returned if you did not request portrait effects matte delivery. Note that the portrait effects matte is only embedded in the photo's internal file format container if you set -[AVCapturePhotoSettings setEmbedsPortraitEffectsMatteInPhoto:YES].
 */
@property(nullable, readonly) AVPortraitEffectsMatte *portraitEffectsMatte API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property metadata
 @abstract
    An ImageIO property style dictionary of metadata associated with this photo.

 @discussion
    Valid metadata keys are found in <ImageIO/CGImageProperties.h>, such as kCGImagePropertyOrientation, kCGImagePropertyExifDictionary, kCGImagePropertyMakerAppleDictionary, etc.
 */
@property(readonly) NSDictionary<NSString *, id> *metadata;

/*!
 @property cameraCalibrationData
 @abstract
    An AVCameraCalibrationData object representing the calibration information for the camera providing the photo.

 @discussion
    Camera calibration data is only present if you call -[AVCapturePhotoSettings setCameraCalibrationDataDeliveryEnabled:YES]. When requesting dualCameraDualPhotoDeliveryEnabled plus cameraCalibrationDataDeliveryEnabled, camera calibration information is delivered with both photos. Telephoto camera calibration data is presented with the telephoto AVCapturePhoto, and wide-angle camera calibration data is presented with the wide AVCapturePhoto.
 */
@property(nullable, readonly) AVCameraCalibrationData *cameraCalibrationData;

/*!
 @property resolvedSettings
 @abstract
    The AVCaptureResolvedPhotoSettings associated with all photo results for a given -[AVCapturePhotoOutput capturePhotoWithSettings:delegate:] request.

 @discussion
    Even in the event of an error, the resolved settings are always non nil.
 */
@property(readonly) AVCaptureResolvedPhotoSettings *resolvedSettings;

/*!
 @property photoCount
 @abstract
    This photo's index (1-based) in the total expected photo count.

 @discussion
    The resolvedSettings.expectedPhotoCount property indicates the total number of images that will be returned for a given capture request. This property indicates this photo's index (1-based). When you receive a -captureOutput:didFinishProcessingPhoto:error: callback with a photo whose photoCount matches resolvedSettings.expectedPhotoCount, you know you've received the last one for the given capture request.
 */
@property(readonly) NSInteger photoCount;

/*!
 @property sourceDeviceType
 @abstract
    The device type of the source camera providing the photo.

 @discussion
    When taking a dual photo capture from the DualCamera, you may query this property to find out the source of the photo: AVCaptureDeviceTypeBuiltInWideCamera, or AVCaptureDeviceTypeBuiltInTelephotoCamera. For all other types of capture, the source device type is equal to the -[AVCaptureDevice deviceType] of the AVCaptureDevice to which the AVCapturePhotoOutput is connected. May return nil if the source of the photo is not an AVCaptureDevice.
 */
@property(nullable, readonly) AVCaptureDeviceType sourceDeviceType;

@end

@protocol AVCapturePhotoFileDataRepresentationCustomizer;

@interface AVCapturePhoto (AVCapturePhotoConversions)

/*!
 @method fileDataRepresentation
 @abstract
    Flattens the AVCapturePhoto to an NSData using the file container format (processedFileType or rawFileType) specified in the AVCapturePhotoSettings (e.g. JFIF, HEIF, DNG).
 
 @result
    An NSData containing bits in the file container's format, or nil if the flattening process fails.
 */
- (nullable NSData *)fileDataRepresentation API_AVAILABLE(ios(11.0));

/*!
 @method fileDataRepresentationWithCustomizer:
 @abstract
    Flattens the AVCapturePhoto to an NSData using the file container format (processedFileType or rawFileType) specified in the AVCapturePhotoSettings (e.g. JFIF, HEIF, DNG), and allows you to strip or replace various pieces of metadata in the process.
 
 @param customizer
    An object conforming to the AVCapturePhotoFileDataRepresentationCustomizer protocol that will be called synchronously to provide customization of metadata written to the container format. An NSInvalidArgumentException is thrown if you pass nil.
 @result
    An NSData containing bits in the file container's format, or nil if the flattening process fails.
 */
- (nullable NSData *)fileDataRepresentationWithCustomizer:(id<AVCapturePhotoFileDataRepresentationCustomizer>)customizer API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @method fileDataRepresentationWithReplacementMetadata:replacementEmbeddedThumbnailPhotoFormat:replacementEmbeddedThumbnailPixelBuffer:replacementDepthData:
 @abstract
    Flattens the AVCapturePhoto to an NSData using the file container format (processedFileType or rawFileType) specified in the AVCapturePhotoSettings (e.g. JFIF, HEIF, DNG), and allows you to replace metadata, thumbnail, and depth data in the process.
 
 @param replacementMetadata
    A dictionary of keys and values from <ImageIO/CGImageProperties.h>. To preserve existing metadata to the file, pass self.metadata. To strip existing metadata, pass nil. To replace metadata, pass a replacement dictionary.
 @param replacementEmbeddedThumbnailPhotoFormat
    A dictionary of keys and values from <AVFoundation/AVVideoSettings.h>. If you pass a non-nil dictionary, AVVideoCodecKey is required, with AVVideoWidthKey and AVVideoHeightKey being optional. To preserve the existing embedded thumbnail photo to the file, pass self.embeddedThumbnailPhotoFormat and pass nil as your replacementEmbeddedThumbnailPixelBuffer parameter. To strip the existing embedded thumbnail, pass nil for both replacementEmbeddedThumbnailPhotoFormat and replacementEmbeddedThumbnailPixelBuffer. To replace the existing embedded thumbnail photo, pass both a non-nil replacementThumbnailPixelBuffer and replacementEmbeddedThumbnailPhotoFormat dictionary.
 @param replacementEmbeddedThumbnailPixelBuffer
    A pixel buffer containing a source image to be encoded to the file as the replacement thumbnail image. To preserve the existing embedded thumbnail photo to the file, pass self.embeddedThumbnailPhotoFormat as your replacementEmbeddedThumbnailPhotoFormat parameter and nil as your replacementEmbeddedThumbnailPixelBuffer parameter. To strip the existing embedded thumbnail, pass nil for both replacementEmbeddedThumbnailPhotoFormat and replacementEmbeddedThumbnailPixelBuffer. To replace the existing embedded thumbnail photo, pass both a non-nil replacementThumbnailPixelBuffer and replacementEmbeddedThumbnailPhotoFormat dictionary.
 @param replacementDepthData
    Replacement depth data to be written to the flattened file container. To preserve existing depth data to the file, pass self.depthData. To strip it, pass nil. To replace it, pass a new AVDepthData instance.
 @result
    An NSData containing bits in the file container's format, or nil if the flattening process fails.
 */
- (nullable NSData *)fileDataRepresentationWithReplacementMetadata:(nullable NSDictionary<NSString *, id> *)replacementMetadata replacementEmbeddedThumbnailPhotoFormat:(nullable NSDictionary<NSString *, id> *)replacementEmbeddedThumbnailPhotoFormat replacementEmbeddedThumbnailPixelBuffer:(nullable CVPixelBufferRef)replacementEmbeddedThumbnailPixelBuffer replacementDepthData:(nullable AVDepthData *)replacementDepthData API_DEPRECATED("Use fileDataRepresentationWithCustomizer: instead", ios(11.0, 12.0));

/*!
 @method CGImageRepresentation
 @abstract
    Utility method that converts the AVCapturePhoto's primary photo to a CGImage.
 
 @result
    A CGImageRef, or nil if the conversion process fails.
 
 @discussion
    Each time you access this method, AVCapturePhoto generates a new CGImageRef. When backed by a compressed container (such as HEIC), the CGImageRepresentation is decoded lazily as needed. When backed by an uncompressed format such as BGRA, it is copied into a separate backing buffer whose lifetime is not tied to that of the AVCapturePhoto. For a 12 megapixel image, a BGRA CGImage represents ~48 megabytes per call. If you only intend to use the CGImage for on-screen rendering, use the previewCGImageRepresentation instead. Note that the physical rotation of the CGImageRef matches that of the main image. Exif orientation has not been applied. If you wish to apply rotation when working with UIImage, you can do so by querying the photo's metadata[kCGImagePropertyOrientation] value, and passing it as the orientation parameter to +[UIImage imageWithCGImage:scale:orientation:]. RAW images always return a CGImageRepresentation of nil. If you wish to make a CGImageRef from a RAW image, use CIRAWFilter in the CoreImage framework.
 */
- (nullable CGImageRef)CGImageRepresentation API_AVAILABLE(ios(11.0));

/*!
 @method CGImageRepresentation
 @abstract
    Utility method that converts the AVCapturePhoto's preview photo to a CGImage.
 
 @result
    A CGImageRef, or nil if the conversion process fails, or if you did not request a preview photo.
 
 @discussion
    Each time you access this method, AVCapturePhoto generates a new CGImageRef. This CGImageRepresentation is a RGB rendering of the previewPixelBuffer property. If you did not request a preview photo by setting the -[AVCapturePhotoSettings previewPhotoFormat] property, this method returns nil. Note that the physical rotation of the CGImageRef matches that of the main image. Exif orientation has not been applied. If you wish to apply rotation when working with UIImage, you can do so by querying the photo's metadata[kCGImagePropertyOrientation] value, and passing it as the orientation parameter to +[UIImage imageWithCGImage:scale:orientation:].
 */
- (nullable CGImageRef)previewCGImageRepresentation API_AVAILABLE(ios(11.0));

@end


/*!
 @enum AVCaptureLensStabilizationStatus
 @abstract
    Constants indicating the status of the lens stabilization module (aka OIS).

 @constant AVCaptureLensStabilizationStatusUnsupported
    Indicates that lens stabilization is unsupported.
 @constant AVCaptureLensStabilizationStatusOff
    Indicates that lens stabilization was not in use for this capture.
 @constant AVCaptureLensStabilizationStatusActive
    Indicates that the lens stabilization module was active for the duration of the capture.
 @constant AVCaptureLensStabilizationStatusOutOfRange
    Indicates that device motion or capture duration exceeded the stabilization module's correction limits.
 @constant AVCaptureLensStabilizationStatusUnavailable
    Indicates that the lens stabilization module was unavailable for use at the time of capture. The module may be available in subsequent captures.
 */
typedef NS_ENUM(NSInteger, AVCaptureLensStabilizationStatus) {
    AVCaptureLensStabilizationStatusUnsupported = 0,
    AVCaptureLensStabilizationStatusOff         = 1,
    AVCaptureLensStabilizationStatusActive      = 2,
    AVCaptureLensStabilizationStatusOutOfRange  = 3,
    AVCaptureLensStabilizationStatusUnavailable = 4,
} API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

@interface AVCapturePhoto (AVCapturePhotoBracketedCapture)

/*!
 @property bracketSettings
 @abstract
    The AVCaptureBracketedStillImageSettings associated with this photo.

 @discussion
    When specifying a bracketed capture using AVCapturePhotoBracketSettings, you specify an array of AVCaptureBracketedStillImageSettings -- one per image in the bracket. This property indicates the AVCaptureBracketedStillImageSettings associated with this particular photo, or nil if this photo is not part of a bracketed capture.
 */
@property(nullable, readonly) AVCaptureBracketedStillImageSettings *bracketSettings API_AVAILABLE(ios(11.0));

/*!
 @property sequenceCount
 @abstract
    1-based sequence count of the photo.

 @discussion
    If this photo is part of a bracketed capture (invoked using AVCapturePhotoBracketSettings), this property indicates the current result's count in the sequence, starting with 1 for the first result, or 0 if this photo is not part of a bracketed capture.
 */
@property(readonly) NSInteger sequenceCount API_AVAILABLE(ios(11.0));

/*!
 @property lensStabilizationStatus
 @abstract
    The status of the lens stabilization module during capture of this photo.

 @discussion
    In configurations where lens stabilization (OIS) is unsupported, AVCaptureLensStabilizationStatusUnsupported is returned. If lens stabilization is supported, but this photo is not part of a bracketed capture in which -[AVCapturePhotoBracketSettings setLensStabilizationEnabled:YES] was called, AVCaptureLensStabilizationStatusOff is returned. Otherwise a lens stabilization status is returned indicating how lens stabilization was applied during the capture.
 */
@property(readonly) AVCaptureLensStabilizationStatus lensStabilizationStatus API_AVAILABLE(ios(11.0));

@end


/*!
 @protocol AVCapturePhotoFileDataRepresentationCustomizer
 @abstract
    A set of delegate callbacks to be implemented by a client who calls AVCapturePhoto's -fileDataRepresentationWithCustomizer:.
 
 @discussion
    AVCapturePhoto is a wrapper representing a file-containerized photo in memory. If you simply wish to flatten the photo to an NSData to be written to a file, you may call -[AVCapturePhoto fileDataRepresentation]. For more complex flattening operations in which you wish to replace or strip metadata, you should call -[AVCapturePhoto fileDataRepresentationWithCustomizer:] instead, providing a delegate for customized stripping / replacing behavior. This delegate's methods are called synchronously before the flattening process commences.
 */
API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos) 
@protocol AVCapturePhotoFileDataRepresentationCustomizer <NSObject>

@optional
/*!
 @method replacementMetadataForPhoto:
 @abstract
    A callback in which you may provide replacement metadata, or direct the AVCapturePhoto to strip existing metadata from the flattened file data representation.
 
 @param photo
    The calling instance of AVCapturePhoto.
 @return
    A dictionary of keys and values from <ImageIO/CGImageProperties.h>. To preserve existing metadata, return photo.metadata. To strip existing metadata, return nil. To replace metadata, pass a replacement dictionary.

 @discussion
    This callback is optional. If your delegate does not implement this callback, the existing metadata in the in-memory AVCapturePhoto container will be written to the file data representation.
 */
- (nullable NSDictionary<NSString *, id> *)replacementMetadataForPhoto:(AVCapturePhoto *)photo;

/*!
 @method replacementEmbeddedThumbnailPixelBufferWithPhotoFormat:forPhoto:
 @abstract
    A callback in which you may provide a replacement embedded thumbnail image with compression settings, or strip the existing embedded thumbnail image from the flattened file data representation.
 
 @param replacementEmbeddedThumbnailPhotoFormatOut
    On output, a pointer to a dictionary of keys and values from <AVFoundation/AVVideoSettings.h> If you pass a non-nil dictionary, AVVideoCodecKey is required, with AVVideoWidthKey and AVVideoHeightKey being optional. To preserve the existing embedded thumbnail photo to the flattened data, set *replacementEmbeddedThumbnailPhotoFormatOut to photo.embeddedThumbnailPhotoFormat and return nil. To strip the existing embedded thumbnail, set *replacementEmbeddedThumbnailPhotoFormatOut to nil and return nil. To replace the existing embedded thumbnail photo, pass a replacement photo format dictionary and return a non-nil replacement pixel buffer.
 @param photo
    The calling instance of AVCapturePhoto.
 @return
    A pixel buffer containing a source image to be encoded to the file as the replacement thumbnail image. To preserve the existing embedded thumbnail photo to the flattened data, set *replacementEmbeddedThumbnailPhotoFormatOut to photo.embeddedThumbnailPhotoFormat and return nil. To strip the existing embedded thumbnail, set *replacementEmbeddedThumbnailPhotoFormatOut to nil and return nil. To replace the existing embedded thumbnail photo, pass a replacement photo format dictionary and return a non-nil replacement pixel buffer.

 @discussion
    This callback is optional. If your delegate does not implement this callback, the existing embedded thumbnail photo in the in-memory AVCapturePhoto container will be written to the file data representation.
 */
- (nullable CVPixelBufferRef)replacementEmbeddedThumbnailPixelBufferWithPhotoFormat:(NSDictionary<NSString *, id> *_Nullable * _Nonnull)replacementEmbeddedThumbnailPhotoFormatOut forPhoto:(AVCapturePhoto *)photo;

/*!
 @method replacementDepthDataForPhoto:
 @abstract
    A callback in which you may provide replacement depth data, or strip the existing depth data from the flattened file data representation.
 
 @param photo
    The calling instance of AVCapturePhoto.
 @return
    An instance of AVDepthData. To preserve the existing depth data, return photo.depthData. To strip the existing one, return nil. To replace, provide a replacement AVDepthData instance.

 @discussion
    This callback is optional. If your delegate does not implement this callback, the existing depth data in the in-memory AVCapturePhoto container will be written to the file data representation.
 */
- (nullable AVDepthData *)replacementDepthDataForPhoto:(AVCapturePhoto *)photo;

/*!
 @method replacementPortraitEffectsMatteForPhoto:
 @abstract
    A callback in which you may provide a replacement portrait effects matte, or strip the existing portrait effects matte from the flattened file data representation.
 
 @param photo
    The calling instance of AVCapturePhoto.
 @return
    An instance of AVPortraitEffectsMatte. To preserve the existing portrait effects matte, return photo.portraitEffectsMatte. To strip the existing one, return nil. To replace, provide a replacement AVPortraitEffectsMatte instance.

 @discussion
    This callback is optional. If your delegate does not implement this callback, the existing portrait effects matte in the in-memory AVCapturePhoto container will be written to the file data representation.
 */
- (nullable AVPortraitEffectsMatte *)replacementPortraitEffectsMatteForPhoto:(AVCapturePhoto *)photo;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitTimePitch.h
/*
	File:           AVAudioUnitTimePitch.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitTimePitch.h>

// ==========  AVFoundation.framework/Headers/AVAssetReader.h
/*
	File:  AVAssetReader.h

	Framework:  AVFoundation
 
	Copyright 2010-2016 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <CoreMedia/CMSampleBuffer.h>

@class AVAsset;
@class AVAssetReaderOutput;
@class AVAssetReaderInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @enum AVAssetReaderStatus
 @abstract
	These constants are returned by the AVAssetReader status property to indicate whether it can successfully read samples from its asset.

 @constant	 AVAssetReaderStatusUnknown
	Indicates that the status of the asset reader is not currently known.
 @constant	 AVAssetReaderStatusReading
	Indicates that the asset reader is successfully reading samples from its asset.
 @constant	 AVAssetReaderStatusCompleted
	Indicates that the asset reader has successfully read all of the samples in its time range.
 @constant	 AVAssetReaderStatusFailed
	Indicates that the asset reader can no longer read samples from its asset because of an error. The error is described by the value of the asset reader's error property.
 @constant	 AVAssetReaderStatusCancelled
	Indicates that the asset reader can no longer read samples because reading was canceled with the cancelReading method.
 */
typedef NS_ENUM(NSInteger, AVAssetReaderStatus) {
    AVAssetReaderStatusUnknown = 0,
    AVAssetReaderStatusReading,
    AVAssetReaderStatusCompleted,
    AVAssetReaderStatusFailed,
    AVAssetReaderStatusCancelled,
};

/*!
 @class AVAssetReader
 @abstract
	AVAssetReader provides services for obtaining media data from an asset.
 
 @discussion
	Instances of AVAssetReader read media data from an instance of AVAsset, whether the asset is file-based or represents an assembly of media data from multiple sources, as is the case with AVComposition.
	
	Clients of AVAssetReader can read data from specific tracks of an asset and in specific formats by adding concrete instances of AVAssetReaderOutput to an AVAssetReader instance.
	
	AVAssetReaderTrackOutput, a concrete subclass of AVAssetReaderOutput, can either read the track's media samples in the format in which they are stored by the asset or convert the media samples to a different format.
	
	AVAssetReaderAudioMixOutput mixes multiple audio tracks of the asset after reading them, while AVAssetReaderVideoCompositionOutput composites multiple video tracks after reading them.
 */
NS_CLASS_AVAILABLE(10_7, 4_1)
@interface AVAssetReader : NSObject
{
@private
	AVAssetReaderInternal		*_priv;
}
AV_INIT_UNAVAILABLE

/*!
 @method assetReaderWithAsset:error:
 @abstract
	Returns an instance of AVAssetReader for reading media data from the specified asset.

 @param asset
	The asset from which media data is to be read.
 @param outError
	On return, if initialization of the AVAssetReader fails, points to an NSError describing the nature of the failure.
 @result An instance of AVAssetReader.
 @discussion
	If the specified asset belongs to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie, the results of any asset reading operation are undefined if you mutate the asset after invoking -startReading.
 */
+ (nullable instancetype)assetReaderWithAsset:(AVAsset *)asset error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method initWithAsset:error:
 @abstract
	Creates an instance of AVAssetReader for reading media data from the specified asset.

 @param asset
	The asset from which media data is to be read.
 @param outError
	On return, if initialization of the AVAssetReader fails, points to an NSError describing the nature of the failure.
 @result
	An instance of AVAssetReader.
 @discussion
	If the specified asset belongs to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie, the results of any asset reading operation are undefined if you mutate the asset after invoking -startReading.
 */
- (nullable instancetype)initWithAsset:(AVAsset *)asset error:(NSError * _Nullable * _Nullable)outError NS_DESIGNATED_INITIALIZER;

/*!
 @property asset
 @abstract
	The asset from which the receiver's outputs read sample buffers.

 @discussion
	The value of this property is an AVAsset. Concrete instances of AVAssetReader that are created with specific AVAssetTrack instances must obtain those tracks from the asset returned by this property.
 */
@property (nonatomic, retain, readonly) AVAsset *asset;

/*!
 @property status
 @abstract
	The status of reading sample buffers from the receiver's asset.

 @discussion
	The value of this property is an AVAssetReaderStatus that indicates whether reading is in progress, has completed successfully, has been canceled, or has failed. Clients of AVAssetReaderOutput objects should check the value of this property after -[AVAssetReaderOutput copyNextSampleBuffer] returns NULL to determine why no more samples could be read. This property is thread safe.
 */
@property (readonly) AVAssetReaderStatus status;

/*!
 @property error
 @abstract
	If the receiver's status is AVAssetReaderStatusFailed, this describes the error that caused the failure.

 @discussion
	The value of this property is an NSError that describes what caused the receiver to no longer be able to read its asset. If the receiver's status is not AVAssetReaderStatusFailed, the value of this property is nil. This property is thread safe.
 */
@property (readonly, nullable) NSError *error;

/*!
 @property timeRange
 @abstract
	Specifies a range of time that may limit the temporal portion of the receiver's asset from which media data will be read.

 @discussion
	The intersection of the value of timeRange and CMTimeRangeMake(kCMTimeZero, asset.duration) will determine the time range of the asset from which media data will be read. The default value of timeRange is CMTimeRangeMake(kCMTimeZero, kCMTimePositiveInfinity).
	
	This property cannot be set after reading has started.
 */
@property (nonatomic) CMTimeRange timeRange;

/*!
 @property outputs
 @abstract
	The outputs from which clients of receiver can read media data.

 @discussion
	The value of this property is an NSArray containing concrete instances of AVAssetReaderOutput. Outputs can be added to the receiver using the addOutput: method.
 */
@property (nonatomic, readonly) NSArray<AVAssetReaderOutput *> *outputs;

/*!
 @method canAddOutput:
 @abstract
	Tests whether an output can be added to the receiver.

 @param output
	The AVAssetReaderOutput object to be tested.
 @result
	A BOOL indicating whether the output can be added to the receiver.

 @discussion
	An output that reads from a track of an asset other than the asset used to initialize the receiver cannot be added.
 */
- (BOOL)canAddOutput:(AVAssetReaderOutput *)output;

/*!
 @method addOutput:
 @abstract
	Adds an output to the receiver.

 @param output
	The AVAssetReaderOutput object to be added.

 @discussion
	Outputs are created with a reference to one or more AVAssetTrack objects. These tracks must be owned by the asset returned by the receiver's asset property.
	
	Outputs cannot be added after reading has started.
 */
- (void)addOutput:(AVAssetReaderOutput *)output;

/*!
 @method startReading
 @abstract
	Prepares the receiver for reading sample buffers from the asset.

 @result
	A BOOL indicating whether reading could be started.
 
 @discussion
	This method validates the entire collection of settings for outputs for tracks, for audio mixing, and for video composition and initiates reading from the receiver's asset.
	
	If this method returns NO, clients can determine the nature of the failure by checking the value of the status and error properties.
 */
- (BOOL)startReading;

/*!
 @method cancelReading
 @abstract
	Cancels any background work and prevents the receiver's outputs from reading more samples.

 @discussion
	Clients that want to stop reading samples from the receiver before reaching the end of its time range should call this method to stop any background read ahead operations that the may have been in progress.
 
	This method should not be called concurrently with any calls to -[AVAssetReaderOutput copyNextSampleBuffer].
 */
- (void)cancelReading;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMetadataObject.h
/*
    File:  AVMetadataObject.h
 
    Framework:  AVFoundation
 
    Copyright 2012-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreGraphics/CGGeometry.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @group AVMetadataObjectType string constants
 
 @discussion
    Constants indicating the type of an AVMetadataObject.
 */
typedef NSString * AVMetadataObjectType NS_STRING_ENUM API_AVAILABLE(macos(10.10), ios(6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVMetadataObject

@class AVMetadataObjectInternal;

/*!
 @class AVMetadataObject
 @abstract
    AVMetadataObject is an abstract class that defines an interface for a metadata object used by AVFoundation.
 
 @discussion
    AVMetadataObject provides an abstract interface for metadata associated with a piece of media. One example is face metadata that might be detected in a picture. All metadata objects have a time, duration, bounds, and type.
 
    The concrete AVMetadataFaceObject is used by AVCaptureMetadataOutput for face detection.
 */
API_AVAILABLE(macos(10.10), ios(6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVMetadataObject : NSObject
{
@private
    AVMetadataObjectInternal *_objectInternal;
}

AV_INIT_UNAVAILABLE

/*!
 @property time
 @abstract
    The media time associated with this metadata object.
 
 @discussion
    The value of this property is a CMTime associated with the metadata object. For capture, it is the time at which this object was captured. If this metadata object originates from a CMSampleBuffer, its time matches the sample buffer's presentation time. This property may return kCMTimeInvalid.
 */
@property(readonly) CMTime time;

/*!
 @property duration
 @abstract
    The media duration associated with this metadata object.
 
 @discussion
    The value of this property is a CMTime representing the duration of the metadata object. If this metadata object originates from a CMSampleBuffer, its duration matches the sample buffer's duration. This property may return kCMTimeInvalid.
 */
@property(readonly) CMTime duration;

/*!
 @property bounds
 @abstract
    The bounding rectangle of the receiver.
 
 @discussion
    The value of this property is a CGRect representing the bounding rectangle of the object with respect to the picture in which it resides. The rectangle's origin is top left. If the metadata originates from video, bounds may be expressed as scalar values from 0. - 1. If the original video has been scaled down, the bounds of the metadata object still are meaningful. This property may return CGRectZero if the metadata has no bounds.
 */
@property(readonly) CGRect bounds;

/*!
 @property type
 @abstract
    An identifier for the metadata object.
 
 @discussion
    The value of this property is an AVMetadataObjectType representing the type of the metadata object. Clients inspecting a collection of metadata objects can use this property to filter objects with a matching type.
 */
@property(readonly) AVMetadataObjectType type;

@end


/*!
 @constant AVMetadataObjectTypeFace
 @abstract
    An identifier for an instance of AVMetadataFaceObject.
 
 @discussion
    AVMetadataFaceObject objects return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeFace API_AVAILABLE(macos(10.10), ios(6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVMetadataFaceObject

@class AVMetadataFaceObjectInternal;

/*!
 @class AVMetadataFaceObject
 @abstract
    AVMetadataFaceObject is a concrete subclass of AVMetadataObject defining the features of a detected face.
 
 @discussion
    AVMetadataFaceObject represents a single detected face in a picture. It is an immutable object describing the various features found in the face.
 
    On supported platforms, AVCaptureMetadataOutput outputs arrays of detected face objects. See AVCaptureOutput.h.
 */
API_AVAILABLE(macos(10.10), ios(6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVMetadataFaceObject : AVMetadataObject <NSCopying>
{
@private
    AVMetadataFaceObjectInternal *_internal;
}

/*!
 @property faceID
 @abstract
    A unique number associated with the receiver.
 
 @discussion
    The value of this property is an NSInteger indicating the unique identifier of this face in the picture. When a new face enters the picture, it is assigned a new unique identifier. faceIDs are not re-used as faces leave the picture and new ones enter. Faces that leave the picture then re-enter are assigned a new faceID.
 */
@property(readonly) NSInteger faceID;

/*!
 @property hasRollAngle
 @abstract
    A BOOL indicating whether the rollAngle property is valid for this receiver.
 */
@property(readonly) BOOL hasRollAngle;

/*!
 @property rollAngle
 @abstract
    The roll angle of the face in degrees.
 
 @discussion
    The value of this property is a CGFloat indicating the face's angle of roll (or tilt) in degrees. A value of 0.0 indicates that the face is level in the picture. If -hasRollAngle returns NO, then reading this property throws an NSGenericException.
 */
@property(readonly) CGFloat rollAngle;

/*!
 @property hasYawAngle
 @abstract
    A BOOL indicating whether the yawAngle property is valid for this receiver.
 */
@property(readonly) BOOL hasYawAngle;

/*!
 @property yawAngle
 @abstract
    The yaw angle of the face in degrees.
 
 @discussion
    The value of this property is a CGFloat indicating the face's angle of yaw (or turn) in degrees. A value of 0.0 indicates that the face is straight on in the picture. If -hasYawAngle returns NO, then reading this property throws an NSGenericException.
 */
@property(readonly) CGFloat yawAngle;

@end


/*!
 @constant AVMetadataObjectTypeUPCECode
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeUPCECode.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from UPC-E codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeUPCECode API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeCode39Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeCode39Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Code 39 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeCode39Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeCode39Mod43Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeCode39Mod43Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Code 39 mod 43 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeCode39Mod43Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeEAN13Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeEAN13Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from EAN-13 (including UPC-A) codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeEAN13Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeEAN8Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeEAN8Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from EAN-8 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeEAN8Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeCode93Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeCode93Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Code 93 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeCode93Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeCode128Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeCode128Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Code 128 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeCode128Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypePDF417Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypePDF417Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from PDF417 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypePDF417Code API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeQRCode
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeQRCode.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from QR codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeQRCode API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeAztecCode
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeAztecCode.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Aztec codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeAztecCode API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeInterleaved2of5Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeInterleaved2of5Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from Interleaved 2 of 5 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeInterleaved2of5Code API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeITF14Code
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeITF14Code.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from ITF14 codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeITF14Code API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVMetadataObjectTypeDataMatrixCode
 @abstract
    An identifier for an instance of AVMetadataMachineReadableCodeObject having a type AVMetadataObjectTypeDataMatrixCode.
 
 @discussion
    AVMetadataMachineReadableCodeObject objects generated from DataMatrix codes return this constant as their type.
 */
AVF_EXPORT AVMetadataObjectType const AVMetadataObjectTypeDataMatrixCode API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVMetadataMachineReadableCodeObject

@class AVMetadataMachineReadableCodeObjectInternal;

/*!
 @class AVMetadataMachineReadableCodeObject
 @abstract
    AVMetadataMachineReadableCodeObject is a concrete subclass of AVMetadataObject defining the features of a detected one-dimensional or two-dimensional barcode.
 
 @discussion
    AVMetadataMachineReadableCodeObject represents a single detected machine readable code in a picture. It is an immutable object describing the features and payload of a barcode.
 
    On supported platforms, AVCaptureMetadataOutput outputs arrays of detected machine readable code objects. See AVCaptureMetadataOutput.h.
 */
API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVMetadataMachineReadableCodeObject : AVMetadataObject
{
@private
    AVMetadataMachineReadableCodeObjectInternal *_internal;
}

/*!
 @property corners
 @abstract
    The points defining the (X,Y) locations of the corners of the machine-readable code.
 
 @discussion
    The value of this property is an NSArray of NSDictionaries, each of which has been created from a CGPoint using CGPointCreateDictionaryRepresentation(), representing the coordinates of the corners of the object with respect to the image in which it resides. If the metadata originates from video, the points may be expressed as scalar values from 0. - 1. The points in the corners differ from the bounds rectangle in that bounds is axis-aligned to orientation of the captured image, and the values of the corners reside within the bounds rectangle. The points are arranged in counter-clockwise order (clockwise if the code or image is mirrored), starting with the top-left of the code in its canonical orientation.
 */
@property(readonly) NSArray<NSDictionary *> *corners;

/*!
 @property stringValue
 @abstract
    Returns the receiver's errorCorrectedData decoded into a human-readable string.
 
 @discussion
    The value of this property is an NSString created by decoding the binary payload according to the format of the machine readable code. Returns nil if a string representation cannot be created from the payload.
 */
@property(readonly, nullable) NSString *stringValue;

@end


@class CIBarcodeDescriptor;

@interface AVMetadataMachineReadableCodeObject (AVMetadataMachineReadableCodeDescriptor)

/*!
 @property descriptor
 @abstract
    An abstract representation of a machine readable code's symbol attributes.
 
 @discussion
    The value may be nil if an abstract representation of a machine readable code object is not defined for the code type or could not be detected.
 */
@property(readonly, nullable) CIBarcodeDescriptor *descriptor API_AVAILABLE(ios(11.0));

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAnimation.h
/*
    File:  AVAnimation.h
 
    Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.
 
 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreFoundation/CFDate.h>

/*!
 @constant					AVCoreAnimationBeginTimeAtZero
 @discussion				Use this constant to set the CoreAnimation's animation beginTime property to be time 0.
							The constant is a small, non-zero, positive value which avoids CoreAnimation
							from replacing 0.0 with CACurrentMediaTime().
*/
AVF_EXPORT const CFTimeInterval AVCoreAnimationBeginTimeAtZero NS_AVAILABLE(10_7, 4_0);

typedef NSString * AVLayerVideoGravity NS_STRING_ENUM;

/*!
	@constant		AVLayerVideoGravityResizeAspect
	@abstract		Preserve aspect ratio; fit within layer bounds.
	@discussion		AVLayerVideoGravityResizeAspect may be used when setting the videoGravity
                    property of an AVPlayerLayer or AVCaptureVideoPreviewLayer instance.
 */
AVF_EXPORT AVLayerVideoGravity const AVLayerVideoGravityResizeAspect NS_AVAILABLE(10_7, 4_0);


/*!
	@constant		AVLayerVideoGravityResizeAspectFill
	@abstract		Preserve aspect ratio; fill layer bounds.
    @discussion     AVLayerVideoGravityResizeAspectFill may be used when setting the videoGravity
                    property of an AVPlayerLayer or AVCaptureVideoPreviewLayer instance.
 */
AVF_EXPORT AVLayerVideoGravity const AVLayerVideoGravityResizeAspectFill NS_AVAILABLE(10_7, 4_0);

/*!
	@constant		AVLayerVideoGravityResize
	@abstract		Stretch to fill layer bounds.
    @discussion     AVLayerVideoGravityResize may be used when setting the videoGravity
                    property of an AVPlayerLayer or AVCaptureVideoPreviewLayer instance.
 */
AVF_EXPORT AVLayerVideoGravity const AVLayerVideoGravityResize NS_AVAILABLE(10_7, 4_0);
// ==========  AVFoundation.framework/Headers/AVAudioUnitEQ.h
/*
	File:           AVAudioUnitEQ.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitEQ.h>

// ==========  AVFoundation.framework/Headers/AVCaptureOutput.h
/*
    File:  AVCaptureOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>

#import <AVFoundation/AVCaptureAudioDataOutput.h>
#import <AVFoundation/AVCaptureAudioPreviewOutput.h>
#import <AVFoundation/AVCaptureDepthDataOutput.h>
#import <AVFoundation/AVCaptureFileOutput.h>
#import <AVFoundation/AVCaptureMetadataOutput.h>
#import <AVFoundation/AVCapturePhotoOutput.h>
#import <AVFoundation/AVCaptureStillImageOutput.h>
#import <AVFoundation/AVCaptureVideoDataOutput.h>
// ==========  AVFoundation.framework/Headers/AVAssetExportSession.h
/*
	File:  AVAssetExportSession.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/


#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVAudioProcessingSettings.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>

// for CGSize
#import <CoreGraphics/CoreGraphics.h>

NS_ASSUME_NONNULL_BEGIN

/*!
	@class		AVAssetExportSession

	@abstract	An AVAssetExportSession creates a new timed media resource from the contents of an 
				existing AVAsset in the form described by a specified export preset.

	@discussion
				Prior to initializing an instance of AVAssetExportSession, you can invoke
				+allExportPresets to obtain the complete list of presets available. Use
				+exportPresetsCompatibleWithAsset: to obtain a list of presets that are compatible
				with a specific AVAsset.

				To configure an export, initialize an AVAssetExportSession with an AVAsset that contains
				the source media, an AVAssetExportPreset, the output file type, (a UTI string from 
				those defined in AVMediaFormat.h) and the output URL.

				After configuration is complete, invoke exportAsynchronouslyWithCompletionHandler: 
				to start the export process. This method returns immediately; the export is performed 
				asynchronously. Invoke the -progress method to check on the progress. Note that in 
				some cases, depending on the capabilities of the device, when multiple exports are 
				attempted at the same time some may be queued until others have been completed. When 
				this happens, the status of a queued export will indicate that it's "waiting".

				Whether the export fails, completes, or is cancelled, the completion handler you
				supply to -exportAsynchronouslyWithCompletionHandler: will be called. Upon
				completion, the status property indicates whether the export has completed
				successfully. If it has failed, the value of the error property supplies additional
				information about the reason for the failure.

*/

// -- Export Preset Names --


/* These export options can be used to produce movie files with video size appropriate to the device.
   The export will not scale the video up from a smaller size. The video will be compressed using
   H.264 and the audio will be compressed using AAC.  */
AVF_EXPORT NSString *const AVAssetExportPresetLowQuality         NS_AVAILABLE(10_11, 4_0);
AVF_EXPORT NSString *const AVAssetExportPresetMediumQuality      NS_AVAILABLE(10_11, 4_0);
AVF_EXPORT NSString *const AVAssetExportPresetHighestQuality     NS_AVAILABLE(10_11, 4_0);

/* These export options can be used to produce movie files with video size appropriate to the device.
   The export will not scale the video up from a smaller size. The video will be compressed using
   HEVC and the audio will be compressed using AAC.  */
AVF_EXPORT NSString *const AVAssetExportPresetHEVCHighestQuality NS_AVAILABLE(10_13, 11_0);

/* These export options can be used to produce movie files with the specified video size.
   The export will not scale the video up from a smaller size. The video will be compressed using
   H.264 and the audio will be compressed using AAC.  Some devices cannot support some sizes. */
AVF_EXPORT NSString *const AVAssetExportPreset640x480           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVAssetExportPreset960x540           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVAssetExportPreset1280x720          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT NSString *const AVAssetExportPreset1920x1080         NS_AVAILABLE(10_7, 5_0);
AVF_EXPORT NSString *const AVAssetExportPreset3840x2160         NS_AVAILABLE(10_10, 9_0);

/* These export options can be used to produce movie files with the specified video size.
   The export will not scale the video up from a smaller size. The video will be compressed using
   HEVC and the audio will be compressed using AAC.  Some devices cannot support some sizes. */
AVF_EXPORT NSString *const AVAssetExportPresetHEVC1920x1080     NS_AVAILABLE(10_13, 11_0);
AVF_EXPORT NSString *const AVAssetExportPresetHEVC3840x2160     NS_AVAILABLE(10_13, 11_0);

/*  This export option will produce an audio-only .m4a file with appropriate iTunes gapless playback data */
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4A			NS_AVAILABLE(10_7, 4_0);

/* This export option will cause the media of all tracks to be passed through to the output exactly as stored in the source asset, except for
   tracks for which passthrough is not possible, usually because of constraints of the container format as indicated by the specified outputFileType.
   This option is not included in the arrays returned by -allExportPresets and -exportPresetsCompatibleWithAsset. */
AVF_EXPORT NSString *const AVAssetExportPresetPassthrough		NS_AVAILABLE(10_7, 4_0);

#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))

/* These export options are used to produce files that can be played on the specified Apple devices. 
	These presets are available for Desktop export only.
	The files should have .m4v extensions (or .m4a for exports with audio only sources). */
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4VCellular	NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4ViPod		NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4V480pSD	NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4VAppleTV	NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4VWiFi		NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4V720pHD	NS_AVAILABLE(10_7, NA);
AVF_EXPORT NSString *const AVAssetExportPresetAppleM4V1080pHD	NS_AVAILABLE(10_8, NA);

/* This export option will produce a QuickTime movie with Apple ProRes 422 video and LPCM audio. */
AVF_EXPORT NSString *const AVAssetExportPresetAppleProRes422LPCM	NS_AVAILABLE(10_7, NA);

#endif // (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))


@class AVAsset;
@class AVAssetExportSessionInternal;
@class AVAudioMix;
@class AVVideoComposition;
@class AVMetadataItemFilter;
@protocol AVVideoCompositing;
@class AVMetadataItem;

typedef NS_ENUM(NSInteger, AVAssetExportSessionStatus) {
	AVAssetExportSessionStatusUnknown,
    AVAssetExportSessionStatusWaiting,
    AVAssetExportSessionStatusExporting,
    AVAssetExportSessionStatusCompleted,
    AVAssetExportSessionStatusFailed,
    AVAssetExportSessionStatusCancelled
};

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAssetExportSession : NSObject
{
@private
	AVAssetExportSessionInternal  *_exportSession;
}
AV_INIT_UNAVAILABLE

/*!
	@method						exportSessionWithAsset:presetName:
	@abstract					Returns an instance of AVAssetExportSession for the specified source asset and preset.
	@param		asset			An AVAsset object that is intended to be exported.
	@param		presetName		An NSString specifying the name of the preset template for the export.
	@result						An instance of AVAssetExportSession.
	@discussion					If the specified asset belongs to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie, the results of any export-related operation are undefined if you mutate the asset after the operation commences. These operations include but are not limited to: 1) testing the compatibility of export presets with the asset, 2) calculating the maximum duration or estimated length of the output file, and 3) the export operation itself.
*/
+ (nullable instancetype)exportSessionWithAsset:(AVAsset *)asset presetName:(NSString *)presetName NS_AVAILABLE(10_7, 4_1);

/*!
	@method						initWithAsset:presetName:
	@abstract					Initialize an AVAssetExportSession with the specified preset and set the source to the contents of the asset.
	@param		asset			An AVAsset object that is intended to be exported.
	@param		presetName		An NSString specifying the name of the preset template for the export.
	@result						Returns the initialized AVAssetExportSession.
	@discussion					If the specified asset belongs to a mutable subclass of AVAsset, AVMutableComposition or AVMutableMovie, the results of any export-related operation are undefined if you mutate the asset after the operation commences. These operations include but are not limited to: 1) testing the compatibility of export presets with the asset, 2) calculating the maximum duration or estimated length of the output file, and 3) the export operation itself.
*/
- (nullable instancetype)initWithAsset:(AVAsset *)asset presetName:(NSString *)presetName NS_DESIGNATED_INITIALIZER;

/* AVAssetExortSession properties are key-value observable unless documented otherwise */

/* Indicates the name of the preset with which the AVExportSession was initialized */
@property (nonatomic, readonly) NSString *presetName;

/* Indicates the instance of AVAsset with which the AVExportSession was initialized  */
@property (nonatomic, retain, readonly) AVAsset *asset NS_AVAILABLE(10_8, 5_0);

/* Indicates the type of file to be written by the session.
   The value of this property must be set before you invoke -exportAsynchronouslyWithCompletionHandler:; otherwise -exportAsynchronouslyWithCompletionHandler: will raise an NSInternalInconsistencyException.
   Setting the value of this property to a file type that's not among the session's supported file types will result in an NSInvalidArgumentException. See supportedFileTypes. */
@property (nonatomic, copy, nullable) AVFileType outputFileType;

/* Indicates the URL of the export session's output. You may use UTTypeCopyPreferredTagWithClass(outputFileType, kUTTagClassFilenameExtension) to obtain an appropriate path extension for the outputFileType you have specified. For more information about UTTypeCopyPreferredTagWithClass and kUTTagClassFilenameExtension, on iOS see <MobileCoreServices/UTType.h> and on Mac OS X see <LaunchServices/UTType.h>.  */
@property (nonatomic, copy, nullable) NSURL *outputURL;

/* indicates that the output file should be optimized for network use, e.g. that a QuickTime movie file should support "fast start" */
@property (nonatomic) BOOL shouldOptimizeForNetworkUse;

/* indicates the status of the export session */
@property (nonatomic, readonly) AVAssetExportSessionStatus status;

/* describes the error that occured if the export status is AVAssetExportSessionStatusFailed */
@property (nonatomic, readonly, nullable) NSError *error;

/*!
	@method						exportAsynchronouslyWithCompletionHandler:
	@abstract					Starts the asynchronous execution of an export session.
	@param						handler
								If internal preparation for export fails, the handler will be invoked synchronously.
								The handler may also be called asynchronously after -exportAsynchronouslyWithCompletionHandler: returns,
								in the following cases: 
								1) if a failure occurs during the export, including failures of loading, re-encoding, or writing media data to the output,
								2) if -cancelExport is invoked, 
								3) if export session succeeds, having completely written its output to the outputURL. 
								In each case, AVAssetExportSession.status will signal the terminal state of the asset reader, and if a failure occurs, the NSError 
								that describes the failure can be obtained from the error property.
	@discussion					Initiates an asynchronous export operation and returns immediately.
*/
- (void)exportAsynchronouslyWithCompletionHandler:(void (^)(void))handler;

/* Specifies the progress of the export on a scale from 0 to 1.0.  A value of 0 means the export has not yet begun, A value of 1.0 means the export is complete. This property is not key-value observable. */
@property (nonatomic, readonly) float progress;

/*!
	@method						cancelExport
	@abstract					Cancels the execution of an export session.
	@discussion					Cancel can be invoked when the export is running.
*/
- (void)cancelExport;

@end

@interface AVAssetExportSession (AVAssetExportSessionPresets)

/*!
	@method						allExportPresets
	@abstract					Returns all available export preset names.
	@discussion					Returns an array of NSStrings with the names of all available presets. Note that not all presets are 
								compatible with all AVAssets.
	@result						An NSArray containing an NSString for each of the available preset names.
*/
+ (NSArray<NSString *> *)allExportPresets;

/*!
	@method						exportPresetsCompatibleWithAsset:
	@abstract					Returns only the identifiers compatible with the given AVAsset object.
	@discussion					Not all export presets are compatible with all AVAssets. For example an video only asset is not compatible with an audio only preset.
								This method returns only the identifiers for presets that will be compatible with the given asset. 
								A client should pass in an AVAsset that is ready to be exported.
								In order to ensure that the setup and running of an export operation will succeed using a given preset no significant changes 
								(such as adding or deleting tracks) should be made to the asset between retrieving compatible identifiers and performing the export operation.
								This method will access the tracks property of the AVAsset to build the returned NSArray.  To avoid blocking the calling thread, 
								the tracks property should be loaded using the AVAsynchronousKeyValueLoading protocol before calling this method.
	@param asset				An AVAsset object that is intended to be exported.
	@result						An NSArray containing NSString values for the identifiers of compatible export types.  
								The array is a complete list of the valid identifiers that can be used as arguments to 
								initWithAsset:presetName: with the specified asset.
*/
+ (NSArray<NSString *> *)exportPresetsCompatibleWithAsset:(AVAsset *)asset;

/*!
	@method						determineCompatibilityOfExportPreset:withAsset:outputFileType:completionHandler:
	@abstract					Performs an inspection on the compatibility of an export preset, AVAsset and output file type.  Calls the completion handler with YES if
								the arguments are compatible; NO otherwise.
	@discussion					Not all export presets are compatible with all AVAssets and file types.  This method can be used to query compatibility.
								In order to ensure that the setup and running of an export operation will succeed using a given preset no significant changes 
								(such as adding or deleting tracks) should be made to the asset between retrieving compatible identifiers and performing the export operation.
	@param presetName			An NSString specifying the name of the preset template for the export.
	@param asset				An AVAsset object that is intended to be exported.
	@param outputFileType		An AVFileType indicating a file type to check; or nil, to query whether there are any compatible types.
	@param completionHandler	A block called with the compatibility result.
 */
+ (void)determineCompatibilityOfExportPreset:(NSString *)presetName withAsset:(AVAsset *)asset outputFileType:(nullable AVFileType)outputFileType completionHandler:(void (^)(BOOL compatible))handler NS_AVAILABLE(10_9, 6_0);

@end

@interface AVAssetExportSession (AVAssetExportSessionFileTypes)

/* Indicates the types of files the target can write, according to the preset the target was initialized with.
   Does not perform an inspection of the AVAsset to determine whether its contents are compatible with the supported file types. If you need to make that determination before initiating the export, use - (void)determineCompatibleFileTypesWithCompletionHandler:(void (^)(NSArray *compatibleFileTypes))handler:. */
@property (nonatomic, readonly) NSArray<AVFileType> *supportedFileTypes;

/*!
	@method						determineCompatibleFileTypesWithCompletionHandler:
	@abstract					Performs an inspection on the AVAsset and Preset the object was initialized with to determine a list of file types the ExportSession can write.
	@param						handler
								Called when the inspection completes with an array of file types the ExportSession can write.  Note that this may have a count of zero.
	@discussion					This method is different than the supportedFileTypes property in that it performs an inspection of the AVAsset in order to determine its compatibility with each of the session's supported file types.
*/
- (void)determineCompatibleFileTypesWithCompletionHandler:(void (^)(NSArray<AVFileType> *compatibleFileTypes))handler NS_AVAILABLE(10_9, 6_0);

@end

@interface AVAssetExportSession (AVAssetExportSessionDurationAndLength)

/* Specifies a time range to be exported from the source.  The default timeRange of an export session is kCMTimeZero..kCMTimePositiveInfinity, meaning that the full duration of the asset will be exported. */
@property (nonatomic) CMTimeRange timeRange;

/* Provides an estimate of the maximum duration of exported media that is possible given the source asset, the export preset, and the current value of fileLengthLimit.  The export will not stop when it reaches this maximum duration; set the timeRange property to export only a certain time range.  */
@property (nonatomic, readonly) CMTime maxDuration API_AVAILABLE(macos(10.14), ios(4.0), tvos(9.0)) __WATCHOS_PROHIBITED;

/* Indicates the estimated byte size of exported file. Returns zero when export preset is AVAssetExportPresetPassthrough or AVAssetExportPresetAppleProRes422LPCM. This property will also return zero if a numeric value (ie. not invalid, indefinite, or infinite) for the timeRange property has not been set. */
@property (nonatomic, readonly) long long estimatedOutputFileLength NS_AVAILABLE(10_9, 5_0);

/* Indicates the file length that the output of the session should not exceed.  Depending on the content of the source asset, it is possible for the output to slightly exceed the file length limit.  The length of the output file should be tested if you require that a strict limit be observed before making use of the output.  See also maxDuration and timeRange. */
@property (nonatomic) long long fileLengthLimit API_AVAILABLE(macos(10.14), ios(4.0), tvos(9.0)) __WATCHOS_PROHIBITED;

@end

@interface AVAssetExportSession (AVAssetExportSessionMetadata)

/* Specifies an NSArray of AVMetadataItems that are to be written to the output file by the export session.
   If the value of this key is nil, any existing metadata in the exported asset will be translated as accurately as possible into
   the appropriate metadata keyspace for the output file and written to the output. */
@property (nonatomic, copy, nullable) NSArray<AVMetadataItem *> *metadata;

/* Specifies a filter object to be used during export to determine which metadata items should be transferred from the source asset.
   If the value of this key is nil, no filter will be applied.  This is the default.
   The filter will not be applied to metadata set with via the metadata property.  To apply the filter to metadata before it is set on the metadata property, see the methods in AVMetadataItem's AVMetadataItemArrayFiltering category. */
@property (nonatomic, retain, nullable) AVMetadataItemFilter *metadataItemFilter NS_AVAILABLE(10_9, 7_0);

@end

@interface AVAssetExportSession (AVAssetExportSessionMediaProcessing)

/* Indicates the processing algorithm used to manage audio pitch for scaled audio edits.
   Constants for various time pitch algorithms, e.g. AVAudioTimePitchAlgorithmSpectral, are defined in AVAudioProcessingSettings.h. An NSInvalidArgumentException will be raised if this property is set to a value other than the constants defined in that file.
   The default value is AVAudioTimePitchAlgorithmSpectral. */
@property (nonatomic, copy) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_9, 7_0);

/* Indicates whether non-default audio mixing is enabled for export and supplies the parameters for audio mixing.  Ignored when export preset is AVAssetExportPresetPassthrough. */
@property (nonatomic, copy, nullable) AVAudioMix *audioMix;

/* Indicates whether video composition is enabled for export and supplies the instructions for video composition.  Ignored when export preset is AVAssetExportPresetPassthrough. */
@property (nonatomic, copy, nullable) AVVideoComposition *videoComposition;

/* Indicates the custom video compositor instance used, if any */
@property (nonatomic, readonly, nullable) id <AVVideoCompositing> customVideoCompositor NS_AVAILABLE(10_9, 7_0);

@end

@interface AVAssetExportSession (AVAssetExportSessionMultipass)

/*!
	@property	canPerformMultiplePassesOverSourceMediaData
	@abstract
		Determines whether the export session can perform multiple passes over the source media to achieve better results.
 
	@discussion
		When the value for this property is YES, the export session can produce higher quality results at the expense of longer export times.  Setting this property to YES may also require the export session to write temporary data to disk during the export.  To control the location of temporary data, use the property directoryForTemporaryFiles.
 
		The default value is NO.  Not all export session configurations can benefit from performing multiple passes over the source media.  In these cases, setting this property to YES has no effect.

		This property cannot be set after the export has started.
*/
@property (nonatomic) BOOL canPerformMultiplePassesOverSourceMediaData NS_AVAILABLE(10_10, 8_0);

/*!
	@property directoryForTemporaryFiles
	@abstract 
		Specifies a directory that is suitable for containing temporary files generated during the export process
 
	@discussion
		AVAssetExportSession may need to write temporary files when configured in certain ways, such as when canPerformMultiplePassesOverSourceMediaData is set to YES.  This property can be used to control where in the filesystem those temporary files are created.  All temporary files will be deleted when the export is completed, is canceled, or fails.
 
		When the value of this property is nil, the export session will choose a suitable location when writing temporary files.  The default value is nil.
	
		This property cannot be set after the export has started.  The export will fail if the URL points to a location that is not a directory, does not exist, is not on the local file system, or if a file cannot be created in this directory (for example, due to insufficient permissions or sandboxing restrictions).
*/
@property (nonatomic, copy, nullable) NSURL *directoryForTemporaryFiles NS_AVAILABLE(10_10, 8_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVRouteDetector.h
/*
	File:  AVRouteDetector.h
 
	Framework:  AVFoundation
 
	Copyright 2017 Apple Inc. All rights reserved.
 
 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

@class AVRouteDetectorInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
 @class		AVRouteDetector
 @abstract	AVRouteDetector detects the presence of media playback routes.
 @discussion	
	If route detection is enabled (it is disabled by default), AVRouteDetector reports whether or not multiple playback routes have been detected. If this is the case, AVKit's AVRoutePickerView can be used to allow users to pick from the set of available routes.
 */

API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) API_UNAVAILABLE(watchos)
@interface AVRouteDetector : NSObject
{
@private
	AVRouteDetectorInternal *_routeDetectorInternal;
}

/*!
 @property	routeDetectionEnabled
 @abstract	Whether or not route detection is enabled. The default value is NO.
 @discussion	
	Route detection significantly increases power consumption and must be turned off when it's no longer needed.
 */
@property (getter=isRouteDetectionEnabled) BOOL routeDetectionEnabled;

/*!
 @property	multipleRoutesDetected
 @abstract	This property is YES if, in addition to the local playback route, at least one more playback route has been detected.
 @discussion	
	If multiple route have been detected, AVKit's AVRoutePickerView can be used to allow users to pick from the set of available routes. When the values of this property changes AVRouteDetectorMultipleRoutesDetectedDidChangeNotification is posted.
 */
@property (readonly) BOOL multipleRoutesDetected;

/*!
 @constant	AVRouteDetectorMultipleRoutesDetectedDidChangeNotification
 @abstract	Posted when the value of multipleRoutesDetected changes.
 */
AVF_EXPORT NSNotificationName const AVRouteDetectorMultipleRoutesDetectedDidChangeNotification API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) API_UNAVAILABLE(watchos);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioMixing.h
/*
	File:           AVAudioMixing.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioMixing.h>

// ==========  AVFoundation.framework/Headers/AVAudioMixerNode.h
/*
	File:           AVAudioMixerNode.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioMixerNode.h>

// ==========  AVFoundation.framework/Headers/AVAssetTrack.h
/*
	File:  AVAssetTrack.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

/*!
	@class		AVAssetTrack
 
	@abstract	An AVAssetTrack object provides provides the track-level inspection interface for all assets.
 
	@discussion
		AVAssetTrack adopts the AVAsynchronousKeyValueLoading protocol. Methods in the protocol should be used to access a track's properties without blocking the current thread. To cancel load requests for all keys of AVAssetTrack one must message the parent AVAsset object (for example, [track.asset cancelLoading])
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAsynchronousKeyValueLoading.h>
#import <AVFoundation/AVAsset.h>
#import <AVFoundation/AVAssetTrackSegment.h>
#import <AVFoundation/AVMediaFormat.h>
#import <AVFoundation/AVMetadataFormat.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAssetTrackInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVAssetTrack : NSObject <NSCopying, AVAsynchronousKeyValueLoading>
{
@private
	AVAssetTrackInternal     *_track;
}
AV_INIT_UNAVAILABLE

/* provides a reference to the AVAsset of which the AVAssetTrack is a part  */
@property (nonatomic, readonly, weak) AVAsset *asset;

/* indicates the persistent unique identifier for this track of the asset  */
@property (nonatomic, readonly) CMPersistentTrackID trackID;

/* Note that cancellation of loading requests for all keys of AVAssetTrack must be made on the parent AVAsset, e.g. [[track.asset] cancelLoading] */

@end


@interface AVAssetTrack (AVAssetTrackBasicPropertiesAndCharacteristics)

/* indicates the media type for this track, e.g. AVMediaTypeVideo, AVMediaTypeAudio, etc., as defined in AVMediaFormat.h. */
@property (nonatomic, readonly) AVMediaType mediaType;

/* provides an array of CMFormatDescriptions
   each of which indicates the format of media samples referenced by the track;
   a track that presents uniform media, e.g. encoded according to the same encoding settings,
   will provide an array with a count of 1 */
@property (nonatomic, readonly) NSArray *formatDescriptions;

/* Indicates whether the receiver is playable in the current environment; if YES, an AVPlayerItemTrack of an AVPlayerItem initialized with the receiver's asset can be enabled for playback.  */
@property (nonatomic, readonly, getter=isPlayable) BOOL playable NS_AVAILABLE(10_8, 5_0);

/* Indicates whether the receiver is decodable in the current environment; if YES, the track can be decoded even though decoding may be too slow for real time playback.  */
@property (nonatomic, readonly, getter=isDecodable) BOOL decodable NS_AVAILABLE(10_13, 11_0);

/* indicates whether the track is enabled according to state stored in its container or construct;
   note that its presentation state can be changed from this default via AVPlayerItemTrack */
@property (nonatomic, readonly, getter=isEnabled) BOOL enabled;

/* indicates whether the track references sample data only within its storage container */
@property (nonatomic, readonly, getter=isSelfContained) BOOL selfContained;

/* indicates the total number of bytes of sample data required by the track */
@property (nonatomic, readonly) long long totalSampleDataLength;

/*!
	@method			hasMediaCharacteristic:
	@abstract		Reports whether the track references media with the specified media characteristic.
	@param			mediaCharacteristic
					The media characteristic of interest, e.g. AVMediaCharacteristicVisual, AVMediaCharacteristicAudible, AVMediaCharacteristicLegible, etc.,
					as defined above.
	@result			YES if the track references media with the specified characteristic, otherwise NO.
*/
- (BOOL)hasMediaCharacteristic:(AVMediaCharacteristic)mediaCharacteristic;

@end


@interface AVAssetTrack (AVAssetTrackTemporalProperties)

/* Indicates the timeRange of the track within the overall timeline of the asset;
   a track with CMTIME_COMPARE_INLINE(timeRange.start, >, kCMTimeZero) will initially present an empty interval. */
@property (nonatomic, readonly) CMTimeRange timeRange;

/*	indicates a timescale in which time values for the track can be operated upon without extraneous numerical conversion  */
@property (nonatomic, readonly) CMTimeScale naturalTimeScale;

/* indicates the estimated data rate of the media data referenced by the track, in units of bits per second */
@property (nonatomic, readonly) float estimatedDataRate;

@end


@interface AVAssetTrack (AVAssetTrackLanguageProperties)

/* indicates the language associated with the track, as an ISO 639-2/T language code;
   may be nil if no language is indicated */
@property (nonatomic, readonly, nullable) NSString *languageCode;

/* indicates the language tag associated with the track, as an IETF BCP 47 (RFC 4646) language identifier;
   may be nil if no language tag is indicated */
@property (nonatomic, readonly, nullable) NSString *extendedLanguageTag;

@end


@interface AVAssetTrack (AVAssetTrackPropertiesForVisualCharacteristic)

/* indicates the natural dimensions of the media data referenced by the track as a CGSize */
@property (nonatomic, readonly) CGSize naturalSize;

/* indicates the transform specified in the track's storage container as the preferred transformation of the visual media data for display purposes;
   its value is often but not always CGAffineTransformIdentity */
@property (nonatomic, readonly) CGAffineTransform preferredTransform;

@end


@interface AVAssetTrack (AVAssetTrackPropertiesForAudibleCharacteristic)

/* indicates the volume specified in the track's storage container as the preferred volume of the audible media data */
@property (nonatomic, readonly) float preferredVolume;

@end


@interface AVAssetTrack (AVAssetTrackPropertiesForFrameBasedCharacteristic)

/*!
	@property		nominalFrameRate
	@abstract		For tracks that carry a full frame per media sample, indicates the frame rate of the track in units of frames per second.
	@discussion		For field-based video tracks that carry one field per media sample, the value of this property is the field rate, not the frame rate.
*/
@property (nonatomic, readonly) float nominalFrameRate;

/* indicates the minimum duration of the track's frames; the value will be kCMTimeInvalid if the minimum frame duration is not known or cannot be calculated */
@property (nonatomic, readonly) CMTime minFrameDuration NS_AVAILABLE(10_10, 7_0);

/*!
	@property       requiresFrameReordering
	@abstract       Indicates whether samples in the track may have different values for their presentation and decode timestamps.
*/
@property (nonatomic, readonly) BOOL requiresFrameReordering NS_AVAILABLE(10_10, 8_0);

@end


@interface AVAssetTrack (AVAssetTrackSegments)

/* Provides an array of AVAssetTrackSegments with time mappings from the timeline of the track's media samples to the timeline of the track.
   Empty edits, i.e. timeRanges for which no media data is available to be presented, have a value of AVAssetTrackSegment.empty equal to YES. */
@property (nonatomic, copy, readonly) NSArray<AVAssetTrackSegment *> *segments;

/*!
	@method			segmentForTrackTime:
	@abstract		Supplies the AVAssetTrackSegment from the segments array with a target timeRange that either contains the specified track time or is the closest to it among the target timeRanges of the track's segments.
	@param			trackTime
					The trackTime for which an AVAssetTrackSegment is requested.
	@result			An AVAssetTrackSegment.
	@discussion		If the trackTime does not map to a sample presentation time (e.g. it's outside the track's timeRange), the segment closest in time to the specified trackTime is returned. 
*/
- (nullable AVAssetTrackSegment *)segmentForTrackTime:(CMTime)trackTime;

/*!
	@method			samplePresentationTimeForTrackTime:
	@abstract		Maps the specified trackTime through the appropriate time mapping and returns the resulting sample presentation time.
	@param			trackTime
					The trackTime for which a sample presentation time is requested.
	@result			A CMTime; will be invalid if the trackTime is out of range
*/
- (CMTime)samplePresentationTimeForTrackTime:(CMTime)trackTime;

@end


@interface AVAssetTrack (AVAssetTrackMetadataReading)

// high-level access to selected metadata of common interest

/* provides access to an array of AVMetadataItems for each common metadata key for which a value is available */
@property (nonatomic, readonly) NSArray<AVMetadataItem *> *commonMetadata;

/* Provides access to an array of AVMetadataItems for all metadata identifiers for which a value is available; items can be filtered according to language via +[AVMetadataItem metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:] and according to identifier via +[AVMetadataItem metadataItemsFromArray:filteredByIdentifier:].
*/
@property (nonatomic, readonly) NSArray<AVMetadataItem *> *metadata NS_AVAILABLE(10_10, 8_0);

/* provides an NSArray of NSStrings, each representing a format of metadata that's available for the track (e.g. QuickTime userdata, etc.)
   Metadata formats are defined in AVMetadataItem.h. */
@property (nonatomic, readonly) NSArray<AVMetadataFormat> *availableMetadataFormats;

/*!
	@method			metadataForFormat:
	@abstract		Provides an NSArray of AVMetadataItems, one for each metadata item in the container of the specified format.
	@param			format
					The metadata format for which items are requested.
	@result			An NSArray containing AVMetadataItems.
	@discussion		Becomes callable without blocking when the key @"availableMetadataFormats" has been loaded
*/
- (NSArray<AVMetadataItem *> *)metadataForFormat:(AVMetadataFormat)format;

@end


@interface AVAssetTrack (AVAssetTrackTrackAssociations)

/*!
 @typedef AVTrackAssociationType
 @abstract
    The type of a track association.
*/
typedef NSString * AVTrackAssociationType NS_STRING_ENUM;

/*
 @constant		AVTrackAssociationTypeAudioFallback
 @abstract		Indicates an association between an audio track with another audio track that contains the same content but is typically encoded in a different format that's more widely supported, used to nominate a track that should be used in place of an unsupported track.
 
 @discussion
	Associations of type AVTrackAssociationTypeAudioFallback are supported only between audio tracks.  This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the receiver should be an instance of AVAssetWriterInput with a corresponding track that has content that's less widely supported, and the input parameter should be an instance of AVAssetWriterInput with a corresponding track that has content that's more widely supported.
	
	Example: Using AVTrackAssociationTypeAudioFallback, a stereo audio track with media subtype kAudioFormatMPEG4AAC could be nominated as the "fallback" for an audio track encoding the same source material but with media subtype kAudioFormatAC3 and a 5.1 channel layout.  This would ensure that all clients are capable of playing back some form of the audio.

 */
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeAudioFallback NS_AVAILABLE(10_9, 7_0);

/*
 @constant		AVTrackAssociationTypeChapterList
 @abstract		Indicates an association between a track with another track that contains chapter information.  The track containing chapter information may be a text track, a video track, or a timed metadata track.
 
 @discussion
	This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the receiver should be an instance of AVAssetWriterInput with a corresponding track that has renderable content while the input parameter should be an instance of AVAssetWriterInput with a corresponding track that contains chapter metadata.
 */
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeChapterList NS_AVAILABLE(10_9, 7_0);

/*
 @constant		AVTrackAssociationTypeForcedSubtitlesOnly
 @abstract		Indicates an association between a subtitle track typically containing both forced and non-forced subtitles with another subtitle track that contains only forced subtitles, for use when the user indicates that only essential subtitles should be displayed.  When such an association is established, the forced subtitles in both tracks are expected to present the same content in the same language but may have different timing.
 
 @discussion
	Associations of type AVTrackAssociationTypeForcedSubtitlesOnly are supported only between subtitle tracks.  This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the receiver should be an instance of AVAssetWriterInput with a corresponding subtitle track that contains non-forced subtitles, and the input parameter should be an instance of AVAssetWriterInput with a corresponding subtitle track that contains forced subtitles only.
 */
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeForcedSubtitlesOnly NS_AVAILABLE(10_9, 7_0);

/*
 @constant		AVTrackAssociationTypeSelectionFollower
 @abstract		Indicates an association between a pair of tracks that specifies that, when the first of the pair is selected, the second of the pair should be considered an appropriate default for selection also.  Example: a subtitle track in the same language as an audio track may be associated with that audio track using AVTrackAssociationTypeSelectionFollower, to indicate that selection of the subtitle track, in the absence of a directive for subtitle selection from the user, can "follow" the selection of the audio track.
 
 @discussion
	This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the input parameter should be an instance of AVAssetWriterInput whose selection may depend on the selection of the receiver.  In the example above, the receiver would be the instance of AVAssetWriterInput corresponding with the audio track and the input parameter would be the instance of AVAssetWriterInput corresponding with the subtitle track.
 */
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeSelectionFollower NS_AVAILABLE(10_9, 7_0);

/*
 @constant		AVTrackAssociationTypeTimecode
 @abstract		Indicates an association between a track with another track that contains timecode information.  The track containing timecode information should be a timecode track.
 
 @discussion
	This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the receiver should be an instance of AVAssetWriterInput with a corresponding track that may be a video track or an audio track while the input parameter should be an instance of AVAssetWriterInput with a corresponding timecode track.
 */
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeTimecode NS_AVAILABLE(10_9, 7_0);

/*
@constant		AVTrackAssociationTypeMetadataReferent
@abstract		Indicates an association between a metadata track and the track that's described or annotated via the contents of the metadata track.

@discussion
	This track association is optional for AVAssetTracks with the mediaType AVMediaTypeMetadata. When a metadata track lacks this track association, its contents are assumed to describe or annotate the asset as a whole.
	This association is not symmetric; when used with -[AVAssetWriterInput addTrackAssociationWithTrackOfInput:type:], the receiver should be an instance of AVAssetWriterInput with mediaType AVMediaTypeMetadata while the input parameter should be an instance of AVAssetWriterInput that's used to create the track to which the contents of the receiver's corresponding metadata track refer.
*/
AVF_EXPORT AVTrackAssociationType const AVTrackAssociationTypeMetadataReferent NS_AVAILABLE(10_10, 8_0);

/* Provides an NSArray of NSStrings, each representing a type of track association that the receiver has with one or more of the other tracks of the asset (e.g. AVTrackAssociationTypeChapterList, AVTrackAssociationTypeTimecode, etc.).
   Track association types are defined immediately above. */
@property (nonatomic, readonly) NSArray<AVTrackAssociationType> *availableTrackAssociationTypes NS_AVAILABLE(10_9, 7_0);

/*!
	@method			associatedTracksOfType:
	@abstract		Provides an NSArray of AVAssetTracks, one for each track associated with the receiver with the specified type of track association.
	@param			trackAssociationType
					The type of track association for which associated tracks are requested.
	@result			An NSArray containing AVAssetTracks; may be empty if there is no associated tracks of the specified type.
	@discussion		Becomes callable without blocking when the key @"availableTrackAssociationTypes" has been loaded.
*/
- (NSArray<AVAssetTrack *> *)associatedTracksOfType:(AVTrackAssociationType)trackAssociationType NS_AVAILABLE(10_9, 7_0);

@end


#if !TARGET_OS_IPHONE

@class AVSampleCursor;

@interface AVAssetTrack (AVAssetTrackSampleCursorProvision)

/* Indicates whether the receiver can provide instances of AVSampleCursor for traversing its media samples and discovering information about them. */
@property (nonatomic, readonly) BOOL canProvideSampleCursors NS_AVAILABLE_MAC(10_10);

/*!
	@method			makeSampleCursorWithPresentationTimeStamp:
	@abstract		Creates an instance of AVSampleCursor and positions it at or near the specified presentation timestamp.
	@param			presentationTimeStamp
					The desired initial presentation timestamp of the returned AVSampleCursor.
	@result			An instance of AVSampleCursor.
	@discussion		If the receiver's asset has a value of YES for providesPreciseDurationAndTiming, the sample cursor will be accurately positioned at the receiver's last media sample with presentation timestamp less than or equal to the desired timestamp, or, if there are no such samples, the first sample in presentation order.
					If the receiver's asset has a value of NO for providesPreciseDurationAndTiming, and it is prohibitively expensive to locate the precise sample at the desired timestamp, the sample cursor may be approximately positioned.
*/
- (nullable AVSampleCursor *)makeSampleCursorWithPresentationTimeStamp:(CMTime)presentationTimeStamp NS_AVAILABLE_MAC(10_10);

/*!
	@method			makeSampleCursorAtFirstSampleInDecodeOrder:
	@abstract		Creates an instance of AVSampleCursor and positions it at the receiver's first media sample in decode order.
	@result			An instance of AVSampleCursor.
*/
- (nullable AVSampleCursor *)makeSampleCursorAtFirstSampleInDecodeOrder NS_AVAILABLE_MAC(10_10);

/*!
	@method			makeSampleCursorAtLastSampleInDecodeOrder:
	@abstract		Creates an instance of AVSampleCursor and positions it at the receiver's last media sample in decode order.
	@result			An instance of AVSampleCursor.
*/
- (nullable AVSampleCursor *)makeSampleCursorAtLastSampleInDecodeOrder NS_AVAILABLE_MAC(10_10);

@end

#endif // !TARGET_OS_IPHONE

#pragma mark --- AVAssetTrack change notifications ---

/*
	AVAssetTrack change notifications are posted by instances of mutable subclasses, AVMutableCompositionTrack and AVMutableMovieTrack.
	Some of the notifications are also posted by instances of dynamic subclasses, AVFragmentedAssetTrack and AVFragmentedMovieTrack, but these are capable of changing only in well-defined ways and only under specific conditions that you control. 
*/

/*!
 @constant       AVAssetTrackTimeRangeDidChangeNotification
 @abstract       Posted when the timeRange of an AVFragmentedAssetTrack changes while the associated instance of AVFragmentedAsset is being minded by an AVFragmentedAssetMinder, but only for changes that occur after the status of the value of @"timeRange" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetTrackTimeRangeDidChangeNotification NS_AVAILABLE(10_11, 9_0);

/*!
 @constant       AVAssetTrackSegmentsDidChangeNotification
 @abstract       Posted when the array of segments of an AVFragmentedAssetTrack changes while the associated instance of AVFragmentedAsset is being minded by an AVFragmentedAssetMinder, but only for changes that occur after the status of the value of @"segments" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetTrackSegmentsDidChangeNotification NS_AVAILABLE(10_11, 9_0);

/*!
 @constant       AVAssetTrackTrackAssociationsDidChangeNotification
 @abstract       Posted when the collection of track associations of an AVAssetTrack changes, but only for changes that occur after the status of the value of @"availableTrackAssociationTypes" has reached AVKeyValueStatusLoaded.
*/
AVF_EXPORT NSString *const AVAssetTrackTrackAssociationsDidChangeNotification NS_AVAILABLE(10_11, 9_0);

#pragma mark --- AVFragmentedAssetTrack ---
/*!
	@class			AVFragmentedAssetTrack
	@abstract		A subclass of AVAssetTrack for handling tracks of fragmented assets. An AVFragmentedAssetTrack is capable of changing the values of certain of its properties, if its parent asset is associated with an instance of AVFragmentedAssetMinder when one or more fragments are appended to the underlying media resource.
	@discussion		While its parent asset is associated with an AVFragmentedAssetMinder, AVFragmentedAssetTrack posts AVAssetTrackTimeRangeDidChangeNotification and AVAssetTrackSegmentsDidChangeNotification whenever new fragments are detected, as appropriate.
*/

@class AVFragmentedAssetTrackInternal;

API_AVAILABLE(macos(10.11), ios(12.0), tvos(12.0)) API_UNAVAILABLE(watchos)
@interface AVFragmentedAssetTrack : AVAssetTrack
{
@private
	AVFragmentedAssetTrackInternal	*_fragmentedAssetTrack __attribute__((unused));
}

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMetadataFormat.h
/*
    File:  AVMetadataFormat.h

    Framework:  AVFoundation
 
    Copyright 2010-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

typedef NSString * AVMetadataFormat NS_STRING_ENUM;
typedef NSString * AVMetadataKeySpace NS_EXTENSIBLE_STRING_ENUM;
typedef NSString * AVMetadataKey NS_EXTENSIBLE_STRING_ENUM;

// CommonMetadata
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceCommon                                 NS_AVAILABLE(10_7, 4_0);

// Metadata common keys
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyTitle                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyCreator                                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeySubject                                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyDescription                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyPublisher                                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyContributor                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyCreationDate                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyLastModifiedDate                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyType                                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyFormat                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyIdentifier                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeySource                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyLanguage                                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyRelation                                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyLocation                                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyCopyrights                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyAlbumName                                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyAuthor                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyArtist                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyArtwork                                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyMake                                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeyModel                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataCommonKeySoftware                                   NS_AVAILABLE(10_7, 4_0);

// QuickTimeUserData
AVF_EXPORT AVMetadataFormat const AVMetadataFormatQuickTimeUserData                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceQuickTimeUserData                      NS_AVAILABLE(10_7, 4_0);

// QuickTimeUserData keys
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyAlbum                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyArranger                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyArtist                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyAuthor                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyChapter                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyComment                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyComposer                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyCopyright                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyCreationDate                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyDescription                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyDirector                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyDisclaimer                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyEncodedBy                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyFullName                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyGenre                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyHostComputer                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyInformation                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyKeywords                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyMake                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyModel                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyOriginalArtist                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyOriginalFormat                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyOriginalSource                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyPerformers                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyProducer                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyPublisher                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyProduct                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeySoftware                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeySpecialPlaybackRequirements     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyTrack                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyWarning                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyWriter                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyURLLink                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyLocationISO6709                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyTrackName                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyCredits                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyPhonogramRights                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeUserDataKeyTaggedCharacteristic            NS_AVAILABLE(10_8, 5_0);

// ISO UserData
AVF_EXPORT AVMetadataFormat const AVMetadataFormatISOUserData                                NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceISOUserData                            NS_AVAILABLE(10_9, 7_0);

// ISO UserData keys (includes 3GPP keys)
AVF_EXPORT AVMetadataKey const AVMetadataISOUserDataKeyCopyright                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataISOUserDataKeyTaggedCharacteristic                  NS_AVAILABLE(10_10, 8_0);

/*!
 @constant		AVMetadataISOUserDataKeyDate
 @abstract		ISO User data key for the content creation date/time.
 @discussion	The value is date and time, formatted according to ISO 8601, when the content was created. For clips captured by recording devices, this is typically the date and time when the clip’s recording started. When stored in AV(Mutable)MetadataItem, the value type must be either NSDate or NSString. When NSString is used, the value uses one of ISO 8601 formats such as "2016-01-11T17:31:10Z".
*/
AVF_EXPORT AVMetadataKey const AVMetadataISOUserDataKeyDate                                  NS_AVAILABLE(10_12, 10_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyCopyright                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyAuthor                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyPerformer                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyGenre                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyRecordingYear                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyLocation                              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyTitle                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyDescription                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyCollection                            NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyUserRating                            NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyThumbnail                             NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyAlbumAndTrack                         NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyKeywordList                           NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyMediaClassification                   NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVMetadataKey const AVMetadata3GPUserDataKeyMediaRating                           NS_AVAILABLE(10_9, 7_0);

// QuickTimeMetadata
AVF_EXPORT AVMetadataFormat const AVMetadataFormatQuickTimeMetadata                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceQuickTimeMetadata                      NS_AVAILABLE(10_7, 4_0);

// QuickTimeMetadata keys. For more information, see the QuickTime File Format Specification, available as part of the Mac OS X Reference Library at http://developer.apple.com/library/mac/navigation/
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyAuthor                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyComment                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCopyright                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCreationDate                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyDirector                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyDisplayName                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyInformation                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyKeywords                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyProducer                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyPublisher                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyAlbum                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyArtist                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyArtwork                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyDescription                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeySoftware                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyYear                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyGenre                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyiXML                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationISO6709                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyMake                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyModel                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyArranger                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyEncodedBy                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyOriginalArtist                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyPerformer                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyComposer                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCredits                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyPhonogramRights                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCameraIdentifier                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCameraFrameReadoutTime          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyTitle                           NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyCollectionUser                  NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyRatingUser                      NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationName                    NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationBody                    NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationNote                    NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationRole                    NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyLocationDate                    NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyDirectionFacing                 NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyDirectionMotion                 NS_AVAILABLE(10_7, 4_3);
AVF_EXPORT AVMetadataKey const AVMetadataQuickTimeMetadataKeyContentIdentifier               NS_AVAILABLE(10_11, 9_0);

// iTunesMetadata
AVF_EXPORT AVMetadataFormat const AVMetadataFormatiTunesMetadata                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceiTunes                                 NS_AVAILABLE(10_7, 4_0);

// iTunesMetadata keys
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAlbum                              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyArtist                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyUserComment                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyCoverArt                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyCopyright                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyReleaseDate                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyEncodedBy                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyPredefinedGenre                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyUserGenre                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeySongName                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyTrackSubTitle                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyEncodingTool                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyComposer                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAlbumArtist                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAccountKind                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAppleID                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyArtistID                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeySongID                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyDiscCompilation                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyDiscNumber                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyGenreID                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyGrouping                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyPlaylistID                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyContentRating                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyBeatsPerMin                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyTrackNumber                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyArtDirector                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyArranger                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAuthor                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyLyrics                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyAcknowledgement                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyConductor                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyDescription                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyDirector                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyEQ                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyLinerNotes                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyRecordCompany                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyOriginalArtist                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyPhonogramRights                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyProducer                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyPerformer                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyPublisher                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeySoundEngineer                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeySoloist                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyCredits                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyThanks                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyOnlineExtras                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataiTunesMetadataKeyExecProducer                       NS_AVAILABLE(10_7, 4_0);

// ID3Metadata
AVF_EXPORT AVMetadataFormat const AVMetadataFormatID3Metadata                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceID3                                    NS_AVAILABLE(10_7, 4_0);

// ID3Metadata keys
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyAudioEncryption                       /* AENC Audio encryption */                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyAttachedPicture                       /* APIC Attached picture */                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyAudioSeekPointIndex                   /* ASPI Audio seek point index */                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyComments                              /* COMM Comments */                                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyCommercial                            /* COMR Commercial frame */                                     NS_AVAILABLE(10_11, 9_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyCommerical                            /* COMR Commercial frame */                                     NS_DEPRECATED(10_7, 10_11, 4_0, 9_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEncryption                            /* ENCR Encryption method registration */                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEqualization                          /* EQUA Equalization */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEqualization2                         /* EQU2 Equalisation (2) */                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEventTimingCodes                      /* ETCO Event timing codes */                                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyGeneralEncapsulatedObject             /* GEOB General encapsulated object */                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyGroupIdentifier                       /* GRID Group identification registration */                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInvolvedPeopleList_v23                /* IPLS Involved people list */                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyLink                                  /* LINK Linked information */                                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyMusicCDIdentifier                     /* MCDI Music CD identifier */                                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyMPEGLocationLookupTable               /* MLLT MPEG location lookup table */                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOwnership                             /* OWNE Ownership frame */                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPrivate                               /* PRIV Private frame */                                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPlayCounter                           /* PCNT Play counter */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPopularimeter                         /* POPM Popularimeter */                                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPositionSynchronization               /* POSS Position synchronisation frame */                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyRecommendedBufferSize                 /* RBUF Recommended buffer size */                              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyRelativeVolumeAdjustment              /* RVAD Relative volume adjustment */                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyRelativeVolumeAdjustment2             /* RVA2 Relative volume adjustment (2) */                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyReverb                                /* RVRB Reverb */                                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySeek                                  /* SEEK Seek frame */                                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySignature                             /* SIGN Signature frame */                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySynchronizedLyric                     /* SYLT Synchronized lyric/text */                              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySynchronizedTempoCodes                /* SYTC Synchronized tempo codes */                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyAlbumTitle                            /* TALB Album/Movie/Show title */                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyBeatsPerMinute                        /* TBPM BPM (beats per minute) */                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyComposer                              /* TCOM Composer */                                             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyContentType                           /* TCON Content type */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyCopyright                             /* TCOP Copyright message */                                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyDate                                  /* TDAT Date */                                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEncodingTime                          /* TDEN Encoding time */                                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPlaylistDelay                         /* TDLY Playlist delay */                                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalReleaseTime                   /* TDOR Original release time */                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyRecordingTime                         /* TDRC Recording time */                                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyReleaseTime                           /* TDRL Release time */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTaggingTime                           /* TDTG Tagging time */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEncodedBy                             /* TENC Encoded by */                                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyLyricist                              /* TEXT Lyricist/Text writer */                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyFileType                              /* TFLT File type */                                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTime                                  /* TIME Time */                                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInvolvedPeopleList_v24                /* TIPL Involved people list */                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyContentGroupDescription               /* TIT1 Content group description */                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTitleDescription                      /* TIT2 Title/songname/content description */                   NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySubTitle                              /* TIT3 Subtitle/Description refinement */                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInitialKey                            /* TKEY Initial key */                                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyLanguage                              /* TLAN Language(s) */                                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyLength                                /* TLEN Length */                                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyMusicianCreditsList                   /* TMCL Musician credits list */                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyMediaType                             /* TMED Media type */                                           NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyMood                                  /* TMOO Mood */                                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalAlbumTitle                    /* TOAL Original album/movie/show title */                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalFilename                      /* TOFN Original filename */                                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalLyricist                      /* TOLY Original lyricist(s)/text writer(s) */                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalArtist                        /* TOPE Original artist(s)/performer(s) */                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOriginalReleaseYear                   /* TORY Original release year */                                NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyFileOwner                             /* TOWN File owner/licensee */                                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyLeadPerformer                         /* TPE1 Lead performer(s)/Soloist(s) */                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyBand                                  /* TPE2 Band/orchestra/accompaniment */                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyConductor                             /* TPE3 Conductor/performer refinement */                       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyModifiedBy                            /* TPE4 Interpreted, remixed, or otherwise modified by */       NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPartOfASet                            /* TPOS Part of a set */                                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyProducedNotice                        /* TPRO Produced notice */                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPublisher                             /* TPUB Publisher */                                            NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTrackNumber                           /* TRCK Track number/Position in set */                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyRecordingDates                        /* TRDA Recording dates */                                      NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInternetRadioStationName              /* TRSN Internet radio station name */                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInternetRadioStationOwner             /* TRSO Internet radio station owner */                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySize                                  /* TSIZ Size */                                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyAlbumSortOrder                        /* TSOA Album sort order */                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPerformerSortOrder                    /* TSOP Performer sort order */                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTitleSortOrder                        /* TSOT Title sort order */                                     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyInternationalStandardRecordingCode    /* TSRC ISRC (international standard recording code) */         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyEncodedWith                           /* TSSE Software/Hardware and settings used for encoding */     NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeySetSubtitle                           /* TSST Set subtitle */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyYear                                  /* TYER Year */                                                 NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyUserText                              /* TXXX User defined text information frame */                  NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyUniqueFileIdentifier                  /* UFID Unique file identifier */                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyTermsOfUse                            /* USER Terms of use */                                         NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyUnsynchronizedLyric                   /* USLT Unsynchronized lyric/text transcription */              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyCommercialInformation                 /* WCOM Commercial information */                               NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyCopyrightInformation                  /* WCOP Copyright/Legal information */                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOfficialAudioFileWebpage              /* WOAF Official audio file webpage */                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOfficialArtistWebpage                 /* WOAR Official artist/performer webpage */                    NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOfficialAudioSourceWebpage            /* WOAS Official audio source webpage */                        NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOfficialInternetRadioStationHomepage  /* WORS Official Internet radio station homepage */             NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyPayment                               /* WPAY Payment */                                              NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyOfficialPublisherWebpage              /* WPUB Publishers official webpage */                          NS_AVAILABLE(10_7, 4_0);
AVF_EXPORT AVMetadataKey const AVMetadataID3MetadataKeyUserURL                               /* WXXX User defined URL link frame */                          NS_AVAILABLE(10_7, 4_0);

// Icecast/ShoutCAST streaming metadata
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceIcy                                    NS_AVAILABLE(10_10, 8_0);

AVF_EXPORT AVMetadataKey const AVMetadataIcyMetadataKeyStreamTitle                           NS_AVAILABLE(10_10, 8_0);
AVF_EXPORT AVMetadataKey const AVMetadataIcyMetadataKeyStreamURL                             NS_AVAILABLE(10_10, 8_0);

// HTTP Live Streaming metadata
AVF_EXPORT AVMetadataFormat const AVMetadataFormatHLSMetadata                                NS_AVAILABLE(10_10, 8_0);
// HLS Metadata does not define its own keySpace or keys. Use of the keySpace AVMetadataKeySpaceQuickTimeMetadata and its keys is recommended.
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceHLSDateRange                           NS_AVAILABLE(10_11_3, 9_3);

// Keys for metadata provided by AudioToolbox's AudioFile interface. See <AudioToolbox/AudioFile.h>
AVF_EXPORT AVMetadataKeySpace const AVMetadataKeySpaceAudioFile                              NS_AVAILABLE(10_13, 11_0);

// Metadata format for AVMetadataItems of unknown provenance. This can occur when metadata is provided generically by an intermediate interface, such as AudioToolbox's AudioFile interface.
AVF_EXPORT AVMetadataFormat const AVMetadataFormatUnknown                                    NS_AVAILABLE(10_13, 11_0);

// Extra attribute keys
typedef NSString * AVMetadataExtraAttributeKey NS_EXTENSIBLE_STRING_ENUM;

/*!
 @constant		AVMetadataExtraAttributeValueURIKey
 @abstract
	When present in an item's extraAttributes dictionary, identifies the resource to be used as the item's value. Values for this key are of type NSString.
*/
AVF_EXPORT AVMetadataExtraAttributeKey const AVMetadataExtraAttributeValueURIKey             NS_AVAILABLE(10_10, 8_0);

/*!
 @constant		AVMetadataExtraAttributeBaseURIKey
 @abstract
	When present in an item's extraAttributes dictionary, identifies the base URI against which other URIs related to the item are to be resolved, e.g. AVMetadataExtraAttributeValueURIKey. Values for this key are of type NSString.
*/
AVF_EXPORT AVMetadataExtraAttributeKey const AVMetadataExtraAttributeBaseURIKey              NS_AVAILABLE(10_10, 8_0);

/*!
	@constant		AVMetadataExtraAttributeInfoKey
	@abstract		More information about the item; specific to the 
					item keySpace & key.
	@discussion		For example, this key is used with the following ID3 tags:
					TXXX, WXXX, APIC, GEOB: carries the Description
					PRIV: carries the Owner Identifier
 */
AVF_EXPORT AVMetadataExtraAttributeKey const AVMetadataExtraAttributeInfoKey                 NS_AVAILABLE(10_11, 9_0);

// ==========  AVFoundation.framework/Headers/AVAudioFormat.h
/*
	File:           AVAudioFormat.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioFormat.h>

// ==========  AVFoundation.framework/Headers/AVPlayerItemOutput.h
/*
    File:  AVPlayerItemOutput.h

	Framework:  AVFoundation
 
	Copyright 2011-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreVideo/CVPixelBuffer.h>

#if ! TARGET_OS_IPHONE

#import <CoreVideo/CVHostTime.h>

#endif // ! TARGET_OS_IPHONE

NS_ASSUME_NONNULL_BEGIN

/*!
	@class			AVPlayerItemOutput
	@abstract		AVPlayerItemOutput is an abstract class encapsulating the common API for all AVPlayerItemOutput subclasses.
	@discussion
		Instances of AVPlayerItemOutput permit the acquisition of individual samples from an AVAsset during playback by an AVPlayer. To provide graceful degradation of service across multiple AVPlayerItemOutput instances for a single source, all AVPlayerItemOutput subclasses only offer the current sample and/or any readily available future samples. All samples earlier than the current sample are automatically discarded by the AVPlayerItemOutput.
		
		You manage an association of an AVPlayerItemOutput instance with an AVPlayerItem as the source input using the AVPlayerItem methods:
		
		• addOutput:
		• removeOutput:
		
		When an AVPlayerItemOutput is associated with an AVPlayerItem, samples are provided for a media type in accordance with the rules for mixing, composition, or exclusion that the AVPlayer honors among multiple enabled tracks of that media type for its own rendering purposes. For example, video media will be composed according to the instructions provided via AVPlayerItem.videoComposition, if present.
 */

@class AVPlayerItemOutputInternal;

NS_CLASS_AVAILABLE(10_8, 6_0)
@interface AVPlayerItemOutput : NSObject
{
	@private
	AVPlayerItemOutputInternal *_outputInternal;
}

/*!
	@method			itemTimeForHostTime:
	@abstract		Convert a host time, expressed in seconds, to item time.
	@discussion
		Converts a host time value (for example a CADisplayLink timestamp, or the value returned by CACurrentMediaTime()) to the equivalent time on the item's timebase.
		
		Note: The Core Animation CADisplayLink timestamp property expresses the most recent, or previous, screen refresh time. You need to increment this timestamp by the CADisplayLink's duration property to find the next appropriate item time.
	@param			hostTimeInSeconds
					The timestamp value to convert to item time.
	@result			The equivalent item time.
 */

- (CMTime)itemTimeForHostTime:(CFTimeInterval)hostTimeInSeconds;

/*!
	@method			itemTimeForMachAbsoluteTime:
	@abstract		Convenience method to convert a Mach host time to item time.
	@discussion
		Converts Mach host time to the equivalent time on the item's timebase.
		mach_absolute_time() returns time awake since boot in system-specific rational units that can be queried by calling mach_timebase_info().
	@param			machAbsoluteTime
					The Mach host time to convert to item time.
	@result			The equivalent item time.
 */

- (CMTime)itemTimeForMachAbsoluteTime:(int64_t)machAbsoluteTime;

#if !TARGET_OS_IPHONE

/*!
	@method			itemTimeForCVTimeStamp:
	@abstract		Convenience method to convert a CoreVideo timestamp to the equivalent time on the item's timebase.
	@discussion
		Note: A CVDisplayLink provides a parameter inOutputTimestamp that expresses a future screen refresh time. You can use this value directly to find the next appropriate item time.
	@param			timestamp
					The CoreVideo timestamp value to convert to item time.
	@result			The equivalent item time.
 */

- (CMTime)itemTimeForCVTimeStamp:(CVTimeStamp)timestamp NS_AVAILABLE(10_8, NA);

#endif // !TARGET_OS_IPHONE

/*!
	@property		suppressesPlayerRendering
	@abstract		Indicates whether the output, when added to an AVPlayerItem, will be used in addition to normal rendering of media data by the player or instead of normal rendering.
	@discussion
		The default value is NO, indicating that the output will be used in addition to normal rendering. If you want to render the media data provided by the output yourself instead of allowing it to be rendered as in normally would be by AVPlayer, set suppressesPlayerRendering to YES.
 
		 Whenever any output is added to an AVPlayerItem that has suppressesPlayerRendering set to YES, the media data supplied to the output will not be rendered by AVPlayer. Other media data associated with the item but not provided to such an output is not affected. For example, if an output of class AVPlayerItemVideoOutput with a value of YES for suppressesPlayerRendering is added to an AVPlayerItem, video media for that item will not be rendered by the AVPlayer, while audio media, subtitle media, and other kinds of media, if present, will be rendered.
*/
@property (nonatomic, readwrite) BOOL suppressesPlayerRendering NS_AVAILABLE(10_8, 6_0);

@end

/*!
	@class			AVPlayerItemVideoOutput
	@abstract		A concrete subclass of AVPlayerItemOutput that vends video images as CVPixelBuffers.
	@discussion
		It is best to use a AVPlayerItemVideoOutput in conjunction with the services of a CVDisplayLink or CADisplayLink to accurately synchronize with screen device refreshes. For optimum efficiency there are opportunities to quiesce these services. Examples include when playback is paused or during playback of empty edits. Below is sample code that illustrates how you might quiesce a CVDisplayLink when used with a AVPlayerItemVideoOutput.


	(void)CVDisplayLinkCreateWithActiveCGDisplays( &myDisplayLink );
	CVDisplayLinkSetOutputCallback( myDisplayLink, myDisplayCallback, self );
	
	[myPlayerItem addOutput:myVideoOutput];
	[myVideoOutput setDelegate:self queue:myDispatchQueue];
	
	...
	
	static CVReturn myDisplayCallback ( CVDisplayLinkRef displayLink, 
										const CVTimeStamp *inNow, 
										const CVTimeStamp *inOutputTime, 
										CVOptionFlags flagsIn, 
										CVOptionFlags *flagsOut, 
										void *displayLinkContext )
	{
		MYObject *self = displayLinkContext;
	 
		CMTime outputItemTime = [[self myVideoOutput] itemTimeForCVTimeStamp:*inOutputTime];
		if ( [[self myVideoOutput] hasNewPixelBufferForItemTime:outputItemTime] ) {
			CVPixelBufferRef pixBuff = NULL;
			CMTime presentationItemTime = kCMTimeZero;
			self->myLastHostTime = inOutputTime->hostTime;
			pixBuff = [[self myVideoOutput] copyPixelBufferForItemTime:outputItemTime itemTimeForDisplay:&presentationItemTime];
	 
			// Use pixBuff here
			// presentationItemTime is the item time appropriate for
			// the next screen refresh
	
			CVBufferRelease( pixBuff );
		}
		else {
			CMTime elapsedTime = CMClockMakeHostTimeFromSystemUnits( inNow->hostTime - self->myLastHostTime );
			if ( CMTimeGetSeconds( elapsedTime ) > NON_QUIESCENT_PERIOD_IN_SECONDS ) {
				[[self myVideoOutput] requestNotificationOfMediaDataChangeWithAdvanceInterval:MY_STARTUP_TIME_IN_SECONDS];
				CVDisplayLinkStop( displayLink );
			}
		}
		return kCVReturnSuccess;
	}
	
	- (void)outputMediaDataWillChange:(AVPlayerItemOutput *)output
	{	
		// Start running again
		myLastHostTime = CVGetCurrentHostTime();
		CVDisplayLinkStart( myDisplayLink );
	}


 */
 
@protocol AVPlayerItemOutputPullDelegate;

@class AVPlayerItemVideoOutputInternal;

NS_CLASS_AVAILABLE(10_8, 6_0)
@interface AVPlayerItemVideoOutput : AVPlayerItemOutput
{
@private
	AVPlayerItemVideoOutputInternal *_videoOutputInternal;
}

/*!
	@method			initWithPixelBufferAttributes:
	@abstract		Returns an instance of AVPlayerItemVideoOutput, initialized with the specified pixel buffer attributes, for video image output.
	@param			pixelBufferAttributes
					The client requirements for output CVPixelBuffers, expressed using the constants in <CoreVideo/CVPixelBuffer.h>.
	@result			An instance of AVPlayerItemVideoOutput.
 */

- (instancetype)initWithPixelBufferAttributes:(nullable NSDictionary<NSString *, id> *)pixelBufferAttributes NS_DESIGNATED_INITIALIZER;

/*!
	@method			initWithOutputSettings:
	@abstract		Returns an instance of AVPlayerItemVideoOutput, initialized with the specified output settings, for video image output.
	@param			outputSettings
					The client requirements for output CVPixelBuffers, expressed using the constants in AVVideoSettings.h.
 
					For uncompressed video output, start with kCVPixelBuffer* keys in <CoreVideo/CVPixelBuffer.h>.
	
					In addition to the keys in CVPixelBuffer.h, uncompressed video settings dictionaries may also contain the following keys:
 
					AVVideoAllowWideColorKey
 
	@result			An instance of AVPlayerItemVideoOutput.
 */

- (instancetype)initWithOutputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings NS_AVAILABLE(10_12, 10_0) NS_DESIGNATED_INITIALIZER;

/*!
	@method			hasNewPixelBufferForItemTime:
	@abstract		Query if any new video output is available for an item time.
	@discussion
		This method returns YES if there is available video output, appropriate for display, at the specified item time not marked as acquired. If you require multiple objects to acquire video output from the same AVPlayerItem, you should instantiate more than one AVPlayerItemVideoOutput and add each via addOutput:. Each AVPlayerItemVideoOutput maintains a separate record of client acquisition.
	@param			itemTime
					The item time to query.
	@result			A BOOL indicating if there is newer output.
 */

- (BOOL)hasNewPixelBufferForItemTime:(CMTime)itemTime;

/*!
	@method			copyPixelBufferForItemTime:itemTimeForDisplay:
	@abstract		Retrieves an image that is appropriate for display at the specified item time, and marks the image as acquired.
	@discussion
		The client is responsible for calling CVBufferRelease on the returned CVPixelBuffer when finished with it. 
		
		Typically you would call this method in response to a CVDisplayLink callback or CADisplayLink delegate invocation and if hasNewPixelBufferForItemTime: also returns YES. 
		
		The buffer reference retrieved from copyPixelBufferForItemTime:itemTimeForDisplay: may itself be NULL. A reference to a NULL pixel buffer communicates that nothing should be displayed for the supplied item time.
	@param			itemTime
					A CMTime that expresses a desired item time.
	@param			itemTimeForDisplay
					A CMTime pointer whose value will contain the true display deadline for the copied pixel buffer. Can be NULL.
 */

- (nullable CVPixelBufferRef)copyPixelBufferForItemTime:(CMTime)itemTime itemTimeForDisplay:(nullable CMTime *)outItemTimeForDisplay CF_RETURNS_RETAINED;

/*!
	@method			setDelegate:queue:
	@abstract		Sets the receiver's delegate and a dispatch queue on which the delegate will be called.
	@param			delegate
					An object conforming to AVPlayerItemOutputPullDelegate protocol.
	@param			delegateQueue
					A dispatch queue on which all delegate methods will be called.
 */

- (void)setDelegate:(nullable id<AVPlayerItemOutputPullDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
	@method			requestNotificationOfMediaDataChangeWithAdvanceInterval:
	@abstract		Informs the receiver that the AVPlayerItemVideoOutput client is entering a quiescent state.
	@param			interval
					A wall clock time interval.
	@discussion
		Message this method before you suspend your use of a CVDisplayLink or CADisplayLink. The interval you provide will be used to message your delegate, in advance, that it should resume the display link. If the interval you provide is large, effectively requesting wakeup earlier than the AVPlayerItemVideoOutput is prepared to act, the delegate will be invoked as soon as possible. Do not use this method to force a delegate invocation for each sample.
 */
 
- (void)requestNotificationOfMediaDataChangeWithAdvanceInterval:(NSTimeInterval)interval;

/*!
	@property		delegate
	@abstract		The receiver's delegate.
 */
@property (nonatomic, readonly, assign, nullable) id<AVPlayerItemOutputPullDelegate> delegate;

/*!
	@property		delegateQueue
	@abstract		The dispatch queue where the delegate is messaged.
 */

@property (nonatomic, readonly, nullable) dispatch_queue_t delegateQueue;

@end

/*!
	@protocol		AVPlayerItemOutputPullDelegate
	@abstract		Defines common delegate methods for objects participating in AVPlayerItemOutput pull sample output acquisition.
 */
 
 @protocol AVPlayerItemOutputPullDelegate <NSObject>
 
 @optional
 
 /*!
	@method			outputMediaDataWillChange:
	@abstract		A method invoked once, prior to a new sample, if the AVPlayerItemOutput sender was previously messaged requestNotificationOfMediaDataChangeWithAdvanceInterval:.
	@discussion
		This method is invoked once after the sender is messaged requestNotificationOfMediaDataChangeWithAdvanceInterval:.
  */

- (void)outputMediaDataWillChange:(AVPlayerItemOutput *)sender NS_AVAILABLE(10_8, 6_0);
 
 /*!
	@method			outputSequenceWasFlushed:
	@abstract		A method invoked when the output is commencing a new sequence.
	@discussion
		This method is invoked after any seeking and change in playback direction. If you are maintaining any queued future samples, copied previously, you may want to discard these after receiving this message.
  */

- (void)outputSequenceWasFlushed:(AVPlayerItemOutput *)output NS_AVAILABLE(10_8, 6_0);

@end


@protocol AVPlayerItemLegibleOutputPushDelegate;
@class AVPlayerItemLegibleOutputInternal;

/*!
	@class			AVPlayerItemLegibleOutput
	@abstract		A subclass of AVPlayerItemOutput that can vend media with a legible characteristic as NSAttributedStrings.
	@discussion
		An instance of AVPlayerItemLegibleOutput is typically initialized using the -init method.
 */
NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVPlayerItemLegibleOutput : AVPlayerItemOutput
{
@private
	AVPlayerItemLegibleOutputInternal *_legibleOutputInternal;
}

/*!
	@method			setDelegate:queue:
	@abstract		Sets the receiver's delegate and a dispatch queue on which the delegate will be called.
	@param			delegate
					An object conforming to AVPlayerItemLegibleOutputPushDelegate protocol.
	@param			delegateQueue
					A dispatch queue on which all delegate methods will be called.
	@discussion
		The delegate is held using a zeroing-weak reference, so it is safe to deallocate the delegate while the receiver still has a reference to it.
 */
- (void)setDelegate:(nullable id <AVPlayerItemLegibleOutputPushDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
	@property		delegate
	@abstract		The receiver's delegate.
	@discussion
		The delegate is held using a zeroing-weak reference, so this property will have a value of nil after a delegate that was previously set has been deallocated.  This property is not key-value observable.
 */
@property (nonatomic, readonly, weak, nullable) id <AVPlayerItemLegibleOutputPushDelegate> delegate;

/*!
	@property		delegateQueue
	@abstract		The dispatch queue where the delegate is messaged.
	@discussion
		This property is not key-value observable.
 */
@property (nonatomic, readonly, nullable) dispatch_queue_t delegateQueue;

/*!
	@property		advanceIntervalForDelegateInvocation
	@abstract		Permits advance invocation of the associated delegate, if any.
	@discussion
		If it is possible, an AVPlayerItemLegibleOutput will message its delegate advanceIntervalForDelegateInvocation seconds earlier than otherwise. If the value you provide is large, effectively requesting provision of samples earlier than the AVPlayerItemLegibleOutput is prepared to act on them, the delegate will be invoked as soon as possible.
 */
@property (nonatomic, readwrite) NSTimeInterval advanceIntervalForDelegateInvocation;

@end


@interface AVPlayerItemLegibleOutput (AVPlayerItemLegibleOutput_NativeRepresentation)

/*!
	@method			initWithMediaSubtypesForNativeRepresentation:
	@abstract		Returns an instance of AVPlayerItemLegibleOutput with filtering enabled for AVPlayerItemLegibleOutputPushDelegate's legibleOutput:didOutputAttributedStrings:nativeSampleBuffers:forItemTime:.
	@param			subtypes
					NSArray of NSNumber FourCC codes, e.g. @[ [NSNumber numberWithUnsignedInt:'tx3g'] ]
	@result			An instance of AVPlayerItemLegibleOutput.
	@discussion
		Add media subtype FourCC number objects to the subtypes array to elect to receive that type as a CMSampleBuffer instead of an NSAttributedString.  Initializing an AVPlayerItemLegibleOutput using the -init method is equivalent to calling -initWithMediaSubtypesForNativeRepresentation: with an empty array, which means that all legible data, regardless of media subtype, will be delivered using NSAttributedString in a common format.
 
		If a media subtype for which there is no legible data in the current player item is included in the media subtypes array, no error will occur.  AVPlayerItemLegibleOutput will not vend closed caption data as CMSampleBuffers, so it is an error to include 'c608' in the media subtypes array.
 */	
- (instancetype)initWithMediaSubtypesForNativeRepresentation:(NSArray<NSNumber *> *)subtypes;

@end


@interface AVPlayerItemLegibleOutput (AVPlayerItemLegibleOutput_TextStylingResolution)

/*!
 @typedef AVPlayerItemLegibleOutputTextStylingResolution
 @abstract
    The type of a text styling resolution.
*/
typedef NSString * AVPlayerItemLegibleOutputTextStylingResolution NS_STRING_ENUM;

/*!
 @constant		AVPlayerItemLegibleOutputTextStylingResolutionDefault
 @abstract		Specify this level of text styling resolution to receive attributed strings from an AVPlayerItemLegibleOutput that include the same level of styling information that AVFoundation would use itself to render text within an AVPlayerLayer. The text styling will accommodate user-level Media Accessibility settings.
 */
AVF_EXPORT AVPlayerItemLegibleOutputTextStylingResolution const AVPlayerItemLegibleOutputTextStylingResolutionDefault NS_AVAILABLE(10_9, 7_0);

/*!
 @constant		AVPlayerItemLegibleOutputTextStylingResolutionSourceAndRulesOnly
 @abstract		Specify this level of text styling resolution to receive only the styling present in the source media and the styling provided via AVPlayerItem.textStyleRules.
 @discussion
	This level of resolution excludes styling provided by the user-level Media Accessibility settings. You would typically use it if you wish to override the styling specified in source media. If you do this, you are strongly encouraged to allow your custom styling in turn to be overriden by user preferences for text styling that are available as Media Accessibility settings.
 */
AVF_EXPORT AVPlayerItemLegibleOutputTextStylingResolution const AVPlayerItemLegibleOutputTextStylingResolutionSourceAndRulesOnly NS_AVAILABLE(10_9, 7_0);

/*!
 @property		textStylingResolution
 @abstract		A string identifier indicating the degree of text styling to be applied to attributed strings vended by the receiver
 @discussion
	Valid values are AVPlayerItemLegibleOutputTextStylingResolutionDefault and AVPlayerItemLegibleOutputTextStylingResolutionSourceAndRulesOnly.  An NSInvalidArgumentException is raised if this property is set to any other value.  The default value is AVPlayerItemLegibleOutputTextStylingResolutionDefault, which indicates that attributed strings vended by the receiver will include the same level of styling information that would be used if AVFoundation were rendering the text via AVPlayerLayer.
 */
@property (nonatomic, copy) AVPlayerItemLegibleOutputTextStylingResolution textStylingResolution;

@end


@protocol AVPlayerItemOutputPushDelegate;

/*!
	@protocol		AVPlayerItemLegibleOutputPushDelegate
	@abstract		Extends AVPlayerItemOutputPushDelegate to provide additional methods specific to attributed string output.
 */
@protocol AVPlayerItemLegibleOutputPushDelegate <AVPlayerItemOutputPushDelegate>

@optional

/*!
	@method			legibleOutput:didOutputAttributedStrings:nativeSampleBuffers:forItemTime:
	@abstract		A delegate callback that delivers new textual samples.
	@param			output
					The AVPlayerItemLegibleOutput source.
	@param			strings
					An NSArray of NSAttributedString, each containing both the run of text and descriptive markup.
	@param			nativeSamples
					An NSArray of CMSampleBuffer objects, for media subtypes included in the array passed in to -initWithMediaSubtypesForNativeRepresentation:
	@param			itemTime
					The item time at which the strings should be presented.
	@discussion
		For each media subtype in the array passed in to -initWithMediaSubtypesForNativeRepresentation:, the delegate will receive sample buffers carrying data in its native format via the nativeSamples parameter, if there is media data of that subtype in the media resource.  For all other media subtypes present in the media resource, the delegate will receive attributed strings in a common format via the strings parameter.  See <CoreMedia/CMTextMarkup.h> for the string attributes that are used in the attributed strings.
 */
- (void)legibleOutput:(AVPlayerItemLegibleOutput *)output didOutputAttributedStrings:(NSArray<NSAttributedString *> *)strings nativeSampleBuffers:(NSArray *)nativeSamples forItemTime:(CMTime)itemTime NS_AVAILABLE(10_9, 7_0);

@end


/*!
 @protocol		AVPlayerItemOutputPushDelegate
 @abstract		Defines common delegate methods for objects participating in AVPlayerItemOutput push sample output acquisition.
 */
@protocol AVPlayerItemOutputPushDelegate <NSObject>

@optional

/*!
	@method			outputSequenceWasFlushed:
	@abstract		A method invoked when the output is commencing a new sequence of media data.
	@discussion
		This method is invoked after any seeking and change in playback direction. If you are maintaining any queued future media data, received previously, you may want to discard these after receiving this message.
 */
- (void)outputSequenceWasFlushed:(AVPlayerItemOutput *)output;

@end


@protocol AVPlayerItemMetadataOutputPushDelegate;
@class AVPlayerItemMetadataOutputInternal;

/*!
	@class			AVPlayerItemMetadataOutput
	@abstract		A subclass of AVPlayerItemOutput that vends collections of metadata items carried in metadata tracks.
 
	@discussion
		Setting the value of suppressesPlayerRendering on an instance of AVPlayerItemMetadataOutput has no effect.
 */
NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVPlayerItemMetadataOutput : AVPlayerItemOutput
{
@private
	AVPlayerItemMetadataOutputInternal *_metadataOutputInternal;
}

/*!
	@method			initWithIdentifiers:
	@abstract		Creates an instance of AVPlayerItemMetadataOutput.
	@param			identifiers
					A array of metadata identifiers indicating the metadata items that the output should provide.
	@discussion
		See AVMetadataIdentifiers.h for publicly defined metadata identifiers. Pass nil to receive all of the timed metadata from all enabled AVPlayerItemTracks that carry timed metadata.
 */
- (instancetype)initWithIdentifiers:(nullable NSArray<NSString *> *)identifiers NS_DESIGNATED_INITIALIZER;

/*!
	@method			setDelegate:queue:
	@abstract		Sets the receiver's delegate and a dispatch queue on which the delegate will be called.
	@param			delegate
					An object conforming to AVPlayerItemMetadataOutputPushDelegate protocol.
	@param			delegateQueue
					A dispatch queue on which all delegate methods will be called.
 */
- (void)setDelegate:(nullable id <AVPlayerItemMetadataOutputPushDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
	@property		delegate
	@abstract		The receiver's delegate.
	@discussion
		The delegate is held using a zeroing-weak reference, so this property will have a value of nil after a delegate that was previously set has been deallocated.  This property is not key-value observable.
 */
@property (nonatomic, readonly, weak, nullable) id <AVPlayerItemMetadataOutputPushDelegate> delegate;

/*!
	@property		delegateQueue
	@abstract		The dispatch queue on which messages are sent to the delegate.
	@discussion
		This property is not key-value observable.
 */
@property (nonatomic, readonly, nullable) dispatch_queue_t delegateQueue;

/*!
	@property		advanceIntervalForDelegateInvocation
	@abstract		Permits advance invocation of the associated delegate, if any.
	@discussion
		If it is possible, an AVPlayerItemMetadataOutput will message its delegate advanceIntervalForDelegateInvocation seconds earlier than otherwise. If the value you provide is large, effectively requesting provision of samples earlier than the AVPlayerItemMetadataOutput is prepared to act on them, the delegate will be invoked as soon as possible.
 */
@property (nonatomic, readwrite) NSTimeInterval advanceIntervalForDelegateInvocation;

@end

@class AVTimedMetadataGroup;
@class AVPlayerItemTrack;

/*!
	@protocol		AVPlayerItemMetadataOutputPushDelegate
	@abstract		Extends AVPlayerItemOutputPushDelegate to provide additional methods specific to metadata output.
 */
@protocol AVPlayerItemMetadataOutputPushDelegate <AVPlayerItemOutputPushDelegate>

@optional
/*!
	@method			metadataOutput:didOutputTimedMetadataGroups:fromPlayerItemTrack:
	@abstract		A delegate callback that delivers a new collection of metadata items.
	@param			output
					The AVPlayerItemMetadataOutput source.
	@param			groups
					An NSArray of AVTimedMetadataGroups that may contain metadata items with requested identifiers, according to the format descriptions associated with the underlying tracks.
	@param			track
					An instance of AVPlayerItemTrack that indicates the source of the metadata items in the group.
	@discussion
		Each group provided in a single invocation of this method will have timing that does not overlap with any other group in the array.
		Note that for some timed metadata formats carried by HTTP live streaming, the timeRange of each group must be reported as kCMTimeIndefinite, because its duration will be unknown until the next metadata group in the stream arrives. In these cases, the groups parameter will always contain a single group.
		Groups are typically packaged into arrays for delivery to your delegate according to the chunking or interleaving of the underlying metadata data.
		Note that if the item carries multiple metadata tracks containing metadata with the same metadata identifiers, this method can be invoked for each one separately, each with reference to the associated AVPlayerItemTrack.
		Note that the associated AVPlayerItemTrack parameter can be nil which implies that the metadata describes the asset as a whole, not just a single track of the asset.
 */
- (void)metadataOutput:(AVPlayerItemMetadataOutput *)output didOutputTimedMetadataGroups:(NSArray<AVTimedMetadataGroup *> *)groups fromPlayerItemTrack:(nullable AVPlayerItemTrack *)track NS_AVAILABLE(10_10, 8_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitVarispeed.h
/*
	File:           AVAudioUnitVarispeed.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitVarispeed.h>

// ==========  AVFoundation.framework/Headers/AVAssetDownloadStorageManager.h
/*
	File:  AVAssetDownloadStorageManager.h
 
	Framework:  AVFoundation
 
	Copyright 2017 Apple Inc. All rights reserved.
 
 */

/*!
	@class		AVAssetDownloadStorageManager
 
	@abstract	An AVAssetDownloadStorageManager manages the policy for automatic purging of downloaded AVAssets. The policy is vended as  AVAssetDownloadStorageManagementPolicy object.

	@discussion	When a storage management policy needs to be set on an asset, sharedDownloadStorageManager singleton needs to be fetched. 
 				The new policy can then be set by using setStorageManagementPolicy and the location of the downloaded asset.
 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAssetDownloadStorageManagementPolicy;
/*!
	@group		AVAssetDownloadedAssetEvictionPriority string constants
	@brief		Used by AVAssetDownloadStorageManagementPolicy.
*/
typedef NSString *AVAssetDownloadedAssetEvictionPriority NS_STRING_ENUM API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
	@enum		AVAssetDownloadedAssetEvictionPriority
	@abstract	These constants represents the eviction priority of downloaded assets.

	@constant	AVAssetDownloadedAssetEvictionPriorityImportant
				Used to mark assets with the highest priority. They will be the last to be purged.
	@constant	AVAssetDownloadedAssetEvictionPriorityDefault
				Used to mark assets have the default priority. They will be the first to be purged.
*/
AVF_EXPORT AVAssetDownloadedAssetEvictionPriority const AVAssetDownloadedAssetEvictionPriorityImportant				API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos);
AVF_EXPORT AVAssetDownloadedAssetEvictionPriority const AVAssetDownloadedAssetEvictionPriorityDefault				API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos);

API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos)
@interface AVAssetDownloadStorageManager : NSObject

/*!
	@method		sharedDownloadStorageManager
	@abstract	returns singleton instance.
*/
+ (AVAssetDownloadStorageManager *)sharedDownloadStorageManager;

/*!
	@method		setStorageManagementPolicy: forURL
	@abstract	Sets the policy for asset with disk backing at downloadStorageURL.
	@param		downloadStorageURL
 				The location of downloaded asset.
*/
- (void)setStorageManagementPolicy:(AVAssetDownloadStorageManagementPolicy *)storageManagementPolicy forURL:(NSURL *)downloadStorageURL;

/*!
	@method		storageManagementPolicyForURL:downloadStorageURL
	@abstract	Returns the storage management policy for asset downloaded at downloadStorageURL.
                This may be nil if a storageManagementPolicy was never set on the downloaded asset.
	@param		downloadStorageURL
				The location of downloaded asset.
*/
- (nullable AVAssetDownloadStorageManagementPolicy *)storageManagementPolicyForURL:(NSURL *)downloadStorageURL;

@end

/*!
	@class		AVAssetDownloadStorageManagementPolicy
 
	@abstract	A class to inform the system of a policy for automatic purging of downloaded AVAssets.
 
	@discussion	System will put in best-effort to evict all the assets based on expirationDate before evicting based on priority.
 */
@class AVAssetDownloadStorageManagementPolicyInternal;

API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos)
@interface AVAssetDownloadStorageManagementPolicy : NSObject <NSCopying, NSMutableCopying> {
@private
	AVAssetDownloadStorageManagementPolicyInternal    *_storageManagementPolicy;
}

/*
 	@property	priority
 	@abstract	Indicates the eviction priority of downloaded asset.
 	@discussion	Assets with default priority will be purged first before assets with higher priorities.
				In case this is not set, default priority is used.
 */
@property (nonatomic, readonly, copy) AVAssetDownloadedAssetEvictionPriority priority;

/*
 	@property	expirationDate
 	@abstract	Returns the expiration date of asset.
 */
@property (nonatomic, readonly, copy) NSDate *expirationDate;

@end
/*!
	@class		AVMutableAssetDownloadStorageManagementPolicy
 
	@abstract	A mutable subclass of AVAssetDownloadStorageManagementPolicy.
 
	@discussion	System will put in best-effort to evict all the assets based on expirationDate before evicting based on priority.
 */
API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos, tvos, watchos)
@interface AVMutableAssetDownloadStorageManagementPolicy : AVAssetDownloadStorageManagementPolicy

/*
	@property	priority
	@abstract	Indicates the eviction priority of downloaded asset.
	@discussion	Assets with default priority will be purged first before assets with higher priorities.
 				In case this is not set, default priority is used.
 */
@property (nonatomic, copy) AVAssetDownloadedAssetEvictionPriority priority;

/*
	@property	expirationDate
 	@abstract	Returns the expiration date of asset.
 */
@property (nonatomic, copy) NSDate *expirationDate;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVMetadataItem.h
/*
    File:  AVMetadataItem.h

	Framework:  AVFoundation
 
	Copyright 2010-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <AVFoundation/AVMetadataFormat.h>
#import <AVFoundation/AVMetadataIdentifiers.h>
#import <AVFoundation/AVAsynchronousKeyValueLoading.h>

#import <CoreGraphics/CoreGraphics.h>

NS_ASSUME_NONNULL_BEGIN

@class AVMetadataItemFilter;

/*!
    @class			AVMetadataItem

    @abstract		AVMetadataItem represents an item of metadata associated with an audiovisual asset or with
    				one of its tracks.
    
	@discussion		AVMetadataItems have keys that accord with the specification of the container format from
					which they're drawn. Full details of the metadata formats, metadata keys, and metadata keyspaces
					supported by AVFoundation are available among the defines in AVMetadataFormat.h.
	
					Note that arrays of AVMetadataItems vended by AVAsset and other classes are "lazy", similar
					to array-based keys that support key-value observing, meaning that you can obtain
					objects from those arrays without incurring overhead for items you don't ultimately inspect.
					
					You can filter arrays of AVMetadataItems by locale or by key and keySpace via the category
					AVMetadataItemArrayFiltering defined below.
*/

@class AVMetadataItemInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMetadataItem : NSObject <AVAsynchronousKeyValueLoading, NSCopying, NSMutableCopying>
{
	AVMetadataItemInternal	*_priv;
}

/* Indicates the identifier of the metadata item. Publicly defined identifiers are declared in AVMetadataIdentifiers.h. */
@property (nonatomic, readonly, copy, nullable) AVMetadataIdentifier identifier NS_AVAILABLE(10_10, 8_0);

/* indicates the IETF BCP 47 (RFC 4646) language identifier of the metadata item; may be nil if no language tag information is available */
@property (nonatomic, readonly, copy, nullable) NSString *extendedLanguageTag NS_AVAILABLE(10_10, 8_0);

/* indicates the locale of the metadata item; may be nil if no locale information is available for the metadata item */
@property (nonatomic, readonly, copy, nullable) NSLocale *locale;

/* indicates the timestamp of the metadata item. */
@property (nonatomic, readonly) CMTime time;

/* indicates the duration of the metadata item */
@property (nonatomic, readonly) CMTime duration NS_AVAILABLE(10_7, 4_2);

/* indicates the data type of the metadata item's value.  Publicly defined data types are declared in <CoreMedia/CMMetadata.h> */
@property (nonatomic, readonly, copy, nullable) NSString *dataType NS_AVAILABLE(10_10, 8_0);

/* provides the value of the metadata item */
@property (nonatomic, readonly, copy, nullable) id<NSObject, NSCopying> value;

/* provides a dictionary of the additional attributes */
@property (nonatomic, readonly, copy, nullable) NSDictionary<AVMetadataExtraAttributeKey, id> *extraAttributes;

@end


@interface AVMetadataItem (AVMetadataItemDateRepresentation)

/* indicates the start date of the timed metadata; nil if no date is indicated */
@property (nonatomic, readonly, copy, nullable) NSDate *startDate NS_AVAILABLE(10_11, 9_0);

@end


@interface AVMetadataItem (AVMetadataItemTypeCoercion)

/* provides the value of the metadata item as a string; will be nil if the value cannot be represented as a string */
@property (nonatomic, readonly, nullable) NSString *stringValue;

/* provides the value of the metadata item as an NSNumber. If the metadata item's value can't be coerced to a number, @"numberValue" will be nil. */
@property (nonatomic, readonly, nullable) NSNumber *numberValue;

/* provides the value of the metadata item as an NSDate. If the metadata item's value can't be coerced to a date, @"dateValue" will be nil. */
@property (nonatomic, readonly, nullable) NSDate *dateValue;

/* provides the raw bytes of the value of the metadata item */
@property (nonatomic, readonly, nullable) NSData *dataValue;

@end


@interface AVMetadataItem (AVAsynchronousKeyValueLoading)

/* The following two methods of the AVAsynchronousKeyValueLoading protocol are re-declared here so that they can be annotated with availability information. See AVAsynchronousKeyValueLoading.h for documentation. */

- (AVKeyValueStatus)statusOfValueForKey:(NSString *)key error:(NSError * _Nullable * _Nullable)outError NS_AVAILABLE(10_7, 4_2);

- (void)loadValuesAsynchronouslyForKeys:(NSArray<NSString *> *)keys completionHandler:(nullable void (^)(void))handler NS_AVAILABLE(10_7, 4_2);

@end


@interface AVMetadataItem (AVMetadataItemArrayFiltering)

/*!
 @method		metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:
 @abstract		Filters an array of AVMetadataItems according to whether their locales match any language identifier in the specified array of preferred languages. The returned array is sorted according to the order of preference of the language each matches.
 @param			metadataItems
				An array of AVMetadataItems to be filtered and sorted.
 @param			preferredLanguages
				An array of language identifiers in order of preference, each of which is an IETF BCP 47 (RFC 4646) language identifier. Use +[NSLocale preferredLanguages] to obtain the user's list of preferred languages.
 @result		An instance of NSArray containing metadata items of the specified NSArray that match a preferred language, sorted according to the order of preference of the language each matches.
*/
+ (NSArray<AVMetadataItem *> *)metadataItemsFromArray:(NSArray<AVMetadataItem *> *)metadataItems filteredAndSortedAccordingToPreferredLanguages:(NSArray<NSString *> *)preferredLanguages NS_AVAILABLE(10_8, 6_0);

/*!
	@method			metadataItemsFromArray:filteredByIdentifier:
	@abstract			Filters an array of AVMetadataItems according to identifier.
	@param			metadataItems
	An array of AVMetadataItems to be filtered by identifier.
	@param			identifier
	The identifier that must be matched for a metadata item to be copied to the output array. Items are considered a match not only when their identifiers are equal to the specified identifier, and also when their identifiers conform to the specified identifier.
	@result			An instance of NSArray containing the metadata items of the target NSArray that match the specified identifier.
*/
+ (NSArray<AVMetadataItem *> *)metadataItemsFromArray:(NSArray<AVMetadataItem *> *)metadataItems filteredByIdentifier:(AVMetadataIdentifier)identifier NS_AVAILABLE(10_10, 8_0);

/*!
	@method			metadataItemsFromArray:filteredByMetadataItemFilter:
	@abstract		Filters an array of AVMetadataItems using the supplied AVMetadataItemFilter.
	@param			metadataItems
					An array of AVMetadataItems to be filtered.
	@param			metadataItemFilter
					The AVMetadataItemFilter object for filtering the metadataItems.
	@result			An instance of NSArray containing the metadata items of the target NSArray that have not been removed by metadataItemFilter.
*/
+ (NSArray<AVMetadataItem *> *)metadataItemsFromArray:(NSArray<AVMetadataItem *> *)metadataItems filteredByMetadataItemFilter:(AVMetadataItemFilter *)metadataItemFilter NS_AVAILABLE(10_9, 7_0);

@end

@interface AVMetadataItem (AVMetadataItemKeyAndKeyspace)

/*!
	@method			identifierForKey:keySpace:
	@abstract		Provides the metadata identifier that's equivalent to a key and keySpace.
	@param			key
					The metadata key.
	@param			keySpace
					The metadata keySpace.
	@result			A metadata identifier equivalent to the given key and keySpace, or nil if no identifier can be constructed from the given key and keySpace.
	@discussion
		Metadata keys that are not instances of NSString, NSNumber, or NSData cannot be converted to metadata identifiers; they also cannot be written to media resources via AVAssetExportSession or AVAssetWriter.  Metadata item keySpaces must be a string of one to four printable ASCII characters.
 
		For custom identifiers, the keySpace AVMetadataKeySpaceQuickTimeMetadata is recommended.  This keySpace defines its key values to be expressed as reverse-DNS strings, which allows third parties to define their own keys in a well established way that avoids collisions.
*/
+ (nullable AVMetadataIdentifier)identifierForKey:(id)key keySpace:(AVMetadataKeySpace)keySpace NS_AVAILABLE(10_10, 8_0);

/* provides the metadata keySpace indicated by the identifier  */
+ (nullable AVMetadataKeySpace)keySpaceForIdentifier:(AVMetadataIdentifier)identifier NS_AVAILABLE(10_10, 8_0);

/* provides the metadata key indicated by the identifier  */
+ (nullable id)keyForIdentifier:(AVMetadataIdentifier)identifier NS_AVAILABLE(10_10, 8_0);

/* indicates the key of the metadata item */
@property (nonatomic, readonly, copy, nullable) id<NSObject, NSCopying> key;

/* indicates the common key of the metadata item */
@property (nonatomic, readonly, copy, nullable) AVMetadataKey commonKey;

/* indicates the keyspace of the metadata item's key; this will typically
 be the default keyspace for the metadata container in which the metadata item is stored */
@property (nonatomic, readonly, copy, nullable) AVMetadataKeySpace keySpace;

@end

/*!
    @class			AVMutableMetadataItem

    @abstract		AVMutableMetadataItem provides support for building collections of metadata to be written
    				to asset files via AVAssetExportSession, AVAssetWriter or AVAssetWriterInput.
    
	@discussion		Can be initialized from an existing AVMetadataItem or with a one or more of the basic properties
					of a metadata item: a key, a keySpace, a locale, and a value.
*/

@class AVMutableMetadataItemInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableMetadataItem : AVMetadataItem
{
	AVMutableMetadataItemInternal	*_mutablePriv;
}

/* Indicates the identifier of the metadata item. Publicly defined identifiers are declared in AVMetadataIdentifiers.h. */
@property (nonatomic, readwrite, copy, nullable) AVMetadataIdentifier identifier NS_AVAILABLE(10_10, 8_0);

/* indicates the IETF BCP 47 (RFC 4646) language identifier of the metadata item; may be nil if no language tag information is available */
@property (nonatomic, readwrite, copy, nullable) NSString *extendedLanguageTag NS_AVAILABLE(10_10, 8_0);

/* indicates the locale of the metadata item; may be nil if no locale information is available for the metadata item */
@property (nonatomic, readwrite, copy, nullable) NSLocale *locale;

/* indicates the timestamp of the metadata item. */
@property (nonatomic, readwrite) CMTime time;

/* indicates the duration of the metadata item */
@property (nonatomic, readwrite) CMTime duration NS_AVAILABLE(10_7, 4_2);

/* indicates the data type of the metadata item's value.  Publicly defined data types are declared in <CoreMedia/CMMetadata.h> */
@property (nonatomic, readwrite, copy, nullable) NSString *dataType NS_AVAILABLE(10_10, 8_0);

/* provides the value of the metadata item */
@property (nonatomic, readwrite, copy, nullable) id<NSObject, NSCopying> value;

/* Provides a dictionary of the additional attributes. Extra attributes of metadata items are related to specifics of their carriage in their container format. Keys for extra attributes are declared in AVMetadataFormat.h. */
@property (nonatomic, readwrite, copy, nullable) NSDictionary<AVMetadataExtraAttributeKey, id> *extraAttributes;

/*!
	@method			metadataItem
	@abstract		Returns an instance of AVMutableMetadataItem.
*/
+ (AVMutableMetadataItem *)metadataItem;

@end

@interface AVMutableMetadataItem (AVMutableMetadataItemDateRepresentation)

/* indicates the start date of the timed metadata; nil if no date is indicated */
@property (nonatomic, readwrite, copy, nullable) NSDate *startDate NS_AVAILABLE(10_11, 9_0);

@end

@interface AVMutableMetadataItem (AVMutableMetadataItemKeyAndKeyspace)

/* Indicates the keyspace of the metadata item's key; this will typically be the default keyspace for the metadata container in which the metadata item is stored. */
@property (nonatomic, readwrite, copy, nullable) AVMetadataKeySpace keySpace;

/* Indicates the key of the metadata item. Metadata item keys that are not instances NSString, NSNumber, or NSData cannot be converted to metadata identifiers; they also cannot be written to media resources via AVAssetExportSession or AVAssetWriter. */
@property (nonatomic, readwrite, copy, nullable) id<NSObject, NSCopying> key;

@end

@class AVMetadataItemValueRequest;

@interface AVMetadataItem (AVMetadataItemLazyValueLoading)

/*!
	@method			metadataItemWithPropertiesOfMetadataItem:valueLoadingHandler:
	@abstract		Creates an instance of AVMutableMetadataItem with a value that you do not wish to load unless required, e.g. a large image value that needn't be loaded into memory until another module wants to display it.
	@param			metadataItem
					An instance of AVMetadataItem with the identifier, extendedLanguageTag, and other property values that you want the newly created instance of AVMetadataItem to share. The value of metadataItem is ignored.
	@param			handler
					A block that loads the value of the metadata item.
	@result			An instance of AVMetadataItem.
	@discussion
 		This method is intended for the creation of metadata items for optional display purposes, when there is no immediate need to load specific metadata values. For example, see the interface for navigation markers as consumed by AVPlayerViewController. It's not intended for the creation of metadata items with values that are required immediately, such as metadata items that are provided for impending serialization operations (e.g. via -[AVAssetExportSession setMetadata:] and other similar methods defined on AVAssetWriter and AVAssetWriterInput). 
		When -loadValuesAsynchronouslyForKeys:completionHandler: is invoked on an AVMetadataItem created via +metadataItemWithPropertiesOfMetadataItem:valueLoadingHandler: and @"value" is among the keys for which loading is requested, the block you provide as the value loading handler will be executed on an arbitrary dispatch queue, off the main thread. The handler can perform I/O and other necessary operations to obtain the value. If loading of the value succeeds, provide the value by invoking -[AVMetadataItemValueRequest respondWithValue:]. If loading of the value fails, provide an instance of NSError that describes the failure by invoking -[AVMetadataItemValueRequest respondWithError:].
*/
+ (AVMetadataItem *)metadataItemWithPropertiesOfMetadataItem:(AVMetadataItem *)metadataItem valueLoadingHandler:(void (^)(AVMetadataItemValueRequest *valueRequest))handler NS_AVAILABLE(10_11, 9_0);

@end

@class AVMetadataItemValueRequestInternal;

NS_CLASS_AVAILABLE(10_11, 9_0)
@interface AVMetadataItemValueRequest : NSObject {
@private
	AVMetadataItemValueRequestInternal	*_valueRequest;
}

/* Indicates the AVMetadataItem for which a value is being loaded. */
@property (readonly, weak) AVMetadataItem *metadataItem;

/*!
	@method			respondWithValue:
	@abstract		Allows you to respond to an AVMetadataItemValueRequest by providing a value.
	@param			value
					The value of the AVMetadataItem.
*/
- (void)respondWithValue:(id<NSObject, NSCopying>)value;

/*!
	@method			respondWithError:
	@abstract		Allows you to respond to an AVMetadataItemValueRequest in the case of failure.
	@param			error
					An instance of NSError that describes a failure encountered while loading the value of an AVMetadataItem.
*/
- (void)respondWithError:(NSError *)error;

@end

/*!
    @class			AVMetadataItemFilter

    @abstract		AVMetadataItemFilter is a tool used to filter AVMetadataItems.
    
	@discussion		Instances of AVMetadataItemFilter are used to filter AVMetadataItems.  They are opaque, unmodifiable objects, created via AVMetadataItemFilter class methods.
*/

@class AVMetadataItemFilterInternal;

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVMetadataItemFilter : NSObject {
@private
	AVMetadataItemFilterInternal	*_itemFilterInternal __attribute__((unused));
}

/* Provides an instance of an AVMetadataItemFilter useful for sharing assets.  Removes many user-identifying metadata items, such as location information, leaving only playback-, copyright- and commercial-related metadata (such as the purchaser's Apple ID), along with metadata either derivable from the media itself or necessary for its proper behavior.  */
+ (AVMetadataItemFilter *)metadataItemFilterForSharing;

@end

@interface AVMetadataItem (AVMetadataItemArrayFilteringDeprecable)

/*!
 @method			metadataItemsFromArray:withLocale:
 @discussion		Instead, use metadataItemsFromArray:filteredAndSortedAccordingToPreferredLanguages:.
 */
+ (NSArray<AVMetadataItem *> *)metadataItemsFromArray:(NSArray<AVMetadataItem *> *)metadataItems withLocale:(NSLocale *)locale;

/*!
 @method			metadataItemsFromArray:withKey:keySpace:
 @discussion		Instead, use metadataItemsFromArray:filteredByIdentifier:.
 */
+ (NSArray<AVMetadataItem *> *)metadataItemsFromArray:(NSArray<AVMetadataItem *> *)metadataItems withKey:(nullable id)key keySpace:(nullable AVMetadataKeySpace)keySpace;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureDevice.h
/*
    File:  AVCaptureDevice.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVCaptureSessionPreset.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMFormatDescription.h>
#import <CoreGraphics/CGBase.h>
#import <CoreGraphics/CGGeometry.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @constant AVCaptureDeviceWasConnectedNotification
 @abstract
    Posted when a device becomes available on the system.
 
 @discussion
    The notification object is an AVCaptureDevice instance representing the device that became available.
 */
AVF_EXPORT NSString *const AVCaptureDeviceWasConnectedNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceWasDisconnectedNotification
 @abstract
    Posted when a device becomes unavailable on the system.
 
 @discussion
    The notification object is an AVCaptureDevice instance representing the device that became unavailable.
 */
AVF_EXPORT NSString *const AVCaptureDeviceWasDisconnectedNotification API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceSubjectAreaDidChangeNotification
 @abstract
    Posted when the instance of AVCaptureDevice has detected a substantial change to the video subject area.
 
 @discussion
    Clients may observe the AVCaptureDeviceSubjectAreaDidChangeNotification to know when an instance of AVCaptureDevice has detected a substantial change to the video subject area. This notification is only sent if you first set subjectAreaChangeMonitoringEnabled to YES.
  */
AVF_EXPORT NSString *const AVCaptureDeviceSubjectAreaDidChangeNotification API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVCaptureDevice

@class AVCaptureDeviceFormat;
@class AVCaptureDeviceInputSource;
@class AVCaptureSystemPressureState;
@class AVCaptureDeviceInternal;

/*!
 @class AVCaptureDevice
 @abstract
    An AVCaptureDevice represents a physical device that provides realtime input media data, such as video and audio.
 
 @discussion
    Each instance of AVCaptureDevice corresponds to a device, such as a camera or microphone. Instances of AVCaptureDevice cannot be created directly. An array of all currently available devices can also be obtained using the AVCaptureDeviceDiscoverySession. Devices can provide one or more streams of a given media type. Applications can search for devices matching desired criteria by using AVCaptureDeviceDiscoverySession, or may obtain a reference to the default device matching desired criteria by using +[AVCaptureDevice defaultDeviceWithDeviceType:mediaType:position:].
 
    Instances of AVCaptureDevice can be used to provide media data to an AVCaptureSession by creating an AVCaptureDeviceInput with the device and adding that to the capture session.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDevice : NSObject
{
@private
    AVCaptureDeviceInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @method devices
 @abstract
    Returns an array of devices currently available for use as media input sources.
 
 @result
    An NSArray of AVCaptureDevice instances for each available device.
 
 @discussion
    This method returns an array of AVCaptureDevice instances for input devices currently connected and available for capture. The returned array contains all devices that are available at the time the method is called. Applications should observe AVCaptureDeviceWasConnectedNotification and AVCaptureDeviceWasDisconnectedNotification to be notified when the list of available devices has changed.
 */
+ (NSArray<AVCaptureDevice *> *)devices API_DEPRECATED("Use AVCaptureDeviceDiscoverySession instead.", ios(4.0, 10.0));

/*!
 @method devicesWithMediaType:
 @abstract
    Returns an array of devices currently available for use as sources of media with the given media type.
 
 @param mediaType
    The media type, such as AVMediaTypeVideo, AVMediaTypeAudio, or AVMediaTypeMuxed, supported by each returned device.
 @result
    An NSArray of AVCaptureDevice instances for each available device.
 
 @discussion
    This method returns an array of AVCaptureDevice instances for input devices currently connected and available for capture that provide media of the given type. Media type constants are defined in AVMediaFormat.h. The returned array contains all devices that are available at the time the method is called. Applications should observe AVCaptureDeviceWasConnectedNotification and AVCaptureDeviceWasDisconnectedNotification to be notified when the list of available devices has changed.
 */
+ (NSArray<AVCaptureDevice *> *)devicesWithMediaType:(AVMediaType)mediaType API_DEPRECATED("Use AVCaptureDeviceDiscoverySession instead.", ios(4.0, 10.0));

/*!
 @method defaultDeviceWithMediaType:
 @abstract
    Returns an AVCaptureDevice instance for the default device of the given media type.
 
 @param mediaType
    The media type, such as AVMediaTypeVideo, AVMediaTypeAudio, or AVMediaTypeMuxed, supported by the returned device.
 @result
    The default device with the given media type, or nil if no device with that media type exists.
 
 @discussion
    This method returns the default device of the given media type currently available on the system. For example, for AVMediaTypeVideo, this method will return the built in camera that is primarily used for capture and recording. Media type constants are defined in AVMediaFormat.h.
 */
+ (nullable AVCaptureDevice *)defaultDeviceWithMediaType:(AVMediaType)mediaType;

/*!
 @method deviceWithUniqueID:
 @abstract
    Returns an AVCaptureDevice instance with the given unique ID.
 
 @param deviceUniqueID
    The unique ID of the device instance to be returned.
 @result
    An AVCaptureDevice instance with the given unique ID, or nil if no device with that unique ID is available.
 
 @discussion
    Every available capture device has a unique ID that persists on one system across device connections and disconnections, application restarts, and reboots of the system itself. This method can be used to recall or track the status of a specific device whose unique ID has previously been saved.
 */
+ (nullable AVCaptureDevice *)deviceWithUniqueID:(NSString *)deviceUniqueID;

/*!
 @property uniqueID
 @abstract
    An ID unique to the model of device corresponding to the receiver.
 
 @discussion
    Every available capture device has a unique ID that persists on one system across device connections and disconnections, application restarts, and reboots of the system itself. Applications can store the value returned by this property to recall or track the status of a specific device in the future.
 */
@property(nonatomic, readonly) NSString *uniqueID;

/*!
 @property modelID
 @abstract
    The model ID of the receiver.
 
 @discussion
    The value of this property is an identifier unique to all devices of the same model. The value is persistent across device connections and disconnections, and across different systems. For example, the model ID of the camera built in to two identical iPhone models will be the same even though they are different physical devices.
 */
@property(nonatomic, readonly) NSString *modelID;

/*!
 @property localizedName
 @abstract
    A localized human-readable name for the receiver.
 
 @discussion
    This property can be used for displaying the name of a capture device in a user interface.
 */
@property(nonatomic, readonly) NSString *localizedName;

/*!
 @property manufacturer
 @abstract
    The human-readable manufacturer name for the receiver.
 
 @discussion
    This property can be used to identify capture devices from a particular manufacturer. All Apple devices return "Apple Inc.". Devices from third party manufacturers may return an empty string.
 */
@property(nonatomic, readonly) NSString *manufacturer API_AVAILABLE(macos(10.9)) API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property transportType
 @abstract
    The transport type of the receiver (e.g. USB, PCI, etc).
 
 @discussion
    This property can be used to discover the transport type of a capture device. Transport types are defined in <IOKit/audio/IOAudioTypes.h> as kIOAudioDeviceTransportType*.
 */
@property(nonatomic, readonly) int32_t transportType API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @method hasMediaType:
 @abstract
    Returns whether the receiver provides media with the given media type.
 
 @param mediaType
    A media type, such as AVMediaTypeVideo, AVMediaTypeAudio, or AVMediaTypeMuxed.
 @result
    YES if the device outputs the given media type, NO otherwise.
 
 @discussion
    Media type constants are defined in AVMediaFormat.h.
 */
- (BOOL)hasMediaType:(AVMediaType)mediaType;

/*!
 @method lockForConfiguration:
 @abstract
    Requests exclusive access to configure device hardware properties.
 
 @param outError
    On return, if the device could not be locked, points to an NSError describing why the failure occurred.
 @result
    A BOOL indicating whether the device was successfully locked for configuration.
 
 @discussion
    In order to set hardware properties on an AVCaptureDevice, such as focusMode and exposureMode, clients must first acquire a lock on the device. Clients should only hold the device lock if they require settable device properties to remain unchanged. Holding the device lock unnecessarily may degrade capture quality in other applications sharing the device.
 */
- (BOOL)lockForConfiguration:(NSError * _Nullable * _Nullable)outError;

/*!
 @method unlockForConfiguration
 @abstract
    Release exclusive control over device hardware properties.
 
 @discussion
    This method should be called to match an invocation of lockForConfiguration: when an application no longer needs to keep device hardware properties from changing automatically.
 */
- (void)unlockForConfiguration;

/*!
 @method supportsAVCaptureSessionPreset:
 @abstract
    Returns whether the receiver can be used in an AVCaptureSession configured with the given preset.
 
 @param preset
    An AVCaptureSession preset.
 @result
    YES if the receiver can be used with the given preset, NO otherwise.
 
 @discussion
    An AVCaptureSession instance can be associated with a preset that configures its inputs and outputs to fulfill common use cases. This method can be used to determine if the receiver can be used in a capture session with the given preset. Presets are defined in AVCaptureSession.h.
 */
- (BOOL)supportsAVCaptureSessionPreset:(AVCaptureSessionPreset)preset;

/*!
 @property connected
 @abstract
    Indicates whether the device is connected and available to the system.
 
 @discussion
    The value of this property is a BOOL indicating whether the device represented by the receiver is connected and available for use as a capture device. Clients can key value observe the value of this property to be notified when a device is no longer available. When the value of this property becomes NO for a given instance, it will not become YES again. If the same physical device again becomes available to the system, it will be represented using a new instance of AVCaptureDevice.
 */
@property(nonatomic, readonly, getter=isConnected) BOOL connected;

/*!
 @property inUseByAnotherApplication
 @abstract
    Indicates whether the device is in use by another application.
 
 @discussion
    The value of this property is a BOOL indicating whether the device represented by the receiver is in use by another application. Clients can key value observe the value of this property to be notified when another app starts or stops using this device.
 */
@property(nonatomic, readonly, getter=isInUseByAnotherApplication) BOOL inUseByAnotherApplication __IOS_PROHIBITED __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @property suspended
 @abstract
    Indicates whether the device is suspended.
 
 @discussion
    The value of this property is a BOOL indicating whether the device represented by the receiver is currently suspended. Some devices disallow data capture due to a feature on the device. For example, isSuspended returns YES for the external iSight when its privacy iris is closed, or for the internal iSight on a notebook when the notebook's display is closed. Clients can key value observe the value of this property to be notified when the device becomes suspended or unsuspended.
 */
@property(nonatomic, readonly, getter=isSuspended) BOOL suspended API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property linkedDevices
 @abstract
    An array of AVCaptureDevice objects physically linked to the receiver.
 
 @discussion
    The value of this property is an array of AVCaptureDevice objects that are a part of the same physical device as the receiver. For example, for the external iSight camera, linkedDevices returns an array containing an AVCaptureDevice for the external iSight microphone.
 */
@property(nonatomic, readonly) NSArray<AVCaptureDevice *> *linkedDevices API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property formats
 @abstract
    An array of AVCaptureDeviceFormat objects supported by the receiver.
 
 @discussion
    This property can be used to enumerate the formats natively supported by the receiver. The capture device's activeFormat property may be set to one of the formats in this array. Clients can observe automatic changes to the receiver's formats by key value observing this property.
 */
@property(nonatomic, readonly) NSArray<AVCaptureDeviceFormat *> *formats API_AVAILABLE(ios(7.0));

/*!
 @property activeFormat
 @abstract
    The currently active format of the receiver.
 
 @discussion
    This property can be used to get or set the currently active device format.
 
    -setActiveFormat: throws an NSInvalidArgumentException if set to a format not present in the formats array.
 
    -setActiveFormat: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 
    Clients can observe automatic changes to the receiver's activeFormat by key value observing this property.
 
    On iOS, use of AVCaptureDevice's setActiveFormat: and AVCaptureSession's setSessionPreset: are mutually exclusive. If you set a capture device's active format, the session to which it is attached changes its preset to AVCaptureSessionPresetInputPriority. Likewise if you set the AVCaptureSession's sessionPreset property, the session assumes control of its input devices, and configures their activeFormat appropriately. Note that audio devices do not expose any user-configurable formats on iOS. To configure audio input on iOS, you should use the AVAudioSession APIs instead (see AVAudioSession.h).
 
    The activeFormat, activeVideoMinFrameDuration, and activeVideoMaxFrameDuration properties may be set simultaneously by using AVCaptureSession's begin/commitConfiguration methods:
 
    [session beginConfiguration]; // the session to which the receiver's AVCaptureDeviceInput is added.
    if ( [device lockForConfiguration:&error] ) {
        [device setActiveFormat:newFormat];
        [device setActiveVideoMinFrameDuration:newMinDuration];
        [device setActiveVideoMaxFrameDuration:newMaxDuration];
        [device unlockForConfiguration];
    }
    [session commitConfiguration]; // The new format and frame rates are applied together in commitConfiguration
 
    Note that when configuring a session to use an active format intended for high resolution still photography and applying one or more of the following operations to an AVCaptureVideoDataOutput, the system may not meet the target framerate: zoom, orientation changes, format conversion.
 */
@property(nonatomic, retain) AVCaptureDeviceFormat *activeFormat API_AVAILABLE(ios(7.0));

/*!
 @property activeVideoMinFrameDuration
 @abstract
    A property indicating the receiver's current active minimum frame duration (the reciprocal of its max frame rate).
 
 @discussion
    An AVCaptureDevice's activeVideoMinFrameDuration property is the reciprocal of its active maximum frame rate. To limit the max frame rate of the capture device, clients may set this property to a value supported by the receiver's activeFormat (see AVCaptureDeviceFormat's videoSupportedFrameRateRanges property). Clients may set this property's value to kCMTimeInvalid to return activeVideoMinFrameDuration to its default value for the given activeFormat.
 
    -setActiveVideoMinFrameDuration: throws an NSInvalidArgumentException if set to an unsupported value.
 
    -setActiveVideoMinFrameDuration: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 
    Clients can observe automatic changes to the receiver's activeVideoMinFrameDuration by key value observing this property.
 
    On iOS, the receiver's activeVideoMinFrameDuration resets to its default value under the following conditions:
        - The receiver's activeFormat changes
        - The receiver's AVCaptureDeviceInput's session's sessionPreset changes
        - The receiver's AVCaptureDeviceInput is added to a session
 
    When exposureMode is AVCaptureExposureModeCustom, setting the activeVideoMinFrameDuration affects max frame rate, but not exposureDuration. You may use setExposureModeCustomWithDuration:ISO:completionHandler: to set a shorter exposureDuration than your activeVideoMinFrameDuration, if desired.
 */
@property(nonatomic) CMTime activeVideoMinFrameDuration API_AVAILABLE(ios(7.0));

/*!
 @property activeVideoMaxFrameDuration
 @abstract
    A property indicating the receiver's current active maximum frame duration (the reciprocal of its min frame rate).
 
 @discussion
    An AVCaptureDevice's activeVideoMaxFrameDuration property is the reciprocal of its active minimum frame rate. To limit the min frame rate of the capture device, clients may set this property to a value supported by the receiver's activeFormat (see AVCaptureDeviceFormat's videoSupportedFrameRateRanges property). Clients may set this property's value to kCMTimeInvalid to return activeVideoMaxFrameDuration to its default value for the given activeFormat.
 
    -setActiveVideoMaxFrameDuration: throws an NSInvalidArgumentException if set to an unsupported value.
 
    -setActiveVideoMaxFrameDuration: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 
    Clients can observe automatic changes to the receiver's activeVideoMaxFrameDuration by key value observing this property.
 
    On iOS, the receiver's activeVideoMaxFrameDuration resets to its default value under the following conditions:
        - The receiver's activeFormat changes
        - The receiver's AVCaptureDeviceInput's session's sessionPreset changes
        - The receiver's AVCaptureDeviceInput is added to a session
 
    When exposureMode is AVCaptureExposureModeCustom, frame rate and exposure duration are interrelated. If you call setExposureModeCustomWithDuration:ISO:completionHandler: with an exposureDuration longer than the current activeVideoMaxFrameDuration, the activeVideoMaxFrameDuration will be lengthened to accommodate the longer exposure time. Setting a shorter exposure duration does not automatically change the activeVideoMinFrameDuration or activeVideoMaxFrameDuration. To explicitly increase the frame rate in custom exposure mode, you must set the activeVideoMaxFrameDuration to a shorter value. If your new max frame duration is shorter than the current exposureDuration, the exposureDuration will shorten as well to accommodate the new frame rate.
 */
@property(nonatomic) CMTime activeVideoMaxFrameDuration API_AVAILABLE(macos(10.9), ios(7.0));

/*!
 @property inputSources
 @abstract
    An array of AVCaptureDeviceInputSource objects supported by the receiver.
 
 @discussion
    Some devices can capture data from one of multiple data sources (different input jacks on the same audio device, for example). For devices with multiple possible data sources, inputSources can be used to enumerate the possible choices. Clients can observe automatic changes to the receiver's inputSources by key value observing this property.
 */
@property(nonatomic, readonly) NSArray<AVCaptureDeviceInputSource *> *inputSources API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property activeInputSource
 @abstract
    The currently active input source of the receiver.
 
 @discussion
    This property can be used to get or set the currently active device input source. -setActiveInputSource: throws an NSInvalidArgumentException if set to a value not present in the inputSources array. -setActiveInputSource: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's activeInputSource by key value observing this property.
 */
@property(nonatomic, retain, nullable) AVCaptureDeviceInputSource *activeInputSource API_UNAVAILABLE(ios, watchos, tvos);

@end


/*!
 @enum AVCaptureDevicePosition
 @abstract
    Constants indicating the physical position of an AVCaptureDevice's hardware on the system.
 
 @constant AVCaptureDevicePositionUnspecified
    Indicates that the device's position relative to the system hardware is unspecified.
 @constant AVCaptureDevicePositionBack
    Indicates that the device is physically located on the back of the system hardware.
 @constant AVCaptureDevicePositionFront
    Indicates that the device is physically located on the front of the system hardware.
 */
typedef NS_ENUM(NSInteger, AVCaptureDevicePosition) {
    AVCaptureDevicePositionUnspecified = 0,
    AVCaptureDevicePositionBack        = 1,
    AVCaptureDevicePositionFront       = 2,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDevicePosition)

/*!
 @property position
 @abstract
    Indicates the physical position of an AVCaptureDevice's hardware on the system.
 
 @discussion
    The value of this property is an AVCaptureDevicePosition indicating where the receiver's device is physically located on the system hardware.
 */
@property(nonatomic, readonly) AVCaptureDevicePosition position;

@end


/*!
 @group AVCaptureDeviceType string constants
 
 @discussion
    The AVCaptureDeviceType string constants are intended to be used in combination with the AVCaptureDeviceDiscoverySession class to obtain a list of devices matching certain search criteria.
 */
typedef NSString *AVCaptureDeviceType NS_STRING_ENUM API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInMicrophone
    A built-in microphone.
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInMicrophone API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInWideAngleCamera
    A built-in wide angle camera device. These devices are suitable for general purpose use.
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInWideAngleCamera API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInTelephotoCamera
    A built-in camera device with a longer focal length than a wide angle camera. Note that devices of this type may only be discovered using an AVCaptureDeviceDiscoverySession.
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInTelephotoCamera API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInDualCamera
    A device that consists of two fixed focal length cameras, one wide and one telephoto. Note that devices of this type may only be discovered using an AVCaptureDeviceDiscoverySession or -[AVCaptureDevice defaultDeviceWithDeviceType:mediaType:position:].
 
    A device of this device type supports the following features:
    - Auto switching from one camera to the other when zoom factor, light level, and focus position allow this.
    - Higher quality zoom for still captures by fusing images from both cameras.
    - Depth data delivery by measuring the disparity of matched features between the wide and telephoto cameras.
 
    A device of this device type does not support the following features:
    - AVCaptureExposureModeCustom and manual exposure bracketing.
    - Locking focus with a lens position other than AVCaptureLensPositionCurrent.
    - Locking auto white balance with device white balance gains other than AVCaptureWhiteBalanceGainsCurrent.
 
    Even when locked, exposure duration, ISO, aperture, white balance gains, or lens position may change when the device switches from one camera to the other. The overall exposure, white balance, and focus position however should be consistent.
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInDualCamera API_AVAILABLE(ios(10.2)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInTrueDepthCamera
    A device that consists of two cameras, one YUV and one Infrared. The infrared camera provides high quality depth information that is synchronized and perspective corrected to frames produced by the YUV camera. While the resolution of the depth data and YUV frames may differ, their field of view and aspect ratio always match. Note that devices of this type may only be discovered using an AVCaptureDeviceDiscoverySession or -[AVCaptureDevice defaultDeviceWithDeviceType:mediaType:position:].
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInTrueDepthCamera API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureDeviceTypeBuiltInDuoCamera
    A deprecated synonym for AVCaptureDeviceTypeBuiltInDualCamera. Please use AVCaptureDeviceTypeBuiltInDualCamera instead.
 */
AVF_EXPORT AVCaptureDeviceType const AVCaptureDeviceTypeBuiltInDuoCamera API_DEPRECATED("Use AVCaptureDeviceTypeBuiltInDualCamera instead.", ios(10.0, 10.2)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceType)

/*!
 @property deviceType
 @abstract
    The type of the capture device.
 
 @discussion
    A capture device's type never changes.
 */
@property(nonatomic, readonly) AVCaptureDeviceType deviceType API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @method defaultDeviceWithDeviceType:mediaType:position:
 @abstract
    Returns an AVCaptureDevice instance for the default device of the given device type, media type, and position.
 
 @param deviceType
    The device type supported by the returned device. It must be a valid AVCaptureDeviceType.
 @param mediaType
    The media type, such as AVMediaTypeVideo, AVMediaTypeAudio, or AVMediaTypeMuxed, supported by the returned device. Pass nil to consider devices with any media type.
 @param position
    The position supported by the returned device. Pass AVCaptureDevicePositionUnspecified to consider devices with any position.
 @result
    The default device with the given device type, media type and position or nil if no device with that media type exists and nil otherwise.
 
 @discussion
    This method returns the default device of the given combination of device type, media type, and position currently available on the system.
 */
+ (nullable AVCaptureDevice *)defaultDeviceWithDeviceType:(AVCaptureDeviceType)deviceType mediaType:(nullable AVMediaType)mediaType position:(AVCaptureDevicePosition)position API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDevice (AVCaptureDeviceSystemPressure)

/*!
 @property systemPressureState
 @abstract
    A key-value observable property indicating the capture device's current system pressure state.
 
 @discussion
    This property indicates whether the capture device is currently subject to an elevated system pressure condition. When system pressure reaches AVCaptureSystemPressureLevelShutdown, the capture device cannot continue to provide input, so the AVCaptureSession becomes interrupted until the pressured state abates. System pressure can be effectively mitigated by lowering the device's activeVideoMinFrameDuration in response to changes in the systemPressureState. Clients are encouraged to implement frame rate throttling to bring system pressure down if their capture use case can tolerate a reduced frame rate.
 */
@property(nonatomic, readonly) AVCaptureSystemPressureState *systemPressureState API_AVAILABLE(ios(11.1)) API_UNAVAILABLE(macos, watchos, tvos) ;

@end


/*!
 @enum AVCaptureFlashMode
 @abstract
    Constants indicating the mode of the flash on the receiver's device, if it has one.
 
 @constant AVCaptureFlashModeOff
    Indicates that the flash should always be off.
 @constant AVCaptureFlashModeOn
    Indicates that the flash should always be on.
 @constant AVCaptureFlashModeAuto
    Indicates that the flash should be used automatically depending on ambient light conditions.
 */
typedef NS_ENUM(NSInteger, AVCaptureFlashMode) {
    AVCaptureFlashModeOff  = 0,
    AVCaptureFlashModeOn   = 1,
    AVCaptureFlashModeAuto = 2,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceFlash)

/*!
 @property hasFlash
 @abstract
    Indicates whether the receiver has a flash.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver has a flash. The receiver's flashMode property can only be set when this property returns YES.
 */
@property(nonatomic, readonly) BOOL hasFlash;

/*!
 @property flashAvailable
 @abstract
    Indicates whether the receiver's flash is currently available for use.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's flash is currently available. The flash may become unavailable if, for example, the device overheats and needs to cool off. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isFlashAvailable) BOOL flashAvailable API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

/*!
 @property flashActive
 @abstract
    Indicates whether the receiver's flash is currently active.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's flash is currently active. When the flash is active, it will flash if a still image is captured. When a still image is captured with the flash active, exposure and white balance settings are overridden for the still. This is true even when using AVCaptureExposureModeCustom and/or AVCaptureWhiteBalanceModeLocked. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isFlashActive) BOOL flashActive API_DEPRECATED("Use AVCapturePhotoOutput's -isFlashScene instead.", ios(5.0, 10.0)) API_UNAVAILABLE(macos);

/*!
 @method isFlashModeSupported:
 @abstract
    Returns whether the receiver supports the given flash mode.
 
 @param flashMode
    An AVCaptureFlashMode to be checked.
 @result
    YES if the receiver supports the given flash mode, NO otherwise.
 
 @discussion
    The receiver's flashMode property can only be set to a certain mode if this method returns YES for that mode.
 */
- (BOOL)isFlashModeSupported:(AVCaptureFlashMode)flashMode API_DEPRECATED("Use AVCapturePhotoOutput's -supportedFlashModes instead.", ios(4.0, 10.0));

/*!
 @property flashMode
 @abstract
    Indicates current mode of the receiver's flash, if it has one.
 
 @discussion
    The value of this property is an AVCaptureFlashMode that determines the mode of the receiver's flash, if it has one. -setFlashMode: throws an NSInvalidArgumentException if set to an unsupported value (see -isFlashModeSupported:). -setFlashMode: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's flashMode by key value observing this property.
 
    When using AVCapturePhotoOutput, AVCaptureDevice's flashMode property is ignored. You specify flashMode on a per photo basis by setting the AVCapturePhotoSettings.flashMode property.
 */
@property(nonatomic) AVCaptureFlashMode flashMode API_DEPRECATED("Use AVCapturePhotoSettings.flashMode instead.", ios(4.0, 10.0));

@end


/*!
 @enum AVCaptureTorchMode
 @abstract
    Constants indicating the mode of the torch on the receiver's device, if it has one.
 
 @constant AVCaptureTorchModeOff
    Indicates that the torch should always be off.
 @constant AVCaptureTorchModeOn
    Indicates that the torch should always be on.
 @constant AVCaptureTorchModeAuto
    Indicates that the torch should be used automatically depending on ambient light conditions.
 */
typedef NS_ENUM(NSInteger, AVCaptureTorchMode) {
    AVCaptureTorchModeOff  = 0,
    AVCaptureTorchModeOn   = 1,
    AVCaptureTorchModeAuto = 2,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @constant AVCaptureMaxAvailableTorchLevel
    A special value that may be passed to -setTorchModeWithLevel:error: to set the torch to the maximum level currently available. Under thermal duress, the maximum available torch level may be less than 1.0.
 */
AVF_EXPORT const float AVCaptureMaxAvailableTorchLevel API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceTorch)

/*!
 @property hasTorch
 @abstract
    Indicates whether the receiver has a torch.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver has a torch. The receiver's torchMode property can only be set when this property returns YES.
 */
@property(nonatomic, readonly) BOOL hasTorch;

/*!
 @property torchAvailable
 @abstract
    Indicates whether the receiver's torch is currently available for use.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's torch is currently available. The torch may become unavailable if, for example, the device overheats and needs to cool off. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isTorchAvailable) BOOL torchAvailable API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

/*!
 @property torchActive
 @abstract
    Indicates whether the receiver's torch is currently active.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's torch is currently active. If the current torchMode is AVCaptureTorchModeAuto and isTorchActive is YES, the torch will illuminate once a recording starts (see AVCaptureOutput.h -startRecordingToOutputFileURL:recordingDelegate:). This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isTorchActive) BOOL torchActive API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @property torchLevel
 @abstract
    Indicates the receiver's current torch brightness level as a floating point value.
 
 @discussion
    The value of this property is a float indicating the receiver's torch level from 0.0 (off) -> 1.0 (full). This property is key-value observable.
 */
@property(nonatomic, readonly) float torchLevel API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

/*!
 @method isTorchModeSupported:
 @abstract
    Returns whether the receiver supports the given torch mode.
 
 @param torchMode
    An AVCaptureTorchMode to be checked.
 @result
    YES if the receiver supports the given torch mode, NO otherwise.
 
 @discussion
    The receiver's torchMode property can only be set to a certain mode if this method returns YES for that mode.
 */
- (BOOL)isTorchModeSupported:(AVCaptureTorchMode)torchMode;

/*!
 @property torchMode
 @abstract
    Indicates current mode of the receiver's torch, if it has one.
 
 @discussion
    The value of this property is an AVCaptureTorchMode that determines the mode of the receiver's torch, if it has one. -setTorchMode: throws an NSInvalidArgumentException if set to an unsupported value (see -isTorchModeSupported:). -setTorchMode: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's torchMode by key value observing this property.
 */
@property(nonatomic) AVCaptureTorchMode torchMode;

/*!
 @method setTorchModeOnWithLevel:error:
 @abstract
    Sets the current mode of the receiver's torch to AVCaptureTorchModeOn at the specified level.
 
 @discussion
    This method sets the torch mode to AVCaptureTorchModeOn at a specified level. torchLevel must be a value between 0 and 1, or the special value AVCaptureMaxAvailableTorchLevel. The specified value may not be available if the iOS device is too hot. This method throws an NSInvalidArgumentException if set to an unsupported level. If the specified level is valid, but unavailable, the method returns NO with AVErrorTorchLevelUnavailable. -setTorchModeOnWithLevel:error: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's torchMode by key value observing the torchMode property.
 */
- (BOOL)setTorchModeOnWithLevel:(float)torchLevel error:(NSError * _Nullable * _Nullable)outError API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVCaptureFocusMode
 @abstract
    Constants indicating the mode of the focus on the receiver's device, if it has one.
 
 @constant AVCaptureFocusModeLocked
    Indicates that the focus should be locked at the lens' current position.
 @constant AVCaptureFocusModeAutoFocus
    Indicates that the device should autofocus once and then change the focus mode to AVCaptureFocusModeLocked.
 @constant AVCaptureFocusModeContinuousAutoFocus
    Indicates that the device should automatically focus when needed.
 */
typedef NS_ENUM(NSInteger, AVCaptureFocusMode) {
    AVCaptureFocusModeLocked              = 0,
    AVCaptureFocusModeAutoFocus           = 1,
    AVCaptureFocusModeContinuousAutoFocus = 2,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @enum AVCaptureAutoFocusRangeRestriction
 @abstract
    Constants indicating the restriction of the receiver's autofocus system to a particular range of focus scan, if it supports range restrictions.
 
 @constant AVCaptureAutoFocusRangeRestrictionNone
    Indicates that the autofocus system should not restrict the focus range.
 @constant AVCaptureAutoFocusRangeRestrictionNear
    Indicates that the autofocus system should restrict the focus range for subject matter that is near to the camera.
 @constant AVCaptureAutoFocusRangeRestrictionFar
    Indicates that the autofocus system should restrict the focus range for subject matter that is far from the camera.
 */
typedef NS_ENUM(NSInteger, AVCaptureAutoFocusRangeRestriction) {
    AVCaptureAutoFocusRangeRestrictionNone = 0,
    AVCaptureAutoFocusRangeRestrictionNear = 1,
    AVCaptureAutoFocusRangeRestrictionFar  = 2,
} API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceFocus)

/*!
 @method isFocusModeSupported:
 @abstract
    Returns whether the receiver supports the given focus mode.
 
 @param focusMode
    An AVCaptureFocusMode to be checked.
 @result
    YES if the receiver supports the given focus mode, NO otherwise.
 
 @discussion
    The receiver's focusMode property can only be set to a certain mode if this method returns YES for that mode.
 */
- (BOOL)isFocusModeSupported:(AVCaptureFocusMode)focusMode;

/*!
 @property lockingFocusWithCustomLensPositionSupported
 @abstract
    Indicates whether the receiver supports a lens position other than AVCaptureLensPositionCurrent.
 
 @discussion
    If lockingFocusWithCustomLensPositionSupported returns NO, setFocusModeLockedWithLensPosition: may only be called with AVCaptureLensPositionCurrent. Passing any other lens position will result in an exception.
 */
@property(nonatomic, readonly, getter=isLockingFocusWithCustomLensPositionSupported) BOOL lockingFocusWithCustomLensPositionSupported API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @property focusMode
 @abstract
    Indicates current focus mode of the receiver, if it has one.
 
 @discussion
    The value of this property is an AVCaptureFocusMode that determines the receiver's focus mode, if it has one. -setFocusMode: throws an NSInvalidArgumentException if set to an unsupported value (see -isFocusModeSupported:). -setFocusMode: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's focusMode by key value observing this property.
 */
@property(nonatomic) AVCaptureFocusMode focusMode;

/*!
 @property focusPointOfInterestSupported
 @abstract
    Indicates whether the receiver supports focus points of interest.
 
 @discussion
    The receiver's focusPointOfInterest property can only be set if this property returns YES.
 */
@property(nonatomic, readonly, getter=isFocusPointOfInterestSupported) BOOL focusPointOfInterestSupported;

/*!
 @property focusPointOfInterest
 @abstract
    Indicates current focus point of interest of the receiver, if it has one.
 
 @discussion
    The value of this property is a CGPoint that determines the receiver's focus point of interest, if it has one. A value of (0,0) indicates that the camera should focus on the top left corner of the image, while a value of (1,1) indicates that it should focus on the bottom right. The default value is (0.5,0.5). -setFocusPointOfInterest: throws an NSInvalidArgumentException if isFocusPointOfInterestSupported returns NO. -setFocusPointOfInterest: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's focusPointOfInterest by key value observing this property. Note that setting focusPointOfInterest alone does not initiate a focus operation. After setting focusPointOfInterest, call -setFocusMode: to apply the new point of interest.
 */
@property(nonatomic) CGPoint focusPointOfInterest;

/*!
 @property adjustingFocus
 @abstract
    Indicates whether the receiver is currently performing a focus scan to adjust focus.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's camera focus is being automatically adjusted by means of a focus scan, because its focus mode is AVCaptureFocusModeAutoFocus or AVCaptureFocusModeContinuousAutoFocus. Clients can observe the value of this property to determine whether the camera's focus is stable.
 @seealso lensPosition
 @seealso AVCaptureAutoFocusSystem
 */
@property(nonatomic, readonly, getter=isAdjustingFocus) BOOL adjustingFocus;

/*!
 @property autoFocusRangeRestrictionSupported
 @abstract
    Indicates whether the receiver supports autofocus range restrictions.
 
 @discussion
    The receiver's autoFocusRangeRestriction property can only be set if this property returns YES.
 */
@property(nonatomic, readonly, getter=isAutoFocusRangeRestrictionSupported) BOOL autoFocusRangeRestrictionSupported API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property autoFocusRangeRestriction
 @abstract
    Indicates current restriction of the receiver's autofocus system to a particular range of focus scan, if it supports range restrictions.
 
 @discussion
    The value of this property is an AVCaptureAutoFocusRangeRestriction indicating how the autofocus system should limit its focus scan. The default value is AVCaptureAutoFocusRangeRestrictionNone. -setAutoFocusRangeRestriction: throws an NSInvalidArgumentException if isAutoFocusRangeRestrictionSupported returns NO. -setAutoFocusRangeRestriction: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. This property only has an effect when the focusMode property is set to AVCaptureFocusModeAutoFocus or AVCaptureFocusModeContinuousAutoFocus. Note that setting autoFocusRangeRestriction alone does not initiate a focus operation. After setting autoFocusRangeRestriction, call -setFocusMode: to apply the new restriction.
 */
@property(nonatomic) AVCaptureAutoFocusRangeRestriction autoFocusRangeRestriction API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property smoothAutoFocusSupported
 @abstract
    Indicates whether the receiver supports smooth autofocus.
 
 @discussion
    The receiver's smoothAutoFocusEnabled property can only be set if this property returns YES.
 */
@property(nonatomic, readonly, getter=isSmoothAutoFocusSupported) BOOL smoothAutoFocusSupported API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property smoothAutoFocusEnabled
 @abstract
    Indicates whether the receiver should use smooth autofocus.
 
 @discussion
    On a receiver where -isSmoothAutoFocusSupported returns YES and smoothAutoFocusEnabled is set to YES, a smooth autofocus will be engaged when the focus mode is set to AVCaptureFocusModeAutoFocus or AVCaptureFocusModeContinuousAutoFocus. Enabling smooth autofocus is appropriate for movie recording. Smooth autofocus is slower and less visually invasive. Disabling smooth autofocus is more appropriate for video processing where a fast autofocus is necessary. The default value is NO. Setting this property throws an NSInvalidArgumentException if -isSmoothAutoFocusSupported returns NO. The receiver must be locked for configuration using lockForConfiguration: before clients can set this method, otherwise an NSGenericException is thrown. Note that setting smoothAutoFocusEnabled alone does not initiate a focus operation. After setting smoothAutoFocusEnabled, call -setFocusMode: to apply the new smooth autofocus mode.
 */
@property(nonatomic, getter=isSmoothAutoFocusEnabled) BOOL smoothAutoFocusEnabled API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property lensPosition
 @abstract
    Indicates the focus position of the lens.
 
 @discussion
    The range of possible positions is 0.0 to 1.0, with 0.0 being the shortest distance at which the lens can focus and 1.0 the furthest. Note that 1.0 does not represent focus at infinity. The default value is 1.0. Note that a given lens position value does not correspond to an exact physical distance, nor does it represent a consistent focus distance from device to device. This property is key-value observable. It can be read at any time, regardless of focus mode, but can only be set via setFocusModeLockedWithLensPosition:completionHandler:.
 */
@property(nonatomic, readonly) float lensPosition API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @constant AVCaptureLensPositionCurrent
    A special value that may be passed as the lensPosition parameter of setFocusModeLockedWithLensPosition:completionHandler: to indicate that the caller does not wish to specify a value for the lensPosition property, and that it should instead be set to its current value. Note that the device may be adjusting lensPosition at the time of the call, in which case the value at which lensPosition is locked may differ from the value obtained by querying the lensPosition property.
 */
AVF_EXPORT const float AVCaptureLensPositionCurrent API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @method setFocusModeLockedWithLensPosition:completionHandler:
 @abstract
    Sets focusMode to AVCaptureFocusModeLocked and locks lensPosition at an explicit value.
 
 @param lensPosition
    The lens position, as described in the documentation for the lensPosition property. A value of AVCaptureLensPositionCurrent can be used to indicate that the caller does not wish to specify a value for lensPosition.
 @param handler
    A block to be called when lensPosition has been set to the value specified and focusMode is set to AVCaptureFocusModeLocked. If setFocusModeLockedWithLensPosition:completionHandler: is called multiple times, the completion handlers will be called in FIFO order. The block receives a timestamp which matches that of the first buffer to which all settings have been applied. Note that the timestamp is synchronized to the device clock, and thus must be converted to the master clock prior to comparison with the timestamps of buffers delivered via an AVCaptureVideoDataOutput. The client may pass nil for the handler parameter if knowledge of the operation's completion is not required.
 
 @discussion
    This is the only way of setting lensPosition. This method throws an NSRangeException if lensPosition is set to an unsupported level. This method throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
- (void)setFocusModeLockedWithLensPosition:(float)lensPosition completionHandler:(nullable void (^)(CMTime syncTime))handler API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVCaptureExposureMode
 @abstract
    Constants indicating the mode of the exposure on the receiver's device, if it has adjustable exposure.
 
 @constant AVCaptureExposureModeLocked
    Indicates that the exposure should be locked at its current value.
 @constant AVCaptureExposureModeAutoExpose
    Indicates that the device should automatically adjust exposure once and then change the exposure mode to AVCaptureExposureModeLocked.
 @constant AVCaptureExposureModeContinuousAutoExposure
    Indicates that the device should automatically adjust exposure when needed.
 @constant AVCaptureExposureModeCustom
    Indicates that the device should only adjust exposure according to user provided ISO, exposureDuration values.
 */
typedef NS_ENUM(NSInteger, AVCaptureExposureMode) {
    AVCaptureExposureModeLocked                            = 0,
    AVCaptureExposureModeAutoExpose                        = 1,
    AVCaptureExposureModeContinuousAutoExposure            = 2,
    AVCaptureExposureModeCustom NS_ENUM_AVAILABLE_IOS(8_0) = 3,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceExposure)

/*!
 @method isExposureModeSupported:
 @abstract
    Returns whether the receiver supports the given exposure mode.
 
 @param exposureMode
    An AVCaptureExposureMode to be checked.
 @result
    YES if the receiver supports the given exposure mode, NO otherwise.
 
 @discussion
    The receiver's exposureMode property can only be set to a certain mode if this method returns YES for that mode.
 */
- (BOOL)isExposureModeSupported:(AVCaptureExposureMode)exposureMode;

/*!
 @property exposureMode
 @abstract
    Indicates current exposure mode of the receiver, if it has adjustable exposure.
 
 @discussion
    The value of this property is an AVCaptureExposureMode that determines the receiver's exposure mode, if it has adjustable exposure. -setExposureMode: throws an NSInvalidArgumentException if set to an unsupported value (see -isExposureModeSupported:). -setExposureMode: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. When using AVCaptureStillImageOutput with automaticallyEnablesStillImageStabilizationWhenAvailable set to YES (the default behavior), the receiver's ISO and exposureDuration values may be overridden by automatic still image stabilization values if the scene is dark enough to warrant still image stabilization. To ensure that the receiver's ISO and exposureDuration values are honored while in AVCaptureExposureModeCustom or AVCaptureExposureModeLocked, you must set AVCaptureStillImageOutput's automaticallyEnablesStillImageStabilizationWhenAvailable property to NO. Clients can observe automatic changes to the receiver's exposureMode by key value observing this property.
 */
@property(nonatomic) AVCaptureExposureMode exposureMode;

/*!
 @property exposurePointOfInterestSupported:
 @abstract
    Indicates whether the receiver supports exposure points of interest.
 
 @discussion
    The receiver's exposurePointOfInterest property can only be set if this property returns YES.
 */
@property(nonatomic, readonly, getter=isExposurePointOfInterestSupported) BOOL exposurePointOfInterestSupported;

/*!
 @property exposurePointOfInterest
 @abstract
    Indicates current exposure point of interest of the receiver, if it has one.
 
 @discussion
    The value of this property is a CGPoint that determines the receiver's exposure point of interest, if it has adjustable exposure. A value of (0,0) indicates that the camera should adjust exposure based on the top left corner of the image, while a value of (1,1) indicates that it should adjust exposure based on the bottom right corner. The default value is (0.5,0.5). -setExposurePointOfInterest: throws an NSInvalidArgumentException if isExposurePointOfInterestSupported returns NO. -setExposurePointOfInterest: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Note that setting exposurePointOfInterest alone does not initiate an exposure operation. After setting exposurePointOfInterest, call -setExposureMode: to apply the new point of interest.
 */
@property(nonatomic) CGPoint exposurePointOfInterest;

/*!
 @constant activeMaxExposureDuration
 @abstract
    The maximum exposure (integration) time that may be used by the auto exposure algorithm.
 
 @discussion
    When an AVCaptureDevice's exposureMode is set to AVCaptureExposureModeAutoExpose or AVCaptureExposureModeContinuousAutoExposure, the auto exposure algorithm picks a default max exposure duration that is tuned for the current configuration, balancing low light image quality with motion preservation. By querying or key-value observing this property, you may find out the current max exposure duration in use. You may also override the default value by setting this property to a value between activeFormat.maxExposureDuration and activeFormat.minExposureDuration. An NSRangeException is thrown if you pass an out-of-bounds exposure duration. Setting the property to the special value of kCMTimeInvalid resets the auto exposure max duration to the device's default for your current configuration. When the device's activeFormat or the AVCaptureSession's sessionPreset changes, this property resets to the default max exposure duration for the new format or session preset.
 
    On some devices, the auto exposure algorithm picks a different max exposure duration for a given format depending whether you used the -[AVCaptureSession setSessionPreset:] API or the -[AVCaptureDevice setActiveFormat:] API to set the format. To ensure uniform default handling of max exposure duration, you can set your AVCaptureDeviceInput's unifiedAutoExposureDefaultsEnabled property to YES.
 */
@property(nonatomic) CMTime activeMaxExposureDuration API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @property adjustingExposure
 @abstract
    Indicates whether the receiver is currently adjusting camera exposure.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's camera exposure is being automatically adjusted because its exposure mode is AVCaptureExposureModeAutoExpose or AVCaptureExposureModeContinuousAutoExposure. Clients can observe the value of this property to determine whether the camera exposure is stable or is being automatically adjusted.
 */
@property(nonatomic, readonly, getter=isAdjustingExposure) BOOL adjustingExposure;

/*!
 @property lensAperture
 @abstract
    The size of the lens diaphragm.
 
 @discussion
    The value of this property is a float indicating the size (f number) of the lens diaphragm. This property does not change.
 */
@property(nonatomic, readonly) float lensAperture API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property exposureDuration
 @abstract
    The length of time over which exposure takes place.
 
 @discussion
    Only exposure duration values between activeFormat.minExposureDuration and activeFormat.maxExposureDuration are supported. This property is key-value observable. It can be read at any time, regardless of exposure mode, but can only be set via setExposureModeCustomWithDuration:ISO:completionHandler:.
 */
@property(nonatomic, readonly) CMTime exposureDuration API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property ISO
 @abstract
    The current exposure ISO value.
 
 @discussion
    This property controls the sensor's sensitivity to light by means of a gain value applied to the signal. Only ISO values between activeFormat.minISO and activeFormat.maxISO are supported. Higher values will result in noisier images. This property is key-value observable. It can be read at any time, regardless of exposure mode, but can only be set via setExposureModeCustomWithDuration:ISO:completionHandler:.
 */
@property(nonatomic, readonly) float ISO API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @constant AVCaptureExposureDurationCurrent
    A special value that may be passed as the duration parameter of setExposureModeCustomWithDuration:ISO:completionHandler: to indicate that the caller does not wish to specify a value for the exposureDuration property, and that it should instead be set to its current value. Note that the device may be adjusting exposureDuration at the time of the call, in which case the value to which exposureDuration is set may differ from the value obtained by querying the exposureDuration property.
 */
AVF_EXPORT const CMTime AVCaptureExposureDurationCurrent API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @constant AVCaptureISOCurrent
    A special value that may be passed as the ISO parameter of setExposureModeCustomWithDuration:ISO:completionHandler: to indicate that the caller does not wish to specify a value for the ISO property, and that it should instead be set to its current value. Note that the device may be adjusting ISO at the time of the call, in which case the value to which ISO is set may differ from the value obtained by querying the ISO property.
 */
AVF_EXPORT const float AVCaptureISOCurrent API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @method setExposureModeCustomWithDuration:ISO:completionHandler:
 @abstract
    Sets exposureMode to AVCaptureExposureModeCustom and locks exposureDuration and ISO at explicit values.
 
 @param duration
    The exposure duration, as described in the documentation for the exposureDuration property. A value of AVCaptureExposureDurationCurrent can be used to indicate that the caller does not wish to specify a value for exposureDuration. Note that changes to this property may result in changes to activeVideoMinFrameDuration and/or activeVideoMaxFrameDuration.
 @param ISO
    The exposure ISO value, as described in the documentation for the ISO property. A value of AVCaptureISOCurrent can be used to indicate that the caller does not wish to specify a value for ISO.
 @param handler
    A block to be called when both exposureDuration and ISO have been set to the values specified and exposureMode is set to AVCaptureExposureModeCustom. If setExposureModeCustomWithDuration:ISO:completionHandler: is called multiple times, the completion handlers will be called in FIFO order. The block receives a timestamp which matches that of the first buffer to which all settings have been applied. Note that the timestamp is synchronized to the device clock, and thus must be converted to the master clock prior to comparison with the timestamps of buffers delivered via an AVCaptureVideoDataOutput. The client may pass nil for the handler parameter if knowledge of the operation's completion is not required.
 
 @discussion
    This is the only way of setting exposureDuration and ISO. This method throws an NSRangeException if either exposureDuration or ISO is set to an unsupported level. This method throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. When using AVCaptureStillImageOutput with automaticallyEnablesStillImageStabilizationWhenAvailable set to YES (the default behavior), the receiver's ISO and exposureDuration values may be overridden by automatic still image stabilization values if the scene is dark enough to warrant still image stabilization. To ensure that the receiver's ISO and exposureDuration values are honored while in AVCaptureExposureModeCustom or AVCaptureExposureModeLocked, you must set AVCaptureStillImageOutput's automaticallyEnablesStillImageStabilizationWhenAvailable property to NO.
 */
- (void)setExposureModeCustomWithDuration:(CMTime)duration ISO:(float)ISO completionHandler:(nullable void (^)(CMTime syncTime))handler API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property exposureTargetOffset
 @abstract
    Indicates the metered exposure level's offset from the target exposure value, in EV units.
 
 @discussion
    The value of this read-only property indicates the difference between the metered exposure level of the current scene and the target exposure value. This property is key-value observable.
 */
@property(nonatomic, readonly) float exposureTargetOffset API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property exposureTargetBias
 @abstract
    Bias applied to the target exposure value, in EV units.
 
 @discussion
    When exposureMode is AVCaptureExposureModeContinuousAutoExposure or AVCaptureExposureModeLocked, the bias will affect both metering (exposureTargetOffset), and the actual exposure level (exposureDuration and ISO). When the exposure mode is AVCaptureExposureModeCustom, it will only affect metering. This property is key-value observable. It can be read at any time, but can only be set via setExposureTargetBias:completionHandler:.
 */
@property(nonatomic, readonly) float exposureTargetBias API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property minExposureTargetBias
 @abstract
    A float indicating the minimum supported exposure bias, in EV units.
 
 @discussion
    This read-only property indicates the minimum supported exposure bias.
 */
@property(nonatomic, readonly) float minExposureTargetBias API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property maxExposureTargetBias
 @abstract
    A float indicating the maximum supported exposure bias, in EV units.
 
 @discussion
    This read-only property indicates the maximum supported exposure bias.
 */
@property(nonatomic, readonly) float maxExposureTargetBias API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @constant AVCaptureExposureTargetBiasCurrent
    A special value that may be passed as the bias parameter of setExposureTargetBias:completionHandler: to indicate that the caller does not wish to specify a value for the exposureTargetBias property, and that it should instead be set to its current value.
 */
AVF_EXPORT const float AVCaptureExposureTargetBiasCurrent API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @method setExposureTargetBias:completionHandler:
 @abstract
    Sets the bias to be applied to the target exposure value.
 
 @param bias
    The bias to be applied to the exposure target value, as described in the documentation for the exposureTargetBias property.
 @param handler
    A block to be called when exposureTargetBias has been set to the value specified. If setExposureTargetBias:completionHandler: is called multiple times, the completion handlers will be called in FIFO order. The block receives a timestamp which matches that of the first buffer to which the setting has been applied. Note that the timestamp is synchronized to the device clock, and thus must be converted to the master clock prior to comparison with the timestamps of buffers delivered via an AVCaptureVideoDataOutput. The client may pass nil for the handler parameter if knowledge of the operation's completion is not required.
 
 @discussion
    This is the only way of setting exposureTargetBias. This method throws an NSRangeException if exposureTargetBias is set to an unsupported level. This method throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
- (void)setExposureTargetBias:(float)bias completionHandler:(nullable void (^)(CMTime syncTime))handler API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVCaptureWhiteBalanceMode
 @abstract
    Constants indicating the mode of the white balance on the receiver's device, if it has adjustable white balance.
 
 @constant AVCaptureWhiteBalanceModeLocked
    Indicates that the white balance should be locked at its current value.
 @constant AVCaptureWhiteBalanceModeAutoWhiteBalance
    Indicates that the device should automatically adjust white balance once and then change the white balance mode to AVCaptureWhiteBalanceModeLocked.
 @constant AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance
    Indicates that the device should automatically adjust white balance when needed.
 */
typedef NS_ENUM(NSInteger, AVCaptureWhiteBalanceMode) {
    AVCaptureWhiteBalanceModeLocked                     = 0,
    AVCaptureWhiteBalanceModeAutoWhiteBalance           = 1,
    AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance = 2,
} API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @typedef AVCaptureWhiteBalanceGains
 @abstract
    Structure containing RGB white balance gain values.
 */
typedef struct {
    float redGain;
    float greenGain;
    float blueGain;
} AVCaptureWhiteBalanceGains API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @typedef AVCaptureWhiteBalanceChromaticityValues
 @abstract
    Structure containing CIE 1931 xy chromaticity values.
 */
typedef struct {
    float x;
    float y;
} AVCaptureWhiteBalanceChromaticityValues API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @typedef AVCaptureWhiteBalanceTemperatureAndTintValues
 @abstract
    Structure containing a white balance color correlated temperature in kelvin, plus a tint value in the range of [-150 - +150].
 */
typedef struct {
    float temperature;
    float tint;
} AVCaptureWhiteBalanceTemperatureAndTintValues API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceWhiteBalance)

/*!
 @method isWhiteBalanceModeSupported:
 @abstract
    Returns whether the receiver supports the given white balance mode.
 
 @param whiteBalanceMode
    An AVCaptureWhiteBalanceMode to be checked.
 @result
    YES if the receiver supports the given white balance mode, NO otherwise.
 
 @discussion
    The receiver's whiteBalanceMode property can only be set to a certain mode if this method returns YES for that mode.
 */
- (BOOL)isWhiteBalanceModeSupported:(AVCaptureWhiteBalanceMode)whiteBalanceMode;

/*!
 @property lockingWhiteBalanceWithCustomDeviceGainsSupported
 @abstract
    Indicates whether the receiver supports white balance gains other than AVCaptureWhiteBalanceGainsCurrent.
 
 @discussion
    If lockingWhiteBalanceWithCustomDeviceGainsSupported returns NO, setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains: may only be called with AVCaptureWhiteBalanceGainsCurrent. Passing any other white balance gains will result in an exception.
 */
@property(nonatomic, readonly, getter=isLockingWhiteBalanceWithCustomDeviceGainsSupported) BOOL lockingWhiteBalanceWithCustomDeviceGainsSupported API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @property whiteBalanceMode
 @abstract
    Indicates current white balance mode of the receiver, if it has adjustable white balance.
 
 @discussion
    The value of this property is an AVCaptureWhiteBalanceMode that determines the receiver's white balance mode, if it has adjustable white balance. -setWhiteBalanceMode: throws an NSInvalidArgumentException if set to an unsupported value (see -isWhiteBalanceModeSupported:). -setWhiteBalanceMode: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's whiteBalanceMode by key value observing this property.
 */
@property(nonatomic) AVCaptureWhiteBalanceMode whiteBalanceMode;

/*!
 @property adjustingWhiteBalance
 @abstract
    Indicates whether the receiver is currently adjusting camera white balance.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver's camera white balance is being automatically adjusted because its white balance mode is AVCaptureWhiteBalanceModeAutoWhiteBalance or AVCaptureWhiteBalanceModeContinuousAutoWhiteBalance. Clients can observe the value of this property to determine whether the camera white balance is stable or is being automatically adjusted.
 */
@property(nonatomic, readonly, getter=isAdjustingWhiteBalance) BOOL adjustingWhiteBalance;

/*!
 @property deviceWhiteBalanceGains
 @abstract
    Indicates the current device-specific RGB white balance gain values in use.
 
 @discussion
    This property specifies the current red, green, and blue gain values used for white balance. The values can be used to adjust color casts for a given scene. For each channel, only values between 1.0 and -maxWhiteBalanceGain are supported. This property is key-value observable. It can be read at any time, regardless of white balance mode, but can only be set via setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler:.
 */
@property(nonatomic, readonly) AVCaptureWhiteBalanceGains deviceWhiteBalanceGains API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property grayWorldDeviceWhiteBalanceGains
 @abstract
    Indicates the current device-specific Gray World RGB white balance gain values in use.
 
 @discussion
    This property specifies the current red, green, and blue gain values derived from the current scene to deliver a neutral (or "Gray World") white point for white balance. Gray World values assume a neutral subject (e.g. a gray card) has been placed in the middle of the subject area and fills the center 50% of the frame. Clients can read these values and apply them to the device using setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler:. For each channel, only values between 1.0 and -maxWhiteBalanceGain are supported. This property is key-value observable. It can be read at any time, regardless of white balance mode.
 */
@property(nonatomic, readonly) AVCaptureWhiteBalanceGains grayWorldDeviceWhiteBalanceGains API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property maxWhiteBalanceGain
 @abstract
    Indicates the maximum supported value to which a channel in the AVCaptureWhiteBalanceGains may be set.
 
 @discussion
    This property does not change for the life of the receiver.
 */
@property(nonatomic, readonly) float maxWhiteBalanceGain API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @constant AVCaptureWhiteBalanceGainsCurrent
    A special value that may be passed as a parameter of setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler: to indicate that the caller does not wish to specify a value for deviceWhiteBalanceGains, and that gains should instead be locked at their value at the moment that white balance is locked.
 */
AVF_EXPORT const AVCaptureWhiteBalanceGains AVCaptureWhiteBalanceGainsCurrent API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;

/*!
 @method setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler:
 @abstract
    Sets white balance to locked mode with explicit deviceWhiteBalanceGains values.
 
 @param whiteBalanceGains
    The white balance gain values, as described in the documentation for the deviceWhiteBalanceGains property. A value of AVCaptureWhiteBalanceGainsCurrent can be used to indicate that the caller does not wish to specify a value for deviceWhiteBalanceGains.
 @param handler
    A block to be called when white balance gains have been set to the values specified and whiteBalanceMode is set to AVCaptureWhiteBalanceModeLocked. If setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler: is called multiple times, the completion handlers will be called in FIFO order. The block receives a timestamp which matches that of the first buffer to which all settings have been applied. Note that the timestamp is synchronized to the device clock, and thus must be converted to the master clock prior to comparison with the timestamps of buffers delivered via an AVCaptureVideoDataOutput. This parameter may be nil if synchronization is not required.
 
 @discussion
    For each channel in the whiteBalanceGains struct, only values between 1.0 and -maxWhiteBalanceGain are supported. Gain values are normalized to the minimum channel value to avoid brightness changes (e.g. R:2 G:2 B:4 will be normalized to R:1 G:1 B:2). This method throws an NSRangeException if any of the whiteBalanceGains are set to an unsupported level. This method throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
- (void)setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:(AVCaptureWhiteBalanceGains)whiteBalanceGains completionHandler:(nullable void (^)(CMTime syncTime))handler API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @method chromaticityValuesForDeviceWhiteBalanceGains:
 @abstract
    Converts device-specific white balance RGB gain values to device-independent chromaticity values.
 
 @param whiteBalanceGains
    White balance gain values, as described in the documentation for the deviceWhiteBalanceGains property. A value of AVCaptureWhiteBalanceGainsCurrent may not be used in this function.
 @result
    A fully populated AVCaptureWhiteBalanceChromaticityValues structure containing device-independent values.
 
 @discussion
    This method may be called on the receiver to convert device-specific white balance RGB gain values to device-independent chromaticity (little x, little y) values. For each channel in the whiteBalanceGains struct, only values between 1.0 and -maxWhiteBalanceGain are supported. This method throws an NSRangeException if any of the whiteBalanceGains are set to unsupported values.
 */
- (AVCaptureWhiteBalanceChromaticityValues)chromaticityValuesForDeviceWhiteBalanceGains:(AVCaptureWhiteBalanceGains)whiteBalanceGains API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @method deviceWhiteBalanceGainsForChromaticityValues:
 @abstract
    Converts device-independent chromaticity values to device-specific white balance RGB gain values.
 
 @param chromaticityValues
    Little x, little y chromaticity values as described in the documentation for AVCaptureWhiteBalanceChromaticityValues.
 @result
    A fully populated AVCaptureWhiteBalanceGains structure containing device-specific RGB gain values.
 
 @discussion
    This method may be called on the receiver to convert device-independent chromaticity values to device-specific RGB white balance gain values. This method throws an NSRangeException if any of the chromaticityValues are set outside the range [0,1]. Note that some x,y combinations yield out-of-range device RGB values that will cause an exception to be thrown if passed directly to -setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler:. Be sure to check that red, green, and blue gain values are within the range of [1.0 - maxWhiteBalanceGain].
 */
- (AVCaptureWhiteBalanceGains)deviceWhiteBalanceGainsForChromaticityValues:(AVCaptureWhiteBalanceChromaticityValues)chromaticityValues API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @method temperatureAndTintValuesForDeviceWhiteBalanceGains:
 @abstract
    Converts device-specific white balance RGB gain values to device-independent temperature and tint values.
 
 @param whiteBalanceGains
    White balance gain values, as described in the documentation for the deviceWhiteBalanceGains property. A value of AVCaptureWhiteBalanceGainsCurrent may not be used in this function.
 @result
    A fully populated AVCaptureWhiteBalanceTemperatureAndTintValues structure containing device-independent values.
 
 @discussion
    This method may be called on the receiver to convert device-specific white balance RGB gain values to device-independent temperature (in kelvin) and tint values. For each channel in the whiteBalanceGains struct, only values between 1.0 and -maxWhiteBalanceGain are supported. This method throws an NSRangeException if any of the whiteBalanceGains are set to unsupported values.
 */
- (AVCaptureWhiteBalanceTemperatureAndTintValues)temperatureAndTintValuesForDeviceWhiteBalanceGains:(AVCaptureWhiteBalanceGains)whiteBalanceGains API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @method deviceWhiteBalanceGainsForTemperatureAndTintValues:
 @abstract
    Converts device-independent temperature and tint values to device-specific white balance RGB gain values.
 
 @param tempAndTintValues
    Temperature and tint values as described in the documentation for AVCaptureWhiteBalanceTemperatureAndTintValues.
 @result
    A fully populated AVCaptureWhiteBalanceGains structure containing device-specific RGB gain values.
 
 @discussion
    This method may be called on the receiver to convert device-independent temperature and tint values to device-specific RGB white balance gain values. You may pass any temperature and tint values and corresponding white balance gains will be produced. Note though that some temperature and tint combinations yield out-of-range device RGB values that will cause an exception to be thrown if passed directly to -setWhiteBalanceModeLockedWithDeviceWhiteBalanceGains:completionHandler:. Be sure to check that red, green, and blue gain values are within the range of [1.0 - maxWhiteBalanceGain].
 */
- (AVCaptureWhiteBalanceGains)deviceWhiteBalanceGainsForTemperatureAndTintValues:(AVCaptureWhiteBalanceTemperatureAndTintValues)tempAndTintValues API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDevice (AVCaptureDeviceSubjectAreaChangeMonitoring)

/*!
 @property subjectAreaChangeMonitoringEnabled
 @abstract
    Indicates whether the receiver should monitor the subject area for changes.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver should monitor the video subject area for changes, such as lighting changes, substantial movement, etc. If subject area change monitoring is enabled, the receiver sends an AVCaptureDeviceSubjectAreaDidChangeNotification whenever it detects a change to the subject area, at which time an interested client may wish to re-focus, adjust exposure, white balance, etc. The receiver must be locked for configuration using lockForConfiguration: before clients can set the value of this property.
 */
@property(nonatomic, getter=isSubjectAreaChangeMonitoringEnabled) BOOL subjectAreaChangeMonitoringEnabled API_AVAILABLE(ios(5.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDevice (AVCaptureDeviceLowLightBoost)

/*!
 @property lowLightBoostSupported
 @abstract
    Indicates whether the receiver supports boosting images in low light conditions.
 
 @discussion
    The receiver's automaticallyEnablesLowLightBoostWhenAvailable property can only be set if this property returns YES.
 */
@property(nonatomic, readonly, getter=isLowLightBoostSupported) BOOL lowLightBoostSupported API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @property lowLightBoostEnabled
 @abstract
    Indicates whether the receiver's low light boost feature is enabled.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is currently enhancing images to improve quality due to low light conditions. When -isLowLightBoostEnabled returns YES, the receiver has switched into a special mode in which more light can be perceived in images. This property is key-value observable.
 */
@property(nonatomic, readonly, getter=isLowLightBoostEnabled) BOOL lowLightBoostEnabled API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

/*!
 @property automaticallyEnablesLowLightBoostWhenAvailable
 @abstract
    Indicates whether the receiver should automatically switch to low light boost mode when necessary.
 
 @discussion
    On a receiver where -isLowLightBoostSupported returns YES, a special low light boost mode may be engaged to improve image quality. When the automaticallyEnablesLowLightBoostWhenAvailable property is set to YES, the receiver switches at its discretion to a special boost mode under low light, and back to normal operation when the scene becomes sufficiently lit. An AVCaptureDevice that supports this feature may only engage boost mode for certain source formats or resolutions. Clients may observe changes to the lowLightBoostEnabled property to know when the mode has engaged. The switch between normal operation and low light boost mode may drop one or more video frames. The default value is NO. Setting this property throws an NSInvalidArgumentException if -isLowLightBoostSupported returns NO. The receiver must be locked for configuration using lockForConfiguration: before clients can set this method, otherwise an NSGenericException is thrown.
 */
@property(nonatomic) BOOL automaticallyEnablesLowLightBoostWhenAvailable API_AVAILABLE(ios(6.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDevice (AVCaptureDeviceVideoZoom)

/*!
 @property videoZoomFactor
 @abstract
    Controls zoom level of image outputs
 
 @discussion
    Applies a centered crop for all image outputs, scaling as necessary to maintain output dimensions. Minimum value of 1.0 yields full field of view, increasing values will increase magnification, up to a maximum value specified in the activeFormat's videoMaxZoomFactor property. Modifying the zoom factor will cancel any active rampToVideoZoomFactor:withRate:, and snap directly to the assigned value. Assigning values outside the acceptable range will generate an NSRangeException. Clients can key value observe the value of this property.
 
    -setVideoZoomFactor: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 
 @seealso -[AVCaptureDeviceFormat videoMaxZoomFactor] and -[AVCaptureDeviceFormat videoZoomFactorUpscaleThreshold]
 */
@property(nonatomic) CGFloat videoZoomFactor API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method rampToVideoZoomFactor:withRate:
 @abstract
    Provides smooth changes in zoom factor.
 
 @discussion
    This method provides a change in zoom by compounding magnification at the specified rate over time. Although the zoom factor will grow exponentially, this yields a visually linear zoom in the image over time.
 
    The zoom transition will stop at the specified factor, which must be in the valid range for videoZoomFactor. Assignments to videoZoomFactor while a ramp is in progress will cancel the ramp and snap to the assigned value.
 
    The zoom factor is continuously scaled by pow(2,rate * time). A rate of 0 causes no change in zoom factor, equivalent to calling cancelVideoZoomRamp. A rate of 1 will cause the magnification to double every second (or halve every second if zooming out), and similarly larger or smaller values will zoom faster or slower respectively. Only the absolute value of the rate is significant--sign is corrected for the direction of the target. Changes in rate will be smoothed by an internal acceleration limit.
 
    -rampToVideoZoomFactor:withRate: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
- (void)rampToVideoZoomFactor:(CGFloat)factor withRate:(float)rate API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property rampingVideoZoom
 @abstract
    Indicates if the zoom factor is transitioning to a value set by rampToVideoZoomFactor:withRate:
 
 @discussion
    Clients can observe this value to determine when a ramp begins or completes.
 */
@property(nonatomic, readonly, getter=isRampingVideoZoom) BOOL rampingVideoZoom API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @method cancelVideoZoomRamp
 @abstract
    Eases out of any video zoom transitions initiated by rampToVideoZoomFactor:withRate:
 
 @discussion
    This method is equivalent to calling rampToVideoZoomFactor:withRate: using the current zoom factor target and a rate of 0. This allows a smooth stop to any changes in zoom which were in progress.
 
    -cancelVideoZoomRamp: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
- (void)cancelVideoZoomRamp API_AVAILABLE(ios(7.0)) API_UNAVAILABLE(macos);

/*!
 @property dualCameraSwitchOverVideoZoomFactor
 @abstract
    The video zoom factor at or above which a DualCamera can select between its wide angle camera and its telephoto camera.
 
 @discussion
    This is the zoom factor at which the wide angle camera's field of view matches telephoto camera's full field of view. On non-DualCamera devices this will return 1.0.
 */
@property(atomic, readonly) CGFloat dualCameraSwitchOverVideoZoomFactor API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVAuthorizationStatus
 @abstract
    Constants indicating the client's authorization to the underlying hardware supporting a media type.
 
 @constant AVAuthorizationStatusNotDetermined
    Indicates that the user has not yet made a choice regarding whether the client can access the hardware.
 @constant AVAuthorizationStatusRestricted
    The client is not authorized to access the hardware for the media type. The user cannot change the client's status, possibly due to active restrictions such as parental controls being in place.
 @constant AVAuthorizationStatusDenied
    The user explicitly denied access to the hardware supporting a media type for the client.
 @constant AVAuthorizationStatusAuthorized
    The client is authorized to access the hardware supporting a media type.
 */
typedef NS_ENUM(NSInteger, AVAuthorizationStatus) {
    AVAuthorizationStatusNotDetermined = 0,
    AVAuthorizationStatusRestricted    = 1,
    AVAuthorizationStatusDenied        = 2,
    AVAuthorizationStatusAuthorized    = 3,
} API_AVAILABLE(macos(10.14), ios(7.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceAuthorization)

/*!
 @method authorizationStatusForMediaType:
 @abstract
    Returns the client's authorization status for accessing the underlying hardware that supports a given media type.
 
 @param mediaType
    The media type, either AVMediaTypeVideo or AVMediaTypeAudio
 @result
    The authorization status of the client
 
 @discussion
    This method returns the AVAuthorizationStatus of the client for accessing the underlying hardware supporting the media type. Media type constants are defined in AVMediaFormat.h. If any media type other than AVMediaTypeVideo or AVMediaTypeAudio is supplied, an NSInvalidArgumentException will be thrown. If the status is AVAuthorizationStatusNotDetermined, you may use the +requestAccessForMediaType:completionHandler: method to request access by prompting the user.
 */
+ (AVAuthorizationStatus)authorizationStatusForMediaType:(AVMediaType)mediaType API_AVAILABLE(macos(10.14), ios(7.0));

/*!
 @method requestAccessForMediaType:completionHandler:
 @abstract
    Requests access to the underlying hardware for the media type, showing a dialog to the user if necessary.
 
 @param mediaType
    The media type, either AVMediaTypeVideo or AVMediaTypeAudio
 @param handler
    A block called with the result of requesting access
 
 @discussion
    Use this function to request access to the hardware for a given media type. Media type constants are defined in AVMediaFormat.h. If any media type other than AVMediaTypeVideo or AVMediaTypeAudio is supplied, an NSInvalidArgumentException will be thrown.
 
    This call will not block while the user is being asked for access, allowing the client to continue running. Until access has been granted, any AVCaptureDevices for the media type will vend silent audio samples or black video frames. The user is only asked for permission the first time the client requests access. Later calls use the permission granted by the user.
 
    Note that the authorization dialog will automatically be shown if the status is AVAuthorizationStatusNotDetermined when creating an AVCaptureDeviceInput.
 
    Invoking this method with AVMediaTypeAudio is equivalent to calling -[AVAudioSession requestRecordPermission:].

    The completion handler is called on an arbitrary dispatch queue. Is it the client's responsibility to ensure that any UIKit-related updates are called on the main queue or main thread as a result.
 */
+ (void)requestAccessForMediaType:(AVMediaType)mediaType completionHandler:(void (^)(BOOL granted))handler API_AVAILABLE(macos(10.14), ios(7.0));

@end


/*!
 @typedef
 @abstract
    A constant that is used to specify the transport controls' speed.
 */
typedef float AVCaptureDeviceTransportControlsSpeed API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos);


/*!
 @enum AVCaptureDeviceTransportControlsPlaybackMode
 @abstract
    Constants indicating the transport controls' current mode of play back, if it has one.
 
 @constant AVCaptureDeviceTransportControlsNotPlayingMode
    Indicates that the tape transport is not threaded through the play head.
 @constant AVCaptureDeviceTransportControlsPlayingMode
    Indicates that the tape transport is threaded through the play head.
 */
typedef NS_ENUM(NSInteger, AVCaptureDeviceTransportControlsPlaybackMode) {
    AVCaptureDeviceTransportControlsNotPlayingMode = 0,
    AVCaptureDeviceTransportControlsPlayingMode    = 1,
} API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos);


@interface AVCaptureDevice (AVCaptureDeviceTransportControls)

/*!
 @property transportControlsSupported
 @abstract
    Returns whether the receiver supports transport control commands.
 
 @discussion
    For devices with transport controls, such as AVC tape-based camcorders or pro capture devices with RS422 deck control, the value of this property is YES. If transport controls are not supported, none of the associated transport control methods and properties are available on the receiver.
 */
@property(nonatomic, readonly) BOOL transportControlsSupported API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property transportControlsPlaybackMode
 @abstract
    Returns the receiver's current playback mode.
 
 @discussion
    For devices that support transport control, this property may be queried to discover the current playback mode.
 */
@property(nonatomic, readonly) AVCaptureDeviceTransportControlsPlaybackMode transportControlsPlaybackMode API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property transportControlsSpeed
 @abstract
    Returns the receiver's current playback speed as a floating point value.
 
 @discussion
    For devices that support transport control, this property may be queried to discover the current playback speed of the deck.
    0.0 -> stopped.
    1.0 -> forward at normal speed.
    -1.0-> reverse at normal speed.
    2.0 -> forward at 2x normal speed.
    etc.
 */
@property(nonatomic, readonly) AVCaptureDeviceTransportControlsSpeed transportControlsSpeed API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @method setTransportControlsPlaybackMode:speed:
 @abstract
    Sets both the transport controls playback mode and speed in a single method.
 
 @param mode
    A AVCaptureDeviceTransportControlsPlaybackMode indicating whether the deck should be put into play mode.
 @param speed
    A AVCaptureDeviceTransportControlsSpeed indicating the speed at which to wind or play the tape.
 
 @discussion
    A method for setting the receiver's transport controls playback mode and speed. The receiver must be locked for configuration using lockForConfiguration: before clients can set this method, otherwise an NSGenericException is thrown.
 */
- (void)setTransportControlsPlaybackMode:(AVCaptureDeviceTransportControlsPlaybackMode)mode speed:(AVCaptureDeviceTransportControlsSpeed)speed API_UNAVAILABLE(ios, watchos, tvos);

@end


@interface AVCaptureDevice (AVCaptureDeviceHighDynamicRangeSupport)

/*!
 @property automaticallyAdjustsVideoHDREnabled
 @abstract
    Indicates whether the receiver is allowed to turn high dynamic range streaming on or off.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is free to turn high dynamic range streaming on or off. This property defaults to YES. By default, AVCaptureDevice always turns off videoHDREnabled when a client uses the -setActiveFormat: API to set a new format. When the client uses AVCaptureSession's setSessionPreset: API instead, AVCaptureDevice turns videoHDR on automatically if it's a good fit for the preset. -setAutomaticallyAdjustsVideoHDREnabled: throws an NSGenericException if called without first obtaining exclusive access to the receiver using -lockForConfiguration:. Clients can key-value observe videoHDREnabled to know when the receiver has automatically changed the value.
 */
@property(nonatomic) BOOL automaticallyAdjustsVideoHDREnabled API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property videoHDREnabled
 @abstract
    Indicates whether the receiver's streaming high dynamic range feature is enabled.
 
 @discussion
    The value of this property is a BOOL indicating whether the receiver is currently streaming high dynamic range video buffers. The property may only be set if you first set automaticallyAdjustsVideoHDREnabled to NO, otherwise an NSGenericException is thrown. videoHDREnabled may only be set to YES if the receiver's activeFormat.isVideoHDRSupported property returns YES, otherwise an NSGenericException is thrown. This property may be key-value observed.
 
    Note that setting this property may cause a lengthy reconfiguration of the receiver, similar to setting a new active format or AVCaptureSession sessionPreset. If you are setting either the active format or the AVCaptureSession's sessionPreset AND this property, you should bracket these operations with [session beginConfiguration] and [session commitConfiguration] to minimize reconfiguration time.
 */
@property(nonatomic, getter=isVideoHDREnabled) BOOL videoHDREnabled API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

@end


/*!
 @enum AVCaptureColorSpace
 @abstract
    Constants indicating active or supported video color space.
 
 @constant AVCaptureColorSpace_sRGB
    The sGRB color space ( https://www.w3.org/Graphics/Color/srgb )
 @constant AVCaptureColorSpace_P3_D65
    The P3 D65 wide color space which uses Illuminant D65 as the white point.
 */
typedef NS_ENUM(NSInteger, AVCaptureColorSpace) {
    AVCaptureColorSpace_sRGB   = 0,
    AVCaptureColorSpace_P3_D65 = 1,
} API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


@interface AVCaptureDevice (AVCaptureDeviceColorSpaceSupport)

/*!
 @property activeColorSpace
 @abstract
    Indicates the receiver's current active color space.
 
 @discussion
    By default, an AVCaptureDevice attached to an AVCaptureSession is automatically configured for wide color by the AVCaptureSession (see AVCaptureSession automaticallyConfiguresCaptureDeviceForWideColor). You may also set the activeColorSpace manually. To prevent the AVCaptureSession from undoing your work, remember to set AVCaptureSession's automaticallyConfiguresCaptureDeviceForWideColor property to NO. Changing the receiver's activeColorSpace while the session is running requires a disruptive reconfiguration of the capture render pipeline. Movie captures in progress will be ended immediately; unfulfilled photo requests will be aborted; video preview will temporarily freeze. -setActiveColorSpace: throws an NSGenericException if called without first obtaining exclusive access to the receiver using -lockForConfiguration:.
 */
@property(nonatomic) AVCaptureColorSpace activeColorSpace API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDevice (AVCaptureDeviceDepthSupport)

/*!
 @property activeDepthDataFormat
 @abstract
    The currently active depth data format of the receiver.

 @discussion
    This property can be used to get or set the device's currently active depth data format. -setActiveDepthDataFormat: throws an NSInvalidArgumentException if set to a format not present in the activeFormat's -supportedDepthDataFormats array. -setActiveDepthDataFormat: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:. Clients can observe automatic changes to the receiver's activeDepthDataFormat by key value observing this property. On devices where depth data is not supported, this property returns nil.
 
    The frame rate of depth data may not be set directly. Depth data frame rate is synchronized to the device's activeMin/MaxFrameDurations. It may match the device's current frame rate, or lower, if depth data cannot be produced fast enough for the active video frame rate.
 
    Delivery of depth data to a AVCaptureDepthDataOutput may increase the system load, resulting in a reduced video frame rate for thermal sustainability.
 */
@property(nonatomic, retain, nullable) AVCaptureDeviceFormat *activeDepthDataFormat API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property activeDepthDataMinFrameDuration
 @abstract
    A property indicating the receiver's current active minimum depth data frame duration (the reciprocal of its maximum depth data frame rate).

 @discussion
    This property may be used to set an upper limit to the frame rate at which depth data is produced. Lowering the depth data frame rate typically lowers power consumption which will increase the time the camera can run before an elevated system pressure state is reached.

    Setting this property to kCMTimeInvalid resets it to the active depth data format's default min frame duration. Setting this property to kCMTimePositiveInfinity results in a depth data frame rate of 0.

    The activeDepthDataMinFrameDuration gets reset whenever either the active video format or the active depth data format changes.

    -setActiveDepthDataMinFrameDuration: throws an NSRangeException if set to a value that is outside of the active depth data format's supported frame rate range.
    -setActiveDepthDataMinFrameDuration: throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.
 */
@property(nonatomic) CMTime activeDepthDataMinFrameDuration API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos);

/*!
 @property minAvailableVideoZoomFactor
 @abstract
    Indicates the minimum zoom factor available for the AVCaptureDevice's videoZoomFactor property.
 
 @discussion
    On non-dual camera devices the minAvailableVideoZoomFactor is always 1.0. On a dual camera device the minAvailableVideoZoomFactor can change when the device is delivering depth data to one or more outputs (see -[AVCaptureDeviceFormat videoMinZoomFactorForDepthDataDelivery]). If the device's videoZoomFactor property is assigned a value smaller than 1.0, an NSRangeException is thrown. Setting the videoZoomFactor to a value greater than or equal to 1.0, but lower than minAvailableVideoZoomFactor results in the value being clamped to the minAvailableVideoZoomFactor. Clients can key value observe the value of this property.
 */
@property(nonatomic, readonly) CGFloat minAvailableVideoZoomFactor API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property maxAvailableVideoZoomFactor
 @abstract
    Indicates the maximum zoom factor available for the AVCaptureDevice's videoZoomFactor property.
 
 @discussion
    On non-dual camera devices the maxAvailableVideoZoomFactor is always equal to the activeFormat.videoMaxZoomFactor. On a dual camera device the maxAvailableVideoZoomFactor can change when the device is delivering depth data to one or more outputs (see -[AVCaptureDeviceFormat videoMaxZoomFactorForDepthDataDelivery]). If the device's videoZoomFactor property is assigned a value greater than activeFormat.videoMaxZoomFactor, an NSRangeException is thrown. Setting the videoZoomFactor to a value less than or equal to activeFormat.videoMaxZoomFactor, but greater than maxAvailableVideoZoomFactor results in the value being clamped to the maxAvailableVideoZoomFactor. Clients can key value observe the value of this property.
 */
@property(nonatomic, readonly) CGFloat maxAvailableVideoZoomFactor API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

@end


#pragma mark - AVCaptureDeviceDiscoverySession

/*!
 @class AVCaptureDeviceDiscoverySession
 @abstract
    The AVCaptureDeviceDiscoverySession allows clients to search for devices by certain criteria.
 
 @discussion
    This class allows clients to discover devices by providing certain search criteria. The objective of this class is to help find devices by device type and optionally by media type or position and allow you to key-value observe changes to the returned devices list.
 */
API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDeviceDiscoverySession : NSObject

AV_INIT_UNAVAILABLE

/*!
 @method discoverySessionWithDeviceTypes:
 @abstract
    Returns an AVCaptureDeviceDiscoverySession instance for the given device types, media type, and position.
 
 @param deviceTypes
    An array specifying the device types to include in the list of discovered devices.
 @param mediaType
    The media type, such as AVMediaTypeVideo, AVMediaTypeAudio, or AVMediaTypeMuxed, to include in the list of discovered devices. Pass nil to search for devices with any media type.
 @param position
    The position to include in the list of discovered devices. Pass AVCaptureDevicePositionUnspecified to search for devices with any position.
 @result
    The AVCaptureDeviceDiscoverySession from which the list of devices can be obtained.
 
 @discussion
    The list of device types is mandatory. This is used to make sure that clients only get access to devices of types they expect. This prevents new device types from automatically being included in the list of devices.
 */
+ (instancetype)discoverySessionWithDeviceTypes:(NSArray<AVCaptureDeviceType> *)deviceTypes mediaType:(nullable AVMediaType)mediaType position:(AVCaptureDevicePosition)position;

/*!
 @property devices
 @abstract
    The list of devices that comply to the search criteria specified on the discovery session.
 
 @discussion
    The returned array contains only devices that are available at the time the method is called. Applications can key-value observe this property to be notified when the list of available devices has changed. For apps linked against iOS 10, the devices returned are unsorted. For apps linked against iOS 11 or later, the devices are sorted by AVCaptureDeviceType, matching the order specified in the deviceTypes parameter of +[AVCaptureDeviceDiscoverySession discoverySessionWithDeviceTypes:mediaType:position:]. If a position of AVCaptureDevicePositionUnspecified is specified, the results are further ordered by position in the AVCaptureDevicePosition enum.
 */
@property(nonatomic, readonly) NSArray<AVCaptureDevice *> *devices;

@end


#pragma mark - AVFrameRateRange

@class AVFrameRateRangeInternal;

/*!
 @class AVFrameRateRange
 @abstract
    An AVFrameRateRange expresses a range of valid frame rates as min and max rate and min and max duration.
 
 @discussion
    An AVCaptureDevice exposes an array of formats, and its current activeFormat may be queried. The payload for the formats property is an array of AVCaptureDeviceFormat objects and the activeFormat property payload is an AVCaptureDeviceFormat. AVCaptureDeviceFormat wraps a CMFormatDescription and expresses a range of valid video frame rates as an NSArray of AVFrameRateRange objects. AVFrameRateRange expresses min and max frame rate as a rate in frames per second and duration (CMTime). An AVFrameRateRange object is immutable. Its values do not change for the life of the object.
 */
API_AVAILABLE(macos(10.7), ios(7.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVFrameRateRange : NSObject
{
@private
    AVFrameRateRangeInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property minFrameRate
 @abstract
    A Float64 indicating the minimum frame rate supported by this range.
 
 @discussion
    This read-only property indicates the minimum frame rate supported by this range in frames per second.
 */
@property(readonly) Float64 minFrameRate;

/*!
 @property maxFrameRate
 @abstract
    A Float64 indicating the maximum frame rate supported by this range.
 
 @discussion
    This read-only property indicates the maximum frame rate supported by this range in frames per second.
 */
@property(readonly) Float64 maxFrameRate;

/*!
 @property maxFrameDuration
 @abstract
    A CMTime indicating the maximum frame duration supported by this range.
 
 @discussion
    This read-only property indicates the maximum frame duration supported by this range. It is the reciprocal of minFrameRate, and expresses minFrameRate as a duration.
 */
@property(readonly) CMTime maxFrameDuration;

/*!
 @property minFrameDuration
 @abstract
    A CMTime indicating the minimum frame duration supported by this range.
 
 @discussion
    This read-only property indicates the minimum frame duration supported by this range. It is the reciprocal of maxFrameRate, and expresses maxFrameRate as a duration.
 */
@property(readonly) CMTime minFrameDuration;

@end


/*!
 @enum AVCaptureVideoStabilizationMode
 @abstract
    Constants indicating the modes of video stabilization supported by the device's format.
 
 @constant AVCaptureVideoStabilizationModeOff
    Indicates that video should not be stabilized.
 @constant AVCaptureVideoStabilizationModeStandard
    Indicates that video should be stabilized using the standard video stabilization algorithm introduced with iOS 5.0. Standard video stabilization has a reduced field of view. Enabling video stabilization may introduce additional latency into the video capture pipeline.
 @constant AVCaptureVideoStabilizationModeCinematic
    Indicates that video should be stabilized using the cinematic stabilization algorithm for more dramatic results. Cinematic video stabilization has a reduced field of view compared to standard video stabilization. Enabling cinematic video stabilization introduces much more latency into the video capture pipeline than standard video stabilization and consumes significantly more system memory. Use narrow or identical min and max frame durations in conjunction with this mode.
 @constant AVCaptureVideoStabilizationModeAuto
    Indicates that the most appropriate video stabilization mode for the device and format should be chosen.
 */
typedef NS_ENUM(NSInteger, AVCaptureVideoStabilizationMode) {
    AVCaptureVideoStabilizationModeOff       = 0,
    AVCaptureVideoStabilizationModeStandard  = 1,
    AVCaptureVideoStabilizationModeCinematic = 2,
    AVCaptureVideoStabilizationModeAuto      = -1,
} API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


/*!
 @enum AVCaptureAutoFocusSystem
 @abstract
    Constants indicating the autofocus system.
 
 @constant AVCaptureAutoFocusSystemNone
    Indicates that autofocus is not available.
 @constant AVCaptureAutoFocusSystemContrastDetection
    Indicates that autofocus is achieved by contrast detection. Contrast detection performs a focus scan to find the optimal position.
 @constant AVCaptureAutoFocusSystemPhaseDetection
    Indicates that autofocus is achieved by phase detection. Phase detection has the ability to achieve focus in many cases without a focus scan. Phase detection autofocus is typically less visually intrusive than contrast detection autofocus.
 */
typedef NS_ENUM(NSInteger, AVCaptureAutoFocusSystem) {
    AVCaptureAutoFocusSystemNone              = 0,
    AVCaptureAutoFocusSystemContrastDetection = 1,
    AVCaptureAutoFocusSystemPhaseDetection    = 2,
} API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;


#pragma mark - AVCaptureDeviceFormat

@class AVCaptureOutput;
@class AVCaptureDeviceFormatInternal;

/*!
 @class AVCaptureDeviceFormat
 @abstract
    An AVCaptureDeviceFormat wraps a CMFormatDescription and other format-related information, such as min and max framerate.
 
 @discussion
    An AVCaptureDevice exposes an array of formats, and its current activeFormat may be queried. The payload for the formats property is an array of AVCaptureDeviceFormat objects and the activeFormat property payload is an AVCaptureDeviceFormat. AVCaptureDeviceFormat is a thin wrapper around a CMFormatDescription, and can carry associated device format information that doesn't go in a CMFormatDescription, such as min and max frame rate. An AVCaptureDeviceFormat object is immutable. Its values do not change for the life of the object.
 */
API_AVAILABLE(macos(10.7), ios(7.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureDeviceFormat : NSObject
{
@private
    AVCaptureDeviceFormatInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property mediaType
 @abstract
    An NSString describing the media type of an AVCaptureDevice active or supported format.
 
 @discussion
    Supported mediaTypes are listed in AVMediaFormat.h. This is a read-only property. The caller assumes no ownership of the returned value and should not CFRelease it.
 */
@property(nonatomic, readonly) AVMediaType mediaType;

/*!
 @property formatDescription
 @abstract
    A CMFormatDescription describing an AVCaptureDevice active or supported format.
 
 @discussion
    A CMFormatDescription describing an AVCaptureDevice active or supported format. This is a read-only property. The caller assumes no ownership of the returned value and should not CFRelease it.
 */
@property(nonatomic, readonly) CMFormatDescriptionRef formatDescription;

/*!
 @property videoSupportedFrameRateRanges
 @abstract
    A property indicating the format's supported frame rate ranges.
 
 @discussion
    videoSupportedFrameRateRanges is an array of AVFrameRateRange objects, one for each of the format's supported video frame rate ranges.
 */
@property(nonatomic, readonly) NSArray<AVFrameRateRange *> *videoSupportedFrameRateRanges;

/*!
 @property videoFieldOfView
 @abstract
    A property indicating the format's field of view.
 
 @discussion
    videoFieldOfView is a float value indicating the receiver's field of view in degrees. If field of view is unknown, a value of 0 is returned.
 */
@property(nonatomic, readonly) float videoFieldOfView API_UNAVAILABLE(macos);

/*!
 @property videoBinned
 @abstract
    A property indicating whether the format is binned.
 
 @discussion
    videoBinned is a BOOL indicating whether the format is a binned format. Binning is a pixel-combining process which can result in greater low light sensitivity at the cost of reduced resolution.
 */
@property(nonatomic, readonly, getter=isVideoBinned) BOOL videoBinned API_UNAVAILABLE(macos);

/*!
 @method isVideoStabilizationModeSupported
 @abstract
    Returns whether the format supports the given video stabilization mode.
 
 @param videoStabilizationMode
    An AVCaptureVideoStabilizationMode to be checked.
 
 @discussion
    isVideoStabilizationModeSupported: returns a boolean value indicating whether the format can be stabilized using the given mode with -[AVCaptureConnection setPreferredVideoStabilizationMode:].
 */
- (BOOL)isVideoStabilizationModeSupported:(AVCaptureVideoStabilizationMode)videoStabilizationMode API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property videoStabilizationSupported
 @abstract
    A property indicating whether the format supports video stabilization.
 
 @discussion
    videoStabilizationSupported is a BOOL indicating whether the format can be stabilized using AVCaptureConnection -setEnablesVideoStabilizationWhenAvailable. This property is deprecated. Use isVideoStabilizationModeSupported: instead.
 */
@property(nonatomic, readonly, getter=isVideoStabilizationSupported) BOOL videoStabilizationSupported API_DEPRECATED("Use isVideoStabilizationModeSupported: instead.", ios(7.0, 8.0)) API_UNAVAILABLE(macos);

/*!
 @property videoMaxZoomFactor
 @abstract
    Indicates the maximum zoom factor available for the AVCaptureDevice's videoZoomFactor property.
 
 @discussion
    If the device's videoZoomFactor property is assigned a larger value, an NSRangeException will be thrown. A maximum zoom factor of 1 indicates no zoom is available.
 */
@property(nonatomic, readonly) CGFloat videoMaxZoomFactor API_UNAVAILABLE(macos);

/*!
 @property videoZoomFactorUpscaleThreshold
 @abstract
    Indicates the value of AVCaptureDevice's videoZoomFactor property at which the image output begins to require upscaling.
 
 @discussion
    In some cases the image sensor's dimensions are larger than the dimensions reported by the video AVCaptureDeviceFormat. As long as the sensor crop is larger than the reported dimensions of the AVCaptureDeviceFormat, the image will be downscaled. Setting videoZoomFactor to the value of videoZoomFactorUpscalingThreshold will provide a center crop of the sensor image data without any scaling. If a greater zoom factor is used, then the sensor data will be upscaled to the device format's dimensions.
 */
@property(nonatomic, readonly) CGFloat videoZoomFactorUpscaleThreshold API_UNAVAILABLE(macos);

/*!
 @property minExposureDuration
 @abstract
    A CMTime indicating the minimum supported exposure duration.
 
 @discussion
    This read-only property indicates the minimum supported exposure duration.
 */
@property(nonatomic, readonly) CMTime minExposureDuration API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property maxExposureDuration
 @abstract
    A CMTime indicating the maximum supported exposure duration.
 
 @discussion
    This read-only property indicates the maximum supported exposure duration.
 */
@property(nonatomic, readonly) CMTime maxExposureDuration API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property minISO
 @abstract
    A float indicating the minimum supported exposure ISO value.
 
 @discussion
    This read-only property indicates the minimum supported exposure ISO value.
 */
@property(nonatomic, readonly) float minISO API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property maxISO
 @abstract
    An float indicating the maximum supported exposure ISO value.
 
 @discussion
    This read-only property indicates the maximum supported exposure ISO value.
 */
@property(nonatomic, readonly) float maxISO API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property videoHDRSupported
 @abstract
    A property indicating whether the format supports high dynamic range streaming.
 
 @discussion
    videoHDRSupported is a BOOL indicating whether the format supports high dynamic range streaming. See AVCaptureDevice's videoHDREnabled property.
 */
@property(nonatomic, readonly, getter=isVideoHDRSupported) BOOL videoHDRSupported API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property highResolutionStillImageDimensions
 @abstract
    CMVideoDimensions indicating the highest resolution still image that can be produced by this format.
 
 @discussion
    By default, AVCapturePhotoOutput and AVCaptureStillImageOutput emit images with the same dimensions as their source AVCaptureDevice's activeFormat.formatDescription property. Some device formats support high resolution photo output. That is, they can stream video to an AVCaptureVideoDataOutput or AVCaptureMovieFileOutput at one resolution while outputting photos to AVCapturePhotoOutput at a higher resolution. You may query this property to discover a video format's supported high resolution still image dimensions. See -[AVCapturePhotoOutput highResolutionPhotoEnabled], -[AVCapturePhotoSettings highResolutionPhotoEnabled], and -[AVCaptureStillImageOutput highResolutionStillImageOutputEnabled].
 
    AVCaptureDeviceFormats of type AVMediaTypeDepthData may also support the delivery of a higher resolution depth data map to an AVCapturePhotoOutput. Chief differences are:
       - Depth data accompanying still images is not supported by AVCaptureStillImageOutput. You must use AVCapturePhotoOutput.
       - By opting in for depth data ( -[AVCapturePhotoSettings setDepthDataDeliveryEnabled:YES] ), you implicitly opt in for high resolution depth data if it's available. You may query the -[AVCaptureDevice activeDepthDataFormat]'s highResolutionStillImageDimensions to discover the depth data resolution that will be delivered with captured photos.
 */
@property(nonatomic, readonly) CMVideoDimensions highResolutionStillImageDimensions API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property autoFocusSystem
 @abstract
    A property indicating the autofocus system.
 
 @discussion
    This read-only property indicates the autofocus system.
 */
@property(nonatomic, readonly) AVCaptureAutoFocusSystem autoFocusSystem API_AVAILABLE(ios(8.0)) API_UNAVAILABLE(macos);

/*!
 @property supportedColorSpaces
 @abstract
    A property indicating the receiver's supported color spaces.
 
 @discussion
    This read-only property indicates the receiver's supported color spaces as an array of AVCaptureColorSpace constants sorted from narrow to wide color.
 */
@property(nonatomic, readonly) NSArray<NSNumber *> *supportedColorSpaces API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @property videoMinZoomFactorForDepthDataDelivery
 @abstract
    Indicates the minimum zoom factor available for the AVCaptureDevice's videoZoomFactor property when delivering depth data to one or more outputs.
 
 @discussion
    Dual camera devices support a limited zoom range when delivering depth data to any output. If this device format has no -supportedDepthDataFormats, this property returns 1.0.
 */
@property(nonatomic, readonly) CGFloat videoMinZoomFactorForDepthDataDelivery API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property videoMaxZoomFactorForDepthDataDelivery
 @abstract
    Indicates the maximum zoom factor available for the AVCaptureDevice's videoZoomFactor property when delivering depth data to one or more outputs.
 
 @discussion
    Dual camera devices support a limited zoom range when delivering depth data to any output. If this device format has no -supportedDepthDataFormats, this property returns videoMaxZoomFactor.
 */
@property(nonatomic, readonly) CGFloat videoMaxZoomFactorForDepthDataDelivery API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*
 @property supportedDepthDataFormats
 @abstract
    Indicates this format's companion depth data formats.
 
 @discussion
    If no depth data formats are supported by the receiver, an empty array is returned. On dual camera devices, the supportedDepthDataFormats list items always match the aspect ratio of their paired video format. When the receiver is set as the device's activeFormat, you may set the device's activeDepthDataFormat to one of these supported depth data formats.
 */
@property(nonatomic, readonly) NSArray<AVCaptureDeviceFormat *> *supportedDepthDataFormats API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

/*!
 @property unsupportedCaptureOutputClasses
 @abstract
    A property indicating AVCaptureOutput subclasses the receiver does not support.
 
 @discussion
    As a rule, AVCaptureDeviceFormats of a given mediaType are available for use with all AVCaptureOutputs that accept that media type, but there are exceptions. For instance, on apps linked against iOS versions earlier than 12.0, the photo resolution video formats may not be used as sources for AVCaptureMovieFileOutput due to bandwidth limitations. On DualCamera devices, AVCaptureDepthDataOutput is not supported by the 12 MP device formats when using the -[AVCaptureDevice setActiveFormat:] API due to bandwidth limitations, though their use with -[AVCaptureSession setSessionPreset:AVCaptureSessionPresetPhoto] is supported. When using the photo preset, video is streamed at preview resolution rather than full sensor resolution.  
 */
@property(nonatomic, readonly) NSArray<Class> *unsupportedCaptureOutputClasses API_AVAILABLE(ios(11.0)) API_UNAVAILABLE(macos);

@end


@interface AVCaptureDeviceFormat (AVCaptureDeviceFormatDepthDataAdditions)

/*
 @property portraitEffectsMatteStillImageDeliverySupported
 @abstract
    Indicates whether this depth format supports the delivery of a portrait effects matte.
 
 @discussion
    Some depth formats are capable of producing an auxiliary matting image (similar to an auxiliary depth image) tuned for high quality portrait effects rendering (see AVPortraitEffectsMatte.h). If this property's value is YES, you may request portrait effects matte delivery in your photos using the AVCapturePhotoOutput, provided this format is selected as the activeDepthDataFormat.
 */
@property(nonatomic, readonly, getter=isPortraitEffectsMatteStillImageDeliverySupported) BOOL portraitEffectsMatteStillImageDeliverySupported API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

@end


#pragma mark - AVCaptureDeviceInputSource

@class AVCaptureDeviceInputSourceInternal;

/*!
 @class AVCaptureDeviceInputSource
 @abstract
    An AVCaptureDeviceInputSource represents a distinct input source on an AVCaptureDevice object.
 
 @discussion
    An AVCaptureDevice may optionally present an array of inputSources, representing distinct mutually exclusive inputs to the device, for example, an audio AVCaptureDevice might have ADAT optical and analog input sources. A video AVCaptureDevice might have an HDMI input source, or a component input source.
 */
API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos)
@interface AVCaptureDeviceInputSource : NSObject
{
@private
    AVCaptureDeviceInputSourceInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @property inputSourceID
 @abstract
    An ID unique among the inputSources exposed by a given AVCaptureDevice.
 
 @discussion
    An AVCaptureDevice's inputSources array must contain AVCaptureInputSource objects with unique inputSourceIDs.
 */
@property(nonatomic, readonly) NSString *inputSourceID;

/*!
 @property localizedName
 @abstract
    A localized human-readable name for the receiver.
 
 @discussion
    This property can be used for displaying the name of the capture device input source in a user interface.
 */
@property(nonatomic, readonly) NSString *localizedName;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVSynchronizedLayer.h
/*
	File:  AVSynchronizedLayer.h

	Framework:  AVFoundation
 
	Copyright 2010-2013 Apple Inc. All rights reserved.

*/

/*!
    @class			AVSynchronizedLayer

    @abstract		AVSynchronizedLayer is a subclass of CALayer with layer timing that synchronized with a specific AVPlayerItem.

	@discussion		Note that arbitrary numbers of AVSynchronizedLayers can be created with the same AVPlayerItem.
	
					An AVSynchronizedLayer is similar to a CATransformLayer in that it doesn't display anything itself but only
					confers state upon its layer subtree. AVSynchronizedLayer confers is timing state, synchronizing the
					timing of layers in its subtree with that of an AVPlayerItem.					

					Any CoreAnimation layer with animation property set that is added as a sublayer of AVSynchronizedLayer should 
					set animation beginTime to a non-zero positive value so animations will be interpreted on the AVPlayerItem's 
					timeline, not real-time. CoreAnimation replaces default beginTime of 0.0 with CACurrentMediaTime(). 
					To start the animation from time 0, use a small positive value like AVCoreAnimationBeginTimeAtZero.
					
					Set the removedOnCompletion property to NO on CAAnimations you attach to AVSynchronizedLayer or 
					its sublayers to prevent CoreAnimation from automatically removing them.

					Usage example:
					
					AVPlayerItem *playerItem = ...;
					
					// .. set up an AVSynchronizedLayer, to sync the layer timing of its subtree
					// with the playback of the playerItem
					
					CALayer *superlayer = ...;
					AVSynchronizedLayer *syncLayer = [AVSynchronizedLayer synchronizedLayerWithPlayerItem:playerItem];
					
					[syncLayer addSublayer:...];	// These sublayers will be synchronized
					
					[superlayer addSublayer:syncLayer];
*/

#import <AVFoundation/AVBase.h>
#import <QuartzCore/CAAnimation.h>

@class AVPlayerItem;
@class AVSynchronizedLayerInternal;

NS_ASSUME_NONNULL_BEGIN

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVSynchronizedLayer : CALayer
{
@private
	AVSynchronizedLayerInternal		*_syncLayer;
}

/*!
	@method			synchronizedLayerWithPlayerItem:
	@abstract		Returns an instance of AVSynchronizedLayer with timing synchronized with the specified AVPlayerItem.
	@result			An instance of AVSynchronizedLayer.
*/
+ (AVSynchronizedLayer *)synchronizedLayerWithPlayerItem:(AVPlayerItem *)playerItem;

/* indicates the instance of AVPlayerItem to which the timing of the AVSynchronizedLayer is synchronized */
@property (nonatomic, retain, nullable) AVPlayerItem *playerItem;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVPlayerItem.h
/*
    File:  AVPlayerItem.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

/*!
	@class			AVPlayerItem

	@abstract
	  An AVPlayerItem carries a reference to an AVAsset as well as presentation settings for that asset.

	@discussion
	  Note that inspection of media assets is provided by AVAsset.
	  This class is intended to represent presentation state for an asset that's played by an AVPlayer and to permit observation of that state.

	  To allow clients to add and remove their objects as key-value observers safely, AVPlayerItem serializes notifications of
	  changes that occur dynamically during playback on the same dispatch queue on which notifications of playback state changes
	  are serialized by its associated AVPlayer. By default, this queue is the main queue. See dispatch_get_main_queue().
	  
	  To ensure safe access to AVPlayerItem's nonatomic properties while dynamic changes in playback state may be reported, clients must
	  serialize their access with the associated AVPlayer's notification queue. In the common case, such serialization is naturally
	  achieved by invoking AVPlayerItem's various methods on the main thread or queue.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAudioProcessingSettings.h>
#import <AVFoundation/AVVideoSettings.h>
#import <Foundation/Foundation.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>
#import <CoreMedia/CMSync.h>
#import <CoreGraphics/CGGeometry.h>

NS_ASSUME_NONNULL_BEGIN

/* Note that NSNotifications posted by AVPlayerItem may be posted on a different thread from the one on which the observer was registered. */

// notifications                                                                                description
AVF_EXPORT NSString *const AVPlayerItemTimeJumpedNotification			 NS_AVAILABLE(10_7, 5_0);	// the item's current time has changed discontinuously
AVF_EXPORT NSString *const AVPlayerItemDidPlayToEndTimeNotification      NS_AVAILABLE(10_7, 4_0);   // item has played to its end time
AVF_EXPORT NSString *const AVPlayerItemFailedToPlayToEndTimeNotification NS_AVAILABLE(10_7, 4_3);   // item has failed to play to its end time
AVF_EXPORT NSString *const AVPlayerItemPlaybackStalledNotification       NS_AVAILABLE(10_9, 6_0);    // media did not arrive in time to continue playback
AVF_EXPORT NSString *const AVPlayerItemNewAccessLogEntryNotification	 NS_AVAILABLE(10_9, 6_0);	// a new access log entry has been added
AVF_EXPORT NSString *const AVPlayerItemNewErrorLogEntryNotification		 NS_AVAILABLE(10_9, 6_0);	// a new error log entry has been added

// notification userInfo key                                                                    type
AVF_EXPORT NSString *const AVPlayerItemFailedToPlayToEndTimeErrorKey     NS_AVAILABLE(10_7, 4_3);   // NSError

/*!
 @enum AVPlayerItemStatus
 @abstract
	These constants are returned by the AVPlayerItem status property to indicate whether it can successfully be played.
 
 @constant	 AVPlayerItemStatusUnknown
	Indicates that the status of the player item is not yet known because it has not tried to load new media resources
	for playback.
 @constant	 AVPlayerItemStatusReadyToPlay
	Indicates that the player item is ready to be played.
 @constant	 AVPlayerItemStatusFailed
	Indicates that the player item can no longer be played because of an error. The error is described by the value of
	the player item's error property.
 */
typedef NS_ENUM(NSInteger, AVPlayerItemStatus) {
	AVPlayerItemStatusUnknown,
	AVPlayerItemStatusReadyToPlay,
	AVPlayerItemStatusFailed
};

@class AVPlayer;
@class AVAsset;
@class AVAssetTrack;
@class AVAudioMix;
@class AVVideoComposition;
@class AVMediaSelection;
@class AVMediaSelectionGroup;
@class AVMediaSelectionOption;
@class AVPlayerItemInternal;
@protocol AVVideoCompositing;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVPlayerItem : NSObject <NSCopying>
{
@private
	AVPlayerItemInternal* _playerItem;
}
AV_INIT_UNAVAILABLE

/*!
 @method		playerItemWithURL:
 @abstract		Returns an instance of AVPlayerItem for playing a resource at the specified location.
 @param			URL
 @result		An instance of AVPlayerItem.
 @discussion	Equivalent to +playerItemWithAsset:, passing [AVAsset assetWithURL:URL] as the value of asset.
 */
+ (instancetype)playerItemWithURL:(NSURL *)URL;

/*!
 @method		playerItemWithAsset:
 @abstract		Returns an instance of AVPlayerItem for playing an AVAsset.
 @param			asset
 @result		An instance of AVPlayerItem.
 @discussion	Equivalent to +playerItemWithAsset:automaticallyLoadedAssetKeys:, passing @[ @"duration" ] as the value of automaticallyLoadedAssetKeys.
  */
+ (instancetype)playerItemWithAsset:(AVAsset *)asset;

/*!
 @method		playerItemWithAsset:automaticallyLoadedAssetKeys:
 @abstract		Returns an instance of AVPlayerItem for playing an AVAsset.
 @param			asset
 @param			automaticallyLoadedAssetKeys
 				An NSArray of NSStrings, each representing a property key defined by AVAsset. See AVAsset.h for property keys, e.g. duration.
 @result		An instance of AVPlayerItem.
 @discussion	The value of each key in automaticallyLoadedAssetKeys will be automatically be loaded by the underlying AVAsset before the receiver achieves the status AVPlayerItemStatusReadyToPlay; i.e. when the item is ready to play, the value of -[[AVPlayerItem asset] statusOfValueForKey:error:] will be one of the terminal status values greater than AVKeyValueStatusLoading.
 */
+ (instancetype)playerItemWithAsset:(AVAsset *)asset automaticallyLoadedAssetKeys:(nullable NSArray<NSString *> *)automaticallyLoadedAssetKeys NS_AVAILABLE(10_9, 7_0);

/*!
 @method		initWithURL:
 @abstract		Initializes an AVPlayerItem with an NSURL.
 @param			URL
 @result		An instance of AVPlayerItem
 @discussion	Equivalent to -initWithAsset:, passing [AVAsset assetWithURL:URL] as the value of asset.
 */
- (instancetype)initWithURL:(NSURL *)URL;

/*!
 @method		initWithAsset:
 @abstract		Initializes an AVPlayerItem with an AVAsset.
 @param			asset
 @result		An instance of AVPlayerItem
 @discussion	Equivalent to -initWithAsset:automaticallyLoadedAssetKeys:, passing @[ @"duration" ] as the value of automaticallyLoadedAssetKeys.
 */
- (instancetype)initWithAsset:(AVAsset *)asset;

/*!
 @method		initWithAsset:automaticallyLoadedAssetKeys:
 @abstract		Initializes an AVPlayerItem with an AVAsset.
 @param			asset
 				An instance of AVAsset.
 @param			automaticallyLoadedAssetKeys
 				An NSArray of NSStrings, each representing a property key defined by AVAsset. See AVAsset.h for property keys, e.g. duration.
 @result		An instance of AVPlayerItem
 @discussion	The value of each key in automaticallyLoadedAssetKeys will be automatically be loaded by the underlying AVAsset before the receiver achieves the status AVPlayerItemStatusReadyToPlay; i.e. when the item is ready to play, the value of -[[AVPlayerItem asset] statusOfValueForKey:error:] will be one of the terminal status values greater than AVKeyValueStatusLoading.
 */
- (instancetype)initWithAsset:(AVAsset *)asset automaticallyLoadedAssetKeys:(nullable NSArray<NSString *> *)automaticallyLoadedAssetKeys NS_DESIGNATED_INITIALIZER NS_AVAILABLE(10_9, 7_0);

/*!
 @property status
 @abstract
	The ability of the receiver to be used for playback.
 
 @discussion
	The value of this property is an AVPlayerItemStatus that indicates whether the receiver can be used for playback.
	When the value of this property is AVPlayerItemStatusFailed, the receiver can no longer be used for playback and
	a new instance needs to be created in its place. When this happens, clients can check the value of the error
	property to determine the nature of the failure. This property is key value observable.
 */
@property (nonatomic, readonly) AVPlayerItemStatus status;

/*!
 @property error
 @abstract
	If the receiver's status is AVPlayerItemStatusFailed, this describes the error that caused the failure.
 
 @discussion
	The value of this property is an NSError that describes what caused the receiver to no longer be able to be played.
	If the receiver's status is not AVPlayerItemStatusFailed, the value of this property is nil.
 */
@property (nonatomic, readonly, nullable) NSError *error;

@end


@class AVPlayerItemTrack;
@class AVMetadataItem;

@interface AVPlayerItem (AVPlayerItemInspection)

/*!
 @property asset
 @abstract Accessor for underlying AVAsset.
 */
@property (nonatomic, readonly) AVAsset *asset;

/*!
 @property tracks
 @abstract Provides array of AVPlayerItem tracks. Observable (can change dynamically during playback).
	
 @discussion
	The value of this property will accord with the properties of the underlying media resource when the receiver becomes ready to play.
	Before the underlying media resource has been sufficiently loaded, its value is an empty NSArray. Use key-value observation to obtain
	a valid array of tracks as soon as it becomes available.
 */
@property (nonatomic, readonly) NSArray<AVPlayerItemTrack *> *tracks;

/*!
 @property duration
 @abstract Indicates the duration of the item, not considering either its forwardPlaybackEndTime or reversePlaybackEndTime.
 
 @discussion
	This property is observable. The duration of an item can change dynamically during playback.
	
	Unless you omit @"duration" from the array of asset keys you pass to +playerItemWithAsset:automaticallyLoadedAssetKeys: or
	-initWithAsset:automaticallyLoadedAssetKeys:, the value of this property will accord with the properties of the underlying
	AVAsset and the current state of playback once the receiver becomes ready to play.

	Before the underlying duration has been loaded, the value of this property is kCMTimeIndefinite. Use key-value observation to
	obtain a valid duration as soon as it becomes available. (Note that the value of duration may remain kCMTimeIndefinite,
	e.g. for live streams.)
 */
@property (nonatomic, readonly) CMTime duration NS_AVAILABLE(10_7, 4_3);

/*!
 @property presentationSize
 @abstract The size of the receiver as presented by the player.
 
 @discussion 
	Indicates the size at which the visual portion of the item is presented by the player; can be scaled from this 
	size to fit within the bounds of an AVPlayerLayer via its videoGravity property. Can be scaled arbitarily for presentation
	via the frame property of an AVPlayerLayer.
	
	The value of this property will accord with the properties of the underlying media resource when the receiver becomes ready to play.
	Before the underlying media resource is sufficiently loaded, its value is CGSizeZero. Use key-value observation to obtain a valid
	presentationSize as soon as it becomes available. (Note that the value of presentationSize may remain CGSizeZero, e.g. for audio-only items.)
 */
@property (nonatomic, readonly) CGSize presentationSize;

/*!
 @property timedMetadata
 @abstract Provides an NSArray of AVMetadataItems representing the timed metadata encountered most recently within the media as it plays. May be nil.
 @discussion
   Notifications of changes are available via key-value observation.
   As an optimization for playback, AVPlayerItem may omit the processing of timed metadata when no observer of this property is registered. Therefore, when no such observer is registered, the value of the timedMetadata property may remain nil regardless of the contents of the underlying media.
 */
@property (nonatomic, readonly, nullable) NSArray<AVMetadataItem *> *timedMetadata;

/*!
 @property automaticallyLoadedAssetKeys
 @abstract An array of property keys defined on AVAsset. The value of each key in the array is automatically loaded while the receiver is being made ready to play.
 @discussion
   The value of each key in automaticallyLoadedAssetKeys will be automatically be loaded by the underlying AVAsset before the receiver achieves the status AVPlayerItemStatusReadyToPlay; i.e. when the item is ready to play, the value of -[[AVPlayerItem asset] statusOfValueForKey:error:] will be AVKeyValueStatusLoaded. If loading of any of the values fails, the status of the AVPlayerItem will change instead to AVPlayerItemStatusFailed..
 */
@property (nonatomic, readonly) NSArray<NSString *> *automaticallyLoadedAssetKeys NS_AVAILABLE(10_9, 7_0);

@end


@interface AVPlayerItem (AVPlayerItemRateAndSteppingSupport)

/* For releases of OS X prior to 10.9 and releases of iOS prior to 7.0, indicates whether the item can be played at rates greater than 1.0.
   Starting with OS X 10.9 and iOS 7.0, all AVPlayerItems with status AVPlayerItemReadyToPlay can be played at rates between 1.0 and 2.0, inclusive, even if canPlayFastForward is NO; for those releases canPlayFastForward indicates whether the item can be played at rates greater than 2.0.
*/
@property (nonatomic, readonly) BOOL canPlayFastForward NS_AVAILABLE(10_8, 5_0);

/* indicates whether the item can be played at rates between 0.0 and 1.0 */
@property (nonatomic, readonly) BOOL canPlaySlowForward NS_AVAILABLE(10_8, 6_0);

/* indicates whether the item can be played at rate -1.0 */
@property (nonatomic, readonly) BOOL canPlayReverse NS_AVAILABLE(10_8, 6_0);

/* indicates whether the item can be played at rates less between 0.0 and -1.0 */
@property (nonatomic, readonly) BOOL canPlaySlowReverse NS_AVAILABLE(10_8, 6_0);

/* indicates whether the item can be played at rates less than -1.0 */
@property (nonatomic, readonly) BOOL canPlayFastReverse NS_AVAILABLE(10_8, 5_0);

/* Indicates whether the item supports stepping forward; see -stepByCount:. Once the item has become ready to play, the value of canStepForward does not change even when boundary conditions are reached, such as when the item's currentTime is its end time. */
@property (nonatomic, readonly) BOOL canStepForward NS_AVAILABLE(10_8, 6_0);

/* indicates whether the item supports stepping backward; see -stepByCount:. Once the item has become ready to play, the value of canStepBackward does not change even when boundary conditions are reached, such as when the item's currentTime is equal to kCMTimeZero. */
@property (nonatomic, readonly) BOOL canStepBackward NS_AVAILABLE(10_8, 6_0);

@end


@interface AVPlayerItem (AVPlayerItemTimeControl)

/*!
 @method			currentTime
 @abstract			Returns the current time of the item.
 @result			A CMTime
 @discussion		Returns the current time of the item. Not key-value observable; use -[AVPlayer addPeriodicTimeObserverForInterval:queue:usingBlock:] instead.
 */
- (CMTime)currentTime;

/*!
 @property forwardPlaybackEndTime
 @abstract
	The end time for forward playback.
 
 @discussion
	Specifies the time at which playback should end when the playback rate is positive (see AVPlayer's rate property).
	The default value is kCMTimeInvalid, which indicates that no end time for forward playback is specified.
	In this case, the effective end time for forward playback is the receiver's duration.
	
	When the end time is reached, the receiver will post AVPlayerItemDidPlayToEndTimeNotification and the AVPlayer will take
	the action indicated by the value of its actionAtItemEnd property (see AVPlayerActionAtItemEnd in AVPlayer.h). 

	The value of this property has no effect on playback when the rate is negative.
 */
@property (nonatomic) CMTime forwardPlaybackEndTime;

/*!
 @property reversePlaybackEndTime
 @abstract
	The end time for reverse playback.
 
 @discussion
	Specifies the time at which playback should end when the playback rate is negative (see AVPlayer's rate property).
	The default value is kCMTimeInvalid, which indicates that no end time for reverse playback is specified.
	In this case, the effective end time for reverse playback is kCMTimeZero.

	When the end time is reached, the receiver will post AVPlayerItemDidPlayToEndTimeNotification and the AVPlayer will take
	the action indicated by the value of its actionAtItemEnd property (see AVPlayerActionAtItemEnd in AVPlayer.h). 

	The value of this property has no effect on playback when the rate is positive.
 */
@property (nonatomic) CMTime reversePlaybackEndTime;

/*!
 @property seekableTimeRanges
 @abstract This property provides a collection of time ranges that the player item can seek to. The ranges provided might be discontinous.
 @discussion Returns an NSArray of NSValues containing CMTimeRanges.
 */
@property (nonatomic, readonly) NSArray<NSValue *> *seekableTimeRanges;

/*!
 @method			seekToTime:completionHandler:
 @abstract			Moves the playback cursor and invokes the specified block when the seek operation has either been completed or been interrupted.
 @param				time
 @param				completionHandler
 @discussion		Use this method to seek to a specified time for the item and to be notified when the seek operation is complete.
 					The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter 
 					set to NO. If the new request completes without being interrupted by another seek request or by any other operation the specified 
 					completion handler will be invoked with the finished parameter set to YES. 
					If the seek time is outside of seekable time ranges as indicated by seekableTimeRanges property, the seek request will be cancelled and the completion handler will be invoked with the finished parameter set to NO.
 */
- (void)seekToTime:(CMTime)time completionHandler:(void (^_Nullable)(BOOL finished))completionHandler NS_AVAILABLE(10_7, 5_0);

/*!
 @method			seekToTime:toleranceBefore:toleranceAfter:completionHandler:
 @abstract			Moves the playback cursor within a specified time bound and invokes the specified block when the seek operation has either been completed or been interrupted.
 @param				time
 @param				toleranceBefore
 @param				toleranceAfter
 @param				completionHandler
 @discussion		Use this method to seek to a specified time for the item and to be notified when the seek operation is complete.
					The time seeked to will be within the range [time-toleranceBefore, time+toleranceAfter] and may differ from the specified time for efficiency.
					Pass kCMTimeZero for both toleranceBefore and toleranceAfter to request sample accurate seeking which may incur additional decoding delay. 
					Messaging this method with beforeTolerance:kCMTimePositiveInfinity and afterTolerance:kCMTimePositiveInfinity is the same as messaging seekToTime: directly.
					The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter set to NO. If the new 
					request completes without being interrupted by another seek request or by any other operation the specified completion handler will be invoked with the 
					finished parameter set to YES.
					If the seek time is outside of seekable time ranges as indicated by seekableTimeRanges property, the seek request will be cancelled and the completion handler will be invoked with the finished parameter set to NO.
 */
- (void)seekToTime:(CMTime)time toleranceBefore:(CMTime)toleranceBefore toleranceAfter:(CMTime)toleranceAfter completionHandler:(void (^_Nullable)(BOOL finished))completionHandler NS_AVAILABLE(10_7, 5_0);

/*!
 @method			cancelPendingSeeks
 @abstract			Cancel any pending seek requests and invoke the corresponding completion handlers if present.
 @discussion		Use this method to cancel and release the completion handlers of pending seeks. The finished parameter of the completion handlers will
 					be set to NO.
 */
- (void)cancelPendingSeeks NS_AVAILABLE(10_7, 5_0);

/*!
	@method	currentDate
	@abstract	If currentTime is mapped to a particular (real-time) date, return that date.
	@result		Returns the date of current playback, or nil if playback is not mapped to any date.
*/
- (nullable NSDate *)currentDate;

/*!
 @method		seekToDate:completionHandler:
 @abstract		move playhead to a point corresponding to a particular date, and invokes the specified block when the seek operation has either been completed or been interrupted.
 @discussion
   For playback content that is associated with a range of dates, move the
   playhead to point within that range and invokes the completion handler when the seek operation is complete. 
   Will fail if the supplied date is outside the range or if the content is not associated with a range of dates.  
   The completion handler for any prior seek request that is still in process will be invoked immediately with the finished parameter 
   set to NO. If the new request completes without being interrupted by another seek request or by any other operation, the specified 
   completion handler will be invoked with the finished parameter set to YES. 
 @param			date				The new position for the playhead.
 @param			completionHandler	The block to invoke when seek operation is complete
 @result		Returns true if the playhead was moved to the supplied date.
 */
- (BOOL)seekToDate:(NSDate *)date completionHandler:(void (^_Nullable)(BOOL finished))completionHandler NS_AVAILABLE(10_9, 6_0);

/*!
 @method		stepByCount:
 @abstract		Moves player's current item's current time forward or backward by the specified number of steps.
 @param 		stepCount
   The number of steps by which to move. A positive number results in stepping forward, a negative number in stepping backward.
 @discussion
   The size of each step depends on the enabled AVPlayerItemTracks of the AVPlayerItem. 
 */
- (void)stepByCount:(NSInteger)stepCount;

/*!
 @property		timebase
 @abstract		The item's timebase.
 @discussion 
   You can examine the timebase to discover the relationship between the item's time and the master clock used for drift synchronization.
   This timebase is read-only; you cannot set its time or rate to affect playback.  The value of this property may change during playback.
 */
@property (nonatomic, readonly, nullable) __attribute__((NSObject)) CMTimebaseRef timebase NS_AVAILABLE(10_8, 6_0);

@end


@class AVTextStyleRule;

@interface AVPlayerItem (AVPlayerItemVisualPresentation)

/*!
 @property videoComposition
 @abstract Indicates the video composition settings to be applied during playback.
 */
@property (nonatomic, copy, nullable) AVVideoComposition *videoComposition;

/*!
 @property customVideoCompositor
 @abstract Indicates the custom video compositor instance.
 @discussion
 	This property is nil if there is no video compositor, or if the internal video compositor is in use. This reference can be used to provide
	extra context to the custom video compositor instance if required.
 */
@property (nonatomic, readonly, nullable) id<AVVideoCompositing> customVideoCompositor NS_AVAILABLE(10_9, 7_0);

/*!
 @property seekingWaitsForVideoCompositionRendering
 @abstract Indicates whether the item's timing follows the displayed video frame when seeking with a video composition
 @discussion
   By default, item timing is updated as quickly as possible, not waiting for media at new times to be rendered when seeking or 
   during normal playback. The latency that occurs, for example, between the completion of a seek operation and the display of a 
   video frame at a new time is negligible in most situations. However, when video compositions are in use, the processing of 
   video for any particular time may introduce noticeable latency. Therefore it may be desirable when a video composition is in 
   use for the item's timing be updated only after the video frame for a time has been displayed. This allows, for instance, an 
   AVSynchronizedLayer associated with an AVPlayerItem to remain in synchronization with the displayed video and for the 
   currentTime property to return the time of the displayed video.

   This property has no effect on items for which videoComposition is nil.

 */
@property (nonatomic) BOOL seekingWaitsForVideoCompositionRendering NS_AVAILABLE(10_9, 6_0);

/*!
 @property textStyleRules
 @abstract An array of AVTextStyleRules representing text styling that can be applied to subtitles and other legible media.
 @discussion
	The styling information contained in each AVTextStyleRule object in the array is used only when no equivalent styling information is provided by the media resource being played.  For example, if the text style rules specify Courier font but the media resource specifies Helvetica font, the text will be drawn using Helvetica font.
 
	This property has an effect only for tracks with media subtype kCMSubtitleFormatType_WebVTT.
*/
@property (nonatomic, copy, nullable) NSArray<AVTextStyleRule *> *textStyleRules NS_AVAILABLE(10_9, 6_0);

/*!
 @property	videoApertureMode
 @abstract	Specifies the video aperture mode to apply during playback.
 @discussion
	See AVVideoApertureMode constants defined in AVVideoSettings.h. Default is AVVideoApertureModeCleanAperture.
 */
@property (nonatomic, copy) AVVideoApertureMode videoApertureMode API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

@end


@interface AVPlayerItem (AVPlayerItemAudioProcessing)

/*!
 @property	audioTimePitchAlgorithm
 @abstract	Indicates the processing algorithm used to manage audio pitch at varying rates and for scaled audio edits.
 @discussion
   Constants for various time pitch algorithms, e.g. AVAudioTimePitchSpectral, are defined in AVAudioProcessingSettings.h.
   The default value on iOS is AVAudioTimePitchAlgorithmLowQualityZeroLatency and on OS X is AVAudioTimePitchAlgorithmSpectral.
*/
@property (nonatomic, copy) AVAudioTimePitchAlgorithm audioTimePitchAlgorithm NS_AVAILABLE(10_9, 7_0);

/*!
 @property audioMix
 @abstract Indicates the audio mix parameters to be applied during playback
 @discussion
   The inputParameters of the AVAudioMix must have trackIDs that correspond to a track of the receiver's asset. Otherwise they will be ignored. (See AVAudioMix.h for the declaration of AVAudioMixInputParameters and AVPlayerItem's asset property.)
 */
@property (nonatomic, copy, nullable) AVAudioMix *audioMix;

@end


@interface AVPlayerItem (AVPlayerItemPlayability)

/*!
 @property loadedTimeRanges
 @abstract This property provides a collection of time ranges for which the player has the media data readily available. The ranges provided might be discontinuous.
 @discussion Returns an NSArray of NSValues containing CMTimeRanges.
 */
@property (nonatomic, readonly) NSArray<NSValue *> *loadedTimeRanges;

/*!
 @property playbackLikelyToKeepUp
 @abstract Indicates whether the item will likely play through without stalling.
 @discussion This property communicates a prediction of playability. Factors considered in this prediction
	include I/O throughput and media decode performance. It is possible for playbackLikelyToKeepUp to
	indicate NO while the property playbackBufferFull indicates YES. In this event the playback buffer has
	reached capacity but there isn't the statistical data to support a prediction that playback is likely to 
	keep up. It is left to the application programmer to decide to continue media playback or not. 
	See playbackBufferFull below.
  */
@property (nonatomic, readonly, getter=isPlaybackLikelyToKeepUp) BOOL playbackLikelyToKeepUp;

/*! 
 @property playbackBufferFull
 @abstract Indicates that the internal media buffer is full and that further I/O is suspended.
 @discussion This property reports that the data buffer used for playback has reach capacity.
	Despite the playback buffer reaching capacity there might not exist sufficient statistical 
	data to support a playbackLikelyToKeepUp prediction of YES. See playbackLikelyToKeepUp above.
 */
@property (nonatomic, readonly, getter=isPlaybackBufferFull) BOOL playbackBufferFull;

/* indicates that playback has consumed all buffered media and that playback will stall or end */
@property (nonatomic, readonly, getter=isPlaybackBufferEmpty) BOOL playbackBufferEmpty;

/*!
 @property canUseNetworkResourcesForLiveStreamingWhilePaused
 @abstract Indicates whether the player item can use network resources to keep playback state up to date while paused
 @discussion
	For live streaming content, the player item may need to use extra networking and power resources to keep playback state up to date when paused.  For example, when this property is set to YES, the seekableTimeRanges property will be periodically updated to reflect the current state of the live stream.
 
	For clients linked on or after OS X 10.11 or iOS 9.0, the default value is NO.  To minimize power usage, avoid setting this property to YES when you do not need playback state to stay up to date while paused.
 */
@property (nonatomic, assign) BOOL canUseNetworkResourcesForLiveStreamingWhilePaused NS_AVAILABLE(10_11, 9_0);

/*!
@property	preferredForwardBufferDuration
@abstract	Indicates the media duration the caller prefers the player to buffer from the network ahead of the playhead to guard against playback disruption. 
@discussion	The value is in seconds. If it is set to 0, the player will choose an appropriate level of buffering for most use cases.
			Note that setting this property to a low value will increase the chance that playback will stall and re-buffer, while setting it to a high value will increase demand on system resources.
			Note that the system may buffer less than the value of this property in order to manage resource consumption.
*/
@property (nonatomic) NSTimeInterval preferredForwardBufferDuration NS_AVAILABLE(10_12, 10_0);

@end


@interface AVPlayerItem (AVPlayerItemVariantControl)

/*!
 @property preferredPeakBitRate
 @abstract Indicates the desired limit of network bandwidth consumption for this item.

 @discussion
	Set preferredPeakBitRate to non-zero to indicate that the player should attempt to limit item playback to that bit rate, expressed in bits per second.

	If network bandwidth consumption cannot be lowered to meet the preferredPeakBitRate, it will be reduced as much as possible while continuing to play the item.
*/
@property (nonatomic) double preferredPeakBitRate NS_AVAILABLE(10_10, 8_0);

/*!
 @property preferredMaximumResolution
 @abstract Indicates a preferred upper limit on the resolution of the video to be downloaded (or otherwise transferred) and rendered by the player.
 @discussion
	The default value is CGSizeZero, which indicates that the client enforces no limit on video resolution. Other values indicate a preferred maximum video resolution.
	It only applies to HTTP Live Streaming asset.
 */
@property (nonatomic) CGSize preferredMaximumResolution NS_AVAILABLE(10_13, 11_0);

@end


@interface AVPlayerItem (AVPlayerItemMediaSelection) 

/*!
 @method		selectMediaOption:inMediaSelectionGroup:
 @abstract
   Selects the media option described by the specified instance of AVMediaSelectionOption in the specified AVMediaSelectionGroup and deselects all other options in that group.
 @param 		mediaSelectionOption	The option to select.
 @param 		mediaSelectionGroup		The media selection group, obtained from the receiver's asset, that contains the specified option.
 @discussion
   If the specified media selection option isn't a member of the specified media selection group, no change in presentation state will result.
   If the value of the property allowsEmptySelection of the AVMediaSelectionGroup is YES, you can pass nil for mediaSelectionOption to deselect
   all media selection options in the group.
   Note that if multiple options within a group meet your criteria for selection according to locale or other considerations, and if these options are otherwise indistinguishable to you according to media characteristics that are meaningful for your application, content is typically authored so that the first available option that meets your criteria is appropriate for selection.
 */
- (void)selectMediaOption:(nullable AVMediaSelectionOption *)mediaSelectionOption inMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup NS_AVAILABLE(10_8, 5_0);

/*!
 @method		selectMediaOptionAutomaticallyInMediaSelectionGroup:
 @abstract
    Selects the media option in the specified media selection group that best matches the AVPlayer's current automatic selection criteria. Also allows automatic selection to be re-applied to the specified group subsequently if the relevant criteria are changed.
 @param 		mediaSelectionGroup		The media selection group, obtained from the receiver's asset, that contains the specified option.
 @discussion
   Has no effect unless the appliesMediaSelectionCriteriaAutomatically property of the associated AVPlayer is YES and unless automatic media selection has previously been overridden via -[AVPlayerItem selectMediaOption:inMediaSelectionGroup:].
 */
- (void)selectMediaOptionAutomaticallyInMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup NS_AVAILABLE(10_9, 7_0);

/*!
  @property		currentMediaSelection
  @abstract		Provides an instance of AVMediaSelection carrying current selections for each of the receiver's media selection groups.
*/
@property (nonatomic, readonly) AVMediaSelection *currentMediaSelection NS_AVAILABLE(10_11, 9_0);

@end


@class AVPlayerItemAccessLog;
@class AVPlayerItemErrorLog;
@class AVPlayerItemAccessLogInternal;
@class AVPlayerItemErrorLogInternal;
@class AVPlayerItemAccessLogEventInternal;
@class AVPlayerItemErrorLogEventInternal;

@interface AVPlayerItem (AVPlayerItemLogging)

/*!
 @method		accessLog
 @abstract		Returns an object that represents a snapshot of the network access log. Can be nil.
 @discussion	An AVPlayerItemAccessLog provides methods to retrieve the network access log in a format suitable for serialization.
 				If nil is returned then there is no logging information currently available for this AVPlayerItem.
				An AVPlayerItemNewAccessLogEntryNotification will be posted when new logging information becomes available. However, accessLog might already return a non-nil value even before the first notification is posted.
 @result		An autoreleased AVPlayerItemAccessLog instance.
 */
- (nullable AVPlayerItemAccessLog *)accessLog NS_AVAILABLE(10_7, 4_3);

/*!
 @method		errorLog
 @abstract		Returns an object that represents a snapshot of the error log. Can be nil.
 @discussion	An AVPlayerItemErrorLog provides methods to retrieve the error log in a format suitable for serialization.
 				If nil is returned then there is no logging information currently available for this AVPlayerItem.
 @result		An autoreleased AVPlayerItemErrorLog instance.
 */
- (nullable AVPlayerItemErrorLog *)errorLog NS_AVAILABLE(10_7, 4_3);

@end

@class AVPlayerItemOutput;

@interface AVPlayerItem (AVPlayerItemOutputs)

/*!
 @method		addOutput:
 @abstract		Adds the specified instance of AVPlayerItemOutput to the receiver's collection of outputs.
 @discussion	
	The class of AVPlayerItemOutput provided dictates the data structure that decoded samples are vended in. 
 
 	When an AVPlayerItemOutput is associated with an AVPlayerItem, samples are provided for a media type in accordance with the rules for mixing, composition, or exclusion that the AVPlayer honors among multiple enabled tracks of that media type for its own rendering purposes. For example, video media will be composed according to the instructions provided via AVPlayerItem.videoComposition, if present. Audio media will be mixed according to the parameters provided via AVPlayerItem.audioMix, if present.
 @param			output
				An instance of AVPlayerItemOutput
 */

- (void)addOutput:(AVPlayerItemOutput *)output NS_AVAILABLE(10_8, 6_0);

/*!
 @method		removeOutput:
 @abstract		Removes the specified instance of AVPlayerItemOutput from the receiver's collection of outputs.
 @param			output
				An instance of AVPlayerItemOutput
 */

- (void)removeOutput:(AVPlayerItemOutput *)output NS_AVAILABLE(10_8, 6_0);

/*!
 @property		outputs
 @abstract		The collection of associated outputs.
 */

@property (nonatomic, readonly) NSArray<AVPlayerItemOutput *> *outputs NS_AVAILABLE(10_8, 6_0);

@end

@class AVPlayerItemMediaDataCollector;

@interface AVPlayerItem (AVPlayerItemMediaDataCollectors)

/*!
 @method		addMediaDataCollector:
 @abstract		Adds the specified instance of AVPlayerItemMediaDataCollector to the receiver's collection of mediaDataCollectors.
 @discussion
	This method may incur additional I/O to collect the requested media data asynchronously.
 @param			collector
				An instance of AVPlayerItemMediaDataCollector
*/
- (void)addMediaDataCollector:(AVPlayerItemMediaDataCollector *)collector NS_AVAILABLE(10_11_3, 9_3);

/*!
 @method		removeMediaDataCollector:
 @abstract		Removes the specified instance of AVPlayerItemMediaDataCollector from the receiver's collection of mediaDataCollectors.
 @param			collector
				An instance of AVPlayerItemMediaDataCollector
*/
- (void)removeMediaDataCollector:(AVPlayerItemMediaDataCollector *)collector NS_AVAILABLE(10_11_3, 9_3);

/*!
 @property		mediaDataCollectors
 @abstract		The collection of associated mediaDataCollectors.
*/
@property (nonatomic, readonly) NSArray<AVPlayerItemMediaDataCollector *> *mediaDataCollectors NS_AVAILABLE(10_11_3, 9_3);;

@end

@interface AVPlayerItem (AVPlayerItemDeprecated)

/*!
 @method			seekToTime:
 @abstract			Moves the playback cursor.
 @param				time
 @discussion		Use this method to seek to a specified time for the item.
					The time seeked to may differ from the specified time for efficiency. For sample accurate seeking see seekToTime:toleranceBefore:toleranceAfter:.
					If the seek time is outside of seekable time ranges as indicated by seekableTimeRanges property, the seek request will be cancelled.
 */
- (void)seekToTime:(CMTime)time NS_DEPRECATED(10_7, 10_13, 4_0, 11_0, "Use -seekToTime:completionHandler:, passing nil for the completionHandler if you don't require notification of completion");

/*!
 @method			seekToTime:toleranceBefore:toleranceAfter:
 @abstract			Moves the playback cursor within a specified time bound.
 @param				time
 @param				toleranceBefore
 @param				toleranceAfter
 @discussion		Use this method to seek to a specified time for the item.
					The time seeked to will be within the range [time-toleranceBefore, time+toleranceAfter] and may differ from the specified time for efficiency.
					Pass kCMTimeZero for both toleranceBefore and toleranceAfter to request sample accurate seeking which may incur additional decoding delay. 
					Messaging this method with beforeTolerance:kCMTimePositiveInfinity and afterTolerance:kCMTimePositiveInfinity is the same as messaging seekToTime: directly.
					Seeking is constrained by the collection of seekable time ranges. If you seek to a time outside all of the seekable ranges the seek will result in a currentTime
					within the seekable ranges.
					If the seek time is outside of seekable time ranges as indicated by seekableTimeRanges property, the seek request will be cancelled.
 */
- (void)seekToTime:(CMTime)time toleranceBefore:(CMTime)toleranceBefore toleranceAfter:(CMTime)toleranceAfter NS_DEPRECATED(10_7, 10_13, 4_0, 11_0, "Use -seekToTime:toleranceBefore:toleranceAfter:completionHandler:, passing nil for the completionHandler if you don't require notification of completion");

/*!
 @method		seekToDate
 @abstract		move playhead to a point corresponding to a particular date.
 @discussion
   For playback content that is associated with a range of dates, move the
   playhead to point within that range. Will fail if the supplied date is outside
   the range or if the content is not associated with a range of dates.
 @param			date	The new position for the playhead.
 @result		Returns true if the playhead was moved to the supplied date.
 */
- (BOOL)seekToDate:(NSDate *)date NS_DEPRECATED(10_7, 10_13, 4_0, 11_0, "Use -seekToDate:completionHandler:, passing nil for the completionHandler if you don't require notification of completion");

/*!
 @method		selectedMediaOptionInMediaSelectionGroup:
 @abstract		Indicates the media selection option that's currently selected from the specified group. May be nil.
 @param 		mediaSelectionGroup		A media selection group obtained from the receiver's asset.
 @result		An instance of AVMediaSelectionOption that describes the currently selection option in the group.
 @discussion
   If the value of the property allowsEmptySelection of the AVMediaSelectionGroup is YES, the currently selected option in the group may be nil.
 */
- (nullable AVMediaSelectionOption *)selectedMediaOptionInMediaSelectionGroup:(AVMediaSelectionGroup *)mediaSelectionGroup NS_DEPRECATED(10_8, 10_13, 5_0, 11_0, "Use currentMediaSelection to obtain an instance of AVMediaSelection, which encompasses the currently selected AVMediaSelectionOption in each of the available AVMediaSelectionGroups");

@end

@class AVPlayerItemAccessLogEvent;

/*!
 @class			AVPlayerItemAccessLog
 @abstract		An AVPlayerItemAccessLog provides methods to retrieve the access log in a format suitable for serialization.
 @discussion	An AVPlayerItemAccessLog acculumulates key metrics about network playback and presents them as a collection 
 				of AVPlayerItemAccessLogEvent instances. Each AVPlayerItemAccessLogEvent instance collates the data 
 				that relates to each uninterrupted period of playback.
*/
NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVPlayerItemAccessLog : NSObject <NSCopying>
{
@private
	AVPlayerItemAccessLogInternal	*_playerItemAccessLog;
}
AV_INIT_UNAVAILABLE

/*!
 @method		extendedLogData
 @abstract		Serializes an AVPlayerItemAccessLog in the Extended Log File Format.
 @discussion	This method converts the webserver access log into a textual format that conforms to the
				W3C Extended Log File Format for web server log files.
				For more information see: http://www.w3.org/pub/WWW/TR/WD-logfile.html
 @result		An autoreleased NSData instance.
 */
- (nullable NSData *)extendedLogData;

/*!
 @property		extendedLogDataStringEncoding
 @abstract		Returns the NSStringEncoding for extendedLogData, see above.
 @discussion	A string suitable for console output is obtainable by: 
 				[[NSString alloc] initWithData:[myLog extendedLogData] encoding:[myLog extendedLogDataStringEncoding]]
 */
 @property (nonatomic, readonly) NSStringEncoding extendedLogDataStringEncoding;

/*!
 @property		events
 @abstract		An ordered collection of AVPlayerItemAccessLogEvent instances.
 @discussion	An ordered collection of AVPlayerItemAccessLogEvent instances that represent the chronological
 				sequence of events contained in the access log.
 				This property is not observable.
 */
@property (nonatomic, readonly) NSArray<AVPlayerItemAccessLogEvent *> *events;

@end

@class AVPlayerItemErrorLogEvent;
/*!
 @class			AVPlayerItemErrorLog
 @abstract		An AVPlayerItemErrorLog provides methods to retrieve the error log in a format suitable for serialization.
 @discussion	An AVPlayerItemErrorLog provides data to identify if, and when, network resource playback failures occured.
*/
NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVPlayerItemErrorLog : NSObject <NSCopying>
{
@private
	AVPlayerItemErrorLogInternal	*_playerItemErrorLog;
}
AV_INIT_UNAVAILABLE

/*!
 @method		extendedLogData
 @abstract		Serializes an AVPlayerItemErrorLog in the Extended Log File Format.
 @discussion	This method converts the webserver error log into a textual format that conforms to the
				W3C Extended Log File Format for web server log files.
				For more information see: http://www.w3.org/pub/WWW/TR/WD-logfile.html
 @result		An autoreleased NSData instance.
 */
- (nullable NSData *)extendedLogData;

/*!
 @property		extendedLogDataStringEncoding
 @abstract		Returns the NSStringEncoding for extendedLogData, see above.
 @discussion	A string suitable for console output is obtainable by: 
 				[[NSString alloc] initWithData:[myLog extendedLogData] encoding:[myLog extendedLogDataStringEncoding]]
 */
 @property (nonatomic, readonly) NSStringEncoding extendedLogDataStringEncoding;

/*!
 @property		events
 @abstract		An ordered collection of AVPlayerItemErrorLogEvent instances.
 @discussion	An ordered collection of AVPlayerItemErrorLogEvent instances that represent the chronological
 				sequence of events contained in the error log.
 				This property is not observable.
 */
@property (nonatomic, readonly) NSArray<AVPlayerItemErrorLogEvent *> *events;

@end

/*!
 @class			AVPlayerItemAccessLogEvent
 @abstract		An AVPlayerItemAccessLogEvent represents a single log entry.
 @discussion	An AVPlayerItemAccessLogEvent provides named properties for accessing the data
				fields of each log event. None of the properties of this class are observable.
*/

NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVPlayerItemAccessLogEvent : NSObject <NSCopying>
{
@private
	AVPlayerItemAccessLogEventInternal	*_playerItemAccessLogEvent;
}
AV_INIT_UNAVAILABLE

/*!
 @property		numberOfSegmentsDownloaded
 @abstract		A count of media segments downloaded.
 @discussion	Value is negative if unknown. A count of media segments downloaded from the server to this client. Corresponds to "sc-count".
 				This property is not observable.
 				This property is deprecated. Use numberOfMediaRequests instead.
 */
@property (nonatomic, readonly) NSInteger numberOfSegmentsDownloaded NS_DEPRECATED(10_7, 10_9, 4_3, 7_0);

/*!
 @property		numberOfMediaRequests
 @abstract		A count of media read requests.
 @discussion	Value is negative if unknown. A count of media read requests from the server to this client. Corresponds to "sc-count".
				For HTTP live Streaming, a count of media segments downloaded from the server to this client.
				For progressive-style HTTP media downloads, a count of HTTP GET (byte-range) requests for the resource.
 				This property is not observable. 
 */
@property (nonatomic, readonly) NSInteger numberOfMediaRequests NS_AVAILABLE(10_9, 6_0);

/*!
 @property		playbackStartDate
 @abstract		The date/time at which playback began for this event. Can be nil.
 @discussion	If nil is returned the date is unknown. Corresponds to "date".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSDate *playbackStartDate;

/*!
 @property		URI
 @abstract		The URI of the playback item. Can be nil.
 @discussion	If nil is returned the URI is unknown. Corresponds to "uri".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *URI;

/*!
 @property		serverAddress
 @abstract		The IP address of the server that was the source of the last delivered media segment. Can be nil.
 @discussion	If nil is returned the address is unknown. Can be either an IPv4 or IPv6 address. Corresponds to "s-ip".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *serverAddress;

/*!
 @property		numberOfServerAddressChanges
 @abstract		A count of changes to the property serverAddress, see above, over the last uninterrupted period of playback.
 @discussion	Value is negative if unknown. Corresponds to "s-ip-changes".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger numberOfServerAddressChanges;

/*!
 @property		playbackSessionID
 @abstract		A GUID that identifies the playback session. This value is used in HTTP requests. Can be nil.
 @discussion	If nil is returned the GUID is unknown. Corresponds to "cs-guid".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *playbackSessionID;

/*!
 @property		playbackStartOffset
 @abstract		An offset into the playlist where the last uninterrupted period of playback began. Measured in seconds.
 @discussion	Value is negative if unknown. Corresponds to "c-start-time".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSTimeInterval playbackStartOffset;

/*!
 @property		segmentsDownloadedDuration
 @abstract		The accumulated duration of the media downloaded. Measured in seconds.
 @discussion	Value is negative if unknown. Corresponds to "c-duration-downloaded".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSTimeInterval segmentsDownloadedDuration;

/*!
 @property		durationWatched
 @abstract		The accumulated duration of the media played. Measured in seconds.
 @discussion	Value is negative if unknown. Corresponds to "c-duration-watched".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSTimeInterval durationWatched;

/*!
 @property		numberOfStalls
 @abstract		The total number of playback stalls encountered.
 @discussion	Value is negative if unknown. Corresponds to "c-stalls".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger numberOfStalls;

/*!
 @property		numberOfBytesTransferred
 @abstract		The accumulated number of bytes transferred.
 @discussion	Value is negative if unknown. Corresponds to "bytes".
 				This property is not observable.
 */
@property (nonatomic, readonly) long long numberOfBytesTransferred;

/*!
 @property		transferDuration
 @abstract		The accumulated duration of active network transfer of bytes. Measured in seconds.
 @discussion	Value is negative if unknown. Corresponds to "c-transfer-duration".
				This property is not observable.
 */
@property (nonatomic, readonly) NSTimeInterval transferDuration NS_AVAILABLE(10_9, 7_0);

/*!
 @property		observedBitrate
 @abstract		The empirical throughput across all media downloaded. Measured in bits per second.
 @discussion	Value is negative if unknown. Corresponds to "c-observed-bitrate".
 				This property is not observable.
 */
@property (nonatomic, readonly) double observedBitrate;

/*!
 @property		indicatedBitrate
 @abstract		The throughput required to play the stream, as advertised by the server. Measured in bits per second.
 @discussion	Value is negative if unknown. Corresponds to "sc-indicated-bitrate".
 				This property is not observable.
 */
@property (nonatomic, readonly) double indicatedBitrate;

/*!
 @property		indicatedAverageBitrate
 @abstract		Average throughput required to play the stream, as advertised by the server. Measured in bits per second.
 @discussion	Value is negative if unknown. Corresponds to "sc-indicated-avg-bitrate".
 This property is not observable.
 */
@property (nonatomic, readonly) double indicatedAverageBitrate NS_AVAILABLE(10_12, 10_0);

/*!
 @property		averageVideoBitrate
 @abstract		The average bitrate of video track if it is unmuxed. Average bitrate of combined content if muxed. Measured in bits per second.
 @discussion	Value is negative if unknown. Corresponds to "c-avg-video-bitrate".
 This property is not observable.
 */
@property (nonatomic, readonly) double averageVideoBitrate NS_AVAILABLE(10_12, 10_0);

/*!
 @property		averageAudioBitrate
 @abstract		The average bitrate of audio track. This is not available if audio is muxed with video. Measured in bits per second.
 @discussion	Value is negative if unknown. Corresponds to "c-avg-audio-bitrate".
 This property is not observable.
 */
@property (nonatomic, readonly) double averageAudioBitrate NS_AVAILABLE(10_12, 10_0);

/*!
 @property		numberOfDroppedVideoFrames
 @abstract		The total number of dropped video frames.
 @discussion	Value is negative if unknown. Corresponds to "c-frames-dropped".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger numberOfDroppedVideoFrames;

/*!
 @property		startupTime
 @abstract		The accumulated duration until player item is ready to play. Measured in seconds.
 @discussion	Value is negative if unknown. Corresponds to "c-startup-time".
				This property is not observable.
 */
@property (nonatomic, readonly) NSTimeInterval startupTime NS_AVAILABLE(10_9, 7_0);

/*!
 @property		downloadOverdue
 @abstract		The total number of times the download of the segments took too long.
 @discussion	Value is negative if unknown. Corresponds to "c-overdue".
				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger downloadOverdue NS_AVAILABLE(10_9, 7_0);

/*!
 @property		observedMaxBitrate
 @abstract		Maximum observed segment download bit rate.
 @discussion	Value is negative if unknown. Corresponds to "c-observed-max-bitrate".
				This property is not observable.
 */
@property (nonatomic, readonly) double observedMaxBitrate NS_AVAILABLE(10_9, 7_0);

/*!
 @property		observedMinBitrate
 @abstract		Minimum observed segment download bit rate.
 @discussion	Value is negative if unknown. Corresponds to "c-observed-min-bitrate".
				This property is not observable.
 */
@property (nonatomic, readonly) double observedMinBitrate NS_AVAILABLE(10_9, 7_0);

/*!
 @property		observedBitrateStandardDeviation
 @abstract		Standard deviation of observed segment download bit rates.
 @discussion	Value is negative if unknown. Corresponds to "c-observed-bitrate-sd".
				This property is not observable.
 */
@property (nonatomic, readonly) double observedBitrateStandardDeviation NS_AVAILABLE(10_9, 7_0);

/*!
 @property		playbackType
 @abstract		Playback type (LIVE, VOD, FILE).
 @discussion	If nil is returned the playback type is unknown. Corresponds to "s-playback-type".
				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *playbackType NS_AVAILABLE(10_9, 7_0);

/*!
 @property		mediaRequestsWWAN
 @abstract		Number of network read requests over WWAN.
 @discussion	Value is negative if unknown. Corresponds to "sc-wwan-count".
				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger mediaRequestsWWAN NS_AVAILABLE(10_9, 7_0);

/*!
 @property		switchBitrate
 @abstract		Bandwidth that caused us to switch (up or down).
 @discussion	Value is negative if unknown. Corresponds to "c-switch-bitrate".
				This property is not observable.
 */
@property (nonatomic, readonly) double switchBitrate NS_AVAILABLE(10_9, 7_0);

@end

/*!
 @class			AVPlayerItemErrorLogEvent
 @abstract		An AVPlayerItemErrorLogEvent represents a single log entry.
 @discussion	An AVPlayerItemErrorLogEvent provides named properties for accessing the data
				fields of each log event. None of the properties of this class are observable.
*/
NS_CLASS_AVAILABLE(10_7, 4_3)
@interface AVPlayerItemErrorLogEvent : NSObject <NSCopying>
{
@private
	AVPlayerItemErrorLogEventInternal	*_playerItemErrorLogEvent;
}
AV_INIT_UNAVAILABLE

/*!
 @property		date
 @abstract		The date and time when the error occured. Can be nil.
 @discussion	If nil is returned the date is unknown. Corresponds to "date".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSDate *date;

/*!
 @property		URI
 @abstract		The URI of the playback item. Can be nil.
 @discussion	If nil is returned the URI is unknown. Corresponds to "uri".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *URI;

/*!
 @property		serverAddress
 @abstract		The IP address of the server that was the source of the error. Can be nil.
 @discussion	If nil is returned the address is unknown. Can be either an IPv4 or IPv6 address. Corresponds to "s-ip".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *serverAddress;

/*!
 @property		playbackSessionID
 @abstract		A GUID that identifies the playback session. This value is used in HTTP requests. Can be nil.
 @discussion	If nil is returned the GUID is unknown. Corresponds to "cs-guid".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *playbackSessionID;

/*!
 @property		errorStatusCode
 @abstract		A unique error code identifier.
 @discussion	Corresponds to "status".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSInteger errorStatusCode;

/*!
 @property		errorDomain
 @abstract		The domain of the error.
 @discussion	Corresponds to "domain".
 				This property is not observable.
 */
@property (nonatomic, readonly) NSString *errorDomain;

/*!
 @property		errorComment
 @abstract		A description of the error encountered. Can be nil.
 @discussion	If nil is returned further information is not available. Corresponds to "comment".
 				This property is not observable.
 */
@property (nonatomic, readonly, nullable) NSString *errorComment;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAssetResourceLoader.h
/*
	File:  AVAssetResourceLoader.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

/*!
  @class		AVAssetResourceLoader

  @abstract		An AVAssetResourceLoader mediates requests to load resources required by an AVURLAsset by asking a delegate object that you provide for assistance. When a resource is required that cannot be loaded by the AVURLAsset itself, the resource loader makes a request of its delegate to load it and proceeds according to the delegate's response.
	
  @discussion
	You should not create resource loader objects. Instead, you should retrieve a resource loader from the resourceLoader property of an AVURLAsset and use it to assign your delegate object for resource loading.
 
	The delegate associated with this object must adopt the AVAssetResourceLoaderDelegate protocol.
*/
#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@protocol AVAssetResourceLoaderDelegate;

@class AVAssetResourceLoadingRequest;
@class AVAssetResourceRenewalRequest;
@class AVAssetResourceLoaderInternal;

NS_CLASS_AVAILABLE(10_9, 6_0)
@interface AVAssetResourceLoader : NSObject {
@private
	AVAssetResourceLoaderInternal *_resourceLoader;
}
AV_INIT_UNAVAILABLE

/*!
 @method 		setDelegate:queue:
 @abstract		Sets the receiver's delegate that will mediate resource loading and the dispatch queue on which delegate methods will be invoked.
 @param			delegate
				An object conforming to the AVAssetResourceLoaderDelegate protocol.
 @param			delegateQueue
				A dispatch queue on which all delegate methods will be invoked.
 @discussion
  If you employ an AVAssetResourceLoader delegate that loads media data for playback, you should set the value of your AVPlayer’s automaticallyWaitsToMinimizeStalling property to NO. Allowing the value of automaticallyWaitsToMinimizeStalling to remain YES — its default value — when an AVAssetResourceLoader delegate is used for the loading of media data can result in poor start-up times for playback and poor recovery from stalls, because the behaviors provided by AVPlayer when automaticallyWaitsToMinimizeStalling has a value of YES depend on predictions of the future availability of media data that that do not function as expected when data is loaded via a client-controlled means, using the AVAssetResourceLoader delegate interface.

  You can allow the value of automaticallyWaitsToMinimizeStalling to remain YES if you use an AVAssetResourceLoader delegate to manage content keys for FairPlay Streaming, to provide dynamically-generated master playlists for HTTP Live Streaming, or to respond to authentication challenges, but not to load media data for playback.
*/
- (void)setDelegate:(nullable id <AVAssetResourceLoaderDelegate>)delegate queue:(nullable dispatch_queue_t)delegateQueue;

/*!
 @property 		delegate
 @abstract		The receiver's delegate.
 @discussion
  The value of this property is an object conforming to the AVAssetResourceLoaderDelegate protocol. The delegate is set using the setDelegate:queue: method. The delegate is held using a zeroing-weak reference, so this property will have a value of nil after a delegate that was previously set has been deallocated.
*/
@property (nonatomic, readonly, weak, nullable) id <AVAssetResourceLoaderDelegate> delegate;

/*!
 @property 		delegateQueue
 @abstract		The dispatch queue on which all delegate methods will be invoked.
 @discussion
  The value of this property is a dispatch_queue_t. The queue is set using the setDelegate:queue: method.
*/
@property (nonatomic, readonly, nullable) dispatch_queue_t delegateQueue;

@end

/*!
	@protocol		AVAssetResourceLoaderDelegate
	@abstract		The AVAssetResourceLoaderDelegate protocol defines methods that allow your code to handle resource loading requests coming from an AVULRAsset.
*/

@class NSURLAuthenticationChallenge;

@protocol AVAssetResourceLoaderDelegate <NSObject>

@optional

/*!
 @method 		resourceLoader:shouldWaitForLoadingOfRequestedResource:
 @abstract		Invoked when assistance is required of the application to load a resource.
 @param 		resourceLoader
				The instance of AVAssetResourceLoader for which the loading request is being made.
 @param 		loadingRequest
				An instance of AVAssetResourceLoadingRequest that provides information about the requested resource. 
 @result 		YES if the delegate can load the resource indicated by the AVAssetResourceLoadingRequest; otherwise NO.
 @discussion
  Delegates receive this message when assistance is required of the application to load a resource. For example, this method is invoked to load decryption keys that have been specified using custom URL schemes.
  If the result is YES, the resource loader expects invocation, either subsequently or immediately, of either -[AVAssetResourceLoadingRequest finishLoading] or -[AVAssetResourceLoadingRequest finishLoadingWithError:]. If you intend to finish loading the resource after your handling of this message returns, you must retain the instance of AVAssetResourceLoadingRequest until after loading is finished.
  If the result is NO, the resource loader treats the loading of the resource as having failed.
  Note that if the delegate's implementation of -resourceLoader:shouldWaitForLoadingOfRequestedResource: returns YES without finishing the loading request immediately, it may be invoked again with another loading request before the prior request is finished; therefore in such cases the delegate should be prepared to manage multiple loading requests.
 
  If an AVURLAsset is added to an AVContentKeySession object and a delegate is set on its AVAssetResourceLoader, that delegate's resourceLoader:shouldWaitForLoadingOfRequestedResource: method must specify which custom URL requests should be handled as content keys. This is done by returning YES and passing either AVStreamingKeyDeliveryPersistentContentKeyType or AVStreamingKeyDeliveryContentKeyType into -[AVAssetResourceLoadingContentInformationRequest setContentType:] and then calling -[AVAssetResourceLoadingRequest finishLoading].

*/
- (BOOL)resourceLoader:(AVAssetResourceLoader *)resourceLoader shouldWaitForLoadingOfRequestedResource:(AVAssetResourceLoadingRequest *)loadingRequest NS_AVAILABLE(10_9, 6_0);

/*!
 @method 		resourceLoader:shouldWaitForRenewalOfRequestedResource:
 @abstract		Invoked when assistance is required of the application to renew a resource.
 @param 		resourceLoader
 The instance of AVAssetResourceLoader for which the loading request is being made.
 @param 		renewalRequest
 An instance of AVAssetResourceRenewalRequest that provides information about the requested resource.
 @result 		YES if the delegate can renew the resource indicated by the AVAssetResourceLoadingRequest; otherwise NO.
 @discussion
 Delegates receive this message when assistance is required of the application to renew a resource previously loaded by resourceLoader:shouldWaitForLoadingOfRequestedResource:. For example, this method is invoked to renew decryption keys that require renewal, as indicated in a response to a prior invocation of resourceLoader:shouldWaitForLoadingOfRequestedResource:.
 If the result is YES, the resource loader expects invocation, either subsequently or immediately, of either -[AVAssetResourceRenewalRequest finishLoading] or -[AVAssetResourceRenewalRequest finishLoadingWithError:]. If you intend to finish loading the resource after your handling of this message returns, you must retain the instance of AVAssetResourceRenewalRequest until after loading is finished.
 If the result is NO, the resource loader treats the loading of the resource as having failed.
 Note that if the delegate's implementation of -resourceLoader:shouldWaitForRenewalOfRequestedResource: returns YES without finishing the loading request immediately, it may be invoked again with another loading request before the prior request is finished; therefore in such cases the delegate should be prepared to manage multiple loading requests.
 
  If an AVURLAsset is added to an AVContentKeySession object and a delegate is set on its AVAssetResourceLoader, that delegate's resourceLoader:shouldWaitForRenewalOfRequestedResource:renewalRequest method must specify which custom URL requests should be handled as content keys. This is done by returning YES and passing either AVStreamingKeyDeliveryPersistentContentKeyType or AVStreamingKeyDeliveryContentKeyType into -[AVAssetResourceLoadingContentInformationRequest setContentType:] and then calling -[AVAssetResourceLoadingRequest finishLoading].
*/
- (BOOL)resourceLoader:(AVAssetResourceLoader *)resourceLoader shouldWaitForRenewalOfRequestedResource:(AVAssetResourceRenewalRequest *)renewalRequest NS_AVAILABLE(10_10, 8_0);

/*!
 @method 		resourceLoader:didCancelLoadingRequest:
 @abstract		Informs the delegate that a prior loading request has been cancelled.
 @param 		loadingRequest
				The loading request that has been cancelled. 
 @discussion	Previously issued loading requests can be cancelled when data from the resource is no longer required or when a loading request is superseded by new requests for data from the same resource. For example, if to complete a seek operation it becomes necessary to load a range of bytes that's different from a range previously requested, the prior request may be cancelled while the delegate is still handling it.
*/
- (void)resourceLoader:(AVAssetResourceLoader *)resourceLoader didCancelLoadingRequest:(AVAssetResourceLoadingRequest *)loadingRequest NS_AVAILABLE(10_9, 7_0);

/*!
 @method 		resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
 @abstract		Invoked when assistance is required of the application to respond to an authentication challenge.
 @param 		resourceLoader
				The instance of AVAssetResourceLoader asking for help with an authentication challenge.
 @param 		authenticationChallenge
				An instance of NSURLAuthenticationChallenge. 
 @discussion
  Delegates receive this message when assistance is required of the application to respond to an authentication challenge.
  If the result is YES, the resource loader expects you to send an appropriate response, either subsequently or immediately, to the NSURLAuthenticationChallenge's sender, i.e. [authenticationChallenge sender], via use of one of the messages defined in the NSURLAuthenticationChallengeSender protocol (see NSAuthenticationChallenge.h). If you intend to respond to the authentication challenge after your handling of -resourceLoader:shouldWaitForResponseToAuthenticationChallenge: returns, you must retain the instance of NSURLAuthenticationChallenge until after your response has been made.
*/
- (BOOL)resourceLoader:(AVAssetResourceLoader *)resourceLoader shouldWaitForResponseToAuthenticationChallenge:(NSURLAuthenticationChallenge *)authenticationChallenge NS_AVAILABLE(10_10, 8_0);

/*!
 @method 		resourceLoader:didCancelAuthenticationChallenge:
 @abstract		Informs the delegate that a prior authentication challenge has been cancelled.
 @param 		authenticationChallenge
				The authentication challenge that has been cancelled. 
*/
- (void)resourceLoader:(AVAssetResourceLoader *)resourceLoader didCancelAuthenticationChallenge:(NSURLAuthenticationChallenge *)authenticationChallenge NS_AVAILABLE(10_10, 8_0);

@end

/*!
 	@class		AVAssetResourceLoadingRequestor
 
 	@abstract	AVAssetResourceLoadingRequestor represents the originator of loading request
 
 	@discussion
		Information about the originator of a loading request, in order to decide whether or how to fulfill the request.
 */

@class AVAssetResourceLoadingRequestor;
@class AVAssetResourceLoadingRequestorInternal;

API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0))
@interface AVAssetResourceLoadingRequestor : NSObject {
@private
	AVAssetResourceLoadingRequestorInternal *_requestor;
}
AV_INIT_UNAVAILABLE

/*!
 @property 		providesExpiredSessionReports
 @abstract		Whether the requestor provides expired session reports (see AVContentKeySession)
 */
@property (nonatomic, readonly) BOOL providesExpiredSessionReports;

@end

/*!
	@class		AVAssetResourceLoadingRequest
 
	@abstract	AVAssetResourceLoadingRequest encapsulates information about a resource request issued by a resource loader.
 
	@discussion	
		When an AVURLAsset needs help loading a resource, it asks its AVAssetResourceLoader object to assist. The resource loader encapsulates the request information by creating an instance of this object, which it then hands to its delegate for processing. The delegate uses the information in this object to perform the request and report on the success or failure of the operation.
 
*/

@class AVAssetResourceLoadingRequestInternal;
@class AVAssetResourceLoadingContentInformationRequest;
@class AVAssetResourceLoadingDataRequest;

NS_CLASS_AVAILABLE(10_9, 6_0)
@interface AVAssetResourceLoadingRequest : NSObject {
@private
	AVAssetResourceLoadingRequestInternal *_loadingRequest;
}
AV_INIT_UNAVAILABLE

/*! 
 @property 		request
 @abstract		An NSURLRequest for the requested resource.
*/
@property (nonatomic, readonly) NSURLRequest *request;

/*! 
 @property 		finished
 @abstract		Indicates whether loading of the resource has been finished.
 @discussion	The value of this property becomes YES only in response to an invocation of either -finishLoading or -finishLoadingWithError:.
*/
@property (nonatomic, readonly, getter=isFinished) BOOL finished;

/*! 
 @property 		cancelled
 @abstract		Indicates whether the request has been cancelled.
 @discussion	The value of this property becomes YES when the resource loader cancels the loading of a request, just prior to sending the message -resourceLoader:didCancelLoadingRequest: to its delegate.
*/
@property (nonatomic, readonly, getter=isCancelled) BOOL cancelled NS_AVAILABLE(10_9, 7_0);

/*! 
 @property 		contentInformationRequest
 @abstract		An instance of AVAssetResourceLoadingContentInformationRequest that you should populate with information about the resource. The value of this property will be nil if no such information is being requested.
*/
@property (nonatomic, readonly, nullable) AVAssetResourceLoadingContentInformationRequest *contentInformationRequest NS_AVAILABLE(10_9, 7_0);

/*! 
 @property 		dataRequest
 @abstract		An instance of AVAssetResourceLoadingDataRequest that indicates the range of resource data that's being requested. The value of this property will be nil if no data is being requested.
*/
@property (nonatomic, readonly, nullable) AVAssetResourceLoadingDataRequest *dataRequest NS_AVAILABLE(10_9, 7_0);

/*! 
 @property 		response
 @abstract		Set the value of this property to an instance of NSURLResponse indicating a response to the loading request. If no response is needed, leave the value of this property set to nil.
*/
@property (nonatomic, copy, nullable) NSURLResponse *response NS_AVAILABLE(10_9, 7_0);

/*! 
 @property 		redirect
 @abstract		Set the value of this property to an instance of NSURLRequest indicating a redirection of the loading request to another URL. If no redirection is needed, leave the value of this property set to nil.
 @discussion	AVAssetResourceLoader supports redirects to HTTP URLs only. Redirects to other URLs will result in a loading failure.
*/
@property (nonatomic, copy, nullable) NSURLRequest *redirect NS_AVAILABLE(10_9, 7_0);

/*!
 @property 		requestor
 @abstract		The AVAssetResourceLoadingRequestor that made this request
 */
@property (nonatomic, readonly) AVAssetResourceLoadingRequestor *requestor API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0));

/*! 
 @method 		finishLoading   
 @abstract		Causes the receiver to treat the processing of the request as complete.
 @discussion	If a dataRequest is present and the resource does not contain the full extent of the data that has been requested according to the values of the requestedOffset and requestedLength properties of the dataRequest, or if requestsAllDataToEndOfResource has a value of YES, you may invoke -finishLoading after you have provided as much of the requested data as the resource contains.
*/
- (void)finishLoading NS_AVAILABLE(10_9, 7_0);

/*! 
 @method 		finishLoadingWithError:   
 @abstract		Causes the receiver to treat the request as having failed.
 @param			error
 				An instance of NSError indicating the reason for failure.
*/
- (void)finishLoadingWithError:(nullable NSError *)error;

@end

/*!
 @class		AVAssetResourceRenewalRequest

 @abstract	AVAssetResourceRenewalRequest encapsulates information about a resource request issued by a resource loader for the purpose of renewing a request previously issued.

 @discussion
 When an AVURLAsset needs to renew a resource (because contentInformationRequest.renewalDate has been set on a previous loading request), it asks its AVAssetResourceLoader object to assist. The resource loader encapsulates the request information by creating an instance of this object, which it then hands to its delegate for processing. The delegate uses the information in this object to perform the request and report on the success or failure of the operation.

 */
NS_CLASS_AVAILABLE(10_10, 8_0)
@interface AVAssetResourceRenewalRequest : AVAssetResourceLoadingRequest

@end

/*!
	@class		AVAssetResourceLoadingContentInformationRequest
 
	@abstract	An AVAssetResourceLoadingContentInformationRequest represents a query for essential information about a resource referenced by an asset resource loading request.
 
	@discussion
		When a resource loading delegate accepts responsibility for loading a resource by returning YES from its implementation of resourceLoader:shouldWaitForLoadingOfRequestedResource:, it must check whether the contentInformationRequest property of the AVAssetResourceLoadingRequest is not nil. Whenever the value is not nil, the request includes a query for the information that AVAssetResourceLoadingContentInformationRequest encapsulates. In response to such queries, the resource loading delegate should set the values of the content information request's properties appropriately before invoking the AVAssetResourceLoadingRequest method finishLoading.
 
		When finishLoading is invoked, the values of the properties of its contentInformationRequest property will, in part, determine how the requested resource is processed. For example, if the requested resource's URL is the URL of an AVURLAsset and contentType is set by the resource loading delegate to a value that the underlying media system doesn't recognize as a supported media file type, operations on the AVURLAsset, such as playback, are likely to fail.
*/

@class AVAssetResourceLoadingContentInformationRequestInternal;

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVAssetResourceLoadingContentInformationRequest : NSObject {
@private
	AVAssetResourceLoadingContentInformationRequestInternal *_contentInformationRequest;
}
AV_INIT_UNAVAILABLE

/*! 
 @property 		contentType
 @abstract		A UTI that indicates the type of data contained by the requested resource.
 @discussion	Before you finish loading an AVAssetResourceLoadingRequest, if its contentInformationRequest is not nil, you should set the value of this property to a UTI indicating the type of data contained by the requested resource.
*/
@property (nonatomic, copy, nullable) NSString *contentType;

/*!
 @property		allowedContentTypes
 @abstract		An array showing the types of data which will be accepted as a valid response for the requested resource.
 @discussion	If allowedContentTypes is nonnil and the contentType property is not in allowedContentTypes, an exception will be raised.
*/
@property (nonatomic, readonly, nullable) NSArray <NSString *> *allowedContentTypes API_AVAILABLE(ios(11.2), tvos(11.2), macos(10.13.2), watchos(4.2));

/*! 
 @property 		contentLength
 @abstract		Indicates the length of the requested resource, in bytes.
 @discussion	Before you finish loading an AVAssetResourceLoadingRequest, if its contentInformationRequest is not nil, you should set the value of this property to the number of bytes contained by the requested resource.
*/
@property (nonatomic) long long contentLength;

/*! 
 @property 		byteRangeAccessSupported
 @abstract		Indicates whether random access to arbitrary ranges of bytes of the resource is supported. Such support also allows portions of the resource to be requested more than once.
 @discussion	Before you finish loading an AVAssetResourceLoadingRequest, if its contentInformationRequest is not nil, you should set the value of this property to YES if you support random access to arbitrary ranges of bytes of the resource. If you do not set this property to YES for resources that must be loaded incrementally, loading of the resource may fail. Such resources include anything that contains media data.
*/
@property (nonatomic, getter=isByteRangeAccessSupported) BOOL byteRangeAccessSupported;

/*!
 @property		renewalDate
 @abstract		For resources that expire, the date at which a new AVAssetResourceLoadingRequest will be issued for a renewal of this resource, if the media system still requires it.
 @discussion	Before you finish loading an AVAssetResourceLoadingRequest, if the resource is prone to expiry you should set the value of this property to the date at which a renewal should be triggered. This value should be set sufficiently early enough to allow an AVAssetResourceRenewalRequest, delivered to your delegate via -resourceLoader:shouldWaitForRenewalOfRequestedResource:, to finish before the actual expiry time. Otherwise media playback may fail.
 */
@property (nonatomic, copy, nullable) NSDate *renewalDate NS_AVAILABLE(10_10, 8_0);

@end

/*!
	@class		AVAssetResourceLoadingDataRequest
 
	@abstract	An AVAssetResourceLoadingDataRequest is used to request data from a resource referenced by an AVAssetResourceLoadingRequest.
 
	@discussion
		The AVAssetResourceLoaderDelegate uses the AVAssetResourceLoadingDataRequest class to do the actual data reading, and its methods will be invoked, as necessary, to acquire data for the AVAssetResourceLoadingRequest instance.

		When a resource loading delegate accepts responsibility for loading a resource by returning YES from its implementation of resourceLoader:shouldWaitForLoadingOfRequestedResource:, it must check whether the dataRequest property of the AVAssetResourceLoadingRequest instance is not nil. If it is not nil, the resource loading delegate is informed of the range of bytes within the resource that are required by the underlying media system. In response, the data is provided by one or more invocations of respondWithData: as needed for provision of the requested data. The data can be provided in increments determined by the resource loading delegate according to convenience or efficiency.

		When the AVAssetResourceLoadingRequest method finishLoading is invoked, the data request is considered fully satisfied. If the entire range of bytes requested has not yet been provided, the underlying media system assumes that the resource's length is limited to the provided content. 
*/

@class AVAssetResourceLoadingDataRequestInternal;

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVAssetResourceLoadingDataRequest : NSObject {
@private
	AVAssetResourceLoadingDataRequestInternal *_dataRequest;
}
AV_INIT_UNAVAILABLE

/*! 
 @property 		requestedOffset
 @abstract		The position within the resource of the first byte requested.
*/
@property (nonatomic, readonly) long long requestedOffset;

/*! 
 @property 		requestedLength
 @abstract		The length of the data requested.
 @discussion	Note that requestsAllDataToEndOfResource will be set to YES when the entire remaining length of the resource is being requested from requestedOffset to the end of the resource. This can occur even when the content length has not yet been reported by you via a prior finished loading request.
 				When requestsAllDataToEndOfResource has a value of YES, you should disregard the value of requestedLength and incrementally provide as much data starting from the requestedOffset as the resource contains, until you have provided all of the available data successfully and invoked -finishLoading, until you have encountered a failure and invoked -finishLoadingWithError:, or until you have received -resourceLoader:didCancelLoadingRequest: for the AVAssetResourceLoadingRequest from which the AVAssetResourceLoadingDataRequest was obtained.
 				When requestsAllDataToEndOfResource is YES and the content length has not yet been provided by you via a prior finished loading request, the value of requestedLength is set to NSIntegerMax. Starting in OS X 10.11 and iOS 9.0, in 32-bit applications requestedLength is also set to NSIntegerMax when all of the remaining resource data is being requested and the known length of the remaining data exceeds NSIntegerMax.
*/
@property (nonatomic, readonly) NSInteger requestedLength;

/*! 
 @property 		requestsAllDataToEndOfResource
 @abstract		Specifies that the entire remaining length of the resource from requestedOffset to the end of the resource is being requested.
 @discussion	When requestsAllDataToEndOfResource has a value of YES, you should disregard the value of requestedLength and incrementally provide as much data starting from the requestedOffset as the resource contains, until you have provided all of the available data successfully and invoked -finishLoading, until you have encountered a failure and invoked -finishLoadingWithError:, or until you have received -resourceLoader:didCancelLoadingRequest: for the AVAssetResourceLoadingRequest from which the AVAssetResourceLoadingDataRequest was obtained.
*/
@property (nonatomic, readonly) BOOL requestsAllDataToEndOfResource NS_AVAILABLE(10_11, 9_0);

/*! 
 @property 		currentOffset
 @abstract		The position within the resource of the next byte within the resource following the bytes that have already been provided via prior invocations of -respondWithData.
*/
@property (nonatomic, readonly) long long currentOffset;

/*! 
 @method 		respondWithData:   
 @abstract		Provides data to the receiver.
 @param			data
 				An instance of NSData containing some or all of the requested bytes.
 @discussion	May be invoked multiple times on the same instance of AVAssetResourceLoadingDataRequest to provide the full range of requested data incrementally. Upon each invocation, the value of currentOffset will be updated to accord with the amount of data provided.
*/
- (void)respondWithData:(NSData *)data;

@end

@interface AVAssetResourceLoader (AVAssetResourceLoaderContentKeySupport)

/*!
 @property 		preloadsEligibleContentKeys
 @abstract		When YES, eligible content keys will be loaded as eagerly as possible, potentially handled by the delegate. Setting to YES may result in network activity.
 @discussion	Any work done as a result of setting this property will be performed asynchronously.
*/
@property (nonatomic) BOOL preloadsEligibleContentKeys NS_AVAILABLE(10_11, 9_0);

@end

@interface AVAssetResourceLoadingRequest (AVAssetResourceLoadingRequestContentKeyRequestSupport)

/*! 
 @method 		streamingContentKeyRequestDataForApp:contentIdentifier:options:error:   
 @abstract		Obtains a streaming content key request for a specific combination of application and content.
 @param			appIdentifier
 				An opaque identifier for the application. The value of this identifier depends on the particular system used to provide the decryption key.
 @param			contentIdentifier
 				An opaque identifier for the content. The value of this identifier depends on the particular system used to provide the decryption key.
 @param			options
 				Additional information necessary to obtain the key, or nil if none.
 @param			error
 				If obtaining the streaming content key request fails, will be set to an instance of NSError describing the failure.
 @result		The key request data that must be transmitted to the key vendor to obtain the content key.
*/
- (nullable NSData *)streamingContentKeyRequestDataForApp:(NSData *)appIdentifier contentIdentifier:(NSData *)contentIdentifier options:(nullable NSDictionary<NSString *, id> *)options error:(NSError * _Nullable * _Nullable)outError;

/*! 
 @method 		persistentContentKeyFromKeyVendorResponse:options:error:
 @abstract		Obtains a persistable content key from a context.
 @param			keyVendorResponse
 				The response returned from the key vendor as a result of a request generated from streamingContentKeyRequestDataForApp:contentIdentifier:options:error:.
 @param			options
 				Additional information necessary to obtain the persistable content key, or nil if none.
 @param			error
 				If obtaining the persistable content key fails, will be set to an instance of NSError describing the failure.
 @result		The persistable content key data that may be stored offline to answer future loading requests of the same content key.
 @discussion	The data returned from this method may be used to immediately satisfy an AVAssetResourceLoadingDataRequest, as well as any subsequent requests for the same key url. The value of AVAssetResourceLoadingContentInformationRequest.contentType must be set to AVStreamingKeyDeliveryPersistentContentKeyType when responding with data created with this method.
*/
- (nullable NSData *)persistentContentKeyFromKeyVendorResponse:(NSData *)keyVendorResponse options:(nullable NSDictionary<NSString *, id> *)options error:(NSError **)outError NS_AVAILABLE_IOS(9_0);

@end

// Options keys for use with -[AVAssetResourceLoadingRequest streamingContentKeyRequestDataForApp:contentIdentifier:trackID:options:error:]
/*!
 @constant		AVAssetResourceLoadingRequestStreamingContentKeyRequestRequiresPersistentKey
 @abstract		Specifies whether the content key request should require a persistable key to be returned from the key vendor. Value should be a NSNumber created with +[NSNumber numberWithBool:].
*/
AVF_EXPORT NSString *const AVAssetResourceLoadingRequestStreamingContentKeyRequestRequiresPersistentKey API_AVAILABLE(ios(9.0), tvos(9.0)) __WATCHOS_PROHIBITED;

@interface AVAssetResourceLoadingRequest (AVAssetResourceLoadingRequestDeprecated)

/*! 
 @method 		finishLoadingWithResponse:data:redirect:   
 @abstract		Causes the receiver to finish loading a resource that a delegate has previously assumed responsibility for loading by returning YES as the result of -resourceLoader:shouldWaitForLoadingOfRequestedResource:.
 @param			response
 				The NSURLResponse for the NSURLRequest of the receiver. Should be nil if no response is required.
 @param			data
 				An instance of NSData containing the data of the resource. Should be nil if no such data is available.
 @param			redirect
 				An instance of NSURLRequest indicating a redirect of the loading request. Should be nil if no redirect is needed.
 @discussion	This method is deprecated. Use the following methods instead.
					-[AVAssetResourceLoadingRequest setResponse:] to set the response property,
					-[AVAssetResourceLoadingRequest setRedirect:] to set the redirect property,
					-[AVAssetResourceLoadingDataRequest respondWithData:] to provide data, and
					-[AVAssetResourceLoadingRequest finishLoading] to indicate that loading is finished.
*/
- (void)finishLoadingWithResponse:(nullable NSURLResponse *)response data:(nullable NSData *)data redirect:(nullable NSURLRequest *)redirect NS_DEPRECATED_IOS(6_0, 7_0);

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVQueuedSampleBufferRendering.h
/*
	File:  AVQueuedSampleBufferRendering.h

	Framework:  AVFoundation
 
	Copyright 2013-2017 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <CoreMedia/CMSampleBuffer.h>
#import <CoreMedia/CMSync.h>

NS_ASSUME_NONNULL_BEGIN

/*!
	@enum		 AVQueuedSampleBufferRenderingStatus
	@abstract	 These constants are the possible status values for queued sample buffer renderers.
	@constant	 AVQueuedSampleBufferRenderingStatusUnknown
		Indicates that the receiver is in a fresh state without any sample buffers enqueued on it.
	@constant	 AVQueuedSampleBufferRenderingStatusRendering
		Indicates at least one sample buffer has been enqueued on the receiver.
	@constant	 AVQueuedSampleBufferRenderingStatusFailed
		Indicates that the receiver cannot currently enqueue or render sample buffers because of an error.
 */
typedef NS_ENUM(NSInteger, AVQueuedSampleBufferRenderingStatus) {
	AVQueuedSampleBufferRenderingStatusUnknown,
	AVQueuedSampleBufferRenderingStatusRendering,
	AVQueuedSampleBufferRenderingStatusFailed
} API_AVAILABLE(macos(10.10), ios(8.0), tvos(10.2)) __WATCHOS_PROHIBITED;

/*
	@protocol		AVQueuedSampleBufferRendering
	@abstract		Defines methods for enqueueing CMSampleBuffers for presentation.
	@discussion
		AVSampleBufferDisplayLayer and AVSampleBufferAudioRenderer conform to this protocol.  When used in conjunction with an AVSampleBufferRenderSynchronizer, an object conforming to AVQueuedSampleBufferRendering can only be attached to a single synchronizer.
*/
API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED
@protocol AVQueuedSampleBufferRendering <NSObject>

/*!
	@property		timebase
	@abstract		The renderer's timebase, which governs how time stamps are interpreted.
	@discussion
		The timebase is used to interpret time stamps.
 
		The timebase is read-only.  Use the AVSampleBufferRenderSynchronizer to set the rate or time.
*/
@property (retain, readonly) __attribute__((NSObject)) CMTimebaseRef timebase;

/*!
	@method			enqueueSampleBuffer:
	@abstract		Sends a sample buffer in order to render its contents.
	@discussion
		Video-specific notes:
	
		If sampleBuffer has the kCMSampleAttachmentKey_DoNotDisplay attachment set to kCFBooleanTrue, the frame will be decoded but not displayed.  Otherwise, if sampleBuffer has the kCMSampleAttachmentKey_DisplayImmediately attachment set to kCFBooleanTrue, the decoded image will be displayed as soon as possible, replacing all previously enqueued images regardless of their timestamps.  Otherwise, the decoded image will be displayed at sampleBuffer's output presentation timestamp, as interpreted by the timebase.
					
		To schedule the removal of previous images at a specific timestamp, enqueue a marker sample buffer containing no samples, with the kCMSampleBufferAttachmentKey_EmptyMedia attachment set to kCFBooleanTrue.
					
		IMPORTANT NOTE: attachments with the kCMSampleAttachmentKey_ prefix must be set via CMSampleBufferGetSampleAttachmentsArray and CFDictionarySetValue.  Attachments with the kCMSampleBufferAttachmentKey_ prefix must be set via CMSetAttachment.
*/
- (void)enqueueSampleBuffer:(CMSampleBufferRef)sampleBuffer;

/*!
	@method			flush
	@abstract		Instructs the receiver to discard pending enqueued sample buffers.
	@discussion
		Additional sample buffers can be appended after -flush.
	
		Video-specific notes:
					
		It is not possible to determine which sample buffers have been decoded, so the next frame passed to enqueueSampleBuffer: should be an IDR frame (also known as a key frame or sync sample).
*/
- (void)flush;

/*!
	@property		readyForMoreMediaData
	@abstract		Indicates the readiness of the receiver to accept more sample buffers.
	@discussion
		An object conforming to AVQueuedSampleBufferRendering keeps track of the occupancy levels of its internal queues for the benefit of clients that enqueue sample buffers from non-real-time sources -- i.e., clients that can supply sample buffers faster than they are consumed, and so need to decide when to hold back.
					
		Clients enqueueing sample buffers from non-real-time sources may hold off from generating or obtaining more sample buffers to enqueue when the value of readyForMoreMediaData is NO.
					
		It is safe to call enqueueSampleBuffer: when readyForMoreMediaData is NO, but it is a bad idea to enqueue sample buffers without bound.
					
		To help with control of the non-real-time supply of sample buffers, such clients can use -requestMediaDataWhenReadyOnQueue:usingBlock in order to specify a block that the receiver should invoke whenever it's ready for sample buffers to be appended.
 
		The value of readyForMoreMediaData will often change from NO to YES asynchronously, as previously supplied sample buffers are decoded and rendered.
	
		This property is not key value observable.
*/
@property (readonly, getter=isReadyForMoreMediaData) BOOL readyForMoreMediaData;

/*!
	@method			requestMediaDataWhenReadyOnQueue:usingBlock:
	@abstract		Instructs the target to invoke a client-supplied block repeatedly, at its convenience, in order to gather sample buffers for playback.
	@discussion
		The block should enqueue sample buffers to the receiver either until the receiver's readyForMoreMediaData property becomes NO or until there is no more data to supply. When the receiver has decoded enough of the media data it has received that it becomes ready for more media data again, it will invoke the block again in order to obtain more.

		If this method is called multiple times, only the last call is effective. Call stopRequestingMediaData to cancel this request.

		Each call to requestMediaDataWhenReadyOnQueue:usingBlock: should be paired with a corresponding call to stopRequestingMediaData:. Releasing the AVQueuedSampleBufferRendering object without a call to stopRequestingMediaData will result in undefined behavior.
*/
- (void)requestMediaDataWhenReadyOnQueue:(dispatch_queue_t)queue usingBlock:(void (^)(void))block;

/*!
	@method			stopRequestingMediaData
	@abstract		Cancels any current requestMediaDataWhenReadyOnQueue:usingBlock: call.
	@discussion
		This method may be called from outside the block or from within the block.
*/
- (void)stopRequestingMediaData;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCompositionTrack.h
/*
	File:  AVCompositionTrack.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAssetTrack.h>
#import <CoreMedia/CMTime.h>
#import <CoreMedia/CMTimeRange.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAsset;
@class AVComposition;

/*!
    @class          AVCompositionTrack

    @abstract       AVCompositionTrack offers the low-level representation of tracks of AVCompositions, comprising a media type, a track identifier, and an array of AVCompositionTrackSegments, each comprising a URL, and track identifier, and a time mapping.
*/

@class AVCompositionTrackInternal;
@class AVCompositionTrackSegment;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVCompositionTrack : AVAssetTrack
{
@private
    AVCompositionTrackInternal    *_priv;
}

/*!
    @property       segments
    @abstract       Provides read-only access to the array of track segments, each an instance of AVCompositionTrackSegment.
    @discussion
      Note that timeMapping.target.start of the first AVCompositionTrackSegment must be kCMTimeZero, and the timeMapping.target.start of each subsequent AVCompositionTrackSegment must equal CMTimeRangeGetEnd(the previous AVCompositionTrackSegment's timeMapping.target).
      Use -validateTrackSegments:error: to perform a test to ensure that an array of AVCompositionTrackSegments conforms to this rule.
*/
@property (nonatomic, readonly, copy) NSArray<AVCompositionTrackSegment *> *segments;

/*!
	@method			segmentForTrackTime:
	@abstract		Supplies the AVCompositionTrackSegment from the segments array with a target timeRange that either contains the specified track time or is the closest to it among the target timeRanges of the track's segments.
	@param			trackTime
					The trackTime for which an AVCompositionTrackSegment is requested.
	@result			An AVCompositionTrackSegment.
	@discussion		If the trackTime does not map to a sample presentation time (e.g. it's outside the track's timeRange), the segment closest in time to the specified trackTime is returned. 
 */
- (nullable AVCompositionTrackSegment *)segmentForTrackTime:(CMTime)trackTime;

@end


/*!
    @class          AVMutableCompositionTrack

    @abstract       AVMutableCompositionTrack provides a convenient interface for insertions, removals, and scaling of track segments without direct manipulation of their low-level representation.
*/

@class AVMutableCompositionTrackInternal;

NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVMutableCompositionTrack : AVCompositionTrack
{
@private
    AVMutableCompositionTrackInternal    *_mutablePriv __attribute__((unused));
}

/*!
    @property       naturalTimeScale
    @abstract       Indicates a timescale in which time values for the track can be operated upon without extraneous numerical conversion.
    @discussion
      If not set, the value is the naturalTimeScale of the first non-empty edit, or 600 if there are no non-empty edits.
      Set to 0 to revert to default behavior.
*/
@property (nonatomic) CMTimeScale naturalTimeScale;

/*!
    @property       languageCode
    @abstract       Indicates the language associated with the track, as an ISO 639-2/T language code.
    @discussion
      The default value is nil.
*/
@property (nonatomic, copy, nullable) NSString *languageCode;

/*!
    @property       extendedLanguageTag
    @abstract       Indicates the language tag associated with the track, as an IETF BCP 47 (RFC 4646) language identifier.
    @discussion
      The default value is nil.
*/
@property (nonatomic, copy, nullable) NSString *extendedLanguageTag;

/*!
    @property       preferredTransform
    @abstract       The preferred transformation of the visual media data for display purposes.
    @discussion
      The default value is CGAffineTransformIdentity.
*/
@property (nonatomic) CGAffineTransform preferredTransform;

/*!
    @property       preferredVolume
    @abstract       The preferred volume of the audible media data.
    @discussion
      The default value is 1.0.
*/
@property (nonatomic) float preferredVolume;

/*!
    @property       segments
    @abstract       Provides read/write access to the array of track segments, each an instance of AVCompositionTrackSegment.
    @discussion
      Note that timeMapping.target.start of the first AVCompositionTrackSegment must be kCMTimeZero, and the timeMapping.target.start of each subsequent AVCompositionTrackSegment must equal CMTimeRangeGetEnd(the previous AVCompositionTrackSegment's timeMapping.target).
      Use -validateTrackSegments:error: to perform a test to ensure that an array of AVCompositionTrackSegments conforms to this rule.
*/
@property (nonatomic, copy, null_resettable) NSArray<AVCompositionTrackSegment *> *segments;

/*!
    @method         insertTimeRange:ofTrack:atTime:error:
    @abstract       Inserts a timeRange of a source track into a track of a composition.
    @param          timeRange
                    Specifies the timeRange of the track to be inserted.
    @param          track
                    Specifies the source track to be inserted. Only AVAssetTracks of AVURLAssets and AVCompositions are supported (AVCompositions starting in MacOS X 10.10 and iOS 8.0).
    @param          startTime
                    Specifies the time at which the inserted track is to be presented by the composition track. You may pass kCMTimeInvalid for startTime to indicate that the timeRange should be appended to the end of the track.
    @param          error
                    Describes failures that may be reported to the user, e.g. the asset that was selected for insertion in the composition is restricted by copy-protection.
    @result         A BOOL value indicating the success of the insertion.
    @discussion
      You provide a reference to an AVAssetTrack and the timeRange within it that you want to insert. You specify the start time in the target composition track at which the timeRange should be inserted.

      Note that the inserted track timeRange will be presented at its natural duration and rate. It can be scaled to a different duration (and presented at a different rate) via -scaleTimeRange:toDuration:.
*/
- (BOOL)insertTimeRange:(CMTimeRange)timeRange ofTrack:(AVAssetTrack *)track atTime:(CMTime)startTime error:(NSError * _Nullable * _Nullable)outError;

/*!
    @method         insertTimeRanges:ofTracks:atTime:error:
    @abstract       Inserts the timeRanges of multiple source tracks into a track of a composition.
    @param          timeRanges
                    Specifies the timeRanges to be inserted. An NSArray of NSValues containing CMTimeRange. (See +[NSValue valueWithCMTimeRange:] in AVTime.h.)
    @param          tracks
                    Specifies the source tracks to be inserted. Only AVAssetTracks of AVURLAssets and AVCompositions are supported (AVCompositions starting in MacOS X 10.10 and iOS 8.0).
    @param          startTime
                    Specifies the time at which the inserted tracks are to be presented by the composition track. You may pass kCMTimeInvalid for startTime to indicate that the timeRanges should be appended to the end of the track.
    @param          error
                    Describes failures that may be reported to the user, e.g. the asset that was selected for insertion in the composition is restricted by copy-protection.
    @result         A BOOL value indicating the success of the insertion.
    @discussion
      This method is equivalent to (but more efficient than) calling -insertTimeRange:ofTrack:atTime:error: for each timeRange/track pair. If this method returns an error, none of the time ranges will be inserted into the composition track. To specify an empty time range, pass NSNull for the track and a time range of starting at kCMTimeInvalid with a duration of the desired empty edit.
*/
- (BOOL)insertTimeRanges:(NSArray<NSValue *> *)timeRanges ofTracks:(NSArray<AVAssetTrack *> *)tracks atTime:(CMTime)startTime error:(NSError * _Nullable * _Nullable)outError NS_AVAILABLE(10_8, 5_0);

/*!
    @method         insertEmptyTimeRange:
    @abstract       Adds or extends an empty timeRange within the composition track.
    @param          timeRange
                    Specifies the empty timeRange to be inserted.
    @discussion
      If you insert an empty timeRange into the track, any media that was presented during that interval prior to the insertion will be presented instead immediately afterward.
      The exact meaning of the term "empty timeRange" depends upon the mediaType of the track. For example, an empty timeRange in a sound track presents silence.
      Note that you cannot add empty time ranges to the end of a composition track.
*/
- (void)insertEmptyTimeRange:(CMTimeRange)timeRange;

/*!
    @method         removeTimeRange:
    @abstract       Removes a specified timeRange from the track.
    @param          timeRange
                    Specifies the timeRange to be removed.
    @discussion
      Removal of a timeRange does not cause the track to be removed from the composition. Instead it removes or truncates track segments that intersect with the timeRange.
*/
- (void)removeTimeRange:(CMTimeRange)timeRange;

/*!
    @method         scaleTimeRange:toDuration:
    @abstract       Changes the duration of a timeRange of the track.
    @param          timeRange
                    Specifies the timeRange of the track to be scaled.
    @param          duration
                    Specifies the new duration of the timeRange.
    @discussion
      Each trackSegment affected by the scaling operation will be presented at a rate equal to source.duration / target.duration of its resulting timeMapping.
*/
- (void)scaleTimeRange:(CMTimeRange)timeRange toDuration:(CMTime)duration;

/*!
    @method         validateTrackSegments:error:
    @abstract       Tests an array of AVCompositionTrackSegments to determine whether they conform to the timing rules noted above (see the property key @"trackSegments").
    @param          trackSegments
                    The array of AVCompositionTrackSegments to be validated.
    @param          error
                    If validation fais, returns information about the failure.
    @result         YES if validation suceeds, otherwise NO.
    @discussion
      The array is tested for suitability for setting as the value of the trackSegments property. If a portion of an existing trackSegments array is to be modified, the modification can be made via an instance of NSMutableArray, and the resulting array can be tested via -validateTrackSegments:error:.
*/
- (BOOL)validateTrackSegments:(NSArray<AVCompositionTrackSegment *> *)trackSegments error:(NSError * _Nullable * _Nullable)outError;

/*!
	@method			addTrackAssociationToTrack:type:
	@abstract		Establishes a track association of a specific type between two tracks.
	@param			compositionTrack
					An AVCompositionTrack object that is to be associated with the receiver.
	@param			trackAssociationType
					The type of track association to add between the receiver and the specified compositionTrack (for instance, AVTrackAssociationTypeChapterList).
*/
- (void)addTrackAssociationToTrack:(AVCompositionTrack *)compositionTrack type:(AVTrackAssociationType)trackAssociationType API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0));

/*!
	@method			removeTrackAssociationToTrack:type:
	@abstract		Removes a track association of a specific type between two tracks.
	@param			compositionTrack
					An AVCompositionTrack object that is associated with the receiver.
	@param			trackAssociationType
					The type of track association to remove between the receiver and the specified compositionTrack (for instance, AVTrackAssociationTypeChapterList).
*/
- (void)removeTrackAssociationToTrack:(AVCompositionTrack *)compositionTrack type:(AVTrackAssociationType)trackAssociationType API_AVAILABLE(macos(10.14), ios(12.0), tvos(12.0), watchos(5.0));

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAsynchronousKeyValueLoading.h
/*
    File:  AVAsynchronousKeyValueLoading.h
 
    Framework:  AVFoundation
 
	Copyright 2010-2016 Apple Inc. All rights reserved.
 
 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

typedef NS_ENUM(NSInteger, AVKeyValueStatus) {
	AVKeyValueStatusUnknown,
	AVKeyValueStatusLoading,
	AVKeyValueStatusLoaded,
	AVKeyValueStatusFailed,
	AVKeyValueStatusCancelled
};

/*!
	@protocol	AVAsynchronousKeyValueLoading
 
	@abstract	The AVAsynchronousKeyValueLoading protocol defines methods that let clients use an AVAsset or AVAssetTrack object without blocking a thread. Using methods in the protocol, one can find out the current status of a key (for example, whether the corresponding value has been loaded); and ask the object to load values asynchronously, informing the client when the operation has completed.
 
	@discussion
		Because of the nature of timed audiovisual media, successful initialization of an asset does not necessarily mean that all its data is immediately available. Instead, an asset will wait to load data until an operation is performed on it (for example, directly invoking any relevant AVAsset methods, playback via an AVPlayerItem object, export using AVAssetExportSession, reading using an instance of AVAssetReader, and so on). This means that although you can request the value of any key at any time, and its value will be returned synchronously, the calling thread may be blocked until the request can be satisfied. To avoid blocking, you can:

			1. First, determine whether the value for a given key is available using statusOfValueForKey:error:.
			2. If a value has not been loaded yet, you can ask for to load one or more values and be notified when they become available using loadValuesAsynchronouslyForKeys:completionHandler:.
		
		Even for use cases that may typically support ready access to some keys (such as for assets initialized with URLs for files in the local filesystem), slow I/O may require AVAsset to block before returning their values. Although blocking may be acceptable for OS X API clients in cases where assets are being prepared on background threads or in operation queues, in all cases in which blocking should be avoided you should use loadValuesAsynchronouslyForKeys:completionHandler:. For iOS clients, blocking to obtain the value of a key synchronously is never recommended under any circumstances.
*/
@protocol AVAsynchronousKeyValueLoading
@required

/*!
  @method		statusOfValueForKey:
  @abstract		Reports whether the value for a key is immediately available without blocking.
  @param		key
    An instance of NSString containing the specified key.
  @param		outError
    If the status of the value for the key is AVKeyValueStatusFailed, *outError is set to a non-nil NSError that describes the failure that occurred.
  @result		The value's current loading status.
  @discussion
    Clients can use -statusOfValueForKey: to determine the availability of the value of any key of interest. However, this method alone does not prompt the receiver to load the value of a key that's not yet available. To request values for keys that may not already be loaded, without blocking, use -loadValuesAsynchronouslyForKeys:completionHandler:, await invocation of the completion handler, and test the availability of each key via -statusOfValueForKey: before invoking its getter.
 
    Even if access to values of some keys may be readily available, as can occur with receivers initialized with URLs for resources on local volumes, extensive I/O or parsing may be needed for these same receivers to provide values for other keys. A duration for a local MP3 file, for example, may be expensive to obtain, even if the values for other AVAsset properties may be trivial to obtain.

    Blocking that may occur when calling the getter for any key should therefore be avoided in the general case by loading values for all keys of interest via -loadValuesAsynchronouslyForKeys:completionHandler: and testing the availability of the requested values before fetching them by calling getters.
      
    The sole exception to this general rule is in usage on Mac OS X on the desktop, where it may be acceptable to block in cases in which the client is preparing objects for use on background threads or in operation queues. On iOS, values should always be loaded asynchronously prior to calling getters for the values, in any usage scenario.
*/
- (AVKeyValueStatus)statusOfValueForKey:(NSString *)key error:(NSError * _Nullable * _Nullable)outError;

/*!
  @method		loadValuesAsynchronouslyForKeys:completionHandler:
  @abstract		Directs the target to load the values of any of the specified keys that are not already loaded.
  @param		keys
    An instance of NSArray, containing NSStrings for the specified keys.
  @param		completionHandler
    The block to be invoked when loading succeeds, fails, or is cancelled.
*/
- (void)loadValuesAsynchronouslyForKeys:(NSArray<NSString *> *)keys completionHandler:(nullable void (^)(void))handler;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioProcessingSettings.h
/*
    File:  AVAudioProcessingSettings.h
 
    Framework:  AVFoundation
 
	Copyright 2013-2018 Apple Inc. All rights reserved.
 
 */

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>


/*!
 @typedef AVAudioTimePitchAlgorithm
 @abstract
    The type of a time pitch algorithm.
 @discussion
	On OS X, the default algorithm for all time pitch operations is AVAudioTimePitchAlgorithmSpectral.  On iOS, the default algorithm for playback is AVAudioTimePitchAlgorithmLowQualityZeroLatency and the default for export & other offline processing is AVAudioTimePitchAlgorithmSpectral.

	For scaled audio edits, i.e. when the timeMapping of an AVAssetTrackSegment is between timeRanges of unequal duration, it is important to choose an algorithm that supports the full range of edit rates present in the source media.  AVAudioTimePitchAlgorithmSpectral is often the best choice due to the highly inclusive range of rates it supports, assuming that it is desirable to maintain a constant pitch regardless of the edit rate.  If it is instead desirable to allow the pitch to vary with the edit rate, AVAudioTimePitchAlgorithmVarispeed is the best choice.
 
*/
typedef NSString * AVAudioTimePitchAlgorithm NS_STRING_ENUM;

/*!
 @abstract		Values for time pitch algorithm
 
 @constant      AVAudioTimePitchAlgorithmLowQualityZeroLatency
				Low quality, very inexpensive. Suitable for brief fast-forward/rewind effects, low quality voice.
                Rate snapped to {0.5, 0.666667, 0.8, 1.0, 1.25, 1.5, 2.0}.

 @constant      AVAudioTimePitchAlgorithmTimeDomain
				Modest quality, less expensive. Suitable for voice.
                Variable rate from 1/32 to 32.

 @constant      AVAudioTimePitchAlgorithmSpectral
				Highest quality, most computationally expensive. Suitable for music.
                Variable rate from 1/32 to 32.

 @constant      AVAudioTimePitchAlgorithmVarispeed
				High quality, no pitch correction. Pitch varies with rate.
                Variable rate from 1/32 to 32.
 
*/
AVF_EXPORT AVAudioTimePitchAlgorithm const AVAudioTimePitchAlgorithmLowQualityZeroLatency API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(macos) __WATCHOS_PROHIBITED;
AVF_EXPORT AVAudioTimePitchAlgorithm const AVAudioTimePitchAlgorithmTimeDomain NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVAudioTimePitchAlgorithm const AVAudioTimePitchAlgorithmSpectral NS_AVAILABLE(10_9, 7_0);
AVF_EXPORT AVAudioTimePitchAlgorithm const AVAudioTimePitchAlgorithmVarispeed NS_AVAILABLE(10_9, 7_0);
// ==========  AVFoundation.framework/Headers/AVPlayerItemTrack.h
/*
	File:  AVPlayerItemTrack.h

	Framework:  AVFoundation
 
	Copyright 2010-2018 Apple Inc. All rights reserved.

*/

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

@class AVPlayerImageProvider;
@class AVAssetTrack;
@class AVPlayerItemTrackInternal;

NS_ASSUME_NONNULL_BEGIN

/*!
	@class			AVPlayerItemTrack

	@abstract
		An AVPlayerItemTrack carries a reference to an AVAssetTrack as well as presentation settings for that track.

	@discussion
		Note that inspection of assets tracks is provided by AVAssetTrack.
		This class is intended to represent presentation state for a track of an asset that's played by an AVPlayer and AVPlayerItem.

		To ensure safe access to AVPlayerItemTrack's nonatomic properties while dynamic changes in playback state may be reported,
		clients must serialize their access with the associated AVPlayer's notification queue. In the common case, such serialization
		is naturally achieved by invoking AVPlayerItemTrack's various methods on the main thread or queue.
*/
NS_CLASS_AVAILABLE(10_7, 4_0)
@interface AVPlayerItemTrack : NSObject
{
@private
	AVPlayerItemTrackInternal	*_playerItemTrack;
}

/*!
 @property		assetTrack
 @abstract		Indicates the AVAssetTrack for which the AVPlayerItemTrack represents presentation state.
 @discussion	This property is not observable.
	Clients must serialize their access to the resulting AVAssetTrack and related objects on the associated AVPlayer's
	notification queue.  By default, this queue is the main queue.
*/
@property (nonatomic, readonly, nullable) AVAssetTrack *assetTrack;

/*!
 @property		enabled
 @abstract		Indicates whether the track is enabled for presentation during playback.
*/
@property (nonatomic, assign, getter=isEnabled) BOOL enabled;

/*!
 @property		currentVideoFrameRate
 @abstract		If the media type of the assetTrack is AVMediaTypeVideo, indicates the current frame rate of the track as it plays, in units of frames per second. If the item is not playing, or if the media type of the track is not video, the value of this property is 0.
 @discussion	This property is not observable.
*/
@property (nonatomic, readonly) float currentVideoFrameRate NS_AVAILABLE(10_9, 7_0);

#if (TARGET_OS_MAC && !(TARGET_OS_EMBEDDED || TARGET_OS_IPHONE))

/*!
 @constant		AVPlayerItemTrackVideoFieldModeDeinterlaceFields
 @abstract		Use with videoFieldMode property to request deinterlacing of video fields.
*/
AVF_EXPORT NSString *const AVPlayerItemTrackVideoFieldModeDeinterlaceFields NS_AVAILABLE_MAC(10_10);

/*!
 @property		videoFieldMode
 @abstract		If the media type of the assetTrack is AVMediaTypeVideo, specifies the handling of video frames that contain multiple fields.
 @discussion	A value of nil indicates default processing of video frames. If you want video fields to be deinterlaced, set videoFieldMode to AVPlayerItemTrackVideoFieldModeDeinterlaceFields.
 				You can test whether video being played has multiple fields by examining the underlying AVAssetTrack's format descriptions. See -[AVAssetTrack formatDescriptions] and, for video format descriptions, kCMFormatDescriptionExtension_FieldCount.
*/
@property (nonatomic, copy, nullable) NSString *videoFieldMode NS_AVAILABLE_MAC(10_10);

#endif

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitReverb.h
/*
	File:           AVAudioUnitReverb.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitReverb.h>

// ==========  AVFoundation.framework/Headers/AVPlayerMediaSelectionCriteria.h
/*
	File:  AVPlayerMediaSelectionCriteria.h
 
	Framework:  AVFoundation
 
	Copyright 2012-2018 Apple Inc. All rights reserved.
 
 */

/*!
	@class		AVPlayerMediaSelectionCriteria
	
	@abstract	The AVPlayerMediaSelectionCriteria class specifies the preferred languages and media characteristics for an AVPlayer instance.
 
	@discussion
		The languages and media characteristics of assets containing media selection options that an AVPlayer instance should attempt to select automatically when preparing and playing items. The languages and media characteristics are specified in the preferred order.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVMediaFormat.h>
#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

NS_CLASS_AVAILABLE(10_9, 7_0)
@interface AVPlayerMediaSelectionCriteria : NSObject {
@private
	void *_criteriaInternal;
}

/* An NSArray of NSStrings containing language identifiers, in order of desirability, that are preferred for selection. Can be nil. Languages can be indicated via BCP 47 language identifiers or via ISO 639-2/T language codes. If no option with any of the preferred languages is available, a selection will be made according to indications for the default enabling and disabling of media options as stored in the asset.
   When making selections, AVPlayer treats the preference for languages as the paramount criterion.
 */
@property (nonatomic, readonly, nullable) NSArray<NSString *> *preferredLanguages;

/* An NSArray of NSStrings indicating additional media characteristics, in order of desirability, that are preferred when selecting media with the characteristic for which the receiver is set on the AVPlayer as the selection criteria. Can be nil. See AVMediaFormat.h for declarations of media characteristics of the form AVMediaCharacteristic*. For example, desirable characteristics of legible media may include AVMediaCharacteristicTranscribesSpokenDialogForAccessibility and AVMediaCharacteristicDescribesMusicAndSoundForAccessibility. Simiarly, desirable characteristics of audible media may include AVMediaCharacteristicDescribesVideoForAccessibility.
   If no option is found that possesses all of the desired characteristics, the option that best matches the desired characteristics will be selected.
   When making automatic selections, AVPlayer treats the preference for additional media characteristics as a criterion that's secondary to language preference.
 */
@property (nonatomic, readonly, nullable) NSArray<AVMediaCharacteristic> *preferredMediaCharacteristics;

/*!
  @method		initWithPreferredLanguages:preferredMediaCharacteristics:
  @abstract		Creates an instance of AVPlayerMediaSelectionCriteria.
  @param		preferredLanguages
				An NSArray of NSStrings containing language identifiers, in order of desirability, that are preferred for selection. Can be nil.
  @param		preferredMediaCharacteristics
				An NSArray of NSStrings indicating additional media characteristics, in order of desirability, that are preferred when selecting media with the characteristic for which the receiver is set on the AVPlayer as the selection criteria. Can be nil.
  @result		An instance of AVPlayerMediaSelectionCriteria.
*/
- (instancetype)initWithPreferredLanguages:(nullable NSArray<NSString *> *)preferredLanguages preferredMediaCharacteristics:(nullable NSArray<AVMediaCharacteristic> *)preferredMediaCharacteristics;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVDepthData.h
/*
    File:  AVDepthData.h
 
    Framework:  AVFoundation
 
    Copyright 2016-2017 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVCameraCalibrationData.h>
#import <Foundation/Foundation.h>
#import <CoreVideo/CVPixelBufferPool.h>
#import <ImageIO/CGImageProperties.h>

NS_ASSUME_NONNULL_BEGIN

/*
 @enum AVDepthDataQuality
 @abstract
    Constants indicating the overall quality of depth data map values.
 
 @constant AVDepthDataQualityLow
    The overall quality of values within the map is low (too many invalid depth values).
 @constant AVDepthDataQualityHigh
    The overall quality of values within the map is high.
 
 @discussion
    Depth data maps are typically generated by comparing images and calculating disparity. If features are lacking in either input image, it may be difficult to find matching keypoints, resulting in a depth data map with substantial holes. These holes can be filled with depth data filtering, but still may produce a map of overall poor quality. If a depth data map suffers from insufficient features, it is marked as low quality, and should be considered a substandard candidate for rendering high-quality depth effects or reconstructing a 3D scene. High quality depth is feature-rich, contains a high level of detail, and should be considered a good candidate for rendering high-quality depth effects or reconstructing a 3D scene.
*/
typedef NS_ENUM(NSInteger, AVDepthDataQuality) {
    AVDepthDataQualityLow    = 0,
    AVDepthDataQualityHigh   = 1,
} API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

/*
 @enum AVDepthDataAccuracy
 @abstract
    Constants indicating the accuracy of the units expressed by depth data map values.

 @constant AVDepthDataAccuracyRelative
    Values within the depth data map are usable for foreground / background separation, but are not absolutely accurate in the physical world.
 @constant AVDepthDataAccuracyAbsolute
    Values within the depth map are absolutely accurate within the physical world.
 
 @discussion
    The accuracy of a depth data map is highly dependent on the camera calibration data used to generate it. If the camera's focal length cannot be precisely determined at the time of capture, scaling error in the z (depth) plane will be introduced. If the camera's optical center can't be precisely determined at capture time, principal point error will be introduced, leading to an offset error in the disparity estimate. AVDepthDataAccuracy constants report the accuracy of a map's values with respect to its reported units. If the accuracy is reported to be AVDepthDataAccuracyRelative, the values within the map are usable relative to one another (that is, larger depth values are farther away than smaller depth values), but do not accurately convey real world distance. Disparity maps with relative accuracy may still be used to reliably determine the difference in disparity between two points in the same map.
*/
typedef NS_ENUM(NSInteger, AVDepthDataAccuracy) {
    AVDepthDataAccuracyRelative    = 0,
    AVDepthDataAccuracyAbsolute    = 1,
} API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED;

@class AVDepthDataInternal;

/*!
 @class AVDepthData
 @abstract
    An object wrapping a map of disparity or depth pixel data, plus metadata.
 
 @discussion
    "Depth Data" is a generic term for a map of pixel data containing depth-related information. AVDepthData wraps a disparity or depth map and provides conversion methods, focus information, and camera calibration data to aid in using the map for rendering or computer vision tasks. CoreVideo supports the following four depth data pixel formats:
          kCVPixelFormatType_DisparityFloat16    = 'hdis'
          kCVPixelFormatType_DisparityFloat32    = 'fdis'
          kCVPixelFormatType_DepthFloat16        = 'hdep'
          kCVPixelFormatType_DepthFloat32        = 'fdep'
 
    The disparity formats describe normalized shift values when comparing two images. Units are 1/meters: ( pixelShift / (pixelFocalLength * baselineInMeters) ). 
    The depth formats describe the distance to an object in meters.
 
    Disparity / depth maps are generated from camera images containing non-rectilinear data. Camera lenses have small imperfections that cause small distortions in their resultant images compared to a pinhole camera. AVDepthData maps contain non-rectilinear (non-distortion-corrected) data as well. Their values are warped to match the lens distortion characteristics present in their accompanying YUV image. Therefore an AVDepthData map can be used as a proxy for depth when rendering effects to its accompanying image, but not to correlate points in 3D space. In order to use AVDepthData for computer vision tasks, you should use its accompanying camera calibration data to rectify the depth data (see AVCameraCalibrationData).
 
    When capturing depth data from a camera using AVCaptureDepthDataOutput, AVDepthData objects are delivered to your AVCaptureDepthDataOutputDelegate in a streaming fashion. When capturing depth data along with photos using AVCapturePhotoOutput, depth data is delivered to your AVCapturePhotoCaptureDelegate as a property of an AVCapturePhoto (see -[AVCapturePhotoCaptureDelegate captureOutput:didFinishProcessingPhoto:error:]). When working with image files containing depth information, AVDepthData may be instantiated using information obtained from ImageIO. When editing images containing depth information, derivative AVDepthData objects may be instantiated reflecting the edits that have been performed.
 */
API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) __WATCHOS_PROHIBITED
@interface AVDepthData : NSObject
{
@private
    AVDepthDataInternal *_internal;
}

AV_INIT_UNAVAILABLE

/*!
 @method depthDataFromDictionaryRepresentation:error:
 @abstract
    Returns an AVDepthData instance from depth information in an image file.
 
 @param imageSourceAuxDataInfoDictionary
    A dictionary of primitive depth-related information obtained from CGImageSourceCopyAuxiliaryDataInfoAtIndex.
 @param outError
    On return, if the depth data cannot be created, points to an NSError describing the problem.
 @result
    An AVDepthData instance, or nil if the auxiliary data info dictionary was malformed.
 
 @discussion
    When using ImageIO framework's CGImageSource API to read from a HEIF or JPEG file containing depth data, AVDepthData can be instantiated using the result of CGImageSourceCopyAuxiliaryDataInfoAtIndex, which returns a CFDictionary of primitive map information.
 */
+ (nullable instancetype)depthDataFromDictionaryRepresentation:(NSDictionary *)imageSourceAuxDataInfoDictionary error:(NSError * _Nullable * _Nullable)outError;

/*!
 @method depthDataByConvertingToDepthDataType:
 @abstract
    Returns a converted, derivative AVDepthData instance in the specified depthDataType.
 
 @param depthDataType
    The OSType of depthData object to which you'd like to convert. Must be present in availableDepthDataTypes.
 @result
    An AVDepthData instance.
 
 @discussion
    This method throws an NSInvalidArgumentException if you pass an unrecognized depthDataType. See
 */
- (instancetype)depthDataByConvertingToDepthDataType:(OSType)depthDataType;

/*!
 @method depthDataByApplyingExifOrientation:
 @abstract
    Returns a derivative AVDepthData instance in which the specified Exif orientation has been applied.
 
 @param exifOrientation
    One of the 8 standard Exif orientation tags expressing how the depth data should be rotated / mirrored.
 @result
    An AVDepthData instance.
 
 @discussion
    When applying simple 90 degree rotation or mirroring edits to media containing depth data, you may use this initializer to create a derivative copy of the depth in which the specified orientation is applied to both the underlying pixel map data and the camera calibration data. This method throws an NSInvalidArgumentException if you pass an unrecognized exifOrientation.
 */
- (instancetype)depthDataByApplyingExifOrientation:(CGImagePropertyOrientation)exifOrientation;

/*!
 @method depthDataByReplacingDepthDataMapWithPixelBuffer:error:
 @abstract
    Returns an AVDepthData instance wrapping the replacement depth data map pixel buffer.
 
 @param pixelBuffer
    A pixel buffer containing depth data information in one of the 4 supported disparity / depth pixel formats.
 @param outError
    On return, if the depth data cannot be created, points to an NSError describing the problem.
 @result
    An AVDepthData instance, or nil if the pixel buffer is malformed.
 
 @discussion
    When applying complex edits to media containing depth data, you may create a derivative map with arbitrary transforms applied to it, then use this initializer to create a new AVDepthData. Note that this new depth data object has no camera calibration data, so its cameraCalibrationData property always returns nil.
 */
- (nullable instancetype)depthDataByReplacingDepthDataMapWithPixelBuffer:(CVPixelBufferRef)pixelBuffer error:(NSError * _Nullable * _Nullable)outError;

/*!
 @property availableDepthDataTypes
 @abstract
    Specifies which depth data pixel formats may be used with depthDataByConvertingToDepthDataType:.
 
 @discussion
    This property presents the available pixel format types as an array of NSNumbers, each wrapping an OSType (CV pixel format type).
 */
@property(readonly) NSArray<NSNumber *> *availableDepthDataTypes;

/*!
 @method dictionaryRepresentationForAuxiliaryDataType:
 @abstract
    Returns a dictionary of primitive map information to be used when writing an image file with depth data.
 
 @param outAuxDataType
    On output, either kCGImageAuxiliaryDataTypeDisparity or kCGImageAuxiliaryDataTypeDepth, depending on the depth data's file.
 @result
    A dictionary of CGImageDestination compatible depth information, or nil if the auxDataType is unsupported.
 
 @discussion
    When using ImageIO framework's CGImageDestination API to write depth data to a HEIF or JPEG file, you may use this method to generate a dictionary of primitive map information consumed by CGImageDestinationAddAuxiliaryDataInfo.
 */
- (nullable NSDictionary *)dictionaryRepresentationForAuxiliaryDataType:(NSString * _Nullable * _Nullable)outAuxDataType;

/*!
 @property depthDataType
 @abstract
    Specifies the pixel format type of this depth data object's internal map.
 
 @discussion
    One of kCVPixelFormatType_DisparityFloat16, kCVPixelFormatType_DisparityFloat32, kCVPixelFormatType_DepthFloat16, or kCVPixelFormatType_DepthFloat32.
 */
@property(readonly) OSType depthDataType;

/*!
 @property depthDataMap
 @abstract
    Provides access to the depth data object's internal map.
 
 @discussion
    The depth data map's pixel format can be queried using the depthDataType property.
 */
@property(readonly) __attribute__((NSObject)) CVPixelBufferRef depthDataMap NS_RETURNS_INNER_POINTER;

/*!
 @property depthDataQuality
 @abstract
    Specifies the overall quality of the depth data map's values.
 
 @discussion
    See AVDepthDataQuality documentation for more information.
 */
@property(readonly) AVDepthDataQuality depthDataQuality;

/*!
 @property depthDataFiltered
 @abstract
    Specifies whether the depth data pixel buffer map contains filtered (hole-filled) data.
 
 @discussion
    By setting either AVCaptureDepthDataOutput's filteringEnabled property or AVCapturePhotoSettings' depthDataFiltered property to YES, the resulting depth data are filtered to remove invalid pixel values that may be present due to a variety of factors including low light and lens occlusion. If you've requested depth data filtering, all depth data holes are filled. Note that filtering the depth data makes it more usable for applying effects, but alters the data such that it may no longer be suitable for computer vision tasks. Unfiltered depth maps present missing data as NaN.
 */
@property(readonly, getter=isDepthDataFiltered) BOOL depthDataFiltered;

/*!
 @property depthDataAccuracy
 @abstract
    Specifies the accuracy of the units in the depth data map's values.
 
 @discussion
    See AVDepthDataAccuracy documentation for more information.
 */
@property(readonly) AVDepthDataAccuracy depthDataAccuracy;

/*!
 @property cameraCalibrationData
 @abstract
    The calibration data of the camera with which AVDepthData map's values are aligned.
 
 @discussion
    See AVCameraCalibrationData for more information.
 */
@property(nullable, readonly) AVCameraCalibrationData *cameraCalibrationData;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVCaptureFileOutput.h
/*
    File:  AVCaptureFileOutput.h
 
    Framework:  AVFoundation
 
    Copyright 2010-2018 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVCaptureOutputBase.h>
#import <AVFoundation/AVVideoSettings.h>
#import <CoreMedia/CMSampleBuffer.h>

NS_ASSUME_NONNULL_BEGIN

#pragma mark AVCaptureFileOutput

@class AVCaptureFileOutputInternal;
@class AVMetadataItem;
@protocol AVCaptureFileOutputDelegate;
@protocol AVCaptureFileOutputRecordingDelegate;

/*!
 @class AVCaptureFileOutput
 @abstract
    AVCaptureFileOutput is an abstract subclass of AVCaptureOutput that provides an interface for writing captured media to files.
 
 @discussion
    This abstract superclass defines the interface for outputs that record media samples to files. File outputs can start recording to a new file using the startRecordingToOutputFileURL:recordingDelegate: method. On successive invocations of this method on Mac OS X, the output file can by changed dynamically without losing media samples. A file output can stop recording using the stopRecording method. Because files are recorded in the background, applications will need to specify a delegate for each new file so that they can be notified when recorded files are finished.
 
    On Mac OS X, clients can also set a delegate on the file output itself that can be used to control recording along exact media sample boundaries using the captureOutput:didOutputSampleBuffer:fromConnection: method.
 
    The concrete subclasses of AVCaptureFileOutput are AVCaptureMovieFileOutput, which records media to a QuickTime movie file, and AVCaptureAudioFileOutput, which writes audio media to a variety of audio file formats.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureFileOutput : AVCaptureOutput
{
@private
    AVCaptureFileOutputInternal *_fileOutputInternal;
}

/*!
 @property delegate
 @abstract
    The receiver's delegate.
 
 @discussion
    The value of this property is an object conforming to the AVCaptureFileOutputDelegate protocol that will be able to monitor and control recording along exact sample boundaries.
 */
@property(nonatomic, assign, nullable) id<AVCaptureFileOutputDelegate> delegate API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @property outputFileURL
 @abstract
    The file URL of the file to which the receiver is currently recording incoming buffers.
 
 @discussion
    The value of this property is an NSURL object containing the file URL of the file currently being written by the receiver. Returns nil if the receiver is not recording to any file.
 */
@property(nonatomic, readonly, nullable) NSURL *outputFileURL;

/*!
 @method startRecordingToOutputFileURL:recordingDelegate:
 @abstract
    Tells the receiver to start recording to a new file, and specifies a delegate that will be notified when recording is finished.
 
 @param outputFileURL
    An NSURL object containing the URL of the output file. This method throws an NSInvalidArgumentException if the URL is not a valid file URL.
 @param delegate
    An object conforming to the AVCaptureFileOutputRecordingDelegate protocol. Clients must specify a delegate so that they can be notified when recording to the given URL is finished.
 
 @discussion
    The method sets the file URL to which the receiver is currently writing output media. If a file at the given URL already exists when capturing starts, recording to the new file will fail.
 
    Clients need not call stopRecording before calling this method while another recording is in progress. On Mac OS X, if this method is invoked while an existing output file was already being recorded, no media samples will be discarded between the old file and the new file.
 
    When recording is stopped either by calling stopRecording, by changing files using this method, or because of an error, the remaining data that needs to be included to the file will be written in the background. Therefore, clients must specify a delegate that will be notified when all data has been written to the file using the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: method. The recording delegate can also optionally implement methods that inform it when data starts being written, when recording is paused and resumed, and when recording is about to be finished.
 
    On Mac OS X, if this method is called within the captureOutput:didOutputSampleBuffer:fromConnection: delegate method, the first samples written to the new file are guaranteed to be those contained in the sample buffer passed to that method.
 
    Note: AVCaptureAudioFileOutput does not support -startRecordingToOutputFileURL:recordingDelegate:. Use -startRecordingToOutputFileURL:outputFileType:recordingDelegate: instead.
 */
- (void)startRecordingToOutputFileURL:(NSURL *)outputFileURL recordingDelegate:(id<AVCaptureFileOutputRecordingDelegate>)delegate;

/*!
 @method stopRecording
 @abstract
    Tells the receiver to stop recording to the current file.
 
 @discussion
    Clients can call this method when they want to stop recording new samples to the current file, and do not want to continue recording to another file. Clients that want to switch from one file to another should not call this method. Instead they should simply call startRecordingToOutputFileURL:recordingDelegate: with the new file URL.
 
    When recording is stopped either by calling this method, by changing files using startRecordingToOutputFileURL:recordingDelegate:, or because of an error, the remaining data that needs to be included to the file will be written in the background. Therefore, before using the file, clients must wait until the delegate that was specified in startRecordingToOutputFileURL:recordingDelegate: is notified when all data has been written to the file using the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: method.
 
    On Mac OS X, if this method is called within the captureOutput:didOutputSampleBuffer:fromConnection: delegate method, the last samples written to the current file are guaranteed to be those that were output immediately before those in the sample buffer passed to that method.
 */
- (void)stopRecording;

/*!
 @property recording
 @abstract
    Indicates whether the receiver is currently recording.
 
 @discussion
    The value of this property is YES when the receiver currently has a file to which it is writing new samples, NO otherwise.
 */
@property(nonatomic, readonly, getter=isRecording) BOOL recording;

/*!
 @property recordingPaused
 @abstract
    Indicates whether recording to the current output file is paused.
 
 @discussion
    This property indicates recording to the file returned by outputFileURL has been previously paused using the pauseRecording method. When a recording is paused, captured samples are not written to the output file, but new samples can be written to the same file in the future by calling resumeRecording.
 */
@property(nonatomic, readonly, getter=isRecordingPaused) BOOL recordingPaused __IOS_PROHIBITED __TVOS_PROHIBITED __WATCHOS_PROHIBITED;

/*!
 @method pauseRecording
 @abstract
    Pauses recording to the current output file.
 
 @discussion
    This method causes the receiver to stop writing captured samples to the current output file returned by outputFileURL, but leaves the file open so that samples can be written to it in the future, when resumeRecording is called. This allows clients to record multiple media segments that are not contiguous in time to a single file.
 
    On Mac OS X, if this method is called within the captureOutput:didOutputSampleBuffer:fromConnection: delegate method, the last samples written to the current file are guaranteed to be those that were output immediately before those in the sample buffer passed to that method.
 */
- (void)pauseRecording __IOS_PROHIBITED __TVOS_PROHIBITED __WATCHOS_PROHIBITED;

/*!
 @method resumeRecording
 @abstract
    Resumes recording to the current output file after it was previously paused using pauseRecording.
 
 @discussion
    This method causes the receiver to resume writing captured samples to the current output file returned by outputFileURL, after recording was previously paused using pauseRecording. This allows clients to record multiple media segments that are not contiguous in time to a single file.
 
    On Mac OS X, if this method is called within the captureOutput:didOutputSampleBuffer:fromConnection: delegate method, the first samples written to the current file are guaranteed to be those contained in the sample buffer passed to that method.
 */
- (void)resumeRecording __IOS_PROHIBITED __TVOS_PROHIBITED __WATCHOS_PROHIBITED;

/*!
 @property recordedDuration
 @abstract
    Indicates the duration of the media recorded to the current output file.
 
 @discussion
    If recording is in progress, this property returns the total time recorded so far.
 */
@property(nonatomic, readonly) CMTime recordedDuration;

/*!
 @property recordedFileSize
 @abstract
    Indicates the size, in bytes, of the data recorded to the current output file.
 
 @discussion
    If a recording is in progress, this property returns the size in bytes of the data recorded so far.
 */
@property(nonatomic, readonly) int64_t recordedFileSize;

/*!
 @property maxRecordedDuration
 @abstract
    Specifies the maximum duration of the media that should be recorded by the receiver.
 
 @discussion
    This property specifies a hard limit on the duration of recorded files. Recording is stopped when the limit is reached and the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: delegate method is invoked with an appropriate error. The default value of this property is kCMTimeInvalid, which indicates no limit.
 */
@property(nonatomic) CMTime maxRecordedDuration;

/*!
 @property maxRecordedFileSize
 @abstract
    Specifies the maximum size, in bytes, of the data that should be recorded by the receiver.
 
 @discussion
    This property specifies a hard limit on the data size of recorded files. Recording is stopped when the limit is reached and the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: delegate method is invoked with an appropriate error. The default value of this property is 0, which indicates no limit.
 */
@property(nonatomic) int64_t maxRecordedFileSize;

/*!
 @property minFreeDiskSpaceLimit
 @abstract
    Specifies the minimum amount of free space, in bytes, required for recording to continue on a given volume.
 
 @discussion
    This property specifies a hard lower limit on the amount of free space that must remain on a target volume for recording to continue. Recording is stopped when the limit is reached and the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: delegate method is invoked with an appropriate error.
 */
@property(nonatomic) int64_t minFreeDiskSpaceLimit;

@end


/*!
 @protocol AVCaptureFileOutputRecordingDelegate
 @abstract
    Defines an interface for delegates of AVCaptureFileOutput to respond to events that occur in the process of recording a single file.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@protocol AVCaptureFileOutputRecordingDelegate <NSObject>

@optional

/*!
 @method captureOutput:didStartRecordingToOutputFileAtURL:fromConnections:
 @abstract
    Informs the delegate when the output has started writing to a file.
 
 @param output
    The capture file output that started writing the file.
 @param fileURL
    The file URL of the file that is being written.
 @param connections
    An array of AVCaptureConnection objects attached to the file output that provided the data that is being written to the file.
 
 @discussion
    This method is called when the file output has started writing data to a file. If an error condition prevents any data from being written, this method may not be called. captureOutput:willFinishRecordingToOutputFileAtURL:fromConnections:error: and captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: will always be called, even if no data is written.
 
    Clients should not assume that this method will be called on a specific thread, and should also try to make this method as efficient as possible.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output didStartRecordingToOutputFileAtURL:(NSURL *)fileURL fromConnections:(NSArray<AVCaptureConnection *> *)connections;

/*!
 @method captureOutput:didPauseRecordingToOutputFileAtURL:fromConnections:
 @abstract
    Called whenever the output is recording to a file and successfully pauses the recording at the request of the client.
 
 @param output
    The capture file output that has paused its file recording.
 @param fileURL
    The file URL of the file that is being written.
 @param connections
    An array of AVCaptureConnection objects attached to the file output that provided the data that is being written to the file.
 
 @discussion
    Delegates can use this method to be informed when a request to pause recording is actually respected. It is safe for delegates to change what the file output is currently doing (starting a new file, for example) from within this method. If recording to a file is stopped, either manually or due to an error, this method is not guaranteed to be called, even if a previous call to pauseRecording was made.
 
    Clients should not assume that this method will be called on a specific thread, and should also try to make this method as efficient as possible.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output didPauseRecordingToOutputFileAtURL:(NSURL *)fileURL fromConnections:(NSArray<AVCaptureConnection *> *)connections API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @method captureOutput:didResumeRecordingToOutputFileAtURL:fromConnections:
 @abstract
    Called whenever the output, at the request of the client, successfully resumes a file recording that was paused.
 
 @param output
    The capture file output that has resumed its paused file recording.
 @param fileURL
    The file URL of the file that is being written.
 @param connections
    An array of AVCaptureConnection objects attached to the file output that provided the data that is being written to the file.
 
 @discussion
    Delegates can use this method to be informed when a request to resume recording is actually respected. It is safe for delegates to change what the file output is currently doing (starting a new file, for example) from within this method. If recording to a file is stopped, either manually or due to an error, this method is not guaranteed to be called, even if a previous call to resumeRecording was made.
 
    Clients should not assume that this method will be called on a specific thread, and should also try to make this method as efficient as possible.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output didResumeRecordingToOutputFileAtURL:(NSURL *)fileURL fromConnections:(NSArray<AVCaptureConnection *> *)connections API_UNAVAILABLE(ios, watchos, tvos);

/*!
 @method captureOutput:willFinishRecordingToOutputFileAtURL:fromConnections:error:
 @abstract
    Informs the delegate when the output will stop writing new samples to a file.
 
 @param output
    The capture file output that will finish writing the file.
 @param fileURL
    The file URL of the file that is being written.
 @param connections
    An array of AVCaptureConnection objects attached to the file output that provided the data that is being written to the file.
 @param error
    An error describing what caused the file to stop recording, or nil if there was no error.
 
 @discussion
    This method is called when the file output will stop recording new samples to the file at outputFileURL, either because startRecordingToOutputFileURL:recordingDelegate: or stopRecording were called, or because an error, described by the error parameter, occurred (if no error occurred, the error parameter will be nil). This method will always be called for each recording request, even if no data is successfully written to the file.
 
    Clients should not assume that this method will be called on a specific thread, and should also try to make this method as efficient as possible.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output willFinishRecordingToOutputFileAtURL:(NSURL *)fileURL fromConnections:(NSArray<AVCaptureConnection *> *)connections error:(nullable NSError *)error API_UNAVAILABLE(ios, watchos, tvos);

@required

/*!
 @method captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:
 @abstract
    Informs the delegate when all pending data has been written to an output file.
 
 @param output
    The capture file output that has finished writing the file.
 @param outputFileURL
    The file URL of the file that has been written.
 @param connections
    An array of AVCaptureConnection objects attached to the file output that provided the data that was written to the file.
 @param error
    An error describing what caused the file to stop recording, or nil if there was no error.
 
 @discussion
    This method is called when the file output has finished writing all data to a file whose recording was stopped, either because startRecordingToOutputFileURL:recordingDelegate: or stopRecording were called, or because an error, described by the error parameter, occurred (if no error occurred, the error parameter will be nil). This method will always be called for each recording request, even if no data is successfully written to the file.
 
    Clients should not assume that this method will be called on a specific thread.
 
    Delegates are required to implement this method.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output didFinishRecordingToOutputFileAtURL:(NSURL *)outputFileURL fromConnections:(NSArray<AVCaptureConnection *> *)connections error:(nullable NSError *)error;

@end


/*!
 @protocol AVCaptureFileOutputDelegate
 @abstract
    Defines an interface for delegates of AVCaptureFileOutput to monitor and control recordings along exact sample boundaries.
 */
API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos)
@protocol AVCaptureFileOutputDelegate <NSObject>

@required

/*!
 @method captureOutputShouldProvideSampleAccurateRecordingStart:
 @abstract
    Allows a client to opt in to frame accurate record-start in captureOutput:didOutputSampleBuffer:fromConnection:
 
 @param output
    The AVCaptureFileOutput instance with which the delegate is associated.
 
 @discussion
    In apps linked before Mac OS X 10.8, delegates that implement the captureOutput:didOutputSampleBuffer:fromConnection: method can ensure frame accurate start / stop of a recording by calling startRecordingToOutputFileURL:recordingDelegate: from within the callback. Frame accurate start requires the capture output to apply outputSettings when the session starts running, so it is ready to record on any given frame boundary. Compressing all the time while the session is running has power, thermal, and CPU implications. In apps linked on or after Mac OS X 10.8, delegates must implement captureOutputShouldProvideSampleAccurateRecordingStart: to indicate whether frame accurate start/stop recording is required (returning YES) or not (returning NO). The output calls this method as soon as the delegate is added, and never again. If your delegate returns NO, the capture output applies compression settings when startRecordingToOutputFileURL:recordingDelegate: is called, and disables compression settings after the recording is stopped.
 */
- (BOOL)captureOutputShouldProvideSampleAccurateRecordingStart:(AVCaptureOutput *)output API_AVAILABLE(macos(10.8)) API_UNAVAILABLE(ios, watchos, tvos);

@optional

/*!
 @method captureOutput:didOutputSampleBuffer:fromConnection:
 @abstract
    Gives the delegate the opportunity to inspect samples as they are received by the output and optionally start and stop recording at exact times.
 
 @param output
    The capture file output that is receiving the media data.
 @param sampleBuffer
    A CMSampleBuffer object containing the sample data and additional information about the sample, such as its format and presentation time.
 @param connection
    The AVCaptureConnection object attached to the file output from which the sample data was received.
 
 @discussion
    This method is called whenever the file output receives a single sample buffer (a single video frame or audio buffer, for example) from the given connection. This gives delegates an opportunity to start and stop recording or change output files at an exact sample boundary if -captureOutputShouldProvideSampleAccurateRecordingStart: returns YES. If called from within this method, the file output's startRecordingToOutputFileURL:recordingDelegate: and resumeRecording methods are guaranteed to include the received sample buffer in the new file, whereas calls to stopRecording and pauseRecording are guaranteed to include all samples leading up to those in the current sample buffer in the existing file.
 
    Delegates can gather information particular to the samples by inspecting the CMSampleBuffer object. Sample buffers always contain a single frame of video if called from this method but may also contain multiple samples of audio. For B-frame video formats, samples are always delivered in presentation order.
 
    Clients that need to reference the CMSampleBuffer object outside of the scope of this method must CFRetain it and then CFRelease it when they are finished with it.
 
    Note that to maintain optimal performance, some sample buffers directly reference pools of memory that may need to be reused by the device system and other capture inputs. This is frequently the case for uncompressed device native capture where memory blocks are copied as little as possible. If multiple sample buffers reference such pools of memory for too long, inputs will no longer be able to copy new samples into memory and those samples will be dropped. If your application is causing samples to be dropped by retaining the provided CMSampleBuffer objects for too long, but it needs access to the sample data for a long period of time, consider copying the data into a new buffer and then calling CFRelease on the sample buffer if it was previously retained so that the memory it references can be reused.
 
    Clients should not assume that this method will be called on a specific thread. In addition, this method is called periodically, so it must be efficient to prevent capture performance problems.
 */
- (void)captureOutput:(AVCaptureFileOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection;

@end


#pragma mark - AVCaptureMovieFileOutput

@class AVCaptureMovieFileOutputInternal;

/*!
 @class AVCaptureMovieFileOutput
 @abstract
    AVCaptureMovieFileOutput is a concrete subclass of AVCaptureFileOutput that writes captured media to QuickTime movie files.
 
 @discussion
    AVCaptureMovieFileOutput implements the complete file recording interface declared by AVCaptureFileOutput for writing media data to QuickTime movie files. In addition, instances of AVCaptureMovieFileOutput allow clients to configure options specific to the QuickTime file format, including allowing them to write metadata collections to each file, specify media encoding options for each track (Mac OS X), and specify an interval at which movie fragments should be written.
 */
API_AVAILABLE(macos(10.7), ios(4.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED
@interface AVCaptureMovieFileOutput : AVCaptureFileOutput
{
@private
    AVCaptureMovieFileOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @property movieFragmentInterval
 @abstract
    Specifies the frequency with which movie fragments should be written.
 
 @discussion
    When movie fragments are used, a partially written QuickTime movie file whose writing is unexpectedly interrupted can be successfully opened and played up to multiples of the specified time interval. A value of kCMTimeInvalid indicates that movie fragments should not be used, but that only a movie atom describing all of the media in the file should be written. The default value of this property is ten seconds.
 
    Changing the value of this property will not affect the movie fragment interval of the file currently being written, if there is one.
 */
@property(nonatomic) CMTime movieFragmentInterval;

/*!
 @property metadata
 @abstract
    A collection of metadata to be written to the receiver's output files.
 
 @discussion
    The value of this property is an array of AVMetadataItem objects representing the collection of top-level metadata to be written in each output file.
 */
@property(nonatomic, copy, nullable) NSArray<AVMetadataItem *> *metadata;

/*!
 @property availableVideoCodecTypes
 @abstract
    Indicates the supported video codec formats that can be specified in setOutputSettingsForConnection:.
 
 @discussion
    The value of this property is an NSArray of AVVideoCodecTypes that can be used as values for the AVVideoCodecKey in the receiver's setOutputSettingsForConnection: dictionary. The array of available video codecs may change depending on the current session preset. The first codec in the array is used by default when recording a file.
 */
@property(nonatomic, readonly) NSArray<AVVideoCodecType> *availableVideoCodecTypes API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(macos);

/*!
 @method supportedOutputSettingsKeysForConnection:
 @abstract
    Indicates the supported keys that can be specified in setOutputSettings:forConnection:.
 
 @param connection
    The connection delivering the media to be encoded.
 
 @discussion
    Returns an NSArray of NSStrings listing the allowable keys in the receiver's setOutputSettings:forConnection: dictionary.
 */
- (NSArray<NSString *> *)supportedOutputSettingsKeysForConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(12.0)) API_UNAVAILABLE(macos, tvos, watchos);

/*!
 @method outputSettingsForConnection:
 @abstract
    Returns the options the receiver uses to encode media from the given connection as it is being recorded.
 
 @param connection
    The connection delivering the media to be encoded.
 @result
    An NSDictionary of output settings.
 
 @discussion
    See AVAudioSettings.h for audio connections or AVVideoSettings.h for video connections for more information on the structure of an output settings dictionary. If the returned value is an empty dictionary (i.e. [NSDictionary dictionary], the format of the media from the connection will not be changed before being written to the file. If -setOutputSettings:forConnection: was called with a nil dictionary, this method returns a non-nil dictionary reflecting the settings used by the AVCaptureSession's current sessionPreset.
 */
- (NSDictionary<NSString *, id> *)outputSettingsForConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(10.0));

/*!
 @method setOutputSettings:forConnection:
 @abstract
    Sets the options the receiver uses to encode media from the given connection as it is being recorded.
 
 @param outputSettings
    An NSDictionary of output settings.
 @param connection
    The connection delivering the media to be encoded.
 
 @discussion
    See AVAudioSettings.h for audio connections or AVVideoSettings.h for video connections for more information on how to construct an output settings dictionary. A value of an empty dictionary (i.e. [NSDictionary dictionary], means that the format of the media from the connection should not be changed before being written to the file. A value of nil means that the output format will be determined by the session preset. In this case, -outputSettingsForConnection: will return a non-nil dictionary reflecting the settings used by the AVCaptureSession's current sessionPreset.
 
    On iOS, your outputSettings dictionary may only contain keys listed in - supportedOutputSettingsKeysForConnection:. If you specify any other key, an NSInvalidArgumentException will be thrown. Further restrictions may be imposed on the AVVideoCodecTypeKey. Its value should be present in the -availableVideoCodecTypes array. If AVVideoCompressionPropertiesKey is specified, you must also specify a valid value for AVVideoCodecKey. On iOS versions prior to 12.0, the only settable key for video connections is AVVideoCodecTypeKey. On iOS 12.0 and later, video connections gain support for AVVideoCompressionPropertiesKey.
 
    On iOS, -outputSettingsForConnection: always provides a fully populated dictionary. If you call -outputSettingsForConnection: with the intent of overriding a few of the values, you must take care to exclude keys that are not supported before calling -setOutputSettings:forConnection:. When providing an AVVideoCompressionPropertiesKey sub dictionary, you may specify a sparse dictionary. AVCaptureMovieFileOutput will always fill in missing keys with default values for the current AVCaptureSession configuration.
 */
- (void)setOutputSettings:(nullable NSDictionary<NSString *, id> *)outputSettings forConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(10.0));

/*!
 @method recordsVideoOrientationAndMirroringChangesAsMetadataTrackForConnection:
 @abstract
    Returns YES if the movie file output will create a timed metadata track that records samples which reflect changes made to the given connection's videoOrientation and videoMirrored properties during recording.
 
 @param connection
    A connection delivering video media to the movie file output. This method throws an NSInvalidArgumentException if the connection does not have a mediaType of AVMediaTypeVideo or if the connection does not terminate at the movie file output.
 
 @discussion
    See setRecordsVideoOrientationAndMirroringChanges:asMetadataTrackForConnection: for details on the behavior controlled by this value. The default value returned is NO.
 */
- (BOOL)recordsVideoOrientationAndMirroringChangesAsMetadataTrackForConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos);

/*!
 @method setRecordsVideoOrientationAndMirroringChanges:asMetadataTrackForConnection:
 @abstract
    Controls whether or not the movie file output will create a timed metadata track that records samples which reflect changes made to the given connection's videoOrientation and videoMirrored properties during recording.
 
 @param doRecordChanges
    If YES, the movie file output will create a timed metadata track that records samples which reflect changes made to the given connection's videoOrientation and videoMirrored properties during recording.
 @param connection
    A connection delivering video media to the movie file output. This method throws an NSInvalidArgumentException if the connection does not have a mediaType of AVMediaTypeVideo or if the connection does not terminate at the movie file output.
 
 @discussion
    When a recording is started the current state of a video capture connection's videoOrientation and videoMirrored properties are used to build the display matrix for the created video track. The movie file format allows only one display matrix per track, which means that any changes made during a recording to the videoOrientation and videoMirrored properties are not captured. For example, a user starts a recording with their device in the portrait orientation, and then partway through the recording changes the device to a landscape orientation. The landscape orientation requires a different display matrix, but only the initial display matrix (the portrait display matrix) is recorded for the video track.
 
    By invoking this method the client application directs the movie file output to create an additional track in the captured movie. This track is a timed metadata track that is associated with the video track, and contains one or more samples that contain a Video Orientation value (as defined by EXIF and TIFF specifications, which is enumerated by CGImagePropertyOrientation in <ImageIO/CGImageProperties.h>). The value represents the display matrix corresponding to the AVCaptureConnection's videoOrientation and videoMirrored properties when applied to the input source. The initial sample written to the timed metadata track represents video track's display matrix. During recording additional samples will be written to the timed metadata track whenever the client application changes the video connection's videoOrienation or videoMirrored properties. Using the above example, when the client application detects the user changing the device from portrait to landscape orientation, it updates the video connection's videoOrientation property, thus causing the movie file output to add a new sample to the timed metadata track.
 
    After capture, playback and editing applications can use the timed metadata track to enhance their user's experience. For example, when playing back the captured movie, a playback engine can use the samples to adjust the display of the video samples to keep the video properly oriented. Another example is an editing application that uses the sample the sample times to suggest cut points for breaking the captured movie into separate clips, where each clip is properly oriented.
 
    The default behavior is to not create the timed metadata track.
 
    The doRecordChanges value is only observed at the start of recording. Changes to the value will not have any effect until the next recording is started.
 */
- (void)setRecordsVideoOrientationAndMirroringChanges:(BOOL)doRecordChanges asMetadataTrackForConnection:(AVCaptureConnection *)connection API_AVAILABLE(ios(9.0)) API_UNAVAILABLE(macos);

@end


#pragma mark - AVCaptureAudioFileOutput

@class AVCaptureAudioFileOutputInternal;

/*!
 @class AVCaptureAudioFileOutput
 @abstract
    AVCaptureAudioFileOutput is a concrete subclass of AVCaptureFileOutput that writes captured audio to any audio file type supported by CoreAudio.
 
 @discussion
    AVCaptureAudioFileOutput implements the complete file recording interface declared by AVCaptureFileOutput for writing media data to audio files. In addition, instances of AVCaptureAudioFileOutput allow clients to configure options specific to the audio file formats, including allowing them to write metadata collections to each file and specify audio encoding options.
 */
API_AVAILABLE(macos(10.7)) API_UNAVAILABLE(ios, watchos, tvos)
@interface AVCaptureAudioFileOutput : AVCaptureFileOutput
{
@private
    AVCaptureAudioFileOutputInternal *_internal;
}

- (instancetype)init;

+ (instancetype)new;

/*!
 @method availableOutputFileTypes
 @abstract
    Provides the file types AVCaptureAudioFileOutput can write.
 
 @result
    An NSArray of UTIs identifying the file types the AVCaptureAudioFileOutput class can write.
 */
+ (NSArray<AVFileType> *)availableOutputFileTypes;

/*!
 @method startRecordingToOutputFileURL:outputFileType:recordingDelegate:
 @abstract
    Tells the receiver to start recording to a new file of the specified format, and specifies a delegate that will be notified when recording is finished.
 
 @param outputFileURL
    An NSURL object containing the URL of the output file. This method throws an NSInvalidArgumentException if the URL is not a valid file URL.
 @param fileType
    A UTI indicating the format of the file to be written.
 @param delegate
    An object conforming to the AVCaptureFileOutputRecordingDelegate protocol. Clients must specify a delegate so that they can be notified when recording to the given URL is finished.
 
 @discussion
    The method sets the file URL to which the receiver is currently writing output media. If a file at the given URL already exists when capturing starts, recording to the new file will fail.
 
    The fileType argument is a UTI corresponding to the audio file format that should be written. UTIs for common audio file types are declared in AVMediaFormat.h.
 
    Clients need not call stopRecording before calling this method while another recording is in progress. If this method is invoked while an existing output file was already being recorded, no media samples will be discarded between the old file and the new file.
 
    When recording is stopped either by calling stopRecording, by changing files using this method, or because of an error, the remaining data that needs to be included to the file will be written in the background. Therefore, clients must specify a delegate that will be notified when all data has been written to the file using the captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error: method. The recording delegate can also optionally implement methods that inform it when data starts being written, when recording is paused and resumed, and when recording is about to be finished.
 
    On Mac OS X, if this method is called within the captureOutput:didOutputSampleBuffer:fromConnection: delegate method, the first samples written to the new file are guaranteed to be those contained in the sample buffer passed to that method.
 */
- (void)startRecordingToOutputFileURL:(NSURL *)outputFileURL outputFileType:(AVFileType)fileType recordingDelegate:(id<AVCaptureFileOutputRecordingDelegate>)delegate;

/*!
 @property metadata
 @abstract
    A collection of metadata to be written to the receiver's output files.
 
 @discussion
    The value of this property is an array of AVMetadataItem objects representing the collection of top-level metadata to be written in each output file. Only ID3 v2.2, v2.3, or v2.4 style metadata items are supported.
 */
@property(nonatomic, copy) NSArray<AVMetadataItem *> *metadata;

/*!
 @property audioSettings
 @abstract
    Specifies the options the receiver uses to re-encode audio as it is being recorded.
 
 @discussion
    The output settings dictionary can contain values for keys from AVAudioSettings.h. A value of nil indicates that the format of the audio should not be changed before being written to the file.
 */
@property(nonatomic, copy, nullable) NSDictionary<NSString *, id> *audioSettings;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Headers/AVAudioUnitTimeEffect.h
/*
	File:           AVAudioUnitTimeEffect.h
	Framework:      AVFoundation
	
	Copyright 2016 Apple Inc. All rights reserved.
*/

#import <AVFAudio/AVAudioUnitTimeEffect.h>

// ==========  AVFoundation.framework/Headers/AVUtilities.h
/*
    File:  AVUtilities.h
	
    Framework:  AVFoundation
	
    Copyright 2010-2015 Apple Inc. All rights reserved.
	
 */

#import <AVFoundation/AVBase.h>

#import <CoreGraphics/CGBase.h>
#import <CoreGraphics/CGGeometry.h>

/*!
 @function					AVMakeRectWithAspectRatioInsideRect
 @abstract					Returns a scaled CGRect that maintains the aspect ratio specified by a CGSize within a bounding CGRect.
 @discussion				This is useful when attempting to fit the presentationSize property of an AVPlayerItem within the bounds of another CALayer. 
							You would typically use the return value of this function as an AVPlayerLayer frame property value. For example:
							myPlayerLayer.frame = AVMakeRectWithAspectRatioInsideRect(myPlayerItem.presentationSize, mySuperLayer.bounds);
 @param aspectRatio			The width & height ratio, or aspect, you wish to maintain.
 @param	boundingRect		The bounding CGRect you wish to fit into. 
 */

AVF_EXPORT CGRect AVMakeRectWithAspectRatioInsideRect(CGSize aspectRatio, CGRect boundingRect) NS_AVAILABLE(10_7, 4_0);
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitSampler.h
/*
	File:		AVAudioUnitSampler.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnitMIDIInstrument.h>

NS_ASSUME_NONNULL_BEGIN

/*!
 @class AVAudioUnitSampler
 @abstract Apple's sampler audio unit.
 @discussion
    An AVAudioUnit for Apple's Sampler Audio Unit. The sampler can be configured by loading
    instruments from different types of files such as an aupreset, a DLS or SF2 sound bank,
    an EXS24 instrument, a single audio file, or an array of audio files.

    The output is a single stereo bus. 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitSampler : AVAudioUnitMIDIInstrument

/*! @method loadSoundBankInstrumentAtURL:program:bankMSB:bankLSB:error:
	@abstract loads a specific instrument from the specified sound bank
	@param bankURL
		URL for a Soundbank file. The file can be either a DLS bank (.dls) or a SoundFont bank (.sf2).
	@param program
		program number for the instrument to load
	@param bankMSB
		MSB for the bank number for the instrument to load.  This is usually 0x79 for melodic
		instruments and 0x78 for percussion instruments.
	@param bankLSB
		LSB for the bank number for the instrument to load.  This is often 0, and represents the "bank variation".
	@param outError
    	the status of the operation
	@discussion
 		This method reads from file and allocates memory, so it should not be called on a real time thread.
 */
- (BOOL)loadSoundBankInstrumentAtURL:(NSURL *)bankURL program:(uint8_t)program bankMSB:(uint8_t)bankMSB bankLSB:(uint8_t)bankLSB error:(NSError **)outError;

/*! @method loadInstrumentAtURL:error:
	@abstract configures the sampler by loading the specified preset file.
	@param instrumentURL
    	URL to the preset file or audio file
	@param outError
		the status of the operation
	@discussion
		The file can be of one of the following types: Logic/GarageBand EXS24 instrument,
		the Sampler AU's native aupreset, or an audio file (eg. .caf, .aiff, .wav, .mp3).
	 
		If an audio file URL is loaded, it will become the sole sample in a new default instrument.
		Any information contained in the file regarding its keyboard placement (e.g. root key,
		key range) will be used.
		This method reads from file and allocates memory, so it should not be called on a real time thread.
 
 */
- (BOOL)loadInstrumentAtURL:(NSURL *)instrumentURL error:(NSError **)outError;

/*! @method loadAudioFilesAtURLs:error:
	@abstract configures the sampler by loading a set of audio files.
	@param audioFiles
		array of URLs for audio files to be loaded
	@param outError
		the status of the operation
	@discussion
		The audio files are loaded into a new default instrument with each audio file placed
		into its own sampler zone. Any information contained in the audio file regarding
		their placement on the keyboard (e.g. root key, key range) will be used.
		This method reads from file and allocates memory, so it should not be called on a real time thread.
 
 */
- (BOOL)loadAudioFilesAtURLs:(NSArray<NSURL *> *)audioFiles error:(NSError **)outError;

/*! @property stereoPan
	@abstract
		adjusts the pan for all the notes played.
		Range:     -1 -> +1
		Default:   0
 */
@property (nonatomic) float     stereoPan;

/*! @property masterGain
	@abstract
    	adjusts the gain of all the notes played
		Range:     -90.0 -> +12 db
		Default: 0 db
 */
@property (nonatomic) float     masterGain;

/*! @property globalTuning
	@abstract
		adjusts the tuning of all the notes played.
		Range:     -2400 -> +2400 cents
		Default:   0
 */
@property (nonatomic) float     globalTuning;


@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioEngine.h
/*
	File:		AVAudioEngine.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>
#import <AVFAudio/AVAudioBuffer.h>
#import <AVFAudio/AVAudioConnectionPoint.h>
#import <AVFAudio/AVAudioIONode.h>
#import <AVFAudio/AVAudioTime.h>

#if __has_include(<AudioToolbox/MusicPlayer.h>)
#define AVAUDIOENGINE_HAVE_MUSICPLAYER 1
#import <AudioToolbox/MusicPlayer.h>
#endif

#if __has_include(<AudioToolbox/AUAudioUnit.h>) && __OBJC2__
#define AVAUDIOENGINE_HAVE_AUAUDIOUNIT 1
#import <AudioToolbox/AUAudioUnit.h>
#endif

NS_ASSUME_NONNULL_BEGIN

@class AVAudioFormat, AVAudioNode, AVAudioInputNode, AVAudioOutputNode, AVAudioMixerNode;

/*! @enum AVAudioEngineManualRenderingError
    @abstract 
		Error codes that could be returned from AVAudioEngine manual rendering mode methods,
		e.g. `enableManualRenderingMode:format:maximumFrameCount:error:` and
		`renderOffline:toBuffer:error:`.
		Note that this is not a comprehensive list, and the underlying audio units could
		return other error codes (e.g. see kAudioUnitErr_* in AudioToolbox/AUComponent.h) from these
		methods as applicable.
 
		AVAudioEngineManualRenderingErrorInvalidMode
			The operation cannot be performed because the engine is either not in manual 
			rendering mode or the right variant of it.

		AVAudioEngineManualRenderingErrorInitialized
			The operation cannot be performed because the engine is initialized (i.e. not stopped).

 		AVAudioEngineManualRenderingErrorNotRunning
			The operation cannot be performed because the engine is not running (i.e. not started).
*/
typedef NS_ENUM(OSStatus, AVAudioEngineManualRenderingError) {
	AVAudioEngineManualRenderingErrorInvalidMode = -80800,
	AVAudioEngineManualRenderingErrorInitialized = -80801,
	AVAudioEngineManualRenderingErrorNotRunning  = -80802
} API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @enum AVAudioEngineManualRenderingStatus
    @abstract 
		Status codes returned from the render call to the engine operating in manual rendering mode.
 
		AVAudioEngineManualRenderingStatusError
			An error occurred when rendering and no data was returned. See the returned error code
			for the description of the error.
 
		AVAudioEngineManualRenderingStatusSuccess
			All of the requested data was returned successfully.

		AVAudioEngineManualRenderingStatusInsufficientDataFromInputNode
			Applicable only to the input node, when it provides input data for rendering
			(see `AVAudioInputNode(setManualRenderingInputPCMFormat:inputBlock:)`).
			Indicates that not enough input data was returned by the input node to satisfy the
			render request at the current time. The output buffer may contain data rendered by other
			active sources in the engine's processing graph.
 
 		AVAudioEngineManualRenderingStatusCannotDoInCurrentContext
			The operation could not be performed now, but the client could retry later if needed.
			This is usually to guard a realtime render operation (e.g. rendering through
			`manualRenderingBlock`) when a reconfiguration of the engine's internal state 
			is in progress.
*/
typedef NS_ENUM(NSInteger, AVAudioEngineManualRenderingStatus) {
	AVAudioEngineManualRenderingStatusError = -1,
	AVAudioEngineManualRenderingStatusSuccess = 0,
	AVAudioEngineManualRenderingStatusInsufficientDataFromInputNode = 1,
	AVAudioEngineManualRenderingStatusCannotDoInCurrentContext = 2
} API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @enum AVAudioEngineManualRenderingMode
    @abstract 
		By default, the engine is connected to an audio device and automatically renders in realtime. 
		It can also be configured to operate in manual rendering mode, i.e. not connected to an 
		audio device and rendering in response to requests from the client.
 
		AVAudioEngineManualRenderingModeOffline
			The engine operates in an offline mode without any realtime constraints.
 
		AVAudioEngineManualRenderingModeRealtime
			The engine operates under realtime constraints, i.e. it will not make any blocking call 
			(e.g. calling libdispatch, blocking on a mutex, allocating memory etc.) while rendering.
			Note that only the block based render mechanism can be used in this mode
			(see `AVAudioEngine(manualRenderingBlock)`.
*/
typedef NS_ENUM(NSInteger, AVAudioEngineManualRenderingMode) {
	AVAudioEngineManualRenderingModeOffline = 0,
	AVAudioEngineManualRenderingModeRealtime = 1
} API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!	@typedef AVAudioEngineManualRenderingBlock
	@abstract 
		Block to render the engine when operating in manual rendering mode
	@param numberOfFrames
		The number of PCM sample frames to be rendered
	@param outBuffer
		The PCM buffer to which the engine must render the audio. 
		The buffer pointers (outBuffer->mBuffers[x].mData) may be null on entry, in which case
		the block will render into a memory it owns and modify the mData pointers to point to that
		memory. The block is responsible for preserving the validity of that memory until it is next
		called to render, or `AVAudioEngine(stop)` is called.
	@param outError
		On exit, if an error occurs during rendering, a description of the error (see
		`AVAudioEngineManualRenderingError` for the possible errors)
	@return
		One of the status codes from `AVAudioEngineManualRenderingStatus`. Irrespective of the
		returned status code, on exit, the output buffer's mDataByteSize 
		(outBuffer->mBuffers[x].mDataByteSize) will indicate the number of PCM data bytes rendered by
		the engine.
 	@discussion
		Use this if you want to render the engine from a realtime context when it is operating in
		the manual rendering mode. See `AVAudioEngine(manualRenderingBlock)` for details.
*/
typedef AVAudioEngineManualRenderingStatus (^AVAudioEngineManualRenderingBlock)(AVAudioFrameCount numberOfFrames, AudioBufferList *outBuffer, OSStatus * __nullable outError) API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!
	@class AVAudioEngine
	@discussion
		An AVAudioEngine contains a group of connected AVAudioNodes ("nodes"), each of which performs
		an audio signal generation, processing, or input/output task.
		
		Nodes are created separately and attached to the engine.

		The engine supports dynamic connection, disconnection and removal of nodes while running,
		with only minor limitations:
		- all dynamic reconnections must occur upstream of a mixer
		- while removals of effects will normally result in the automatic connection of the adjacent
			nodes, removal of a node which has differing input vs. output channel counts, or which
			is a mixer, is likely to result in a broken graph.
 
		By default, the engine is connected to an audio device and automatically renders in realtime. 
		It can also be configured to operate in manual rendering mode, i.e. not connected to an
		audio device and rendering in response to requests from the client, normally at or
		faster than realtime rate.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioEngine : NSObject {
@private
	void *_impl;
}

/*! @method init
	@abstract
		Initialize a new engine.
	@discussion
		On creation, the engine is by default connected to an audio device and automatically renders 
		in realtime. It can be configured to operate in manual rendering mode through 
		`enableManualRenderingMode:format:maximumFrameCount:error:`.
*/
- (instancetype)init;

/*!	@method attachNode:
	@abstract
		Take ownership of a new node.
	@param node
		The node to be attached to the engine.
	@discussion
		To support the instantiation of arbitrary AVAudioNode subclasses, instances are created
		externally to the engine, but are not usable until they are attached to the engine via
		this method. Thus the idiom, without ARC, is:

<pre>
// when building engine:
AVAudioNode *_player;	// member of controller class (for example)
...
_player = [[AVAudioPlayerNode alloc] init];
[engine attachNode: _player];
...
// when destroying engine (without ARC)
[_player release];
</pre>
*/
- (void)attachNode:(AVAudioNode *)node;

/*!	@method detachNode:
	@abstract
		Detach a node previously attached to the engine.
	@discussion
		If necessary, the engine will safely disconnect the node before detaching it.
*/
- (void)detachNode:(AVAudioNode *)node;

/*! @method connect:to:fromBus:toBus:format:
	@abstract
		Establish a connection between two nodes.
	@param node1 
		The source node
	@param node2 
		The destination node
	@param bus1 
		The output bus on the source node
	@param bus2 
		The input bus on the destination node
	@param format 
		If non-nil, the format of the source node's output bus is set to this
		format. In all cases, the format of the destination node's input bus is set to
		match that of the source node's output bus.
	@discussion
		Nodes have input and output buses (AVAudioNodeBus). Use this method to establish
		one-to-one connections betweeen nodes. Connections made using this method are always
		one-to-one, never one-to-many or many-to-one.
	
		Note that any pre-existing connection(s) involving the source's output bus or the
		destination's input bus will be broken.
*/
- (void)connect:(AVAudioNode *)node1 to:(AVAudioNode *)node2 fromBus:(AVAudioNodeBus)bus1 toBus:(AVAudioNodeBus)bus2 format:(AVAudioFormat * __nullable)format;

/*!	@method connect:to:format:
	@abstract
		Establish a connection between two nodes
	@discussion
		This calls connect:to:fromBus:toBus:format: using bus 0 on the source node,
		and bus 0 on the destination node, except in the case of a destination which is a mixer,
		in which case the destination is the mixer's nextAvailableInputBus.
*/
- (void)connect:(AVAudioNode *)node1 to:(AVAudioNode *)node2 format:(AVAudioFormat * __nullable)format;

/*! @method connect:toConnectionPoints:fromBus:format:
	@abstract
		Establish connections between a source node and multiple destination nodes.
	@param sourceNode 
		The source node
	@param destNodes 
		An array of AVAudioConnectionPoint objects specifying destination
		nodes and busses
	@param sourceBus 
		The output bus on source node
	@param format 
		If non-nil, the format of the source node's output bus is set to this
		format. In all cases, the format of the destination nodes' input bus is set to
		match that of the source node's output bus
	@discussion
		Use this method to establish connections from a source node to multiple destination nodes.
		Connections made using this method are either one-to-one (when a single destination
		connection is specified) or one-to-many (when multiple connections are specified), but 
		never many-to-one.

		To incrementally add a new connection to a source node, use this method with an array
		of AVAudioConnectionPoint objects comprising of pre-existing connections (obtained from
		`outputConnectionPointsForNode:outputBus:`) and the new connection.
 
		Note that any pre-existing connection involving the destination's input bus will be 
		broken. And, any pre-existing connection on source node which is not a part of the
		specified destination connection array will also be broken.

		Also note that when the output of a node is split into multiple paths, all the paths
		must render at the same rate until they reach a common mixer.
		In other words, starting from the split node until the common mixer node where all split 
		paths terminate, you cannot have:
			- any AVAudioUnitTimeEffect
			- any sample rate conversion
*/
- (void)connect:(AVAudioNode *)sourceNode toConnectionPoints:(NSArray<AVAudioConnectionPoint *> *)destNodes fromBus:(AVAudioNodeBus)sourceBus format:(AVAudioFormat * __nullable)format API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*! @method disconnectNodeInput:bus:
	@abstract
		Remove a connection between two nodes.
	@param node 
		The node whose input is to be disconnected
	@param bus 
		The destination's input bus to disconnect
*/
- (void)disconnectNodeInput:(AVAudioNode *)node bus:(AVAudioNodeBus)bus;

/*!	@method disconnectNodeInput:
	@abstract
		Remove a connection between two nodes.
	@param node 
		The node whose inputs are to be disconnected
	@discussion
		Connections are broken on each of the node's input busses.
*/
- (void)disconnectNodeInput:(AVAudioNode *)node;

/*! @method disconnectNodeOutput:bus:
	@abstract
		Remove a connection between two nodes.
	@param node 
		The node whose output is to be disconnected
	@param bus 
		The source's output bus to disconnect
*/
- (void)disconnectNodeOutput:(AVAudioNode *)node bus:(AVAudioNodeBus)bus;

/*!	@method disconnectNodeOutput:
	@abstract
		Remove a connection between two nodes.
	@param node 
		The node whose outputs are to be disconnected
	@discussion
		Connections are broken on each of the node's output busses.
*/
- (void)disconnectNodeOutput:(AVAudioNode *)node;

/*!	@method prepare
	@abstract
		Prepare the engine for starting.
	@discussion
		This method preallocates many of the resources the engine requires in order to start.
		It can be used to be able to start more responsively.
*/
- (void)prepare;

/*! @method startAndReturnError:
	@abstract
		Start the engine.
	@return
		YES for success
	@discussion
		Calls prepare if it has not already been called since stop.
	
		When the engine is rendering to/from an audio device, starts the audio hardware via the
		AVAudioInputNode and/or AVAudioOutputNode instances in the engine. Audio begins to flow 
		through the engine.
		Reasons for potential failure to start in this mode include:
		1. There is problem in the structure of the graph. Input can't be routed to output or to a
			recording tap through converter type nodes.
		2. An AVAudioSession error.
		3. The driver failed to start the hardware.
 
		In manual rendering mode, prepares the engine to render when requested by the client.
*/
- (BOOL)startAndReturnError:(NSError **)outError;

/*!	@method pause
	@abstract
		Pause the engine.
	@discussion
		When the engine is rendering to/from an audio device, stops the audio hardware and the flow
		of audio through the engine. When operating in this mode, it is recommended that the engine
		be paused or stopped (as applicable) when not in use, to minimize power consumption.

		Pausing the engine does not deallocate the resources allocated by prepare. Resume the
		engine by invoking start again.
*/
- (void)pause;

/*!	@method reset
	@abstract reset
		Reset all of the nodes in the engine.
	@discussion
		This will reset all of the nodes in the engine. This is useful, for example, for silencing
		reverb and delay tails.
 
		In manual rendering mode, the render timeline is reset to a sample time of zero.
*/
- (void)reset;

/*! @method stop
	@abstract
		When the engine is rendering to/from an audio device, stops the audio hardware and the
		engine. When operating in this mode, it is recommended that the engine be paused or stopped
		 (as applicable) when not in use, to minimize power consumption.
 
		Stopping the engine releases the resources allocated by prepare.
*/
- (void)stop;

/*! @method inputConnectionPointForNode:inputBus:
	@abstract 
		Get connection information on a node's input bus.
	@param node 
		The node whose input connection is being queried.
	@param bus 
		The node's input bus on which the connection is being queried.
	@return	
		An AVAudioConnectionPoint object with connection information on the node's
		specified input bus.
	@discussion
		Connections are always one-to-one or one-to-many, never many-to-one.
 
		Returns nil if there is no connection on the node's specified input bus.
*/
- (AVAudioConnectionPoint * __nullable)inputConnectionPointForNode:(AVAudioNode *)node inputBus:(AVAudioNodeBus)bus API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*! @method outputConnectionPointsForNode:outputBus:
	@abstract
		Get connection information on a node's output bus.
	@param node 
		The node whose output connections are being queried.
	@param bus 
		The node's output bus on which connections are being queried.
	@return
		An array of AVAudioConnectionPoint objects with connection information on the node's
		specified output bus.
	@discussion
		Connections are always one-to-one or one-to-many, never many-to-one.
 
		Returns an empty array if there are no connections on the node's specified output bus.
*/
- (NSArray<AVAudioConnectionPoint *> *)outputConnectionPointsForNode:(AVAudioNode *)node outputBus:(AVAudioNodeBus)bus API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

#if AVAUDIOENGINE_HAVE_MUSICPLAYER
/*! @property musicSequence
	@abstract
		The MusicSequence previously attached to the engine (if any).
 */
@property (nonatomic, nullable) MusicSequence musicSequence;
#endif

/*! @property outputNode
	@abstract
		The engine's singleton output node.
	@discussion
		Audio output is performed via an output node. The engine creates a singleton on demand when
		this property is first accessed. Connect another node to the input of the output node, or
		obtain a mixer that is connected there by default, using the "mainMixerNode" property.
 
		When the engine is rendering to/from an audio device, the AVAudioSesssion category and/or
		availability of hardware determine whether an app can perform output. Check the output
		format of output node (i.e. hardware format) for non-zero sample rate and channel count to
		see if output is enabled. 
		Trying to perform output through the output node when it is not enabled or available will 
		cause the engine to throw an error (when possible) or an exception.
 
		In manual rendering mode, the output format of the output node will determine the
		render format of the engine. It can be changed through
		`enableManualRenderingMode:format:maximumFrameCount:error:`.
*/
@property (readonly, nonatomic) AVAudioOutputNode *outputNode;

/*! @property inputNode
	@abstract
		The engine's singleton input node.
	@discussion
		Audio input is performed via an input node. The engine creates a singleton on demand when
		this property is first accessed. To receive input, connect another node from the output of 
		the input node, or create a recording tap on it.
 
		When the engine is rendering to/from an audio device, the AVAudioSesssion category and/or
		availability of hardware determine whether an app can perform input (e.g. input hardware is
		not available on tvos). Check for the input node's input format (i.e. hardware format) for
		non-zero sample rate and channel count to see if input is enabled.
		Trying to perform input through the input node when it is not enabled or available will 
		cause the engine to throw an error (when possible) or an exception.

		In manual rendering mode, the input node can be used to synchronously supply data to
		the engine while it is rendering (see 
		`AVAudioInputNode(setManualRenderingInputPCMFormat:inputBlock:)`.
*/
@property (readonly, nonatomic) AVAudioInputNode *inputNode  API_AVAILABLE(macos(10.10), ios(8.0), watchos(4.0), tvos(11.0));


/*! @property mainMixerNode
	@abstract
		The engine's optional singleton main mixer node.
	@discussion
		The engine will construct a singleton main mixer and connect it to the outputNode on demand,
		when this property is first accessed. You can then connect additional nodes to the mixer.
		
		By default, the mixer's output format (sample rate and channel count) will track the format 
		of the output node. You may however make the connection explicitly with a different format.
*/
@property (readonly, nonatomic) AVAudioMixerNode *mainMixerNode;

/*! @property running
	@abstract
		The engine's running state.
*/
@property (readonly, nonatomic, getter=isRunning) BOOL running;

/*! @property autoShutdownEnabled
	@abstract
		When auto shutdown is enabled, the engine can start and stop the audio hardware dynamically,
		to conserve power. This is the enforced behavior on watchOS and can be optionally enabled on
		other platforms.
	@discussion
		To conserve power, it is advised that the client pause/stop the engine when not in use.
		But when auto shutdown is enabled, the engine will stop the audio hardware if it was running 
		idle for a certain duration, and restart it later when required.
		Note that, because this operation is dynamic, it may affect the start times of the source 
		nodes (e.g. `AVAudioPlayerNode`), if the engine has to resume from its shutdown state.
 
		On watchOS, auto shutdown is always enabled. On other platforms, it is disabled by
		default, but the client can enable it if needed.
 
		This property is applicable only when the engine is rendering to/from an audio device. If
		the value is changed when the engine is in manual rendering mode, it will take effect
		whenever the engine is switched to render to/from the audio device.
*/
@property (nonatomic, getter=isAutoShutdownEnabled) BOOL autoShutdownEnabled API_AVAILABLE(macos(10.13), ios(11.0), tvos(11.0)) API_UNAVAILABLE(watchos);

#pragma mark -
#pragma mark Manual Rendering Mode

/*!	@method enableManualRenderingMode:format:maximumFrameCount:error:
	@abstract
		Set the engine to operate in manual rendering mode with the specified render format and
		maximum frame count.
	@param format
		The format of the output PCM audio data from the engine
	@param maximumFrameCount
		The maximum number of PCM sample frames the engine will be asked to produce in any single
		render call
 	@param outError
		On exit, if the engine cannot switch to the manual rendering mode, a description of the
		error (see `AVAudioEngineManualRenderingError` for the possible errors)
	@return
		YES for success
	@discussion
		Use this method to configure the engine to render in response to requests from the client.
 
		The engine must be in a stopped state before calling this method.
		The render format must be a PCM format and match the format of the buffer to which
		the engine is asked to render (see `renderOffline:toBuffer:error:`).
 
		The input data in manual rendering mode can be supplied through the source nodes, e.g.
		`AVAudioPlayerNode`, `AVAudioInputNode` etc.
 
 		When switching to manual rendering mode, the engine:
		1. Switches the input and output nodes to manual rendering mode. Their input and output
		   formats may change.
		2. Removes any taps previously installed on the input and output nodes.
		3. Maintains all the engine connections as is.

		Reasons for potential failure when switching to manual rendering mode include:
		- Engine is not in a stopped state.
*/
- (BOOL)enableManualRenderingMode:(AVAudioEngineManualRenderingMode)mode format:(AVAudioFormat *)pcmFormat maximumFrameCount:(AVAudioFrameCount)maximumFrameCount error:(NSError **)outError API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!	@method disableManualRenderingMode
	@abstract
		Set the engine to render to/from an audio device.
	@discussion
 		When disabling the manual rendering mode, the engine:
		1. Stops and resets itself (see `stop` and `reset`).
		2. Switches the output/input nodes to render to/from an audio device. Their input and
		   output formats may change.
		3. Removes any taps previously installed on the input and output nodes.
		4. Maintains all the engine connections as is.
 
		Calling this method when the engine is already rendering to/from an audio device has no 
		effect.
*/
- (void)disableManualRenderingMode API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!	@method renderOffline:toBuffer:error:
	@abstract
		Render call to the engine operating in the offline manual rendering mode
	@param numberOfFrames
		The number of PCM sample frames to be rendered
	@param buffer
		The PCM buffer to which the engine must render the audio
	@param outError
		On exit, if an error occurs during rendering, a description of the error (see
		`AVAudioEngineManualRenderingError` for the possible errors)
	@return
		One of the status codes from `AVAudioEngineManualRenderingStatus`. Irrespective of the
		returned status code, on exit, the output buffer's frameLength will indicate the number of
		PCM samples rendered by the engine
	@discussion
		The engine must be in the offline manual rendering mode 
		(`AVAudioEngineManualRenderingModeOffline`) and started before calling this method.
 
		The format of the buffer must match the render format set through 
		`enableManualRenderingMode:format:maximumFrameCount:error:`. The buffer capacity must be
		greater than or equal to the number of samples asked to render.
		On exit, the buffer's frameLength will indicate the number of PCM samples rendered by the 
		engine.
 
 		The engine's timeline in manual rendering mode starts at a sample time of zero, and is in
		terms of the render format's sample rate. Resetting the engine (see `reset`) will reset the
		timeline back to zero.
 
 		When rendering in `AVAudioEngineManualRenderingModeRealtime`, this ObjC render method 
		must not be used, an error is returned otherwise. Use the block based render call
		(`manualRenderingBlock`) in that mode instead.
*/
- (AVAudioEngineManualRenderingStatus)renderOffline:(AVAudioFrameCount)numberOfFrames toBuffer:(AVAudioPCMBuffer *)buffer error:(NSError **)outError API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0)) __attribute__((swift_error(nonnull_error)));

/*!	@property manualRenderingBlock
	@abstract
		Block to render the engine operating in manual rendering mode
	@discussion
		This block based render call must be used to render the engine when operating in
		`AVAudioEngineManualRenderingModeRealtime`. In this mode, the engine operates under
		realtime constraints and will not make any blocking call (e.g. calling libdispatch, blocking 
		on a mutex, allocating memory etc.) while rendering. 
 
		Before invoking the rendering functionality, client must fetch this block and cache the
		result. The block can then be called from a realtime context, without any possibility of 
		blocking.
 
		When rendering in `AVAudioEngineManualRenderingModeOffline`, either this block based render
		call or	`renderOffline:toBuffer:error:` ObjC method can be used.
		All the rules outlined in `renderOffline:toBuffer:error:` are applicable here as well.
*/
@property (readonly, nonatomic) AVAudioEngineManualRenderingBlock manualRenderingBlock API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property isInManualRenderingMode
	@abstract
		Whether or not the engine is operating in manual rendering mode, i.e. not connected
		to an audio device and rendering in response to the requests from the client
*/
@property (readonly, nonatomic) BOOL isInManualRenderingMode API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property manualRenderingMode
	@abstract
		The manual rendering mode configured on the engine
	@discussion
		This property is meaningful only when the engine is operating in manual rendering mode,
		i.e. when `isInManualRenderingMode` returns true.
*/
@property (readonly, nonatomic) AVAudioEngineManualRenderingMode manualRenderingMode API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property manualRenderingFormat
	@abstract
		The render format of the engine in manual rendering mode.
	@discussion
		Querying this property when the engine is not in manual rendering mode will return an
		invalid format, with zero sample rate and channel count.
*/
@property (readonly, nonatomic) AVAudioFormat *manualRenderingFormat API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property manualRenderingMaximumFrameCount
	@abstract
		The maximum number of PCM sample frames the engine can produce in any single render call in 
		the manual rendering mode.
	@discussion
		Querying this property when the engine is not in manual rendering mode will return zero.
*/
@property (readonly, nonatomic) AVAudioFrameCount manualRenderingMaximumFrameCount API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property manualRenderingSampleTime
	@abstract
		Indicates where the engine is on its render timeline in manual rendering mode.
	@discussion
		The timeline in manual rendering mode starts at a sample time of zero, and is in terms
		of the render format's sample rate. Resetting the engine (see `reset`) will reset the
		timeline back to zero.
*/
@property (readonly, nonatomic) AVAudioFramePosition manualRenderingSampleTime API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

#if AVAUDIOENGINE_HAVE_AUAUDIOUNIT
/*! @method connectMIDI:to:format:block:
    @abstract
        Establish a MIDI only connection between two nodes.
    @param sourceNode
        The source node.
    @param destinationNode
        The destination node.
    @param format
        If non-nil, the format of the source node's output bus is set to this format.
        In all cases, the format of the source nodes' output bus has to match with the
        destination nodes' output bus format.
        Although the output bus of the source is not in use, the format needs to be set
        in order to be able to use the sample rate for MIDI event timing calculations.
    @param tapBlock
        If non-nil, this block is called from the source node's `AUMIDIOutputEventBlock`
        on the realtime thread. The host can tap the MIDI data of the source node through
        this block. May be nil.
    @discussion
        Use this method to establish a MIDI only connection between a source node and a
        destination node that has MIDI input capability.

        The source node can only be a AVAudioUnit node of type `kAudioUnitType_MIDIProcessor`.
        The destination node types can be `kAudioUnitType_MusicDevice`,
        `kAudioUnitType_MusicEffect` or `kAudioUnitType_MIDIProcessor`.

        Note that any pre-existing MIDI connection involving the destination will be broken.

        Any client installed block on the source node's audio unit `AUMIDIOutputEventBlock`
        will be overwritten when making the MIDI connection.
 */
- (void)connectMIDI:(AVAudioNode *)sourceNode to:(AVAudioNode *)destinationNode format:(AVAudioFormat * __nullable)format block:(AUMIDIOutputEventBlock __nullable)tapBlock API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

/*! @method connectMIDI:toNodes:format:block:
    @abstract
        Establish a MIDI only connection between a source node and multiple destination nodes.
    @param sourceNode
        The source node.
    @param destinationNodes
        An array of AVAudioNodes specifying destination nodes.
    @param format
        If non-nil, the format of the source node's output bus is set to this format.
        In all cases, the format of the source nodes' output bus has to match with the
        destination nodes' output bus format.
        Although the output bus of the source is not in use, the format needs to be set
        in order to be able to use the sample rate for MIDI event timing calculations.
    @param tapBlock
        If non-nil, this block is called from the source node's `AUMIDIOutputEventBlock`
        on the realtime thread. The host can tap the MIDI data of the source node through
        this block. May be nil.
    @discussion
        Use this method to establish a MIDI only connection between a source node and
        multiple destination nodes.

        The source node can only be a AVAudioUnit node of type `kAudioUnitType_MIDIProcessor`.
        The destination node types can be `kAudioUnitType_MusicDevice`,
        `kAudioUnitType_MusicEffect` or `kAudioUnitType_MIDIProcessor`.

        MIDI connections made using this method are either one-to-one (when a single
        destination connection is specified) or one-to-many (when multiple connections are
        specified), but never many-to-one.

        Note that any pre-existing connection involving the destination will be broken.

        Any client installed block on the source node's audio unit `AUMIDIOutputEventBlock`
        will be overwritten when making the MIDI connection.
 */
- (void)connectMIDI:(AVAudioNode *)sourceNode toNodes:(NSArray<AVAudioNode *> *)destinationNodes format:(AVAudioFormat * __nullable)format block:(AUMIDIOutputEventBlock __nullable)tapBlock API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

/*! @method disconnectMIDI:from:
    @abstract
        Remove a MIDI connection between two nodes.
    @param sourceNode
        The node whose MIDI output is to be disconnected.
    @param destinationNode
        The node whose MIDI input is to be disconnected.
    @discussion
        If a tap block is installed on the source node, it will be removed when the last
        connection from the source node is removed.
 */
- (void)disconnectMIDI:(AVAudioNode *)sourceNode from:(AVAudioNode *)destinationNode API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

/*! @method disconnectMIDI:fromNodes:
    @abstract
        Remove a MIDI connection between one source node and multiple destination nodes.
    @param sourceNode
        The node whose MIDI output is to be disconnected.
    @param destinationNodes
        An array of AVAudioNodes specifying nodes whose MIDI input is to be disconnected.
    @discussion
        If a tap block is installed on the source node, it will be removed when the last
        connection from the source node is removed.
 */
- (void)disconnectMIDI:(AVAudioNode *)sourceNode fromNodes:(NSArray<AVAudioNode *> *)destinationNodes API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

/*! @method disconnectMIDIInput:
    @abstract
        Disconnects all input MIDI connections of this node.
    @param node
        The node whose MIDI input is to be disconnected.
*/
- (void)disconnectMIDIInput:(AVAudioNode *)node API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

/*! @method disconnectMIDIOutput:
    @abstract
        Disconnects all output MIDI connections of this node.
    @param node
        The node whose MIDI outputs are to be disconnected.
*/
- (void)disconnectMIDIOutput:(AVAudioNode *)node API_AVAILABLE(macos(10.14), ios(12.0), watchos(5.0), tvos(12.0));

#endif // AVAUDIOENGINE_HAVE_AUAUDIOUNIT

@end // AVAudioEngine

/*!	@constant AVAudioEngineConfigurationChangeNotification
	@abstract
		A notification generated on engine configuration changes when rendering to/from an audio
		device.
	@discussion
		Register for this notification on your engine instances, as follows:
		
		[[NSNotificationCenter defaultCenter] addObserver: myObject 
			 selector:    @selector(handleInterruption:)
			 name:        AVAudioEngineConfigurationChangeNotification
			 object:      engine];

		When the engine's I/O unit observes a change to the audio input or output hardware's
		channel count or sample rate, the engine stops itself (see `AVAudioEngine(stop)`), and
		issues this notification.
		The nodes remain attached and connected with previously set formats. However, the app 
		must reestablish connections if the connection formats need to change (e.g. in an 
		input node chain, connections must follow the hardware sample rate, while in an output only
		chain, the output node supports rate conversion).
 
		Note that the engine must not be deallocated from within the client's notification handler
		because the callback happens on an internal dispatch queue and can deadlock while trying to 
		synchronously teardown the engine.
*/
AVF_EXPORT
NSString *const AVAudioEngineConfigurationChangeNotification API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0));

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitGenerator.h
/*
    File:		AVAudioUnitGenerator.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#import <AVFAudio/AVAudioUnit.h>
#import <AVFAudio/AVAudioMixing.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitGenerator
    @abstract an AVAudioUnit that generates audio output
    @discussion
    An AVAudioUnitGenerator represents an audio unit of type kAudioUnitType_Generator or
	kAudioUnitType_RemoteGenerator.
    A generator will have no audio input, but will just produce audio output.
    A tone generator is an example of this. 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitGenerator : AVAudioUnit <AVAudioMixing>

#if AVAUDIOUNIT_HAVE_AUDIOUNIT
/*! @method initWithAudioComponentDescription:
    @abstract Create an AVAudioUnitGenerator object.
    
    @param audioComponentDescription
    @abstract AudioComponentDescription of the audio unit to be instantiated.
    @discussion
    The componentType must be kAudioUnitType_Generator or kAudioUnitType_RemoteGenerator
*/
- (instancetype)initWithAudioComponentDescription:(AudioComponentDescription)audioComponentDescription;
#endif

/*! @property bypass
    @abstract Bypass state of the audio unit.
*/
@property (nonatomic) BOOL bypass;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioTime.h
/*
	File:		AVAudioTime.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

NS_ASSUME_NONNULL_BEGIN

/*!
	@class AVAudioTime
	@abstract Represent a moment in time.
	@discussion
		AVAudioTime is used in AVAudioEngine to represent time. Instances are immutable.
		
		A single moment in time may be represented in two different ways:
		1. mach_absolute_time(), the system's basic clock. Commonly referred to as "host time."
		2. audio samples at a particular sample rate
		
		A single AVAudioTime instance may contain either or both representations; it might
		represent only a sample time, only a host time, or both.
		
Rationale for using host time:
[a] internally we are using AudioTimeStamp, which uses host time, and it seems silly to divide
[b] it is consistent with a standard system timing service
[c] we do provide conveniences to convert between host ticks and seconds (host time divided by
	frequency) so client code wanting to do what should be straightforward time computations can at 
	least not be cluttered by ugly multiplications and divisions by the host clock frequency.
*/

OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioTime : NSObject {
@private
	AudioTimeStamp _ats;
	double _sampleRate;
	void *_reserved;
}

/*!	@method initWithAudioTimeStamp:sampleRate:
*/
- (instancetype)initWithAudioTimeStamp: (const AudioTimeStamp *)ts sampleRate: (double)sampleRate;

/*! @method initWithHostTime:
*/
- (instancetype)initWithHostTime:(uint64_t)hostTime;

/*! @method initWithSampleTime:atRate:
*/
- (instancetype)initWithSampleTime:(AVAudioFramePosition)sampleTime atRate:(double)sampleRate;

/*! @method initWithHostTime:sampleTime:atRate:
*/
- (instancetype)initWithHostTime:(uint64_t)hostTime sampleTime:(AVAudioFramePosition)sampleTime atRate:(double)sampleRate;

/*! @method timeWithAudioTimeStamp:sampleRate:
*/
+ (instancetype)timeWithAudioTimeStamp: (const AudioTimeStamp *)ts sampleRate: (double)sampleRate;

/*! @method timeWithHostTime:
*/
+ (instancetype)timeWithHostTime:(uint64_t)hostTime;

/*! @method timeWithSampleTime:atRate:
*/
+ (instancetype)timeWithSampleTime:(AVAudioFramePosition)sampleTime atRate:(double)sampleRate;

/*! @method timeWithHostTime:sampleTime:atRate:
*/
+ (instancetype)timeWithHostTime:(uint64_t)hostTime sampleTime:(AVAudioFramePosition)sampleTime atRate:(double)sampleRate;

/*!	@method hostTimeForSeconds:
	@abstract Convert seconds to host time.
*/
+ (uint64_t)hostTimeForSeconds:(NSTimeInterval)seconds;

/*!	@method secondsForHostTime:
	@abstract Convert host time to seconds.
*/
+ (NSTimeInterval)secondsForHostTime:(uint64_t)hostTime;

/*!	@method extrapolateTimeFromAnchor:
	@abstract Converts between host and sample time.
	@param anchorTime
		An AVAudioTime with a more complete AudioTimeStamp than that of the receiver (self).
	@return
		the extrapolated time
	@discussion
		If anchorTime is an AVAudioTime where both host time and sample time are valid,
		and self is another timestamp where only one of the two is valid, this method
		returns a new AVAudioTime copied from self and where any additional valid fields provided by
		the anchor are also valid.
 
		Note that the anchorTime must have both host and sample time valid, and self must have
		sample rate and at least one of host or sample time valid. Otherwise this method returns nil.

<pre>
// time0 has a valid audio sample representation, but no host time representation.
AVAudioTime *time0 = [AVAudioTime timeWithSampleTime: 0.0 atRate: 44100.0];
// anchor has a valid host time representation and sample time representation.
AVAudioTime *anchor = [player playerTimeForNodeTime: player.lastRenderTime];
// fill in valid host time representation
AVAudioTime *fullTime0 = [time0 extrapolateTimeFromAnchor: anchor];
</pre>
*/
- (nullable AVAudioTime *)extrapolateTimeFromAnchor:(AVAudioTime *)anchorTime;


/*! @property hostTimeValid
	@abstract Whether the hostTime property is valid.
*/
@property (nonatomic, readonly, getter=isHostTimeValid) BOOL hostTimeValid;

/*! @property hostTime
	@abstract The host time.
*/
@property (nonatomic, readonly) uint64_t hostTime;

/*! @property sampleTimeValid
	@abstract Whether the sampleTime and sampleRate properties are valid.
*/
@property (nonatomic, readonly, getter=isSampleTimeValid) BOOL sampleTimeValid;

/*!	@property sampleTime
	@abstract The time as a number of audio samples, as tracked by the current audio device.
*/
@property (nonatomic, readonly) AVAudioFramePosition sampleTime;

/*!	@property sampleRate
	@abstract The sample rate at which sampleTime is being expressed.
*/
@property (nonatomic, readonly) double sampleRate;

/*! @property audioTimeStamp
	@abstract The time expressed as an AudioTimeStamp structure.
	@discussion
		This may be useful for compatibility with lower-level CoreAudio and AudioToolbox API's.
*/
@property (readonly, nonatomic) AudioTimeStamp audioTimeStamp;


@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitMIDIInstrument.h
/*
	File:		AVAudioUnitMIDIInstrument.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnit.h>
#import <AVFAudio/AVAudioMixing.h>

#if defined(__MAC_OS_X_VERSION_MIN_REQUIRED) && __MAC_OS_X_VERSION_MIN_REQUIRED >= 101100
	#define AVAudioUnitMIDIInstrument_MixingConformance <AVAudioMixing>
#elif defined(__IPHONE_OS_VERSION_MIN_REQUIRED) && __IPHONE_OS_VERSION_MIN_REQUIRED >= 90000
	#define AVAudioUnitMIDIInstrument_MixingConformance <AVAudioMixing>
#else
	#define AVAudioUnitMIDIInstrument_MixingConformance
#endif


NS_ASSUME_NONNULL_BEGIN

/*!
 @class AVAudioUnitMIDIInstrument
 @abstract Base class for sample synthesizers.
 @discussion
    This base class represents audio units of type kAudioUnitType_MusicDevice or kAudioUnitType_RemoteInstrument. This can be used in a chain
    that processes realtime input (live) and has general concept of music events i.e. notes.
 */
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitMIDIInstrument : AVAudioUnit AVAudioUnitMIDIInstrument_MixingConformance

#if AVAUDIOUNIT_HAVE_AUDIOUNIT
/*! @method initWithAudioComponentDescription:
 @abstract initialize the node with the component description
 @param description
    audio component description structure that describes the audio component of type kAudioUnitType_MusicDevice
    or kAudioUnitType_RemoteInstrument.
 */
- (instancetype)initWithAudioComponentDescription:(AudioComponentDescription)description;
#endif

/*! @method startNote:withVelocity:onChannel:
 @abstract sends a MIDI Note On event to the instrument
 @param note
    the note number (key) to play.
    Range: 0 -> 127
 @param velocity
    specifies the volume with which the note is played.
    Range: 0 -> 127
 @param channel
    the channel number to which the event is sent.
 */
- (void)startNote:(uint8_t)note withVelocity:(uint8_t)velocity onChannel:(uint8_t)channel;

/*! @method stopNote:onChannel:
 @abstract sends a MIDI Note Off event to the instrument
 @param note
    the note number (key) to stop
    Range: 0 -> 127
 @param channel
    the channel number to which the event is sent. 

 */
- (void)stopNote:(uint8_t)note onChannel:(uint8_t)channel;

/*! @method sendController:withValue:onChannel:
 @abstract send a MIDI controller event to the instrument.
 @param controller
    a standard MIDI controller number. 
    Range: 0 -> 127
 @param  value
    value for the controller. 
    Range: 0 -> 127
 @param channel
    the channel number to which the event is sent. 
 
 */
- (void)sendController:(uint8_t)controller withValue:(uint8_t)value onChannel:(uint8_t)channel;

/*! @method sendPitchBend:onChannel:
 @abstract sends MIDI Pitch Bend event to the instrument.
 @param pitchbend
    value of the pitchbend
    Range: 0 -> 16383
 @param channel
    the channel number to which the pitch bend message is sent
 
 */
- (void)sendPitchBend:(uint16_t)pitchbend onChannel:(uint8_t)channel;

/*! @method sendPressure:onChannel:
 @abstract sends MIDI channel pressure event to the instrument.
 @param pressure 
    value of the pressure.
    Range: 0 -> 127
 @param channel
    the channel number to which the event is sent. 

 */
- (void)sendPressure:(uint8_t)pressure onChannel:(uint8_t)channel;

/*! @method sendPressureForKey:withValue:onChannel:
 @abstract sends MIDI Polyphonic key pressure event to the instrument
 @param key
    the key (note) number to which the pressure event applies
    Range: 0 -> 127
 @param value
    value of the pressure
    Range: 0 -> 127
 @param channel
    channel number to which the event is sent. 

 */
- (void)sendPressureForKey:(uint8_t)key withValue:(uint8_t)value onChannel:(uint8_t)channel;

/*! @method sendProgramChange:onChannel:
 @abstract sends MIDI Program Change event to the instrument
 @param program
    the program number.
    Range: 0 -> 127
 @param channel
    channel number to which the event is sent.
 @discussion
    the instrument will be loaded from the bank that has been previous set by MIDI Bank Select
    controller messages (0 and 31). If none has been set, bank 0 will be used. 
 */
- (void)sendProgramChange:(uint8_t)program onChannel:(uint8_t)channel;

/*! @method sendProgramChange:bankMSB:bankLSB:onChannel:
 @abstract sends a MIDI Program Change and Bank Select events to the instrument
 @param program
    specifies the program (preset) number within the bank to load.
    Range: 0 -> 127
 @param bankMSB
    specifies the most significant byte value for the bank to select.
    Range: 0 -> 127
 @param bankLSB
    specifies the least significant byte value for the bank to select.
    Range: 0 -> 127
 @param channel
    channel number to which the events are sent.
 @discussion
 
 */
- (void)sendProgramChange:(uint8_t)program bankMSB:(uint8_t)bankMSB bankLSB:(uint8_t)bankLSB onChannel:(uint8_t)channel;

/*! @method sendMIDIEvent:data1:data2:
 @abstract sends a MIDI event which contains two data bytes to the instrument.
 @param midiStatus
    the STATUS value of the MIDI event
 @param data1
    the first data byte of the MIDI event
 @param data2
    the second data byte of the MIDI event.
  */
- (void)sendMIDIEvent:(uint8_t)midiStatus data1:(uint8_t)data1 data2:(uint8_t)data2;

/*! @method sendMIDIEvent:data1:
 @abstract sends a MIDI event which contains one data byte to the instrument.
 @param midiStatus
    the STATUS value of the MIDI event
 @param data1
    the first data byte of the MIDI event
 */
- (void)sendMIDIEvent:(uint8_t)midiStatus data1:(uint8_t)data1;

/*! @method sendMIDISysExEvent:
 @abstract sends a MIDI System Exclusive event to the instrument.
 @param midiData
    a NSData object containing the complete SysEx data including start(F0) and termination(F7) bytes.
 
 */
- (void)sendMIDISysExEvent:(NSData *)midiData;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitEffect.h
/*
    File:		AVAudioUnitEffect.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnit.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitEffect
    @abstract an AVAudioUnit that processes audio in real-time
    @discussion
    An AVAudioUnitEffect represents an audio unit of type kAudioUnitType_Effect,
    kAudioUnitType_MusicEffect, kAudioUnitType_Panner, kAudioUnitType_RemoteEffect or 
    kAudioUnitType_RemoteMusicEffect.

    These effects run in real-time and process some x number of audio input 
    samples to produce x number of audio output samples. A delay unit is an 
    example of an effect unit.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitEffect : AVAudioUnit

#if AVAUDIOUNIT_HAVE_AUDIOUNIT
/*! @method initWithAudioComponentDescription:
    @abstract Create an AVAudioUnitEffect object.
    
    @param audioComponentDescription
    @abstract AudioComponentDescription of the audio unit to be instantiated.
    @discussion
    The componentType must be one of these types
    kAudioUnitType_Effect
    kAudioUnitType_MusicEffect
    kAudioUnitType_Panner
    kAudioUnitType_RemoteEffect
    kAudioUnitType_RemoteMusicEffect

*/
- (instancetype)initWithAudioComponentDescription:(AudioComponentDescription)audioComponentDescription;
#endif

/*! @property bypass
    @abstract Bypass state of the audio unit.
*/
@property (nonatomic) BOOL bypass;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioSettings.h
/*
	File:  AVAudioSettings.h
	
	Framework:  AVFoundation
	
	Copyright 2008-2013 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <Foundation/NSObject.h>
#import <Availability.h>

/* This file's methods are available with iPhone 3.0 or later */

/* property keys - values for all keys defined below are NSNumbers */

/* keys for all formats */
AVF_EXPORT NSString *const AVFormatIDKey;								/* value is an integer (format ID) from CoreAudioTypes.h */
AVF_EXPORT NSString *const AVSampleRateKey;								/* value is floating point in Hertz */
AVF_EXPORT NSString *const AVNumberOfChannelsKey;						/* value is an integer */

/* linear PCM keys */
AVF_EXPORT NSString *const AVLinearPCMBitDepthKey;						/* value is an integer, one of: 8, 16, 24, 32 */
AVF_EXPORT NSString *const AVLinearPCMIsBigEndianKey;					/* value is a BOOL */
AVF_EXPORT NSString *const AVLinearPCMIsFloatKey;						/* value is a BOOL */

AVF_EXPORT NSString *const AVLinearPCMIsNonInterleaved                  API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0));   /* value is a BOOL */
#define AVLinearPCMIsNonInterleavedKey AVLinearPCMIsNonInterleaved

/* audio file type key */
AVF_EXPORT NSString *const AVAudioFileTypeKey API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0)); /* value is an integer (audio file type) from AudioFile.h */

/* encoder property keys */
AVF_EXPORT NSString *const AVEncoderAudioQualityKey;					/* value is an integer from enum AVAudioQuality */
AVF_EXPORT NSString *const AVEncoderAudioQualityForVBRKey               API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0)); /* value is an integer from enum AVAudioQuality. only relevant for AVAudioBitRateStrategy_Variable */

	/* only one of AVEncoderBitRateKey and AVEncoderBitRatePerChannelKey should be provided. */
AVF_EXPORT NSString *const AVEncoderBitRateKey;           				/* value is an integer. */
AVF_EXPORT NSString *const AVEncoderBitRatePerChannelKey                API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0)); /* value is an integer */
AVF_EXPORT NSString *const AVEncoderBitRateStrategyKey                  API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0)); /* value is an AVAudioBitRateStrategy constant. see below. */
AVF_EXPORT NSString *const AVEncoderBitDepthHintKey;					/* value is an integer from 8 to 32 */

/* sample rate converter property keys */
AVF_EXPORT NSString *const AVSampleRateConverterAlgorithmKey API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0)); /* value is an AVSampleRateConverterAlgorithm constant. see below. */
AVF_EXPORT NSString *const AVSampleRateConverterAudioQualityKey;		/* value is an integer from enum AVAudioQuality */

/* channel layout */
AVF_EXPORT NSString *const AVChannelLayoutKey API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0));	/* value is an NSData containing an AudioChannelLayout */


/* property values */

/* values for AVEncoderBitRateStrategyKey */
AVF_EXPORT NSString *const AVAudioBitRateStrategy_Constant              API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
AVF_EXPORT NSString *const AVAudioBitRateStrategy_LongTermAverage       API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
AVF_EXPORT NSString *const AVAudioBitRateStrategy_VariableConstrained   API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
AVF_EXPORT NSString *const AVAudioBitRateStrategy_Variable              API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));

/* values for AVSampleRateConverterAlgorithmKey */
AVF_EXPORT NSString *const AVSampleRateConverterAlgorithm_Normal        API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
AVF_EXPORT NSString *const AVSampleRateConverterAlgorithm_Mastering     API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
AVF_EXPORT NSString *const AVSampleRateConverterAlgorithm_MinimumPhase  API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

typedef NS_ENUM(NSInteger, AVAudioQuality) {
	AVAudioQualityMin    = 0,
	AVAudioQualityLow    = 0x20,
	AVAudioQualityMedium = 0x40,
	AVAudioQualityHigh   = 0x60,
	AVAudioQualityMax    = 0x7F
};

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitComponent.h
/*
 File:		AVAudioUnitComponent.h
 Framework:	AVFoundation
 
 Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#if !0

#import <AVFAudio/AVAudioTypes.h>

#if __has_include(<AudioToolbox/AudioComponent.h>)
#define AVAUDIOUNITCOMPONENT_HAVE_AUDIOCOMPONENT 1
#import <AudioToolbox/AudioComponent.h>
#import <AudioToolbox/AUComponent.h>
#endif

NS_ASSUME_NONNULL_BEGIN

// Standard Audio Unit Types
AVF_EXPORT NSString * const AVAudioUnitTypeOutput				API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeMusicDevice			API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeMusicEffect			API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeFormatConverter		API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeEffect				API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeMixer				API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypePanner				API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeGenerator			API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeOfflineEffect		API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);
AVF_EXPORT NSString * const AVAudioUnitTypeMIDIProcessor		API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);

// Standard Audio Unit Manufacturers
AVF_EXPORT NSString * const AVAudioUnitManufacturerNameApple	API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);

#pragma mark AVAudioUnitComponent

/*!
 @class AVAudioUnitComponent
 @discussion
	 AVAudioUnitComponent provides details about an audio unit such as type, subtype, manufacturer, 
	 location etc. User tags can be added to the AVAudioUnitComponent which can be queried later
 	 for display.
 */

OS_EXPORT API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitComponent : NSObject
{
	void *_impl;
}
/*! @property name
	@abstract the name of an audio component
 */
@property (nonatomic, readonly) NSString		*name;

/*! @property typeName
	@abstract standard audio component types returned as strings
 */
@property (nonatomic, readonly) NSString		*typeName;

/*! @property typeName
	@abstract localized string of typeName for display
 */
@property (nonatomic, readonly) NSString		*localizedTypeName;

/*! @property manufacturerName
	@abstract the manufacturer name, extracted from the manufacturer key defined in Info.plist dictionary
 */
@property (nonatomic, readonly) NSString		*manufacturerName;

/*! @property version
	@abstract version number comprised of a hexadecimal number with major, minor, dot-release format: 0xMMMMmmDD
 */
@property (nonatomic, readonly) NSUInteger	version;

/*! @property versionString
	@abstract version number as string
 */
@property (nonatomic, readonly) NSString		*versionString;

/*! @property componentURL
	@abstract URL representing location of component
 */
@property (nonatomic, readonly, nullable) NSURL		*componentURL NS_DEPRECATED(10_10, 10_11, NA, NA);

/*! @property availableArchitectures
	@abstract NSArray of NSNumbers each of which corresponds to one of the constants in Mach-O Architecture in NSBundle Class Reference
 */
@property (nonatomic, readonly) NSArray<NSNumber *>		*availableArchitectures API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

/*! @property sandboxSafe
	@abstract On OSX, YES if the AudioComponent can be loaded into a sandboxed process otherwise NO.
			  On iOS, this is always YES.
 */
@property (nonatomic, readonly, getter=isSandboxSafe) BOOL		sandboxSafe;

/*! @property hasMIDIInput
	@abstract YES if AudioComponent has midi input, otherwise NO
 */
@property (nonatomic, readonly) BOOL		hasMIDIInput;

/*! @property hasMIDIOutput
	@abstract YES if AudioComponent has midi output, otherwise NO
 */
@property (nonatomic, readonly) BOOL		hasMIDIOutput;

#if AVAUDIOUNITCOMPONENT_HAVE_AUDIOCOMPONENT
/*! @property audioComponent
	@abstract the audioComponent that can be used in AudioComponent APIs.
 */
@property (nonatomic, readonly) AudioComponent	audioComponent;
#endif

/*! @property userTagNames
	@abstract User tags represent the tags from the current user.
 */
@property (copy) NSArray<NSString *>		*userTagNames API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

/*! @property allTagNames
	@abstract represent the tags from the current user and the system tags defined by AudioComponent.
 */
@property (nonatomic, readonly) NSArray<NSString *>		*allTagNames;

#if AVAUDIOUNITCOMPONENT_HAVE_AUDIOCOMPONENT
/*! @property audioComponentDescription
	@abstract description of the audio component that can be used in AudioComponent APIs.
 */
@property (nonatomic, readonly) AudioComponentDescription	audioComponentDescription;
#endif

/*! @property iconURL
	@abstract A URL that will specify the location of an icon file that can be used when presenting UI
 for this audio component.
 */
@property (nonatomic, readonly, nullable) NSURL		*iconURL API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

#if TARGET_OS_OSX
/*! @property icon
	@abstract An icon representing the component.
    @discussion
        For a component originating in an app extension, the returned icon will be that of the
        application containing the extension.
        
        For components loaded from bundles, the icon will be that of the bundle.
 */
@property (nonatomic, readonly, nullable) NSImage *icon API_AVAILABLE(macos(10.11)) API_UNAVAILABLE(ios, watchos, tvos);
#endif

/*! @property passesAUVal
	@abstract YES if the AudioComponent has passed the AU validation tests, otherwise NO
 */
@property (nonatomic, readonly) BOOL		passesAUVal API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

/*! @property hasCustomView
	@abstract YES if the AudioComponent provides custom view, otherwise NO
 */
@property (nonatomic, readonly) BOOL		hasCustomView API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

/*! @property configurationDictionary
	@abstract A NSDictionary that contains information describing the capabilities of the AudioComponent.
	The specific information depends on the type and the keys are defined in AudioUnitProperties.h
 */
@property (nonatomic, readonly) NSDictionary<NSString *, id>		*configurationDictionary API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

/*! @property supportsNumberInputChannels: outputChannels:
	@abstract returns YES if the AudioComponent supports the input/output channel configuration
 */
- (BOOL)supportsNumberInputChannels:(NSInteger)numInputChannels outputChannels:(NSInteger)numOutputChannels API_AVAILABLE(macos(10.10)) API_UNAVAILABLE(ios, watchos, tvos);

@end


#pragma mark AVAudioUnitComponentManager

/* The notification object is an AVAudioUnitComponent object */
AVF_EXPORT NSString * const AVAudioUnitComponentTagsDidChangeNotification API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos);

/*!
 @class AVAudioUnitComponentManager
 @discussion 
 		AVAudioUnitComponentManager is a singleton object that provides an easy way to find
 		audio components that are registered with the system. It provides methods to search and
 		query various information about the audio components without opening them.
 
 		Currently audio components that are audio units can only be searched.
 
 		The class also supports predefined system tags and arbitrary user tags. Each audio unit can be 
 		tagged as part of its definition. Refer to AudioComponent.h for more details. AudioUnit Hosts
 		such as Logic or GarageBand can present groupings of audio units based on the tags.
 
 		Searching for audio units can be done in various ways
 			- using a NSPredicate that contains search strings for tags or descriptions
 			- using a block to match on custom criteria 
			- using an AudioComponentDescription
 */

OS_EXPORT API_AVAILABLE(macos(10.10), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitComponentManager : NSObject
{
	void *_impl;
}

/*! @discussion 
 		returns all tags associated with the current user as well as all system tags defined by 
		the audio unit(s).
 */
@property (nonatomic, readonly) NSArray<NSString *>		*tagNames;

/*! @discussion
		returns the localized standard system tags defined by the audio unit(s).
 */

@property (nonatomic, readonly) NSArray<NSString *>		*standardLocalizedTagNames;

/* returns singleton instance of AVAudioUnitComponentManager */
+ (instancetype)sharedAudioUnitComponentManager;

/*!
 @method componentsMatchingPredicate:
 @abstract	returns an array of AVAudioUnitComponent objects that match the search predicate.
 @discussion
 		AudioComponent's information or tags can be used to build a search criteria. 
 		For example, "typeName CONTAINS 'Effect'" or tags IN {'Sampler', 'MIDI'}"
 */
- (NSArray<AVAudioUnitComponent *> *)componentsMatchingPredicate:(NSPredicate *)predicate;

/*!
 @method componentsPassingTest:
 @abstract	returns an array of AVAudioUnitComponent objects that pass the user provided block method.
 @discussion
		For each AudioComponent found by the manager, the block method will be called. If the return
 		value is YES then the AudioComponent is added to the resulting array else it will excluded. 
 		This gives more control to the block provider to filter out the components returned.
 */
- (NSArray<AVAudioUnitComponent *> *)componentsPassingTest:(BOOL(^)(AVAudioUnitComponent *comp, BOOL *stop))testHandler;

#if AVAUDIOUNITCOMPONENT_HAVE_AUDIOCOMPONENT
/*!
 @method componentsMatchingDescription:
 @abstract	returns an array of AVAudioUnitComponent objects that match the description.
 @discussion
 		This method provides a mechanism to search for AudioComponents using AudioComponentDescription
		structure. The type, subtype and manufacturer fields are used to search for audio units. A 
 		value of 0 for any of these fields is a wildcard and returns the first match found.
 */
- (NSArray<AVAudioUnitComponent *> *)componentsMatchingDescription:(AudioComponentDescription)desc;
#endif

@end

NS_ASSUME_NONNULL_END

#endif // !TARGET_OS_BRIDGE
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioSequencer.h
/*
	File:		AVAudioSequencer.h
	Framework:	AVFoundation

	Copyright (c) 2015 Apple Inc. All Rights Reserved.
*/

#if !0

#import <Foundation/Foundation.h>

#if __has_include(<CoreMIDI/MIDIServices.h>)
	#import <CoreMIDI/MIDIServices.h>
#endif

NS_ASSUME_NONNULL_BEGIN

@class AVAudioUnit;
@class AVAudioTime;
@class AVAudioEngine;
@class AVMusicTrack;
@class AVMusicTrackEventIterator;
@class AVAudioSequencer;

/*!	@typedef AVMusicTimeStamp
	@abstract A fractional number of beats
	
	@discussion
		This is used for all sequencer timeline-related methods.  The relationship between this
		value and time in seconds is determined by the sequence's tempo.
*/
typedef Float64 AVMusicTimeStamp;

/*! @typedef AVMusicSequenceLoadOptions
	@abstract Determines whether data on different MIDI channels is mapped to multiple tracks, or
		if the tracks are preserved as-is.
	@discussion
		If AVMusicSequenceLoadSMF_ChannelsToTracks is set, the loaded MIDI Sequence will contain a
		tempo track, one track for each MIDI channel that is found in the SMF, and one track for
		SysEx and/or MetaEvents (this will be the last track in the sequence).

		If AVMusicSequenceLoadSMF_ChannelsToTracks is not set, the loadad MIDI Sequence will
		contain one track for each track that is found in the SMF, plus a tempo track (if not found
		in the SMF).
		
		API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0))
*/
typedef NS_OPTIONS(NSUInteger, AVMusicSequenceLoadOptions) {
	AVMusicSequenceLoadSMF_PreserveTracks		= 0,				// 0x00
	AVMusicSequenceLoadSMF_ChannelsToTracks		= (1UL << 0)		// 0x01
};

/*! @typedef AVBeatRange
	@abstract Used to describe a specific time range within an AVMusicTrack.
*/
typedef struct _AVBeatRange {
	AVMusicTimeStamp start;
	AVMusicTimeStamp length;
} AVBeatRange;

NS_INLINE AVBeatRange AVMakeBeatRange(AVMusicTimeStamp startBeat, AVMusicTimeStamp lengthInBeats) {
	AVBeatRange r;
	r.start = startBeat;
	r.length = lengthInBeats;
	return r;
}

/*! @class AVAudioSequencer
	@abstract A collection of MIDI events organized into AVMusicTracks, plus a player to play back the events.
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioSequencer : NSObject {
@protected
	void *_impl;
}

/*! @method init
	@abstract
		Initialize a new sequencer, which will not be connected to an audio engine.
	@discussion
		This is used to create a sequencer whose tracks will only send events to external MIDI endpoints.
*/
- (instancetype)init	API_UNAVAILABLE(tvos);

/*! @method initWithAudioEngine:
	@abstract
		Initialize a new sequencer, handing it the audio engine.
*/
- (instancetype)initWithAudioEngine:(AVAudioEngine *)engine;

/*! @method loadFromURL:options:error:
	@abstract Load the file referenced by the URL and add the events to the sequence
	@param fileURL
	@param options
		determines how the file's contents are mapped to tracks inside the sequence
	@param outError
*/
- (BOOL)loadFromURL:(NSURL *)fileURL options:(AVMusicSequenceLoadOptions)options error:(NSError **)outError;

/*! @method loadFromData:options:error:
	@abstract Parse the data and add the its events to the sequence
	@param data
	@param options
		determines how the contents are mapped to tracks inside the sequence
	@param outError
*/
- (BOOL)loadFromData:(NSData *)data options:(AVMusicSequenceLoadOptions)options error:(NSError **)outError;

/*! @method writeToURL:SMPTEResolution:replaceExisting:error:
	@abstract Create and write a MIDI file from the events in the sequence
	@param fileURL
		the path for the file to be created
	@param resolution
		the relationship between "tick" and quarter note for saving to a Standard MIDI File - pass in
		zero to use default - this will be the value that is currently set on the tempo track
	@param replace
		if the file already exists, YES will cause it to be overwritten with the new data.
		Otherwise the call will fail with a permission error.
	@param outError
	@discussion
		Only MIDI events are written when writing to the MIDI file. MIDI files are normally beat
		based, but can also have a SMPTE (or real-time rather than beat time) representation.
		The relationship between "tick" and quarter note for saving to Standard MIDI File
		- pass in zero to use default - this will be the value that is currently set on the tempo track
*/
- (BOOL)writeToURL:(NSURL *)fileURL SMPTEResolution:(NSInteger)resolution replaceExisting:(BOOL)replace error:(NSError **)outError;

/*!	@method dataWithSMPTEResolution:error:
	@abstract Return a data object containing the events from the sequence
	@discussion
		All details regarding the SMPTE resolution apply here as well.
		The returned NSData lifetime is controlled by the client.
*/
- (NSData *)dataWithSMPTEResolution:(NSInteger)SMPTEResolution error:(NSError **)outError;

/*!	@method secondsForBeats:
	@abstract Get the time in seconds for the given beat position (timestamp) in the track
*/
- (NSTimeInterval)secondsForBeats:(AVMusicTimeStamp)beats;

/*!	@method beatsForSeconds:
	@abstract Get the beat position (timestamp) for the given time in the track
*/
- (AVMusicTimeStamp)beatsForSeconds:(NSTimeInterval)seconds;

/* properties */

/*!	@property tracks
	@abstract An NSArray containing all the tracks in the sequence
	@discussion
		Track indices count from 0, and do not include the tempo track.
*/
@property (nonatomic, readonly) NSArray<AVMusicTrack *> *tracks;

/*!	@property tempoTrack
	@abstract The tempo track
	 @discussion
		 Each sequence has a single tempo track. All tempo events are placed into this track (as well
		 as other appropriate events (for instance, the time signature from a MIDI file). The tempo
		 track can be edited and iterated upon as any other track. Non-tempo events in a tempo track
		 are ignored.
*/
@property (nonatomic, readonly) AVMusicTrack *tempoTrack;

/*!	@property userInfo
	@abstract A dictionary containing meta-data derived from a sequence
	@discussion
		The dictionary can contain one or more of the kAFInfoDictionary_* keys
		specified in <AudioToolbox/AudioFile.h>
*/
@property (nonatomic, readonly) NSDictionary<NSString *, id> *userInfo;

@end

@interface AVAudioSequencer(AVAudioSequencer_Player)

/*! @property currentPositionInSeconds
	@abstract The current playback position in seconds
	@discussion
		Setting this positions the sequencer's player to the specified time.  This can be set while
		the player is playing, in which case playback will resume at the new position.
*/
@property(nonatomic) NSTimeInterval currentPositionInSeconds;

/*! @property currentPositionInBeats
	@abstract The current playback position in beats
	@discussion
		Setting this positions the sequencer's player to the specified beat.  This can be set while
		the player is playing, in which case playback will resume at the new position.
*/
@property(nonatomic) NSTimeInterval currentPositionInBeats;


/*! @property playing
	@abstract Indicates whether or not the sequencer's player is playing
	@discussion
		Returns TRUE if the sequencer's player has been started and not stopped. It may have
		"played" past the end of the events in the sequence, but it is still considered to be
		playing (and its time value increasing) until it is explicitly stopped.
*/
@property(nonatomic, readonly, getter=isPlaying) BOOL playing;

/*! @property rate
	@abstract The playback rate of the sequencer's player
	@discussion
		1.0 is normal playback rate.  Rate must be > 0.0.
*/
@property (nonatomic) float rate;

/*!	@method hostTimeForBeats:error:
	@abstract Returns the host time that will be (or was) played at the specified beat.
	@discussion
		This call is only valid if the player is playing and will return 0 with an error if the
		player is not playing or if the starting position of the player (its "starting beat") was 
		after the specified beat.  The method uses the sequence's tempo map to translate a beat
		time from the starting time and beat of the player.
*/
- (UInt64)hostTimeForBeats:(AVMusicTimeStamp)inBeats error:(NSError **)outError;

/*!	@method beatsForHostTime:error:
	@abstract Returns the beat that will be (or was) played at the specified host time.
	@discussion
		This call is only valid if the player is playing and will return 0 with an error if the
		player is not playing or if the starting time of the player was after the specified host
		time.  The method uses the sequence's tempo map to retrieve a beat time from the starting
		and specified host time.
*/
- (AVMusicTimeStamp)beatsForHostTime:(UInt64)inHostTime error:(NSError **)outError;

/*! @method prepareToPlay
	@abstract Get ready to play the sequence by prerolling all events
	@discussion
		Happens automatically on play if it has not already been called, but may produce a delay in
		startup.
*/
- (void)prepareToPlay;

/*!	@method	startAndReturnError:
	@abstract	Start the sequencer's player
	@discussion
		If the AVAudioSequencer has not been prerolled, it will pre-roll itself and then start.
*/
- (BOOL)startAndReturnError:(NSError **)outError;

/*!	@method	stop
	@abstract	Stop the sequencer's player
	@discussion
		Stopping the player leaves it in an un-prerolled state, but stores the playback position so
		that a subsequent call to startAndReturnError will resume where it left off. This action
		will not stop an associated audio engine.
*/
- (void)stop;

@end


/*! @class AVMusicTrack
	@abstract A collection of music events which will be sent to a given destination, and which can be 
				offset, muted, etc. independently of events in other tracks.
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVMusicTrack : NSObject {
@protected
	void *_impl;
}

/* properties */

/*!	@property destinationAudioUnit
	@abstract The AVAudioUnit which will receive the track's events
	@discussion
		This is mutually exclusive with setting a destination MIDIEndpoint.  The AU must already be
		attached to an audio engine, and the track must be part of the AVAudioSequencer associated
		with that engine. When playing, the track will send its events to that AVAudioUnit. The
		destination AU cannot be changed while the track's sequence is playing.
*/
@property (nonatomic, retain, nullable) AVAudioUnit *destinationAudioUnit;

/*!	@property destinationMIDIEndpoint
	@abstract Set the track's target to the specified MIDI endpoint
	@discussion
		This is mutually exclusive with setting a destination audio unit.  Setting this will remove
		the track's reference to an AVAudioUnit destination.  When played, the track will send its
		events to the MIDI Endpoint.  See also MIDIDestinationCreate.  The endpoint cannot be
		changed while the track's sequence is playing.
*/
#if TARGET_OS_OSX || TARGET_OS_IOS
@property (nonatomic) MIDIEndpointRef destinationMIDIEndpoint;
#endif

/*!	@property loopRange
	@abstract The timestamp range in beats for the loop
	@discussion
		The loop is set by specifying its beat range.
*/
@property (nonatomic) AVBeatRange loopRange;

/*!	@property loopingEnabled
	@abstract Determines whether or not the track is looped.
	@discussion
		If loopRange has not been set, the full track will be looped.
*/
@property (nonatomic,getter=isLoopingEnabled) BOOL loopingEnabled;

typedef NS_ENUM(NSInteger, AVMusicTrackLoopCount) {
	AVMusicTrackLoopCountForever		= -1
} NS_ENUM_AVAILABLE(10_10, 8_0);

/*!	@property numberOfLoops
	@abstract The number of times that the track's loop will repeat
	@discussion
		If set to AVMusicTrackLoopCountForever, the track will loop forever.
		Otherwise, legal values start with 1.
*/
@property (nonatomic) NSInteger numberOfLoops;

/*! @property offsetTime
	@abstract Offset the track's start time to the specified time in beats
	@discussion
		By default this value is zero.
*/
@property (nonatomic) AVMusicTimeStamp offsetTime;

/*! @property muted
	@abstract Whether the track is muted
*/
@property (nonatomic,getter=isMuted) BOOL muted;

/*! @property soloed
	@abstract Whether the track is soloed
*/
@property (nonatomic,getter=isSoloed) BOOL soloed;

/*! @property lengthInBeats
	@abstract The total duration of the track in beats
	@discussion
		This will return the beat of the last event in the track plus any additional time that may
		be needed for fading out of ending notes or round a loop point to musical bar, etc.  If this
		has not been set by the user, the track length will always be adjusted to the end of the
		last active event in a track and is adjusted dynamically as events are added or removed.

		The property will return the maximum of the user-set track length, or the calculated length.
*/
@property (nonatomic) AVMusicTimeStamp lengthInBeats;

/*! @property lengthInSeconds
	@abstract The total duration of the track in seconds
	@discussion
		This will return time of the last event in the track plus any additional time that may be
		needed for fading out of ending notes or round a loop point to musical bar, etc.  If this
		has not been set by the user, the track length will always be adjusted to the end of the
		last active event in a track and is adjusted dynamically as events are added or removed.

		The property will return the maximum of the user-set track length, or the calculated length.
*/
@property (nonatomic) NSTimeInterval lengthInSeconds;


/*! @property timeResolution
	@abstract The time resolution value for the sequence, in ticks (pulses) per quarter note (PPQN)
	@discussion
		If a MIDI file was used to construct the containing sequence, the resolution will be what
		was in the file. If you want to keep a time resolution when writing a new file, you can
		retrieve this value and then specify it when calling -[AVAudioSequencer
		writeToFile:flags:withResolution]. It has no direct bearing on the rendering or notion of
		time of the sequence itself, just its representation in MIDI files. By default this is set
		to either 480 if the sequence was created manually, or a value based on what was in a MIDI
		file if the sequence was created from a MIDI file.
		
		This can only be retrieved from the tempo track.
*/
@property (nonatomic, readonly) NSUInteger timeResolution;

@end

NS_ASSUME_NONNULL_END

#endif // !TARGET_OS_BRIDGE
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVMIDIPlayer.h
/*
 	File:		AVMIDIPlayer.h
 	Framework:	AVFoundation
 
 	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#if !0

#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAudioTime;

/*! @typedef AVMIDIPlayerCompletionHandler
	@abstract Generic callback block.
 */
typedef void (^AVMIDIPlayerCompletionHandler)(void);

/*! @class AVMIDIPlayer
	@abstract A player for music file formats (MIDI, iMelody).
 */
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVMIDIPlayer : NSObject {
@protected
	void *_impl;
}

/*!	@method initWithContentsOfURL:soundBankURL:error:
 	@abstract Create a player with the contents of the file specified by the URL.
	@discussion
 		'bankURL' should contain the path to a SoundFont2 or DLS bank to be used
 		by the MIDI synthesizer.  For OSX it can be set to nil for the default,
 		but for iOS it must always refer to a valid bank file.
*/
- (nullable instancetype)initWithContentsOfURL:(NSURL *)inURL soundBankURL:(NSURL * __nullable)bankURL error:(NSError **)outError;

/*!	@method initWithData:soundBankURL:error:
	@abstract Create a player with the contents of the data object
	@discussion
		'bankURL' should contain the path to a SoundFont2 or DLS bank to be used
		by the MIDI synthesizer.  For OSX it can be set to nil for the default,
		but for iOS it must always refer to a valid bank file.
 */
- (nullable instancetype)initWithData:(NSData *)data soundBankURL:(NSURL * __nullable)bankURL error:(NSError **)outError;

/* transport control */

/*! @method prepareToPlay
	@abstract Get ready to play the sequence by prerolling all events
	@discussion
		Happens automatically on play if it has not already been called, but may produce a delay in startup.
 */
- (void)prepareToPlay;

/*! @method play:
	@abstract Play the sequence.
 */
- (void)play:(AVMIDIPlayerCompletionHandler __nullable)completionHandler;

/*! @method stop
	@abstract Stop playing the sequence.
 */
- (void)stop;

/* properties */

/*! @property duration
	@abstract The length of the currently loaded file in seconds.
 */
@property(nonatomic, readonly) NSTimeInterval duration;

/*! @property playing
	@abstract Indicates whether or not the player is playing
 */
@property(nonatomic, readonly, getter=isPlaying) BOOL playing;

/*! @property rate
	@abstract The playback rate of the player
	@discussion
		1.0 is normal playback rate.  Rate must be > 0.0.
 */
@property (nonatomic) float rate;

/*! @property currentPosition
	@abstract The current playback position in seconds
	@discussion
		Setting this positions the player to the specified time.  No range checking on the time value is done.
 		This can be set while the player is playing, in which case playback will resume at the new time.
 */
@property(nonatomic) NSTimeInterval currentPosition;

@end

NS_ASSUME_NONNULL_END

#endif // !TARGET_OS_BRIDGE
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioSession.h
/*
	File:  AVAudioSession.h
	
	Framework:  AVFoundation
	
	Copyright © 2009-2018 Apple Inc. All rights reserved.
*/

#ifdef __OBJC2__

#import <AVFoundation/AVBase.h>
#import <Foundation/NSArray.h>
#import <Foundation/NSDate.h> /* for NSTimeInterval */
#import <Foundation/NSObject.h>
#import <CoreAudio/CoreAudioTypes.h>
#import <os/availability.h>
#import <os/base.h>

NS_ASSUME_NONNULL_BEGIN

/* This protocol is available with iPhone 3.0 or later */
@protocol AVAudioSessionDelegate;
@class NSError, NSString, NSNumber;
@class AVAudioSessionChannelDescription, AVAudioSessionPortDescription, AVAudioSessionRouteDescription, AVAudioSessionDataSourceDescription;

/*
 Notes on terminology used in this API.
 Some of the property names and class names in AVAudioSession differ from
 the names used in the 'C' language Audio Session API.  In this API, an audio
 "route" is made up of zero or more input "ports" and zero or more ouput "ports".
 If the current audio category does not support inputs, the route will consist purely of
 outputs.  Conversely, if the category does not support output, the route will
 consist purely of inputs.  Categories that support simultaneous input and output
 will have both inputs and outputs in the route.

 A "port" refers to a single input or output within an audio route.  Examples of
 ports include built-in speaker, wired microphone, or Bluetooth A2DP output.
*/

#pragma mark -- enumerations --

/* Category property */
typedef NSString *AVAudioSessionCategory NS_TYPED_ENUM;

/* Mode property */
typedef NSString *AVAudioSessionMode NS_TYPED_ENUM;

/* AVAudioSessionPort */
typedef NSString *AVAudioSessionPort NS_TYPED_ENUM;

/* AVAudioSessionLocation */
typedef NSString *AVAudioSessionLocation NS_TYPED_ENUM;

/* AVAudioSessionOrientation */
typedef NSString *AVAudioSessionOrientation NS_TYPED_ENUM;

/* AVAudioSessionPolarPattern */
typedef NSString *AVAudioSessionPolarPattern NS_TYPED_ENUM;

/* For use with AVAudioSessionInterruptionNotification */
typedef NS_OPTIONS(NSUInteger, AVAudioSessionInterruptionOptions)
{
	AVAudioSessionInterruptionOptionShouldResume = 1
};

/*  options for use when calling setActive:withOptions:error: 
AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation -- 
Notify an interrupted app that the interruption has ended and it may resume playback. Only valid on 
session deactivation. */
typedef NS_OPTIONS(NSUInteger, AVAudioSessionSetActiveOptions)
{
	AVAudioSessionSetActiveOptionNotifyOthersOnDeactivation = 1
};

/*!
 @enum AVAudioSessionActivationOptions values
 @abstract   For use with -activateWithOptions:completionHandler:.
 @discussion Reserved for future use.
 Added in watchOS 5.0.
 */
typedef NS_OPTIONS(NSUInteger, AVAudioSessionActivationOptions)
{
	AVAudioSessionActivationOptionNone	= 0
};

/* values to use for setting overrideOutputAudioPort property
AVAudioSessionPortOverrideNone -- 
No override.  Return audio routing to the default state for the current audio category.
AVAudioSessionPortOverrideSpeaker -- 
Route audio output to speaker.  Use this override with AVAudioSessionCategoryPlayAndRecord, which by 
default routes the output to the receiver. */
typedef NS_ENUM(NSUInteger, AVAudioSessionPortOverride)
{
	AVAudioSessionPortOverrideNone = 0,
	AVAudioSessionPortOverrideSpeaker API_UNAVAILABLE(tvos, watchos, macos) = 'spkr'
};

/* values for AVAudioSessionRouteChangeReasonKey in AVAudioSessionRouteChangeNotification userInfo dictionary
 AVAudioSessionRouteChangeReasonUnknown
	The reason is unknown.
 AVAudioSessionRouteChangeReasonNewDeviceAvailable
	A new device became available (e.g. headphones have been plugged in).
 AVAudioSessionRouteChangeReasonOldDeviceUnavailable
	The old device became unavailable (e.g. headphones have been unplugged).
 AVAudioSessionRouteChangeReasonCategoryChange
	The audio category has changed (e.g. AVAudioSessionCategoryPlayback has been changed to AVAudioSessionCategoryPlayAndRecord).
 AVAudioSessionRouteChangeReasonOverride
	The route has been overridden (e.g. category is AVAudioSessionCategoryPlayAndRecord and the output 
	has been changed from the receiver, which is the default, to the speaker).
 AVAudioSessionRouteChangeReasonWakeFromSleep
	The device woke from sleep.
 AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory
	Returned when there is no route for the current category (for instance, the category is AVAudioSessionCategoryRecord
	but no input device is available).
 AVAudioSessionRouteChangeReasonRouteConfigurationChange
	Indicates that the set of input and/our output ports has not changed, but some aspect of their
	configuration has changed.  For example, a port's selected data source has changed.
*/
typedef NS_ENUM(NSUInteger, AVAudioSessionRouteChangeReason)
{
	AVAudioSessionRouteChangeReasonUnknown = 0,
	AVAudioSessionRouteChangeReasonNewDeviceAvailable = 1,
	AVAudioSessionRouteChangeReasonOldDeviceUnavailable = 2,
	AVAudioSessionRouteChangeReasonCategoryChange = 3,
	AVAudioSessionRouteChangeReasonOverride = 4,
	AVAudioSessionRouteChangeReasonWakeFromSleep = 6,
	AVAudioSessionRouteChangeReasonNoSuitableRouteForCategory = 7,
	AVAudioSessionRouteChangeReasonRouteConfigurationChange = 8 // added in iOS 7
};

/* values for setCategory:withOptions:error:
AVAudioSessionCategoryOptionMixWithOthers -- 
	This allows an application to set whether or not other active audio apps will be interrupted or mixed with
	when your app's audio session goes active. The typical cases are:
	 (1) AVAudioSessionCategoryPlayAndRecord or AVAudioSessionCategoryMultiRoute
		 this will default to false, but can be set to true. This would allow other applications to play in the background
		 while an app had both audio input and output enabled
	 (2) AVAudioSessionCategoryPlayback
		 this will default to false, but can be set to true. This would allow other applications to play in the background,
		 but an app will still be able to play regardless of the setting of the ringer switch
	 (3) Other categories
		 this defaults to false and cannot be changed (that is, the mix with others setting of these categories
		 cannot be overridden. An application must be prepared for setting this property to fail as behaviour 
		 may change in future releases. If an application changes their category, they should reassert the 
		 option (it is not sticky across category changes).
 
AVAudioSessionCategoryOptionDuckOthers -- 
	This allows an application to set whether or not other active audio apps will be ducked when when your app's audio
	session goes active. An example of this is the Nike app, which provides periodic updates to its user (it reduces the
	volume of any music currently being played while it provides its status). This defaults to off. Note that the other
	audio will be ducked for as long as the current session is active. You will need to deactivate your audio
	session when you want full volume playback of the other audio. 
    If your category is AVAudioSessionCategoryPlayback, AVAudioSessionCategoryPlayAndRecord, or 
	AVAudioSessionCategoryMultiRoute, by default the audio session will be non-mixable and non-ducking. 
	Setting this option will also make your category mixable with others (AVAudioSessionCategoryOptionMixWithOthers
	will be set).
 
AVAudioSessionCategoryOptionAllowBluetooth --
	This allows an application to change the default behaviour of some audio session categories with regards to showing
	bluetooth Hands-Free Profile (HFP) devices as available routes. The current category behavior is:
	 (1) AVAudioSessionCategoryPlayAndRecord
		 this will default to false, but can be set to true. This will allow a paired bluetooth HFP device to show up as
		 an available route for input, while playing through the category-appropriate output
	 (2) AVAudioSessionCategoryRecord
		 this will default to false, but can be set to true. This will allow a paired bluetooth HFP device to show up
		 as an available route for input
	 (3) Other categories
		 this defaults to false and cannot be changed (that is, enabling bluetooth for input in these categories is
		 not allowed)
		 An application must be prepared for setting this option to fail as behaviour may change in future releases.
		 If an application changes their category or mode, they should reassert the override (it is not sticky
		 across category and mode changes).
 
AVAudioSessionCategoryOptionDefaultToSpeaker --
	This allows an application to change the default behaviour of some audio session categories with regards to
	the audio route. The current category behavior is:
	 (1) AVAudioSessionCategoryPlayAndRecord category
		 this will default to false, but can be set to true. this will route to Speaker (instead of Receiver)
		 when no other audio route is connected.
	 (2) Other categories
		 this defaults to false and cannot be changed (that is, the default to speaker setting of these
		 categories cannot be overridden
		 An application must be prepared for setting this property to fail as behaviour may change in future releases.
		 If an application changes their category, they should reassert the override (it is not sticky across
		 category and mode changes). 
 
AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers --
	If another app's audio session mode is set to AVAudioSessionModeSpokenAudio (podcast playback in the background for example),
	then that other app's audio will be interrupted when the current application's audio session goes active. An example of this
	is a navigation app that provides navigation prompts to its user (it pauses any spoken audio currently being played while it
	plays the prompt). This defaults to off. Note that the other app's audio will be paused for as long as the current session is
	active. You will need to deactivate your audio session to allow the other audio to resume playback.
	Setting this option will also make your category mixable with others (AVAudioSessionCategoryOptionMixWithOthers
	will be set).  If you want other non-spoken audio apps to duck their audio when your app's session goes active, also set
	AVAudioSessionCategoryOptionDuckOthers.
 
AVAudioSessionCategoryOptionAllowBluetoothA2DP --
    This allows an application to change the default behaviour of some audio session categories with regards to showing
	bluetooth Advanced Audio Distribution Profile (A2DP), i.e. stereo Bluetooth, devices as available routes. The current 
	category behavior is:
    (1) AVAudioSessionCategoryPlayAndRecord
    this will default to false, but can be set to true. This will allow a paired bluetooth A2DP device to show up as
    an available route for output, while recording through the category-appropriate input
    (2) AVAudioSessionCategoryMultiRoute and AVAudioSessionCategoryRecord
    this will default to false, and cannot be set to true.
    (3) Other categories
    this defaults to true and cannot be changed (that is, bluetooth A2DP ports are always supported in output-only categories).
    An application must be prepared for setting this option to fail as behaviour may change in future releases.
    If an application changes their category or mode, they should reassert the override (it is not sticky
    across category and mode changes).
	Setting both AVAudioSessionCategoryOptionAllowBluetooth and AVAudioSessionCategoryOptionAllowBluetoothA2DP is allowed. In cases
	where a single Bluetooth device supports both HFP and A2DP, the HFP ports will be given a higher priority for routing. For HFP 
	and A2DP ports on separate hardware devices, the last-in wins rule applies.

 AVAudioSessionCategoryOptionAllowAirPlay --
    This allows an application to change the default behaviour of some audio session categories with regards to showing
	AirPlay devices as available routes. See the documentation of AVAudioSessionCategoryOptionAllowBluetoothA2DP for details on 
    how this option applies to specific categories.
 
 */
typedef NS_OPTIONS(NSUInteger, AVAudioSessionCategoryOptions)
{
	/* MixWithOthers is only valid with AVAudioSessionCategoryPlayAndRecord, AVAudioSessionCategoryPlayback, and  AVAudioSessionCategoryMultiRoute */
	AVAudioSessionCategoryOptionMixWithOthers			= 0x1,
	/* DuckOthers is only valid with AVAudioSessionCategoryAmbient, AVAudioSessionCategoryPlayAndRecord, AVAudioSessionCategoryPlayback, and AVAudioSessionCategoryMultiRoute */
	AVAudioSessionCategoryOptionDuckOthers				= 0x2,
	/* AllowBluetooth is only valid with AVAudioSessionCategoryRecord and AVAudioSessionCategoryPlayAndRecord */
	AVAudioSessionCategoryOptionAllowBluetooth	API_UNAVAILABLE(tvos, watchos, macos) = 0x4,
	/* DefaultToSpeaker is only valid with AVAudioSessionCategoryPlayAndRecord */
	AVAudioSessionCategoryOptionDefaultToSpeaker API_UNAVAILABLE(tvos, watchos, macos) = 0x8,
	/* InterruptSpokenAudioAndMixWithOthers is only valid with AVAudioSessionCategoryPlayAndRecord, AVAudioSessionCategoryPlayback, and AVAudioSessionCategoryMultiRoute */
	AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos)  = 0x11,
	/* AllowBluetoothA2DP is only valid with AVAudioSessionCategoryPlayAndRecord */
	AVAudioSessionCategoryOptionAllowBluetoothA2DP API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0)) API_UNAVAILABLE(macos) = 0x20,
	/* AllowAirPlay is only valid with AVAudioSessionCategoryPlayAndRecord */
	AVAudioSessionCategoryOptionAllowAirPlay API_AVAILABLE(ios(10.0), tvos(10.0)) API_UNAVAILABLE(watchos, macos) = 0x40,
};

typedef NS_ENUM(NSUInteger, AVAudioSessionInterruptionType)
{
	AVAudioSessionInterruptionTypeBegan = 1,  /* the system has interrupted your audio session */
	AVAudioSessionInterruptionTypeEnded = 0,  /* the interruption has ended */
};

/* Used in AVAudioSessionSilenceSecondaryAudioHintNotification to indicate whether optional secondary audio muting should begin or end */
typedef NS_ENUM(NSUInteger, AVAudioSessionSilenceSecondaryAudioHintType)
{
	AVAudioSessionSilenceSecondaryAudioHintTypeBegin = 1,  /* the system is indicating that another application's primary audio has started */
	AVAudioSessionSilenceSecondaryAudioHintTypeEnd = 0,    /* the system is indicating that another application's primary audio has stopped */
};

/*!
	@enum AVAudioSessionRecordPermission values
	@abstract   These are the values returned by recordPermission.
	@constant   AVAudioSessionRecordPermissionUndetermined
		The user has not yet been asked for permission.
	@constant   AVAudioSessionRecordPermissionDenied
 		The user has been asked and has denied permission.
	@constant   AVAudioSessionRecordPermissionGranted
 		The user has been asked and has granted permission.

 	Added in iOS 8.0, watchos 4.0.
*/
typedef NS_ENUM(NSUInteger, AVAudioSessionRecordPermission)
{
	AVAudioSessionRecordPermissionUndetermined		= 'undt',
	AVAudioSessionRecordPermissionDenied			= 'deny',
	AVAudioSessionRecordPermissionGranted			= 'grnt'
};

/*
 @enum AVAudioSessionIOType values
	@abstract   Values to be used by setAggregatedIOPreference:error: method.
	@discussion Starting in iOS 10, applications that use AVCaptureSession on iPads and iPhones that
		support taking Live Photos, will have non-aggregated audio I/O unless the app opts out by
		setting its AVAudioSessionIOType to Aggregated. Non-aggregated audio I/O means that separate
		threads will be used to service audio I/O for input and output directions.

		Note that in cases where the I/O is not aggregated, the sample rate and IO buffer duration 
		properties will map to the output audio device. In this scenario, the input and
		output audio hardware may be running at different sample rates and with different IO buffer 
		durations. If your app requires input and output audio to be presented in the same realtime 
		I/O callback, or requires that input and output audio have the same sample rate or IO buffer
		duration, or if your app requires the ability to set a preferred sample rate or IO buffer duration
		for audio input, set the AVAudioSessionIOType to Aggregated.

		Apps that don't use AVCaptureSession and use AVAudioSessionCategoryPlayAndRecord will continue
		to have aggregated audio I/O, as in previous versions of iOS.

	@constant   AVAudioSessionIOTypeNotSpecified
		The default value.  If your app does not use AVCaptureSession or does not have any specific 
		requirement for aggregating input and output audio in the same realtime I/O callback, use this 
		value. Note that if your app does not use AVCaptureSession, it will get aggregated I/O when using 
		AVAudioSessionCategoryPlayAndRecord.

		If your app does utilize AVCaptureSession, use of this value will allow AVCaptureSession to 
		start recording without glitching already running output audio and will allow the system to
		utilize power-saving optimizations.

	@constant	AVAudioSessionIOTypeAggregated
		Use this value if your session uses AVAudioSessionCategoryPlayAndRecord and requires input and
		output audio to be presented in the same realtime I/O callback. For example, if your app will be using
		a RemoteIO with both input and output enabled. 

		Note that your session's preference to use aggregated IO will not be honored if it specifies 
		AVAudioSessionCategoryOptionMixWithOthers AND another app's audio session was already active 
		with non-mixable, non-aggregated input/output.
	
	Added in iOS 10.0. Not applicable on watchos, tvos, macos.
*/
typedef NS_ENUM(NSUInteger, AVAudioSessionIOType)
{
	AVAudioSessionIOTypeNotSpecified = 0,
	AVAudioSessionIOTypeAggregated = 1
};

/*!
	@enum		AVAudioSessionRouteSharingPolicy
 	@abstract   Starting in iOS 11, tvOS 11, and watchOS 5, the route sharing policy allows a session
		to specify that its output audio should be routed somewhere other than the default system output,
		when appropriate alternative routes are available.
	@constant	AVAudioSessionRouteSharingPolicyDefault
		Follow normal rules for routing audio output.
	@constant	AVAudioSessionRouteSharingPolicyLongForm
		Route output to the shared long-form audio output. A session whose primary use case is as a
		music or podcast player may use this value to play to the same output as the built-in Music (iOS), 
		Podcasts, or iTunes (macOS) applications. Typically applications that use this policy will also
		want sign up for remote control events as documented in “Event Handling Guide for UIKit Apps” 
		and will want to utilize MediaPlayer framework’s MPNowPlayingInfoCenter class. All applications
		on the system that use the long-form route sharing policy will have their audio routed to the
		same location.
		Apps running on watchOS using this policy will also be able to play audio in the background,
 		as long as an eligible audio route can be activated. Apps running on watchOS using this policy
 		must use -activateWithOptions:completionHandler: instead of -setActive:withOptions:error: in
  		order to ensure	that the user will be given the opportunity to pick an appropriate audio route
 		in cases where the system is unable to automatically pick the route.
	@constant	AVAudioSessionRouteSharingPolicyIndependent
		Applications should not attempt to set this value directly. On iOS, this value will be set by
		the system in cases where route picker UI is used to direct video to a wireless route.
*/
typedef NS_ENUM(NSUInteger, AVAudioSessionRouteSharingPolicy)
{
	AVAudioSessionRouteSharingPolicyDefault			= 0,
	AVAudioSessionRouteSharingPolicyLongForm		= 1,
	AVAudioSessionRouteSharingPolicyIndependent		= 2,
};

/*!
	@enum AVAudioSession error codes
	@abstract   These are the error codes returned from the AVAudioSession API.
	@constant   AVAudioSessionErrorCodeNone
		Operation succeeded.
	@constant   AVAudioSessionErrorCodeMediaServicesFailed
		The app attempted to use the audio session during or after a Media Services failure.  App should
 		wait for a AVAudioSessionMediaServicesWereResetNotification and then rebuild all its state.
	@constant	AVAudioSessionErrorCodeIsBusy
 		The app attempted to set its audio session inactive or change its AVAudioSessionIOType, but it is still actively playing and/or recording.
 	@constant	AVAudioSessionErrorCodeIncompatibleCategory
 		The app tried to perform an operation on a session but its category does not support it.
 		For instance, if the app calls setPreferredInputNumberOfChannels: while in a playback-only category.
	@constant	AVAudioSessionErrorCodeCannotInterruptOthers
		The app's audio session is non-mixable and trying to go active while in the background.
 		This is allowed only when the app is the NowPlaying app.
	@constant	AVAudioSessionErrorCodeMissingEntitlement
		The app does not have the required entitlements to perform an operation.
	@constant	AVAudioSessionErrorCodeSiriIsRecording
 		The app tried to do something with the audio session that is not allowed while Siri is recording.
 	@constant	AVAudioSessionErrorCodeCannotStartPlaying
		The app is not allowed to start recording and/or playing, usually because of a lack of audio key in
 		its Info.plist.  This could also happen if the app has this key but uses a category that can't record 
 		and/or play in the background (AVAudioSessionCategoryAmbient, AVAudioSessionCategorySoloAmbient, etc.).
	@constant	AVAudioSessionErrorCodeCannotStartRecording
		The app is not allowed to start recording, usually because it is starting a mixable recording from the
 		background and is not an Inter-App Audio app.
	@constant	AVAudioSessionErrorCodeBadParam
 		An illegal value was used for a property.
	@constant	AVAudioSessionErrorCodeInsufficientPriority
 		The app was not allowed to set the audio category because another app (Phone, etc.) is controlling it.
	@constant	AVAudioSessionErrorCodeResourceNotAvailable
		The operation failed because the device does not have sufficient hardware resources to complete the action. 
		For example, the operation requires audio input hardware, but the device has no audio input available.
	@constant	AVAudioSessionErrorCodeUnspecified
 		An unspecified error has occurred.
*/

typedef NS_ENUM(NSInteger, AVAudioSessionErrorCode)
{
	AVAudioSessionErrorCodeNone							=  0,
	AVAudioSessionErrorCodeMediaServicesFailed			= 'msrv',			/* 0x6D737276, 1836282486	*/
	AVAudioSessionErrorCodeIsBusy						= '!act',			/* 0x21616374, 560030580	*/
	AVAudioSessionErrorCodeIncompatibleCategory			= '!cat',			/* 0x21636174, 560161140	*/
	AVAudioSessionErrorCodeCannotInterruptOthers		= '!int',			/* 0x21696E74, 560557684	*/
	AVAudioSessionErrorCodeMissingEntitlement			= 'ent?',			/* 0x656E743F, 1701737535	*/
	AVAudioSessionErrorCodeSiriIsRecording				= 'siri',			/* 0x73697269, 1936290409	*/
	AVAudioSessionErrorCodeCannotStartPlaying			= '!pla',			/* 0x21706C61, 561015905	*/
	AVAudioSessionErrorCodeCannotStartRecording			= '!rec',			/* 0x21726563, 561145187	*/
	AVAudioSessionErrorCodeBadParam						= -50,
	AVAudioSessionErrorCodeInsufficientPriority			= '!pri',			/* 0x21707269, 561017449	*/
	AVAudioSessionErrorCodeResourceNotAvailable			= '!res',			/* 0x21726573, 561145203	*/
	AVAudioSessionErrorCodeUnspecified					= 'what'			/* 0x77686174, 2003329396	*/
};

#pragma mark -- AVAudioSession interface --
OS_EXPORT API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos) 
@interface AVAudioSession : NSObject {
@private
	void *_impl;
}

/* returns singleton instance */
+ (AVAudioSession *)sharedInstance API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Set the session active or inactive. Note that activating an audio session is a synchronous (blocking) operation.
 Therefore, we recommend that applications not activate their session from a thread where a long blocking operation will be problematic.
 When deactivating a session, the caller is required to first stop or pause all running I/Os (e.g. audio queues, players, recorders,
 converters, remote I/Os, etc.). Starting in iOS 8, if the session has running IOs at the time that deactivation is requested,
 the session will be deactivated, but the method will return NO and populate the NSError with the code property set to AVAudioSessionErrorCodeIsBusy
 to indicate the misuse of the API. Prior to iOS 8, the session would have remained active if it had running IOs at the time of the
 deactivation request.
*/
- (BOOL)setActive:(BOOL)active error:(NSError **)outError API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
- (BOOL)setActive:(BOOL)active withOptions:(AVAudioSessionSetActiveOptions)options error:(NSError **)outError API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Asynchronously activate the session. This is relatively time consuming operation. The completion handler will be called when the activation completes or
 if an error occurs while attempting to activate the session. If the session is configured to use AVAudioSessionRouteSharingPolicyLongForm on watchOS, this method
 will also cause a route picker to be presented to the user in cases where an appropriate output route has not already been selected automatically.
 watchOS apps using AVAudioSessionRouteSharingPolicyLongForm should be prepared for this method to fail if no eligible audio route can be activated or if the user
 cancels the route picker view.
 */
- (void)activateWithOptions:(AVAudioSessionActivationOptions)options completionHandler:(void (^)(BOOL activated, NSError * _Nullable error))handler API_AVAILABLE(watchos(5.0)) API_UNAVAILABLE(ios, tvos) API_UNAVAILABLE(macos);

// Get the list of categories available on the device.  Certain categories may be unavailable on particular devices.  For example,
// AVAudioSessionCategoryRecord will not be available on devices that have no support for audio input.
@property (readonly) NSArray<AVAudioSessionCategory> *availableCategories API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* set session category */
- (BOOL)setCategory:(AVAudioSessionCategory)category error:(NSError **)outError API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
/* set session category with options */
- (BOOL)setCategory:(AVAudioSessionCategory)category withOptions:(AVAudioSessionCategoryOptions)options error:(NSError **)outError API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
/* set session category and mode with options */
- (BOOL)setCategory:(AVAudioSessionCategory)category mode:(AVAudioSessionMode)mode options:(AVAudioSessionCategoryOptions)options error:(NSError **)outError API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0)) API_UNAVAILABLE(macos);

/* set session category, mode, routing sharing policy, and options 
 Use of the long-form route sharing policy is only valid in conjunction with a limited set of category, mode, and option values.
 Allowed categories: AVAudioSessionCategoryPlayback
 Allowed modes: AVAudioSessionModeDefault, AVAudioSessionModeMoviePlayback, AVAudioSessionModeSpokenAudio
 Allowed options: None. Options are allowed when changing the routing policy back to Default, however. */
- (BOOL)setCategory:(AVAudioSessionCategory)category mode:(AVAudioSessionMode)mode routeSharingPolicy:(AVAudioSessionRouteSharingPolicy)policy options:(AVAudioSessionCategoryOptions)options error:(NSError **)outError API_AVAILABLE(ios(11.0), tvos(11.0), watchos(5.0)) API_UNAVAILABLE(macos);

/* get session category. Examples: AVAudioSessionCategoryRecord, AVAudioSessionCategoryPlayAndRecord, etc. */
@property (readonly) AVAudioSessionCategory category API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* get the current set of AVAudioSessionCategoryOptions */
@property (readonly) AVAudioSessionCategoryOptions categoryOptions API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Get the routing policy. See AVAudioSessionRouteSharingPolicy for a description of the available policies
 See -setCategory:mode:routeSharingPolicy:options:error: method for additional discussion. */
@property (readonly) AVAudioSessionRouteSharingPolicy routeSharingPolicy API_AVAILABLE(ios(11.0), tvos(11.0), watchos(5.0)) API_UNAVAILABLE(macos);

// Modes modify the audio category in order to introduce behavior that is tailored to the specific
// use of audio within an application. Examples:  AVAudioSessionModeVideoRecording, AVAudioSessionModeVoiceChat,
// AVAudioSessionModeMeasurement, etc.

// Get the list of modes available on the device.  Certain modes may be unavailable on particular devices.  For example,
// AVAudioSessionModeVideoRecording will not be available on devices that have no support for recording video.
@property (readonly) NSArray<AVAudioSessionMode> *availableModes API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

- (BOOL)setMode:(AVAudioSessionMode)mode error:(NSError **)outError API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* set session mode */
@property (readonly) AVAudioSessionMode mode API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* get session mode */

/* Returns an enum indicating whether the user has granted or denied permission to record, or has not been asked */
@property (readonly) AVAudioSessionRecordPermission recordPermission API_AVAILABLE(ios(8.0), watchos(4.0)) API_UNAVAILABLE(macos, tvos);

/* Checks to see if calling process has permission to record audio.  The 'response' block will be called
 immediately if permission has already been granted or denied.  Otherwise, it presents a dialog to notify
 the user and allow them to choose, and calls the block once the UI has been dismissed.  'granted'
 indicates whether permission has been granted. Note that the block may be called in a different thread context.
 */
typedef void (^PermissionBlock)(BOOL granted);

- (void)requestRecordPermission:(PermissionBlock)response API_AVAILABLE(ios(7.0), watchos(4.0)) API_UNAVAILABLE(macos, tvos);

- (BOOL)overrideOutputAudioPort:(AVAudioSessionPortOverride)portOverride error:(NSError **)outError API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Will be true when another application is playing audio.
Note: As of iOS 8.0, Apple recommends that most applications use secondaryAudioShouldBeSilencedHint instead of this property.
The otherAudioPlaying property will be true if any other audio (including audio from an app using AVAudioSessionCategoryAmbient)
is playing, whereas the secondaryAudioShouldBeSilencedHint property is more restrictive in its consideration of whether 
primary audio from another application is playing.  
For additional information, see https://developer.apple.com/library/content/qa/qa1882/_index.html
*/
@property (readonly, getter=isOtherAudioPlaying) BOOL otherAudioPlaying API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Will be true when another application with a non-mixable audio session is playing audio.  Applications may use
this property as a hint to silence audio that is secondary to the functionality of the application. For example, a game app
using AVAudioSessionCategoryAmbient may use this property to decide to mute its soundtrack while leaving its sound effects unmuted.
Note: This property is closely related to AVAudioSessionSilenceSecondaryAudioHintNotification.
*/
@property (readonly) BOOL secondaryAudioShouldBeSilencedHint API_AVAILABLE(ios(8.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* A description of the current route, consisting of zero or more input ports and zero or more output ports */
@property (readonly) AVAudioSessionRouteDescription *currentRoute API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Select a preferred input port for audio routing. If the input port is already part of the current audio route, this will have no effect.
   Otherwise, selecting an input port for routing will initiate a route change to use the preferred input port, provided that the application's
   session controls audio routing. Setting a nil value will clear the preference. */
- (BOOL)setPreferredInput:(nullable AVAudioSessionPortDescription *)inPort error:(NSError **)outError API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly, nullable) AVAudioSessionPortDescription *preferredInput API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos); /* Get the preferred input port.  Will be nil if no preference has been set */

/* Get the set of input ports that are available for routing. Note that this property only applies to the session's current category and mode.
   For example, if the session's current category is AVAudioSessionCategoryPlayback, there will be no available inputs.  */
@property (readonly, nullable) NSArray<AVAudioSessionPortDescription *> *availableInputs API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

@end

/* AVAudioSessionHardwareConfiguration manages the set of properties that reflect the current state of
 audio hardware in the current route.  Applications whose functionality depends on these properties should
 reevaluate them any time the route changes. */
@interface AVAudioSession (AVAudioSessionHardwareConfiguration)

/* Get and set preferred values for hardware properties.  Note: that there are corresponding read-only
 properties that describe the actual values for sample rate, I/O buffer duration, etc. */

/* The preferred hardware sample rate for the session. The actual sample rate may be different. */
- (BOOL)setPreferredSampleRate:(double)sampleRate error:(NSError **)outError API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly) double preferredSampleRate API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* The preferred hardware IO buffer duration in seconds. The actual IO buffer duration may be different.  */
- (BOOL)setPreferredIOBufferDuration:(NSTimeInterval)duration error:(NSError **)outError API_AVAILABLE(ios(3.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly) NSTimeInterval preferredIOBufferDuration API_AVAILABLE(ios(3.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Sets the number of input channels that the app would prefer for the current route */
- (BOOL)setPreferredInputNumberOfChannels:(NSInteger)count error:(NSError **)outError API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly) NSInteger preferredInputNumberOfChannels API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Sets the number of output channels that the app would prefer for the current route */
- (BOOL)setPreferredOutputNumberOfChannels:(NSInteger)count error:(NSError **)outError API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly) NSInteger preferredOutputNumberOfChannels API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Returns the largest number of audio input channels available for the current route */
@property (readonly) NSInteger maximumInputNumberOfChannels API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Returns the largest number of audio output channels available for the current route */
@property (readonly) NSInteger maximumOutputNumberOfChannels API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* A value defined over the range [0.0, 1.0], with 0.0 corresponding to the lowest analog
gain setting and 1.0 corresponding to the highest analog gain setting.  Attempting to set values
outside of the defined range will result in the value being "clamped" to a valid input.  This is
a global input gain setting that applies to the current input source for the entire system.
When no applications are using the input gain control, the system will restore the default input
gain setting for the input source.  Note that some audio accessories, such as USB devices, may
not have a default value.  This property is only valid if inputGainSettable
is true.  Note: inputGain is key-value observable */
- (BOOL)setInputGain:(float)gain error:(NSError **)outError API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);
@property (readonly) float inputGain API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos); /* value in range [0.0, 1.0] */

/* True when audio input gain is available.  Some input ports may not provide the ability to set the
input gain, so check this value before attempting to set input gain. */
@property (readonly, getter=isInputGainSettable) BOOL inputGainSettable API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* True if input hardware is available. */
@property (readonly, getter=isInputAvailable) BOOL inputAvailable API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* DataSource methods are for use with routes that support input or output data source selection.
If the attached accessory supports data source selection, the data source properties/methods provide for discovery and 
selection of input and/or output data sources. Note that the properties and methods for data source selection below are
equivalent to the properties and methods on AVAudioSessionPortDescription. The methods below only apply to the currently 
routed ports. */

/* Key-value observable. */
@property (readonly, nullable) NSArray<AVAudioSessionDataSourceDescription *> *inputDataSources API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Get and set the currently selected data source.  Will be nil if no data sources are available.
Setting a nil value will clear the data source preference. */
@property (readonly, nullable) AVAudioSessionDataSourceDescription *inputDataSource API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
- (BOOL)setInputDataSource:(nullable AVAudioSessionDataSourceDescription *)dataSource error:(NSError **)outError API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Key-value observable. */
@property (readonly, nullable) NSArray<AVAudioSessionDataSourceDescription *> *outputDataSources API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Get and set currently selected data source.  Will be nil if no data sources are available. 
Setting a nil value will clear the data source preference. */
@property (readonly, nullable) AVAudioSessionDataSourceDescription *outputDataSource API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
- (BOOL)setOutputDataSource:(nullable AVAudioSessionDataSourceDescription *)dataSource error:(NSError **)outError API_AVAILABLE(ios(6.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Current values for hardware properties.  Note that most of these properties have corresponding methods 
for getting and setting preferred values.  Input- and output-specific properties will generate an error if they are 
queried if the audio session category does not support them.  Each of these will return 0 (or 0.0) if there is an error.  */

/* The current hardware sample rate */
@property (readonly) double sampleRate API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The current number of hardware input channels. Is key-value observable */
@property (readonly) NSInteger inputNumberOfChannels API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The current number of hardware output channels. Is key-value observable */
@property (readonly) NSInteger outputNumberOfChannels API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The current output volume. Is key-value observable */
@property (readonly) float outputVolume API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* value in range [0.0, 1.0] */

/* The current hardware input latency in seconds. */
@property (readonly) NSTimeInterval inputLatency API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The current hardware output latency in seconds. */
@property (readonly) NSTimeInterval outputLatency API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The current hardware IO buffer duration in seconds. */
@property (readonly) NSTimeInterval IOBufferDuration API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Set inIOType to AVAudioSessionIOTypeAggregated if your app uses AVAudioSessionCategoryPlayAndRecord
 and requires input and output audio to be presented in the same realtime I/O callback. See the AVAudioSessionIOType
 documentation for more details.
 */
- (BOOL)setAggregatedIOPreference:(AVAudioSessionIOType)inIOType error:(NSError **)outError API_AVAILABLE(ios(10.0)) API_UNAVAILABLE(tvos, watchos, macos);

@end

#pragma mark -- Names for NSNotifications --

/* Registered listeners will be notified when the system has interrupted the audio session and when
 the interruption has ended.  Check the notification's userInfo dictionary for the interruption type -- either begin or end.
 In the case of an end interruption notification, check the userInfo dictionary for AVAudioSessionInterruptionOptions that
 indicate whether audio playback should resume.
 In cases where the interruption is a consequence of the application being suspended, the info dictionary will contain
 AVAudioSessionInterruptionWasSuspendedKey, with the boolean value set to true.
 */
AVF_EXPORT NSNotificationName const AVAudioSessionInterruptionNotification API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Registered listeners will be notified when a route change has occurred.  Check the notification's userInfo dictionary for the
 route change reason and for a description of the previous audio route.
 */
AVF_EXPORT NSNotificationName const AVAudioSessionRouteChangeNotification API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Registered listeners will be notified if the media server is killed.  In the event that the server is killed,
 take appropriate steps to handle requests that come in before the server resets.  See Technical Q&A QA1749.
 */
AVF_EXPORT NSNotificationName const AVAudioSessionMediaServicesWereLostNotification API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Registered listeners will be notified when the media server restarts.  In the event that the server restarts,
 take appropriate steps to re-initialize any audio objects used by your application.  See Technical Q&A QA1749.
 */
AVF_EXPORT NSNotificationName const AVAudioSessionMediaServicesWereResetNotification API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Registered listeners that are currently in the foreground and have active audio sessions will be notified 
 when primary audio from other applications starts and stops.  Check the notification's userInfo dictionary 
 for the notification type -- either begin or end.
 Foreground applications may use this notification as a hint to enable or disable audio that is secondary
 to the functionality of the application. For more information, see the related property secondaryAudioShouldBeSilencedHint.
*/
AVF_EXPORT NSNotificationName const AVAudioSessionSilenceSecondaryAudioHintNotification API_AVAILABLE(ios(8.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

#pragma mark -- Keys for NSNotification userInfo dictionaries --

/* keys for AVAudioSessionInterruptionNotification */
/* Value is an NSNumber representing an AVAudioSessionInterruptionType */
AVF_EXPORT NSString *const AVAudioSessionInterruptionTypeKey API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Only present for end interruption events.  Value is of type AVAudioSessionInterruptionOptions.*/
AVF_EXPORT NSString *const AVAudioSessionInterruptionOptionKey API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Only present in begin interruption events, where the interruption is a direct result of the application being suspended
	by the operating sytem. Value is a boolean NSNumber, where a true value indicates that the interruption is the result
	of the application being suspended, rather than being interrupted by another audio session.
	
	Starting in iOS 10, the system will deactivate the audio session of most apps in response to the app process
	being suspended. When the app starts running again, it will receive the notification that its session has been deactivated
	by the system. Note that the notification is necessarily delayed in time, due to the fact that the application was suspended
	at the time the session was deactivated by the system and the notification can only be delivered once the app is running again. */
AVF_EXPORT NSString *const AVAudioSessionInterruptionWasSuspendedKey API_AVAILABLE(ios(10.3), watchos(2.3), tvos(10.3)) API_UNAVAILABLE(macos);

/* keys for AVAudioSessionRouteChangeNotification */
/* value is an NSNumber representing an AVAudioSessionRouteChangeReason */
AVF_EXPORT NSString *const AVAudioSessionRouteChangeReasonKey API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
/* value is AVAudioSessionRouteDescription * */
AVF_EXPORT NSString *const AVAudioSessionRouteChangePreviousRouteKey API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* keys for AVAudioSessionSilenceSecondaryAudioHintNotification */
/* value is an NSNumber representing an AVAudioSessionSilenceSecondaryAudioHintType */
AVF_EXPORT NSString *const AVAudioSessionSilenceSecondaryAudioHintTypeKey API_AVAILABLE(ios(8.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

#pragma mark -- Values for the category property --

/*  Use this category for background sounds such as rain, car engine noise, etc.  
 Mixes with other music. */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryAmbient API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/*  Use this category for background sounds.  Other music will stop playing. */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategorySoloAmbient API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Use this category for music tracks.*/
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryPlayback API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/*  Use this category when recording audio. */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryRecord API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/*  Use this category when recording and playing back audio. */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryPlayAndRecord API_AVAILABLE(ios(3.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/*  Use this category when using a hardware codec or signal processor while
 not playing or recording audio. */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryAudioProcessing API_DEPRECATED("No longer supported", ios(3.0, 10.0)) API_UNAVAILABLE(watchos, tvos) API_UNAVAILABLE(macos);

/*  Use this category to customize the usage of available audio accessories and built-in audio hardware.
 For example, this category provides an application with the ability to use an available USB output 
 and headphone output simultaneously for separate, distinct streams of audio data. Use of 
 this category by an application requires a more detailed knowledge of, and interaction with, 
 the capabilities of the available audio routes.  May be used for input, output, or both.
 Note that not all output types and output combinations are eligible for multi-route.  Input is limited
 to the last-in input port. Eligible inputs consist of the following:
	AVAudioSessionPortUSBAudio, AVAudioSessionPortHeadsetMic, and AVAudioSessionPortBuiltInMic.  
 Eligible outputs consist of the following: 
	AVAudioSessionPortUSBAudio, AVAudioSessionPortLineOut, AVAudioSessionPortHeadphones, AVAudioSessionPortHDMI, 
	and AVAudioSessionPortBuiltInSpeaker.  
 Note that AVAudioSessionPortBuiltInSpeaker is only allowed to be used when there are no other eligible 
 outputs connected.  */
AVF_EXPORT AVAudioSessionCategory const AVAudioSessionCategoryMultiRoute API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

#pragma mark -- Values for the mode property --

/*!
@abstract      Modes modify the audio category in order to introduce behavior that is tailored to the specific
use of audio within an application.  Available in iOS 5.0 and greater.
 */

/* The default mode */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeDefault API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Only valid with AVAudioSessionCategoryPlayAndRecord.  Appropriate for Voice over IP
(VoIP) applications.  Reduces the number of allowable audio routes to be only those
that are appropriate for VoIP applications and may engage appropriate system-supplied
signal processing.  Has the side effect of setting AVAudioSessionCategoryOptionAllowBluetooth */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeVoiceChat API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Set by Game Kit on behalf of an application that uses a GKVoiceChat object; valid
 only with the AVAudioSessionCategoryPlayAndRecord category.
 Do not set this mode directly. If you need similar behavior and are not using
 a GKVoiceChat object, use AVAudioSessionModeVoiceChat instead. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeGameChat API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Only valid with AVAudioSessionCategoryPlayAndRecord or AVAudioSessionCategoryRecord.
 Modifies the audio routing options and may engage appropriate system-supplied signal processing. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeVideoRecording API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Appropriate for applications that wish to minimize the effect of system-supplied signal
processing for input and/or output audio signals. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeMeasurement API_AVAILABLE(ios(5.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Engages appropriate output signal processing for movie playback scenarios.  Currently
only applied during playback over built-in speaker. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeMoviePlayback API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Only valid with kAudioSessionCategory_PlayAndRecord. Reduces the number of allowable audio
routes to be only those that are appropriate for video chat applications. May engage appropriate
system-supplied signal processing.  Has the side effect of setting
AVAudioSessionCategoryOptionAllowBluetooth and AVAudioSessionCategoryOptionDefaultToSpeaker. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeVideoChat API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Appropriate for applications which play spoken audio and wish to be paused (via audio session interruption) rather than ducked
if another app (such as a navigation app) plays a spoken audio prompt.  Examples of apps that would use this are podcast players and
audio books.  For more information, see the related category option AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers. */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeSpokenAudio API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Appropriate for applications which play audio using text to speech. Setting this mode allows for different routing behaviors when
connected to certain audio devices such as CarPlay. An example of an app that would use this mode is a turn by turn navigation app that
plays short prompts to the user. Typically, these same types of applications would also configure their session to use
AVAudioSessionCategoryOptionDuckOthers and AVAudioSessionCategoryOptionInterruptSpokenAudioAndMixWithOthers */
AVF_EXPORT AVAudioSessionMode const AVAudioSessionModeVoicePrompt API_AVAILABLE(ios(12.0), watchos(5.0), tvos(12.0)) API_UNAVAILABLE(macos);

#pragma mark -- constants for port types --

/* input port types */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortLineIn       API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Line level input on a dock connector */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBuiltInMic   API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Built-in microphone on an iOS device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortHeadsetMic   API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Microphone on a wired headset.  Headset refers to an
																				   accessory that has headphone outputs paired with a
																				   microphone. */

/* output port types */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortLineOut          API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Line level output on a dock connector */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortHeadphones       API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Headphone or headset output */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBluetoothA2DP    API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Output on a Bluetooth A2DP device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBuiltInReceiver  API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* The speaker you hold to your ear when on a phone call */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBuiltInSpeaker   API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Built-in speaker on an iOS device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortHDMI             API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Output via High-Definition Multimedia Interface */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortAirPlay          API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Output on a remote Air Play device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBluetoothLE	  API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Output on a Bluetooth Low Energy device */

/* port types that refer to either input or output */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortBluetoothHFP API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Input or output on a Bluetooth Hands-Free Profile device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortUSBAudio     API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Input or output on a Universal Serial Bus device */
AVF_EXPORT AVAudioSessionPort const AVAudioSessionPortCarAudio     API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos); /* Input or output via Car Audio */

#pragma mark -- constants for data source locations, orientations, polar patterns, and channel roles --

/* The following represent the location of a data source on an iOS device. */
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionLocationUpper					API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionLocationLower					API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The following represent the orientation or directionality of a data source on an iOS device. */
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationTop					API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationBottom				API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationFront				API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationBack				API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationLeft				API_AVAILABLE(ios(8.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionOrientationRight				API_AVAILABLE(ios(8.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* The following represent the possible polar patterns for a data source on an iOS device. */
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionPolarPatternOmnidirectional	API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionPolarPatternCardioid			API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
AVF_EXPORT AVAudioSessionLocation const AVAudioSessionPolarPatternSubcardioid		API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

#pragma mark -- helper class interfaces --

/* 
 AVAudioSessionChannelDescription objects provide information about a port's audio channels.
 AudioQueues, AURemoteIO and AUVoiceIO instances can be assigned to communicate with specific 
 hardware channels by setting an array of <port UID, channel index> pairs.
 */
OS_EXPORT API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos) 
@interface AVAudioSessionChannelDescription : NSObject {
@private
	void *_impl;
}

@property(readonly) NSString *			channelName API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
@property(readonly) NSString *			owningPortUID API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);  /* the unique identifier (UID) for the channel's owning port */
@property(readonly) NSUInteger			channelNumber API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);  /* the index of this channel in its owning port's array of channels */
@property(readonly) AudioChannelLabel	channelLabel API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);	/* description of the physical location of this channel.   */

@end

OS_EXPORT API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos) 
@interface AVAudioSessionPortDescription : NSObject {
@private
	void *_impl;
}

/* Value is one of the AVAudioSessionPort constants declared above. */
@property (readonly) AVAudioSessionPort portType API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* A descriptive name for the port */
@property (readonly) NSString *portName API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* A system-assigned unique identifier for the port */
@property (readonly) NSString *UID API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* This property's value will be true if the associated hardware port has built-in processing for two-way 
 voice communication. Applications that use their own proprietary voice processing algorithms should use 
 this property to decide when to disable processing.  On the other hand, if using Apple's Voice Processing 
 I/O unit (subtype kAudioUnitSubType_VoiceProcessingIO), the system will automatically manage this for the 
 application. In particular, ports of type AVAudioSessionPortBluetoothHFP and AVAudioSessionPortCarAudio
 often have hardware voice processing. */
@property (readonly) BOOL hasHardwareVoiceCallProcessing API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0)) API_UNAVAILABLE(macos);

@property (readonly, nullable) NSArray<AVAudioSessionChannelDescription *> *channels API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Will be nil if there are no selectable data sources. */
@property (readonly, nullable) NSArray<AVAudioSessionDataSourceDescription *> *dataSources API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Will be nil if there are no selectable data sources. In all other cases, this
 property reflects the currently selected data source.*/
@property (readonly, nullable) AVAudioSessionDataSourceDescription *selectedDataSource API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* This property reflects the application's preferred data source for the Port.
 Will be nil if there are no selectable data sources or if no preference has been set.*/
@property (readonly, nullable) AVAudioSessionDataSourceDescription *preferredDataSource API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Select the preferred data source for this port. The input dataSource parameter must be one of the dataSources exposed by 
the dataSources property. Setting a nil value will clear the preference.
Note: if the port is part of the active audio route, changing the data source will likely
result in a route reconfiguration.  If the port is not part of the active route, selecting a new data source will
not result in an immediate route reconfiguration.  Use AVAudioSession's setPreferredInput:error: method to activate the port. */
- (BOOL)setPreferredDataSource:(nullable AVAudioSessionDataSourceDescription *)dataSource error:(NSError **)outError API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

@end

OS_EXPORT API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos) 
@interface AVAudioSessionRouteDescription : NSObject {
@private
	void *_impl;
}

@property (readonly) NSArray<AVAudioSessionPortDescription *> *inputs API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

@property (readonly) NSArray<AVAudioSessionPortDescription *> *outputs API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);
@end

OS_EXPORT API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos) 
@interface AVAudioSessionDataSourceDescription : NSObject {
@private
	void *_impl;
}

/* system-assigned ID for the data source */
@property (readonly) NSNumber *dataSourceID API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* human-readable name for the data source */
@property (readonly) NSString *dataSourceName API_AVAILABLE(ios(6.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Location and orientation can be used to distinguish between multiple data sources belonging to a single port.  For example, in the case of a port of type AVAudioSessionPortBuiltInMic, one can
   use these properties to differentiate between an upper/front-facing microphone and a lower/bottom-facing microphone. */

/* Describes the general location of a data source. Will be nil for data sources for which the location is not known. */
@property (readonly, nullable) AVAudioSessionLocation location API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Describes the orientation of a data source.  Will be nil for data sources for which the orientation is not known. */
@property (readonly, nullable) AVAudioSessionOrientation orientation API_AVAILABLE(ios(7.0), watchos(2.0), tvos(9.0)) API_UNAVAILABLE(macos);

/* Array of one or more AVAudioSessionPolarPatterns describing the supported polar patterns for a data source.  Will be nil for data sources that have no selectable patterns. */
@property (readonly, nullable) NSArray<AVAudioSessionPolarPattern> *supportedPolarPatterns API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Describes the currently selected polar pattern.  Will be nil for data sources that have no selectable patterns. */
@property (readonly, nullable) AVAudioSessionPolarPattern selectedPolarPattern API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Describes the preferred polar pattern.  Will be nil for data sources that have no selectable patterns or if no preference has been set. */
@property (readonly, nullable) AVAudioSessionPolarPattern preferredPolarPattern API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

/* Select the desired polar pattern from the set of available patterns. Setting a nil value will clear the preference.
   Note: if the owning port and data source are part of the active audio route,
   changing the polar pattern will likely result in a route reconfiguration. If the owning port and data source are not part of the active route,
   selecting a polar pattern will not result in an immediate route reconfiguration.  Use AVAudioSession's setPreferredInput:error: method
   to activate the port. Use setPreferredDataSource:error: to active the data source on the port. */
- (BOOL)setPreferredPolarPattern:(nullable AVAudioSessionPolarPattern)pattern error:(NSError **)outError API_AVAILABLE(ios(7.0), tvos(9.0)) API_UNAVAILABLE(watchos, macos);

@end


#pragma mark -- Deprecated API --
@interface AVAudioSession (AVAudioSessionDeprecated)

/* The delegate property is deprecated. Instead, you should register for the NSNotifications named below. */
/* For example:
 [[NSNotificationCenter defaultCenter] addObserver: myObject
 selector:    @selector(handleInterruption:)
 name:        AVAudioSessionInterruptionNotification
 object:      [AVAudioSession sharedInstance]];
 */
@property (assign, nullable) id<AVAudioSessionDelegate> delegate API_DEPRECATED("No longer supported", ios(4.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);

/* AVAudioSession is a singleton. Use +sharedInstance instead of -init */
- (instancetype)init API_DEPRECATED_WITH_REPLACEMENT("+sharedInstance", ios(3.0, 10.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);

- (BOOL)setActive:(BOOL)active withFlags:(NSInteger)flags error:(NSError **)outError API_DEPRECATED_WITH_REPLACEMENT("-setActive:withOptions:error:", ios(4.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);

//isInputAvailable
@property (readonly) BOOL inputIsAvailable API_DEPRECATED_WITH_REPLACEMENT("isInputAvailable", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos); /* is input hardware available or not? */

/* deprecated.  Use the corresponding properties without "Hardware" in their names. */
@property (readonly) double currentHardwareSampleRate API_DEPRECATED_WITH_REPLACEMENT("sampleRate", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);
@property (readonly) NSInteger currentHardwareInputNumberOfChannels API_DEPRECATED_WITH_REPLACEMENT("inputNumberOfChannels", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);
@property (readonly) NSInteger currentHardwareOutputNumberOfChannels API_DEPRECATED_WITH_REPLACEMENT("outputNumberOfChannels", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);
- (BOOL)setPreferredHardwareSampleRate:(double)sampleRate error:(NSError **)outError API_DEPRECATED_WITH_REPLACEMENT("setPreferredSampleRate:error:", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);
@property (readonly) double preferredHardwareSampleRate API_DEPRECATED_WITH_REPLACEMENT("preferredSampleRate", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);

@end

#pragma mark -- AVAudioSessionDelegate protocol --
/* The AVAudioSessionDelegate protocol is deprecated. Instead you should register for notifications. */
API_DEPRECATED("No longer supported", ios(3.0, 6.0)) API_UNAVAILABLE(tvos, watchos, macos)
@protocol AVAudioSessionDelegate <NSObject>
@optional

- (void)beginInterruption API_DEPRECATED("No longer supported", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos); /* something has caused your audio session to be interrupted */

/* the interruption is over */
- (void)endInterruptionWithFlags:(NSUInteger)flags API_DEPRECATED("No longer supported", ios(4.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos); /* Currently the only flag is AVAudioSessionInterruptionFlags_ShouldResume. */

- (void)endInterruption API_DEPRECATED("No longer supported", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos); /* endInterruptionWithFlags: will be called instead if implemented. */

/* notification for input become available or unavailable */
- (void)inputIsAvailableChanged:(BOOL)isInputAvailable API_DEPRECATED("No longer supported", ios(3.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos);

@end

#pragma mark -- Deprecated enumerations --

/* Deprecated in iOS 6.0.  Use AVAudioSessionInterruptionOptions instead.
 Flags passed to you when endInterruptionWithFlags: is called on the delegate */
enum {
	AVAudioSessionInterruptionFlags_ShouldResume API_DEPRECATED_WITH_REPLACEMENT("AVAudioSessionInterruptionOptions", ios(4.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos) = 1
};

/* Deprecated in iOS 6.0.  Use AVAudioSessionSetActiveOptions instead.
 flags for use when calling setActive:withFlags:error: */
enum {
	AVAudioSessionSetActiveFlags_NotifyOthersOnDeactivation API_DEPRECATED_WITH_REPLACEMENT("AVAudioSessionSetActiveOptions", ios(4.0, 6.0)) __WATCHOS_PROHIBITED __TVOS_PROHIBITED API_UNAVAILABLE(macos) = 1
};

enum {
	AVAudioSessionErrorInsufficientPriority API_DEPRECATED_WITH_REPLACEMENT("AVAudioSessionErrorCodeInsufficientPriority", ios(7.0, 12.0)) API_UNAVAILABLE(macos) = AVAudioSessionErrorCodeInsufficientPriority
};

NS_ASSUME_NONNULL_END

#elif defined(__OBJC__) && TARGET_OS_OSX
#import <AVFAudio/AVAudioSession_i386.h>
#endif // __OBJC2__
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVSpeechSynthesis.h
/*
 File:  AVSpeechSynthesis.h
 
 Framework:  AVFoundation
 
 Copyright 2013-2015 Apple Inc. All rights reserved.
 */

#if !0

#import <AVFoundation/AVBase.h>
#import <Foundation/Foundation.h>

#ifdef __OBJC2__

@class AVAudioSessionChannelDescription;

NS_ASSUME_NONNULL_BEGIN

typedef NS_ENUM(NSInteger, AVSpeechBoundary) {
    AVSpeechBoundaryImmediate,
    AVSpeechBoundaryWord
} NS_ENUM_AVAILABLE(10_14, 7_0);

typedef NS_ENUM(NSInteger, AVSpeechSynthesisVoiceQuality) {
    AVSpeechSynthesisVoiceQualityDefault = 1,
    AVSpeechSynthesisVoiceQualityEnhanced
} NS_ENUM_AVAILABLE(10_14, 9_0);

AVF_EXPORT const float AVSpeechUtteranceMinimumSpeechRate API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
AVF_EXPORT const float AVSpeechUtteranceMaximumSpeechRate API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
AVF_EXPORT const float AVSpeechUtteranceDefaultSpeechRate API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));

// Use the Alex identifier with voiceWithIdentifier:. If the voice is present on the system,
// an AVSpeechSynthesisVoice will be returned. Alex is en-US only.
AVF_EXPORT NSString *const AVSpeechSynthesisVoiceIdentifierAlex API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0), macos(10.14));

//NSString, containing International Phonetic Alphabet (IPA) symbols. Controls pronunciation of a certain word or phrase, e.g. a proper name.
AVF_EXPORT NSString *const AVSpeechSynthesisIPANotationAttribute API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0), macos(10.14));

@protocol AVSpeechSynthesizerDelegate;

/*!
 @class AVSpeechSynthesisVoice
 @abstract
 AVSpeechSynthesisVoice encapsulates the attributes of the voice used to synthesize speech on the system.
 
 @discussion
 Retrieve a voice by specifying the language code your text should be spoken in, or by using voiceWithIdentifier
 for a known voice identifier.
 */
NS_CLASS_AVAILABLE(10_14, 7_0)
@interface AVSpeechSynthesisVoice : NSObject<NSSecureCoding>

+ (NSArray<AVSpeechSynthesisVoice *> *)speechVoices;
+ (NSString *)currentLanguageCode;

/*!
 @method        voiceWithLanguage:
 @abstract      Use a BCP-47 language tag to specify the desired language and region.
 @param			language
 Specifies the BCP-47 language tag that represents the voice.
 @discussion
 The default is the system's region and language.
 Passing in nil will return the default voice.
 Passing in an invalid languageCode will return nil.
 Will return enhanced quality voice if available, default quality otherwise.
 Examples: en-US (U.S. English), fr-CA (French Canadian)
 */
+ (nullable AVSpeechSynthesisVoice *)voiceWithLanguage:(nullable NSString *)languageCode;

/*!
 @method        voiceWithIdentifier:
 @abstract      Retrieve a voice by its identifier.
 @param			identifier
 A unique identifier for a voice.
 @discussion
 Passing in an invalid identifier will return nil.
 Returns nil if the identifier is valid, but the voice is not available on device (i.e. not yet downloaded by the user).
 */
+ (nullable AVSpeechSynthesisVoice *)voiceWithIdentifier:(NSString *)identifier API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0), macos(10.14));

@property(nonatomic, readonly) NSString *language;
@property(nonatomic, readonly) NSString *identifier API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0), macos(10.14));
@property(nonatomic, readonly) NSString *name API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0), macos(10.14));
@property(nonatomic, readonly) AVSpeechSynthesisVoiceQuality quality API_AVAILABLE(ios(9.0), watchos(2.0), tvos(9.0), macos(10.14));

@end

/*!
 @class AVSpeechUtterance
 @abstract
 AVSpeechUtterance is the atom of speaking a string or pausing the synthesizer.
 
 @discussion
 To start speaking, specify the AVSpeechSynthesisVoice and the string to be spoken, then optionally change the rate, pitch or volume if desired.
 */
NS_CLASS_AVAILABLE(10_14, 7_0)
@interface AVSpeechUtterance : NSObject<NSCopying, NSSecureCoding>

+ (instancetype)speechUtteranceWithString:(NSString *)string;
+ (instancetype)speechUtteranceWithAttributedString:(NSAttributedString *)string API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0), macos(10.14));

- (instancetype)initWithString:(NSString *)string;
- (instancetype)initWithAttributedString:(NSAttributedString *)string API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0), macos(10.14));

/* If no voice is specified, the system's default will be used. */
@property(nonatomic, retain, nullable) AVSpeechSynthesisVoice *voice;

@property(nonatomic, readonly) NSString *speechString;
@property(nonatomic, readonly) NSAttributedString *attributedSpeechString API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0), macos(10.14));

/* Setting these values after a speech utterance has been enqueued will have no effect. */

@property(nonatomic) float rate;             // Values are pinned between AVSpeechUtteranceMinimumSpeechRate and AVSpeechUtteranceMaximumSpeechRate.
@property(nonatomic) float pitchMultiplier;  // [0.5 - 2] Default = 1
@property(nonatomic) float volume;           // [0-1] Default = 1

@property(nonatomic) NSTimeInterval preUtteranceDelay;    // Default is 0.0
@property(nonatomic) NSTimeInterval postUtteranceDelay;   // Default is 0.0


@end

/*!
 @class AVSpeechSynthesizer
 @abstract
 AVSpeechSynthesizer allows speaking of speech utterances with a basic queuing mechanism.
 
 @discussion
 Create an instance of AVSpeechSynthesizer to start generating synthesized speech by using AVSpeechUtterance objects.
 */
NS_CLASS_AVAILABLE(10_14, 7_0)
@interface AVSpeechSynthesizer : NSObject

@property(nonatomic, weak, nullable) id<AVSpeechSynthesizerDelegate> delegate;

@property(nonatomic, readonly, getter=isSpeaking) BOOL speaking;
@property(nonatomic, readonly, getter=isPaused) BOOL paused;

/* AVSpeechUtterances are queued by default. 
   Enqueing the same AVSpeechUtterance that is already enqueued or is speaking will raise an exception. */
- (void)speakUtterance:(AVSpeechUtterance *)utterance;

/* These methods will operate on the speech utterance that is speaking. Returns YES if it succeeds, NO for failure. */

/* Call stopSpeakingAtBoundary: to interrupt current speech and clear the queue. */
- (BOOL)stopSpeakingAtBoundary:(AVSpeechBoundary)boundary;
- (BOOL)pauseSpeakingAtBoundary:(AVSpeechBoundary)boundary;
- (BOOL)continueSpeaking;

// Specify the audio channels to be used for synthesized speech as described by the channel descriptions in AVAudioSession's current route.
// Speech audio will be replicated to each specified channel.
// Default is nil, which implies system defaults.
@property(nonatomic, retain, nullable) NSArray<AVAudioSessionChannelDescription *> *outputChannels API_AVAILABLE(ios(10.0), watchos(3.0), tvos(10.0), macos(10.14));

@end

/*!
 @protocol AVSpeechSynthesizerDelegate
 @abstract
 Defines an interface for delegates of AVSpeechSynthesizer to receive notifications of important speech utterance events.
 */
@protocol AVSpeechSynthesizerDelegate <NSObject>

@optional
- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer didStartSpeechUtterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer didFinishSpeechUtterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer didPauseSpeechUtterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer didContinueSpeechUtterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer didCancelSpeechUtterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));

- (void)speechSynthesizer:(AVSpeechSynthesizer *)synthesizer willSpeakRangeOfSpeechString:(NSRange)characterRange utterance:(AVSpeechUtterance *)utterance API_AVAILABLE(ios(7.0), watchos(1.0), tvos(7.0), macos(10.14));
@end

NS_ASSUME_NONNULL_END

#endif // __OBJC2__

#endif // !TARGET_OS_BRIDGE
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioFile.h
/*
	File:		AVAudioFile.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>
#import <AVFAudio/AVAudioFormat.h>

NS_ASSUME_NONNULL_BEGIN

@class NSURL;
@class AVAudioPCMBuffer;

/*!
	@class AVAudioFile
	@abstract
		AVAudioFile represents an audio file opened for reading or writing.
	@discussion
		Regardless of the file's actual format, reading and writing the file is done via 
		`AVAudioPCMBuffer` objects, containing samples in an `AVAudioCommonFormat`,
		referred to as the file's "processing format." Conversions are performed to and from
		the file's actual format.
		
		Reads and writes are always sequential, but random access is possible by setting the
		framePosition property.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioFile : NSObject {
@private
	void *_impl;
}
/*! @method initForReading:error:
	@abstract Open a file for reading.
	@param fileURL
		the file to open
	@param outError
		on exit, if an error occurs, a description of the error
	@discussion
		This opens the file for reading using the standard format (deinterleaved floating point).
*/
- (nullable instancetype)initForReading:(NSURL *)fileURL error:(NSError **)outError;

/*!	@method initForReading:commonFormat:interleaved:error:
	@abstract Open a file for reading, using a specified processing format.
	@param fileURL
		the file to open
	@param format
		the processing format to use when reading from the file
	@param interleaved
		whether to use an interleaved processing format
	@param outError
		on exit, if an error occurs, a description of the error
*/
- (nullable instancetype)initForReading:(NSURL *)fileURL commonFormat:(AVAudioCommonFormat)format interleaved:(BOOL)interleaved error:(NSError **)outError;

/*! @method initForWriting:settings:error:
	@abstract Open a file for writing.
	@param fileURL
		the path at which to create the file
	@param settings
		the format of the file to create (See `AVAudioRecorder`.)
	@param outError
		on exit, if an error occurs, a description of the error
	@discussion
		The file type to create can be set through the corresponding settings key. If not set, it will be
		inferred from the file extension. Will overwrite a file at the specified URL if a file exists.

		This opens the file for writing using the standard format (deinterleaved floating point).
*/
- (nullable instancetype)initForWriting:(NSURL *)fileURL settings:(NSDictionary<NSString *, id> *)settings error:(NSError **)outError;

/*! @method initForWriting:settings:commonFormat:interleaved:error:
	@abstract Open a file for writing.
	@param fileURL
		the path at which to create the file
	@param settings
		the format of the file to create (See `AVAudioRecorder`.)
	@param format
		the processing format to use when writing to the file
	@param interleaved
		whether to use an interleaved processing format
	@param outError
		on exit, if an error occurs, a description of the error
	@discussion
		The file type to create can be set through the corresponding settings key. If not set, it will be
		inferred from the file extension. Will overwrite a file at the specified URL if a file exists.
*/
- (nullable instancetype)initForWriting:(NSURL *)fileURL settings:(NSDictionary<NSString *, id> *)settings commonFormat:(AVAudioCommonFormat)format interleaved:(BOOL)interleaved error:(NSError **)outError;

/*! @method readIntoBuffer:error:
	@abstract Read an entire buffer.
	@param buffer
		The buffer into which to read from the file. Its format must match the file's
		processing format.
	@param outError
		on exit, if an error occurs, a description of the error
	@return
		YES for success.
	@discussion
		Reading sequentially from framePosition, attempts to fill the buffer to its capacity. On
		return, the buffer's length indicates the number of sample frames successfully read.
*/
- (BOOL)readIntoBuffer:(AVAudioPCMBuffer *)buffer error:(NSError **)outError;

/*! @method readIntoBuffer:frameCount:error:
	@abstract Read a portion of a buffer.
	@param frames
		The number of frames to read.
	@param buffer
		The buffer into which to read from the file. Its format must match the file's
		processing format.
	@param outError
		on exit, if an error occurs, a description of the error
	@return
		YES for success.
	@discussion
		Like `readIntoBuffer:error:`, but can be used to read fewer frames than buffer.frameCapacity.
*/
- (BOOL)readIntoBuffer:(AVAudioPCMBuffer *)buffer frameCount:(AVAudioFrameCount)frames error:(NSError **)outError;

/*! @method writeFromBuffer:error:
	@abstract Write a buffer.
	@param buffer
		The buffer from which to write to the file. Its format must match the file's
		processing format.
	@param outError
		on exit, if an error occurs, a description of the error
	@return
		YES for success.
	@discussion
		Writes sequentially. The buffer's frameLength signifies how much of the buffer is to be written.
*/
- (BOOL)writeFromBuffer:(const AVAudioPCMBuffer *)buffer error:(NSError **)outError;

/*!	@property url
	@abstract The URL the file is reading or writing.
*/
@property (nonatomic, readonly) NSURL *url;

/*! @property fileFormat
	@abstract The on-disk format of the file.
*/
@property (nonatomic, readonly) AVAudioFormat *fileFormat;

/*! @property processingFormat
	@abstract The processing format of the file.
*/
@property (nonatomic, readonly) AVAudioFormat *processingFormat;

/*! @property length
	@abstract The number of sample frames in the file.
	@discussion
		 Note: this can be expensive to compute for the first time.
*/
@property (nonatomic, readonly) AVAudioFramePosition length;

/*! @property framePosition
	@abstract The position in the file at which the next read or write will occur.
	@discussion
		Set framePosition to perform a seek before a read or write. A read or write operation advances the frame position by the number of frames read or written.
*/
@property (nonatomic) AVAudioFramePosition framePosition;
@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioChannelLayout.h
/*
	File:		AVAudioChannelLayout.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>
#import <CoreAudio/CoreAudioTypes.h>

NS_ASSUME_NONNULL_BEGIN

/*!
	@class AVAudioChannelLayout
	@abstract A description of the roles of a set of audio channels.
	@discussion
		This object is a thin wrapper for the AudioChannelLayout structure, described
		in <CoreAudio/CoreAudioTypes.h>.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioChannelLayout : NSObject <NSSecureCoding> {
@private
	AudioChannelLayoutTag _layoutTag;
	AudioChannelLayout * _layout;
	void *_reserved;
}

- (instancetype)init NS_UNAVAILABLE;

/*!	@method initWithLayoutTag:
	@abstract Initialize from a layout tag.
	@param layoutTag
		The tag.
	@discussion
		Returns nil if the tag is either kAudioChannelLayoutTag_UseChannelDescriptions or
		kAudioChannelLayoutTag_UseChannelBitmap.
*/
- (nullable instancetype)initWithLayoutTag:(AudioChannelLayoutTag)layoutTag;

/*!	@method initWithLayout:
	@abstract Initialize from an AudioChannelLayout.
	@param layout
		The AudioChannelLayout.
	@discussion
		If the provided layout's tag is kAudioChannelLayoutTag_UseChannelDescriptions, this
		initializer attempts to convert it to a more specific tag.
*/
- (instancetype)initWithLayout:(const AudioChannelLayout *)layout NS_DESIGNATED_INITIALIZER;

/*!	@method isEqual:
	@abstract Determine whether another AVAudioChannelLayout is exactly equal to this layout.
	@param object
		The AVAudioChannelLayout to compare against.
	@discussion
		The underlying AudioChannelLayoutTag and AudioChannelLayout are compared for equality.
*/
- (BOOL)isEqual:(id)object;

/*!	@method layoutWithLayoutTag:
	@abstract Create from a layout tag.
*/
+ (instancetype)layoutWithLayoutTag:(AudioChannelLayoutTag)layoutTag;

/*!	@method layoutWithLayout:
	@abstract Create from an AudioChannelLayout
*/
+ (instancetype)layoutWithLayout:(const AudioChannelLayout *)layout;

/*!	@property layoutTag
	@abstract The layout's tag. */
@property (nonatomic, readonly) AudioChannelLayoutTag layoutTag;

/*!	@property layout
	@abstract The underlying AudioChannelLayout. */
@property (nonatomic, readonly) const AudioChannelLayout *layout;

/*! @property channelCount
	@abstract The number of channels of audio data.
*/
@property (nonatomic, readonly) AVAudioChannelCount channelCount;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitDelay.h
/*
    File:		AVAudioUnitDelay.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnitEffect.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitDelay
    @abstract an AVAudioUnitEffect that implements a delay effect
    @discussion
        A delay unit delays the input signal by the specified time interval
        and then blends it with the input signal. The amount of high frequency
        roll-off can also be controlled in order to simulate the effect of
        a tape delay.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitDelay : AVAudioUnitEffect

/*! @property delayTime
    Time taken by the delayed input signal to reach the output
    @abstract
    Range:      0 -> 2
    Default:    1
    Unit:       Seconds
 */
@property (nonatomic) NSTimeInterval delayTime;

/*! @property feedback
    @abstract
    Amount of the output signal fed back into the delay line
    Range:      -100 -> 100
    Default:    50
    Unit:       Percent
*/
@property (nonatomic) float feedback;

/*! @property lowPassCutoff
    @abstract
    Cutoff frequency above which high frequency content is rolled off
    Range:      10 -> (samplerate/2)
    Default:    15000
    Unit:       Hertz
*/
@property (nonatomic) float lowPassCutoff;

/*! @property wetDryMix
    @abstract
    Blend of the wet and dry signals
    Range:      0 (all dry) -> 100 (all wet)
    Default:    100
    Unit:       Percent
*/
@property (nonatomic) float wetDryMix;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioEnvironmentNode.h
/*
    File:       AVAudioEnvironmentNode.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioNode.h>
#import <AVFAudio/AVAudioUnitReverb.h>
#import <AVFAudio/AVAudioUnitEQ.h>
#import <AVFAudio/AVAudioMixing.h>

NS_ASSUME_NONNULL_BEGIN

/*! @enum AVAudioEnvironmentDistanceAttenuationModel
    @abstract Types of distance attenuation models
    @discussion
        Distance attenuation is the natural attenuation of sound when traveling from the source to 
        the listener. The different attenuation models listed below describe the drop-off in gain as 
        the source moves away from the listener.
     
        AVAudioEnvironmentDistanceAttenuationModelExponential
            distanceGain = (distance / referenceDistance) ^ (-rolloffFactor)
     
        AVAudioEnvironmentDistanceAttenuationModelInverse
            distanceGain = referenceDistance /  (referenceDistance + rolloffFactor *
                                                (distance – referenceDistance))
     
        AVAudioEnvironmentDistanceAttenuationModelLinear
            distanceGain = (1 – rolloffFactor * (distance – referenceDistance) /
                                                (maximumDistance – referenceDistance))
     
        With all the distance models, if the formula can not be evaluated then the source will not 
        be attenuated. For example, if a linear model is being used with referenceDistance equal 
        to maximumDistance, then the gain equation will have a divide-by-zero error in it. In this case,
        there is no attenuation for that source.
     
        All the values for distance are specified in meters.
*/
typedef NS_ENUM(NSInteger, AVAudioEnvironmentDistanceAttenuationModel) {
    AVAudioEnvironmentDistanceAttenuationModelExponential   = 1,
    AVAudioEnvironmentDistanceAttenuationModelInverse       = 2,
    AVAudioEnvironmentDistanceAttenuationModelLinear        = 3
} NS_ENUM_AVAILABLE(10_10, 8_0);


/*! @class AVAudioEnvironmentDistanceAttenuationParameters
    @abstract Parameters specifying the amount of distance attenuation
    @discussion
        A standalone instance of AVAudioEnvironmentDistanceAttenuationParameters cannot be created. 
        Only an instance vended out by a source object (e.g. AVAudioEnvironmentNode) can be used.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioEnvironmentDistanceAttenuationParameters : NSObject {
@private
	void *_impl;
}

- (instancetype)init NS_UNAVAILABLE;

/*! @property distanceAttenuationModel
    @abstract Type of distance attenuation model
    @discussion
        Default:    AVAudioEnvironmentDistanceAttenuationModelInverse
*/
@property (nonatomic) AVAudioEnvironmentDistanceAttenuationModel distanceAttenuationModel;

/*! @property referenceDistance
    @abstract The minimum distance at which attenuation is applied
    @discussion
        Default:    1.0 meter
        Models:     AVAudioEnvironmentDistanceAttenuationModelInverse,
                    AVAudioEnvironmentDistanceAttenuationModelLinear
*/
@property (nonatomic) float referenceDistance;

/*! @property maximumDistance
    @abstract The distance beyond which no further attenuation is applied
    @discussion
        Default:    100000.0 meters
        Models:     AVAudioEnvironmentDistanceAttenuationModelLinear
*/
@property (nonatomic) float maximumDistance;

/*! @property rolloffFactor
    @abstract Determines the attenuation curve
    @discussion
        A higher value results in a steeper attenuation curve.
        The rolloff factor should be a value greater than 0.
        Default:    1.0
        Models:     AVAudioEnvironmentDistanceAttenuationModelExponential
                    AVAudioEnvironmentDistanceAttenuationModelInverse
                    AVAudioEnvironmentDistanceAttenuationModelLinear
*/
@property (nonatomic) float rolloffFactor;

@end


/*! @class AVAudioEnvironmentReverbParameters
    @abstract Parameters used to control the reverb in AVAudioEnvironmentNode
    @discussion
        Reverberation can be used to simulate the acoustic characteristics of an environment.
        AVAudioEnvironmentNode has a built in reverb that describes the space that the listener 
        is in.
 
        The reverb also has a single filter that sits at the end of the chain. This filter is useful 
        to shape the overall sound of the reverb. For instance, one of the reverb presets can be 
        selected to simulate the general space and then the filter can be used to brighten or darken 
        the overall sound.
 
        A standalone instance of AVAudioEnvironmentReverbParameters cannot be created.
        Only an instance vended out by a source object (e.g. AVAudioEnvironmentNode) can be used.
*/

OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioEnvironmentReverbParameters : NSObject {
@private
	void *_impl;
}

- (instancetype)init NS_UNAVAILABLE;

/*! @property enable
    @abstract Turns on/off the reverb
    @discussion
        Default:    NO
*/
@property (nonatomic) BOOL enable;

/*! @property level
    @abstract Controls the master level of the reverb
    @discussion
        Range:      -40 to 40 dB
        Default:    0.0
*/
@property (nonatomic) float level;

/*! @property filterParameters
    @abstract filter that applies to the output of the reverb
*/
@property (nonatomic, readonly) AVAudioUnitEQFilterParameters *filterParameters;

/*! @method loadFactoryReverbPreset:
    @abstract Load one of the reverb's factory presets
    @param preset
        Reverb preset to be set.
    @discussion
        Loading a factory reverb preset changes the sound of the reverb. This works independently
        of the filter which follows the reverb in the signal chain.
*/
- (void)loadFactoryReverbPreset:(AVAudioUnitReverbPreset)preset;

@end


/*!
    @class AVAudioEnvironmentNode
    @abstract Mixer node that simulates a 3D environment
    @discussion
        AVAudioEnvironmentNode is a mixer node that simulates a 3D audio environment. Any node that 
        conforms to the AVAudioMixing protocol (e.g. AVAudioPlayerNode) can act as a source in this
        environment.
 
        The environment has an implicit "listener". By controlling the listener's position and
        orientation, the application controls the way the user experiences the virtual world. 
        In addition, this node also defines properties for distance attenuation and reverberation 
        that help characterize the environment.
 
        It is important to note that only inputs with a mono channel connection format to the 
        environment node are spatialized. If the input is stereo, the audio is passed through 
        without being spatialized. Currently inputs with connection formats of more than 2 channels 
        are not supported.
 
        In order to set the environment node’s output to a multichannel format, use an AVAudioFormat 
        having one of the following AudioChannelLayoutTags.
 
        kAudioChannelLayoutTag_AudioUnit_4
        kAudioChannelLayoutTag_AudioUnit_5_0;
        kAudioChannelLayoutTag_AudioUnit_6_0;
        kAudioChannelLayoutTag_AudioUnit_7_0;
        kAudioChannelLayoutTag_AudioUnit_7_0_Front;
        kAudioChannelLayoutTag_AudioUnit_8;
*/

OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioEnvironmentNode : AVAudioNode <AVAudioMixing>

- (instancetype)init NS_DESIGNATED_INITIALIZER;

/*! @property outputVolume
	@abstract The mixer's output volume.
	@discussion
        This accesses the mixer's output volume (0.0-1.0, inclusive).
*/
@property (nonatomic) float outputVolume;

/*! @property nextAvailableInputBus
    @abstract Find an unused input bus
    @discussion
        This will find and return the first input bus to which no other node is connected.
*/
@property (nonatomic, readonly) AVAudioNodeBus nextAvailableInputBus;

/*! @property listenerPosition
    @abstract Sets the listener's position in the 3D environment
    @discussion
        The coordinates are specified in meters.
        Default:
            The default position of the listener is at the origin.
            x: 0.0
            y: 0.0
            z: 0.0
*/
@property (nonatomic) AVAudio3DPoint listenerPosition;

/*! @property listenerVectorOrientation
    @abstract The listener's orientation in the environment
    @discussion
    Changing listenerVectorOrientation will result in a corresponding change in listenerAngularOrientation.
        Default:
            The default orientation is with the listener looking directly along the negative Z axis.
            forward: (0, 0, -1)
            up:      (0, 1, 0)
*/
@property (nonatomic) AVAudio3DVectorOrientation listenerVectorOrientation;

/*! @property listenerAngularOrientation
    @abstract The listener's orientation in the environment
    @discussion
    Changing listenerAngularOrientation will result in a corresponding change in listenerVectorOrientation.
        All angles are specified in degrees.
        Default:
            The default orientation is with the listener looking directly along the negative Z axis.
            yaw: 0.0
            pitch: 0.0
            roll: 0.0
*/
@property (nonatomic) AVAudio3DAngularOrientation listenerAngularOrientation;

/*! @property distanceAttenuationParameters
    @abstract The distance attenuation parameters for the environment
*/
@property (nonatomic, readonly) AVAudioEnvironmentDistanceAttenuationParameters *distanceAttenuationParameters;

/*! @property reverbParameters
    @abstract The reverb parameters for the environment
*/
@property (nonatomic, readonly) AVAudioEnvironmentReverbParameters *reverbParameters;

/*! @property applicableRenderingAlgorithms
    @abstract Returns an array of AVAudio3DMixingRenderingAlgorithm values based on the current output format
    @discussion
        AVAudioEnvironmentNode supports several rendering algorithms per input bus which are defined 
        in <AVFAudio/AVAudioMixing.h>.
 
        Depending on the current output format of the environment node, this method returns 
        an immutable array of the applicable rendering algorithms. This is important when the
        environment node has been configured to a multichannel output format because only a subset
        of the available rendering algorithms are designed to render to all of the channels.
        
        This information should be retrieved after a successful connection to the destination node 
        via the engine's connect method.
*/
@property (nonatomic, readonly) NSArray<NSNumber *> *applicableRenderingAlgorithms;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioPlayer.h
/*
	File:  AVAudioPlayer.h
	
	Framework:  AVFoundation

	Copyright 2008-2016 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAudioFormat.h>
#import <Foundation/Foundation.h>
#import <AVFAudio/AVAudioSettings.h>

#if (TARGET_OS_IPHONE && __has_include(<AVFoundation/AVAudioSession.h>))
#import <AVFAudio/AVAudioSession.h>
#endif // #if TARGET_OS_EMBEDDED

#import <Availability.h>

NS_ASSUME_NONNULL_BEGIN

@class NSData, NSURL, NSError;
#if (TARGET_OS_IPHONE && __has_include(<AVFoundation/AVAudioSession.h>))
@class AVAudioSessionChannelDescription;
#endif
@protocol AVAudioPlayerDelegate;

OS_EXPORT API_AVAILABLE(macos(10.7), ios(2.2), watchos(3.0), tvos(9.0))
@interface AVAudioPlayer : NSObject {
@private
	id _impl;
}

/* For all of these init calls, if a return value of nil is given you can check outError to see what the problem was.
 If not nil, then the object is usable for playing
*/

/* all data must be in the form of an audio file understood by CoreAudio */
- (nullable instancetype)initWithContentsOfURL:(NSURL *)url error:(NSError **)outError;
- (nullable instancetype)initWithData:(NSData *)data error:(NSError **)outError;

/* The file type hint is a constant defined in AVMediaFormat.h whose value is a UTI for a file format. e.g. AVFileTypeAIFF. */
/* Sometimes the type of a file cannot be determined from the data, or it is actually corrupt. The file type hint tells the parser what kind of data to look for so that files which are not self identifying or possibly even corrupt can be successfully parsed. */
- (nullable instancetype)initWithContentsOfURL:(NSURL *)url fileTypeHint:(NSString * __nullable)utiString error:(NSError **)outError API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));
- (nullable instancetype)initWithData:(NSData *)data fileTypeHint:(NSString * __nullable)utiString error:(NSError **)outError API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0));

/* transport control */
/* methods that return BOOL return YES on success and NO on failure. */
- (BOOL)prepareToPlay;	/* get ready to play the sound. happens automatically on play. */
- (BOOL)play;			/* sound is played asynchronously. */
- (BOOL)playAtTime:(NSTimeInterval)time API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0)); /* play a sound some time in the future. time is an absolute time based on and greater than deviceCurrentTime. */
- (void)pause;			/* pauses playback, but remains ready to play. */
- (void)stop;			/* stops playback. no longer ready to play. */

/* properties */

@property(readonly, getter=isPlaying) BOOL playing; /* is it playing or not? */

@property(readonly) NSUInteger numberOfChannels;
@property(readonly) NSTimeInterval duration; /* the duration of the sound. */

/* the UID of the current audio device (as a string) */
@property(copy, nullable) NSString *currentDevice API_AVAILABLE(macos(10.13)) API_UNAVAILABLE(ios, watchos, tvos);

/* the delegate will be sent messages from the AVAudioPlayerDelegate protocol */ 
@property(assign, nullable) id<AVAudioPlayerDelegate> delegate;

/* one of these properties will be non-nil based on the init... method used */
@property(readonly, nullable) NSURL *url; /* returns nil if object was not created with a URL */
@property(readonly, nullable) NSData *data; /* returns nil if object was not created with a data object */

@property float pan API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0)); /* set panning. -1.0 is left, 0.0 is center, 1.0 is right. */
@property float volume; /* The volume for the sound. The nominal range is from 0.0 to 1.0. */
- (void)setVolume:(float)volume fadeDuration:(NSTimeInterval)duration API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0)); /* fade to a new volume over a duration */

@property BOOL enableRate API_AVAILABLE(macos(10.8), ios(5.0), watchos(2.0), tvos(9.0)); /* You must set enableRate to YES for the rate property to take effect. You must set this before calling prepareToPlay. */
@property float rate API_AVAILABLE(macos(10.8), ios(5.0), watchos(2.0), tvos(9.0)); /* See enableRate. The playback rate for the sound. 1.0 is normal, 0.5 is half speed, 2.0 is double speed. */


/*  If the sound is playing, currentTime is the offset into the sound of the current playback position.  
If the sound is not playing, currentTime is the offset into the sound where playing would start. */
@property NSTimeInterval currentTime;

/* returns the current time associated with the output device */
@property(readonly) NSTimeInterval deviceCurrentTime API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0));

/* "numberOfLoops" is the number of times that the sound will return to the beginning upon reaching the end. 
A value of zero means to play the sound just once.
A value of one will result in playing the sound twice, and so on..
Any negative number will loop indefinitely until stopped.
*/
@property NSInteger numberOfLoops;

/* settings */
@property(readonly) NSDictionary<NSString *, id> *settings API_AVAILABLE(macos(10.7), ios(4.0), watchos(2.0), tvos(9.0)); /* returns a settings dictionary with keys as described in AVAudioSettings.h */

/* returns the format of the audio data */
@property(readonly) AVAudioFormat *format API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/* metering */

@property(getter=isMeteringEnabled) BOOL meteringEnabled; /* turns level metering on or off. default is off. */

- (void)updateMeters; /* call to refresh meter values */

- (float)peakPowerForChannel:(NSUInteger)channelNumber; /* returns peak power in decibels for a given channel */
- (float)averagePowerForChannel:(NSUInteger)channelNumber; /* returns average power in decibels for a given channel */

#if (TARGET_OS_IPHONE && !0 && __has_include(<AVFoundation/AVAudioSession.h>))
/* The channels property lets you assign the output to play to specific channels as described by AVAudioSession's channels property */
/* This property is nil valued until set. */
/* The array must have the same number of channels as returned by the numberOfChannels property. */
@property(nonatomic, copy, nullable) NSArray<AVAudioSessionChannelDescription *> *channelAssignments API_AVAILABLE(macos(10.9), ios(7.0), watchos(2.0), tvos(9.0)); /* Array of AVAudioSessionChannelDescription objects */
#endif

@end

/* A protocol for delegates of AVAudioPlayer */
__WATCHOS_AVAILABLE(3_0)
@protocol AVAudioPlayerDelegate <NSObject>
@optional 
/* audioPlayerDidFinishPlaying:successfully: is called when a sound has finished playing. This method is NOT called if the player is stopped due to an interruption. */
- (void)audioPlayerDidFinishPlaying:(AVAudioPlayer *)player successfully:(BOOL)flag;

/* if an error occurs while decoding it will be reported to the delegate. */
- (void)audioPlayerDecodeErrorDidOccur:(AVAudioPlayer *)player error:(NSError * __nullable)error;

#if TARGET_OS_IPHONE

/* AVAudioPlayer INTERRUPTION NOTIFICATIONS ARE DEPRECATED - Use AVAudioSession instead. */

/* audioPlayerBeginInterruption: is called when the audio session has been interrupted while the player was playing. The player will have been paused. */
- (void)audioPlayerBeginInterruption:(AVAudioPlayer *)player NS_DEPRECATED_IOS(2_2, 8_0);

/* audioPlayerEndInterruption:withOptions: is called when the audio session interruption has ended and this player had been interrupted while playing. */
/* Currently the only flag is AVAudioSessionInterruptionFlags_ShouldResume. */
- (void)audioPlayerEndInterruption:(AVAudioPlayer *)player withOptions:(NSUInteger)flags NS_DEPRECATED_IOS(6_0, 8_0);

- (void)audioPlayerEndInterruption:(AVAudioPlayer *)player withFlags:(NSUInteger)flags NS_DEPRECATED_IOS(4_0, 6_0);

/* audioPlayerEndInterruption: is called when the preferred method, audioPlayerEndInterruption:withFlags:, is not implemented. */
- (void)audioPlayerEndInterruption:(AVAudioPlayer *)player NS_DEPRECATED_IOS(2_2, 6_0);

#endif // TARGET_OS_IPHONE

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVFAudio.h
//
//  AVFAudio.h
//  Copyright © 2015 Apple. All rights reserved.
//

#import <AVFAudio/AVAudioBuffer.h>
#import <AVFAudio/AVAudioChannelLayout.h>
#import <AVFAudio/AVAudioConnectionPoint.h>
#import <AVFAudio/AVAudioConverter.h>
#import <AVFAudio/AVAudioEngine.h>
#import <AVFAudio/AVAudioEnvironmentNode.h>
#import <AVFAudio/AVAudioFile.h>
#import <AVFAudio/AVAudioFormat.h>
#import <AVFAudio/AVAudioIONode.h>
#import <AVFAudio/AVAudioMixerNode.h>
#import <AVFAudio/AVAudioMixing.h>
#import <AVFAudio/AVAudioNode.h>
#import <AVFAudio/AVAudioPlayer.h>
#import <AVFAudio/AVAudioPlayerNode.h>
#import <AVFAudio/AVAudioRecorder.h>
#import <AVFAudio/AVAudioSequencer.h>
#import <AVFAudio/AVAudioSettings.h>
#import <AVFAudio/AVAudioTime.h>
#import <AVFAudio/AVAudioTypes.h>
#import <AVFAudio/AVAudioUnit.h>
#import <AVFAudio/AVAudioUnitComponent.h>
#import <AVFAudio/AVAudioUnitDelay.h>
#import <AVFAudio/AVAudioUnitDistortion.h>
#import <AVFAudio/AVAudioUnitEQ.h>
#import <AVFAudio/AVAudioUnitEffect.h>
#import <AVFAudio/AVAudioUnitGenerator.h>
#import <AVFAudio/AVAudioUnitMIDIInstrument.h>
#import <AVFAudio/AVAudioUnitReverb.h>
#import <AVFAudio/AVAudioUnitSampler.h>
#import <AVFAudio/AVAudioUnitTimeEffect.h>
#import <AVFAudio/AVAudioUnitTimePitch.h>
#import <AVFAudio/AVAudioUnitVarispeed.h>
#import <AVFAudio/AVMIDIPlayer.h>

#if TARGET_OS_IPHONE
#import <AVFAudio/AVAudioSession.h>
#import <AVFAudio/AVSpeechSynthesis.h>
#endif
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioConnectionPoint.h
/*
	File:		AVAudioConnectionPoint.h
	Framework:	AVFoundation
	
	Copyright (c) 2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAudioNode;

/*! @class AVAudioConnectionPoint
	@abstract A representation of either a source or destination connection point in AVAudioEngine.
	@discussion
		AVAudioConnectionPoint describes either a source or destination connection point (node, bus)
		in AVAudioEngine's graph.
	
		Instances of this class are immutable.
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0))
@interface AVAudioConnectionPoint : NSObject {
@private
	AVAudioNode *_node;
	AVAudioNodeBus _bus;
	void *_reserved;
}

/*! @method initWithNode:bus:
	@abstract Create a connection point object.
	@param node the source or destination node
	@param bus the output or input bus on the node
	@discussion
		If the node is nil, this method fails (returns nil).
*/
- (instancetype)initWithNode:(AVAudioNode *)node bus:(AVAudioNodeBus)bus NS_DESIGNATED_INITIALIZER;

- (instancetype)init NS_UNAVAILABLE;

/*!	@property node
	@abstract Returns the node in the connection point.
*/
@property (nonatomic, readonly, weak) AVAudioNode *node;

/*!	@property bus
	@abstract Returns the bus on the node in the connection point.
*/
@property (nonatomic, readonly) AVAudioNodeBus bus;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioNode.h
/*
	File:		AVAudioNode.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

#if __has_include(<AudioToolbox/AudioUnit.h>) && __OBJC2__
#define AVAUDIONODE_HAVE_AUAUDIOUNIT 1
#import <AudioToolbox/AudioUnit.h>
#endif

NS_ASSUME_NONNULL_BEGIN

@class AVAudioEngine, AVAudioFormat, AVAudioInputNode, AVAudioMixerNode, AVAudioOutputNode, AVAudioPCMBuffer, AVAudioTime;

#if AVAUDIONODE_HAVE_AUAUDIOUNIT
@class AUAudioUnit;
#endif 

/*!	@typedef AVAudioNodeTapBlock
	@abstract A block that receives copies of the output of an AVAudioNode.
	@param buffer
		a buffer of audio captured from the output of an AVAudioNode
	@param when
		the time at which the buffer was captured
	@discussion
		CAUTION: This callback may be invoked on a thread other than the main thread.
*/
typedef void (^AVAudioNodeTapBlock)(AVAudioPCMBuffer *buffer, AVAudioTime *when);

/*!
	@class AVAudioNode
	@abstract Base class for an audio generation, processing, or I/O block.
	@discussion
		`AVAudioEngine` objects contain instances of various AVAudioNode subclasses. This
		base class provides certain common functionality.
		
		Nodes have input and output busses, which can be thought of as connection points.
		For example, an effect typically has one input bus and one output bus. A mixer
		typically has multiple input busses and one output bus.
		
		Busses have formats, expressed in terms of sample rate and channel count. When making
		connections between nodes, often the format must match exactly. There are exceptions
		(e.g. `AVAudioMixerNode` and `AVAudioOutputNode`).

		Nodes do not currently provide useful functionality until attached to an engine.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioNode : NSObject {
@protected
	void *_impl;
}
/*! @method reset
	@abstract Clear a unit's previous processing state.
*/
- (void)reset;

/*! @method inputFormatForBus:
	@abstract Obtain an input bus's format.
*/
- (AVAudioFormat *)inputFormatForBus:(AVAudioNodeBus)bus;

/*! @method outputFormatForBus:
	@abstract Obtain an output bus's format.
*/
- (AVAudioFormat *)outputFormatForBus:(AVAudioNodeBus)bus;

/*!	@method nameForInputBus:
	@abstract Return the name of an input bus.
*/
- (nullable NSString *)nameForInputBus:(AVAudioNodeBus)bus;

/*!	@method nameForOutputBus:
	@abstract Return the name of an output bus.
*/
- (nullable NSString *)nameForOutputBus:(AVAudioNodeBus)bus;

/*! @method installTapOnBus:bufferSize:format:block:
	@abstract Create a "tap" to record/monitor/observe the output of the node.
	@param bus
		the node output bus to which to attach the tap
	@param bufferSize
		the requested size of the incoming buffers in sample frames. Supported range is [100, 400] ms.
	@param format
		If non-nil, attempts to apply this as the format of the specified output bus. This should
		only be done when attaching to an output bus which is not connected to another node; an
		error will result otherwise.
		The tap and connection formats (if non-nil) on the specified bus should be identical. 
		Otherwise, the latter operation will override any previously set format.
	@param tapBlock
		a block to be called with audio buffers
	
	@discussion
		Only one tap may be installed on any bus. Taps may be safely installed and removed while
		the engine is running.
 
		Note that if you have a tap installed on AVAudioOutputNode, there could be a mismatch
		between the tap buffer format and AVAudioOutputNode's output format, depending on the
		underlying physical device. Hence, instead of tapping the AVAudioOutputNode, it is
		advised to tap the node connected to it.

		E.g. to capture audio from input node:
<pre>
AVAudioEngine *engine = [[AVAudioEngine alloc] init];
AVAudioInputNode *input = [engine inputNode];
AVAudioFormat *format = [input outputFormatForBus: 0];
[input installTapOnBus: 0 bufferSize: 8192 format: format block: ^(AVAudioPCMBuffer *buf, AVAudioTime *when) {
// ‘buf' contains audio captured from input node at time 'when'
}];
....
// start engine
</pre>
*/
- (void)installTapOnBus:(AVAudioNodeBus)bus bufferSize:(AVAudioFrameCount)bufferSize format:(AVAudioFormat * __nullable)format block:(AVAudioNodeTapBlock)tapBlock;

/*!	@method removeTapOnBus:
	@abstract Destroy a tap.
	@param bus
		the node output bus whose tap is to be destroyed
	@return
		YES for success.
*/
- (void)removeTapOnBus:(AVAudioNodeBus)bus;

/*!	@property engine
	@abstract The engine to which the node is attached (or nil).
*/
@property (nonatomic, readonly, nullable) AVAudioEngine *engine;

/*! @property numberOfInputs
	@abstract The node's number of input busses.
*/
@property (nonatomic, readonly) NSUInteger numberOfInputs;

/*! @property numberOfOutputs
	@abstract The node's number of output busses.
*/
@property (nonatomic, readonly) NSUInteger numberOfOutputs;

/*! @property lastRenderTime
	@abstract Obtain the time for which the node most recently rendered.
	@discussion
		Will return nil if the engine is not running or if the node is not connected to an input or
		output node.
*/
@property (nonatomic, readonly, nullable) AVAudioTime *lastRenderTime;

#if AVAUDIONODE_HAVE_AUAUDIOUNIT
/*! @property AUAudioUnit
	@abstract An AUAudioUnit wrapping or underlying the implementation's AudioUnit.
	@discussion
		This provides an AUAudioUnit which either wraps or underlies the implementation's
		AudioUnit, depending on how that audio unit is packaged. Applications can interact with this
		AUAudioUnit to control custom properties, select presets, change parameters, etc.

		No operations that may conflict with state maintained by the engine should be performed 
		directly on the audio unit. These include changing initialization state, stream formats, 
		channel layouts or connections to other audio units.
*/
@property (nonatomic, readonly) AUAudioUnit *AUAudioUnit API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));
#endif // AVAUDIONODE_HAVE_AUAUDIOUNIT

/*!	@property latency
	@abstract The processing latency of the node, in seconds.
	@discussion
		This property reflects the delay between when an impulse in the audio stream arrives at the
		input vs. output of the node. This should reflect the delay due to signal processing 
		(e.g. filters, FFT's, etc.), not delay or reverberation which is being applied as an effect. 
		A value of zero indicates either no latency or an unknown latency.
*/
@property (nonatomic, readonly) NSTimeInterval latency API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!	@property outputPresentationLatency
	@abstract The maximum render pipeline latency downstream of the node, in seconds.
	@discussion
		This describes the maximum time it will take for the audio at the output of a node to be
		presented. 
		For instance, the output presentation latency of the output node in the engine is:
			- zero in manual rendering mode
			- the presentation latency of the device itself when rendering to an audio device
			  (see `AVAudioIONode(presentationLatency)`)
		The output presentation latency of a node connected directly to the output node is the
		output node's presentation latency plus the output node's processing latency (see `latency`).
 
		For a node which is exclusively in the input node chain (i.e. not connected to engine's 
		output node), this property reflects the latency for the output of this node to be 
		presented at the output of the terminating node in the input chain.

		A value of zero indicates either an unknown or no latency.
 
		Note that this latency value can change as the engine is reconfigured (started/stopped, 
		connections made/altered downstream of this node etc.). So it is recommended not to cache
		this value and fetch it whenever it's needed.
*/
@property (nonatomic, readonly) NSTimeInterval outputPresentationLatency API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnit.h
/*
    File:       AVAudioUnit.h
    Framework:  AVFoundation
    
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioNode.h>

#if __has_include(<AudioToolbox/AudioUnit.h>)
#define AVAUDIOUNIT_HAVE_AUDIOUNIT 1
#import <AudioToolbox/AudioUnit.h>
#endif

NS_ASSUME_NONNULL_BEGIN

#ifdef __OBJC2__
@class AUAudioUnit;
#endif // __OBJC2__

/*! @class AVAudioUnit
    @abstract An AVAudioNode implemented by an audio unit.
    @discussion
        An AVAudioUnit is an AVAudioNode implemented by an audio unit. Depending on the type of
        the audio unit, audio is processed either in real-time or non real-time.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnit : AVAudioNode

#if AVAUDIOUNIT_HAVE_AUDIOUNIT

/*!	@method	instantiateWithComponentDescription:options:completionHandler:
	@abstract Asynchronously create an instance of an audio unit component, wrapped in an AVAudioUnit.
	@param audioComponentDescription
		The component to instantiate.
	@param options
		Instantiation options.
	@param completionHandler
		Called in an arbitrary thread/queue context when instantiation is complete. The client
		should retain the provided AVAudioUnit.
	@discussion
		Components whose flags include kAudioComponentFlag_RequiresAsyncInstantiation must be 
		instantiated asynchronously, via this method if they are to be used with AVAudioEngine.
		See the discussion of this flag in AudioToolbox/AudioComponent.h.
		
		The returned AVAudioUnit instance normally will be of a subclass (AVAudioUnitEffect,
		AVAudioUnitGenerator, AVAudioUnitMIDIInstrument, or AVAudioUnitTimeEffect), selected
		according to the component's type.
*/
+ (void)instantiateWithComponentDescription:(AudioComponentDescription)audioComponentDescription options:(AudioComponentInstantiationOptions)options completionHandler:(void (^)(__kindof AVAudioUnit * __nullable audioUnit, NSError * __nullable error))completionHandler API_AVAILABLE(macos(10.11), ios(9.0), tvos(9.0));

/*! @method loadAudioUnitPresetAtURL:error:
    @abstract Load an audio unit preset.
    @param url
        NSURL of the .aupreset file.
	@param outError
    @discussion
        If the .aupreset file cannot be successfully loaded, an error is returned.
*/
- (BOOL)loadAudioUnitPresetAtURL:(NSURL *)url error:(NSError **)outError;

/*! @property audioComponentDescription
    @abstract AudioComponentDescription of the underlying audio unit.
*/
@property (nonatomic, readonly) AudioComponentDescription audioComponentDescription;

/*! @property audioUnit
    @abstract Reference to the underlying audio unit.
    @discussion
        A reference to the underlying audio unit is provided so that parameters that are not
        exposed by AVAudioUnit subclasses can be modified using the AudioUnit C API.
 
        No operations that may conflict with state maintained by the engine should be performed
        directly on the audio unit. These include changing initialization state, stream formats,
        channel layouts or connections to other audio units.
*/
@property (nonatomic, readonly) AudioUnit audioUnit;

#ifdef __OBJC2__
/*! @property AUAudioUnit
    @abstract An AUAudioUnit wrapping or underlying the implementation's AudioUnit.
    @discussion
        This provides an AUAudioUnit which either wraps or underlies the implementation's
        AudioUnit, depending on how that audio unit is packaged. Applications can interact with this
        AUAudioUnit to control custom properties, select presets, change parameters, etc.
 
        As with the audioUnit property, no operations that may conflict with state maintained by the
        engine should be performed directly on the audio unit. These include changing initialization
        state, stream formats, channel layouts or connections to other audio units.
*/
@property (nonatomic, readonly) AUAudioUnit *AUAudioUnit API_AVAILABLE(macos(10.11), ios(9.0), tvos(9.0));
#endif // __OBJC2__

/*! @property name
    @abstract Name of the audio unit.
*/
@property (nonatomic, readonly) NSString *name;

/*! @property manufacturerName
    @abstract Manufacturer name of the audio unit.
*/
@property (nonatomic, readonly) NSString *manufacturerName;

/*! @property version
    @abstract Version number of the audio unit.
*/
@property (nonatomic, readonly) NSUInteger version;

#endif //AVAUDIOUNIT_HAVE_AUDIOUNIT

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioBuffer.h
/*
	File:		AVAudioBuffer.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAudioFormat;

/*!
	@class AVAudioBuffer
	@abstract A buffer of audio data, with a format.
	@discussion
		AVAudioBuffer represents a buffer of audio data and its format.
*/

OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioBuffer : NSObject <NSCopying, NSMutableCopying> {
@protected
	void *_impl;
}

/*!
	@property format
	@abstract The format of the audio in the buffer.
*/
@property (nonatomic, readonly) AVAudioFormat *format;

/*!	@property audioBufferList
	@abstract The buffer's underlying AudioBufferList.
	@discussion
		For compatibility with lower-level CoreAudio and AudioToolbox API's, this method accesses
		the buffer implementation's internal AudioBufferList. The buffer list structure must
		not be modified, though you may modify buffer contents.
		
		The mDataByteSize fields of this AudioBufferList express the buffer's current frameLength.
*/
@property (nonatomic, readonly) const AudioBufferList *audioBufferList;

/*!	@property mutableAudioBufferList
	@abstract A mutable version of the buffer's underlying AudioBufferList.
	@discussion
		Some lower-level CoreAudio and AudioToolbox API's require a mutable AudioBufferList,
		for example, AudioConverterConvertComplexBuffer.
		
		The mDataByteSize fields of this AudioBufferList express the buffer's current frameCapacity.
		If they are altered, you should modify the buffer's frameLength to match.
*/
@property (nonatomic, readonly) AudioBufferList *mutableAudioBufferList;

@end

// -------------------------------------------------------------------------------------------------

/*!
	@class AVAudioPCMBuffer
	@abstract A subclass of AVAudioBuffer for use with PCM audio formats.
	@discussion
		AVAudioPCMBuffer provides a number of methods useful for manipulating buffers of
		audio in PCM format.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioPCMBuffer : AVAudioBuffer

/*!	@method initWithPCMFormat:frameCapacity:
	@abstract Initialize a buffer that is to contain PCM audio samples.
	@param format
		The format of the PCM audio to be contained in the buffer.
	@param frameCapacity
		The capacity of the buffer in PCM sample frames.
	@discussion
		An exception is raised if the format is not PCM.
 
		Returns nil in the following cases:
		- if the format has zero bytes per frame (format.streamDescription->mBytesPerFrame == 0)
		- if the buffer byte capacity (frameCapacity * format.streamDescription->mBytesPerFrame)
		  cannot be represented by an uint32_t
*/
- (nullable instancetype)initWithPCMFormat:(AVAudioFormat *)format frameCapacity:(AVAudioFrameCount)frameCapacity NS_DESIGNATED_INITIALIZER;

/*! @property frameCapacity
	@abstract
		The buffer's capacity, in audio sample frames.
*/
@property (nonatomic, readonly) AVAudioFrameCount frameCapacity;

/*!	@property frameLength
	@abstract The current number of valid sample frames in the buffer.
	@discussion
		You may modify the length of the buffer as part of an operation that modifies its contents.
		The length must be less than or equal to the frameCapacity. Modifying frameLength will update
		the mDataByteSize in each of the underlying AudioBufferList's AudioBuffer's correspondingly,
		and vice versa. Note that in the case of deinterleaved formats, mDataByteSize will refers
		the size of one channel's worth of audio samples.
*/
@property (nonatomic) AVAudioFrameCount frameLength;

/*!	@property stride
	@abstract The buffer's number of interleaved channels.
	@discussion
		Useful in conjunction with floatChannelData etc.
*/
@property (nonatomic, readonly) NSUInteger stride;

/*! @property floatChannelData
	@abstract Access the buffer's float audio samples.
	@discussion
		floatChannelData returns pointers to the buffer's audio samples if the buffer's format is
		32-bit float, or nil if it is another format.
	
		The returned pointer is to format.channelCount pointers to float. Each of these pointers
		is to "frameLength" valid samples, which are spaced by "stride" samples.
		
		If format.interleaved is false (as with the standard deinterleaved float format), then 
		the pointers will be to separate chunks of memory. "stride" is 1.
		
		If format.interleaved is true, then the pointers will refer into the same chunk of interleaved
		samples, each offset by 1 frame. "stride" is the number of interleaved channels.
*/
@property (nonatomic, readonly) float * __nonnull const * __nullable floatChannelData;

/*!	@property int16ChannelData
	@abstract Access the buffer's int16_t audio samples.
	@discussion
		int16ChannelData returns the buffer's audio samples if the buffer's format has 2-byte
		integer samples, or nil if it is another format.
		
		See the discussion of floatChannelData.
*/
@property (nonatomic, readonly) int16_t * __nonnull const * __nullable int16ChannelData;

/*!	@property int32ChannelData
	@abstract Access the buffer's int32_t audio samples.
	@discussion
		int32ChannelData returns the buffer's audio samples if the buffer's format has 4-byte
		integer samples, or nil if it is another format.
		
		See the discussion of floatChannelData.
*/
@property (nonatomic, readonly) int32_t * __nonnull const * __nullable int32ChannelData;

@end


// -------------------------------------------------------------------------------------------------

/*!
	@class AVAudioCompressedBuffer
	@abstract A subclass of AVAudioBuffer for use with compressed audio formats.
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0))
@interface AVAudioCompressedBuffer : AVAudioBuffer

/*!	@method initWithFormat:packetCapacity:maximumPacketSize:
	@abstract Initialize a buffer that is to contain compressed audio data. 
	@param format
		The format of the audio to be contained in the buffer.
	@param packetCapacity
		The capacity of the buffer in packets.
	@param maximumPacketSize
		The maximum size in bytes of a compressed packet. 
		The maximum packet size can be obtained from the maximumOutputPacketSize property of an AVAudioConverter configured for encoding this format.
	@discussion
		An exception is raised if the format is PCM.
*/
- (instancetype)initWithFormat:(AVAudioFormat *)format packetCapacity:(AVAudioPacketCount)packetCapacity maximumPacketSize:(NSInteger)maximumPacketSize;

/*!	@method initWithFormat:packetCapacity:
	@abstract Initialize a buffer that is to contain constant bytes per packet compressed audio data.
	@param format
		The format of the audio to be contained in the buffer.
	@param packetCapacity
		The capacity of the buffer in packets.
	@discussion
		This fails if the format is PCM or if the format has variable bytes per packet (format.streamDescription->mBytesPerPacket == 0).
*/
- (instancetype)initWithFormat:(AVAudioFormat *)format packetCapacity:(AVAudioPacketCount)packetCapacity;

/*! @property packetCapacity
	@abstract
		The number of compressed packets the buffer can contain.
*/
@property (nonatomic, readonly) AVAudioPacketCount packetCapacity;

/*!	@property packetCount
	@abstract The current number of compressed packets in the buffer.
	@discussion
		You may modify the packet length as part of an operation that modifies its contents.
		The packet length must be less than or equal to the packetCapacity.
*/
@property (nonatomic) AVAudioPacketCount packetCount;

/*!	@property maximumPacketSize
	@abstract The maximum size of a compressed packet in bytes.
*/
@property (nonatomic, readonly) NSInteger maximumPacketSize;

/*! @property data
	@abstract Access the buffer's data bytes.
*/
@property (nonatomic, readonly) void *data;

/*!
	@property byteCapacity
	@abstract The buffer's capacity in bytes
*/
@property (nonatomic, readonly) uint32_t byteCapacity API_AVAILABLE(macosx(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!
	@property byteLength
	@abstract The current number of valid bytes in the buffer.
	@discussion
		Can be changed as part of an operation that modifies the contents.
*/
@property (nonatomic) uint32_t byteLength API_AVAILABLE(macosx(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @property packetDescriptions
	@abstract Access the buffer's array of packet descriptions, if any.
	@discussion
		If the format has constant bytes per packet (format.streamDescription->mBytesPerPacket != 0), then this will return nil.
*/
@property (nonatomic, readonly, nullable) AudioStreamPacketDescription *packetDescriptions;

@end

NS_ASSUME_NONNULL_END

// -------------------------------------------------------------------------------------------------

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioIONode.h
/*
	File:		AVAudioIONode.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioNode.h>
#import <AVFAudio/AVAudioMixing.h>
#if __has_include(<AudioToolbox/AudioUnit.h>)
#define AVAUDIOIONODE_HAVE_AUDIOUNIT 1
#import <AudioToolbox/AudioUnit.h>
#endif

NS_ASSUME_NONNULL_BEGIN

/*! @typedef AVAudioIONodeInputBlock
    @abstract 
		A block which will be called by AVAudioEngine's render call when operating in the manual 
		rendering mode, to get input data as needed.
	@param inNumberOfFrames
		The number of frames required to complete the request. You may supply either these many 
		frames or none.
	@return
		An AudioBufferList containing data to be rendered, or null if no data is available.
		The data in the returned buffer must not be cleared or re-filled until the input block is 
		called again or the rendering has finished.
		The format of the returned buffer must match the format specified when registering the
		block.
	@discussion
		If you are out of data and return null or less than the requested number of frames, this
		data will not be used for rendering. The engine will try to render from other active
		sources in the processing graph, and will inform about the input node's status in the error
		returned from its render call.
 
		Note that when the engine is configured to operate in 
		`AVAudioEngineManualRenderingModeRealtime`, this block will be called from a realtime 
		context. Care should be taken not to make any blocking call (e.g. calling libdispatch,
		blocking on a mutex, allocating memory etc.) which may cause an overload at the lower layers.
*/
typedef const AudioBufferList * __nullable (^AVAudioIONodeInputBlock)(AVAudioFrameCount inNumberOfFrames) API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));


/*!	@class AVAudioIONode
	@abstract 
		Base class for a node that performs audio input or output in the engine.
	@discussion
		When the engine is configured to render to/from an audio device, on macOS, AVAudioInputNode 
		and AVAudioOutputNode communicate with the system's default input and output devices. 
		On iOS, they communicate with the devices appropriate to the app's AVAudioSession category 
		and other configuration, also considering the user's actions such as 
		connecting/disconnecting external devices.
 
		In the manual rendering mode, the AVAudioInputNode and AVAudioOutputNode perform the input
		and output in the engine, in response to client's request.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioIONode : AVAudioNode

/*!	@property presentationLatency
	@abstract 
		The presentation or hardware latency, applicable when the engine is rendering to/from an
		audio device.
	@discussion
		This corresponds to kAudioDevicePropertyLatency and kAudioStreamPropertyLatency.
		See <CoreAudio/AudioHardwareBase.h>.
*/
@property (nonatomic, readonly) NSTimeInterval presentationLatency;

#if AVAUDIOIONODE_HAVE_AUDIOUNIT
/*!	@property audioUnit
	@abstract 
		The node's underlying AudioUnit, if any.
	@discussion
		This is only necessary for certain advanced usages.
*/
@property (nonatomic, readonly, nullable) AudioUnit audioUnit;
#endif

@end


/*! @class AVAudioInputNode
	@abstract 
		A node that performs audio input in the engine.
	@discussion
 		When the engine is rendering to/from an audio device, this node connects to the system's 
		audio input.
		When the engine is operating in manual rendering mode, this node can be used to supply
		the input data to the engine.
 
		This node has one element.
		The format of the input scope reflects:
			- the audio hardware sample rate and channel count, when connected to the hardware
			- the format of the PCM audio data that the node will supply to the engine, in the
			  manual rendering mode (see `setManualRenderingInputPCMFormat:inputBlock:`)
 
		When rendering from an audio device, the input node does not support format conversion.
		Hence the format of the output scope must be same as that of the input, as well as the
		formats for all the nodes connected in the input node chain.
 
		In the manual rendering mode, the format of the output scope is initially the same as that
		of the input, but you may set it to a different format, in which case the node will convert.
*/
API_AVAILABLE(macos(10.10), ios(8.0), watchos(4.0), tvos(11.0))
@interface AVAudioInputNode : AVAudioIONode <AVAudioMixing>
- (instancetype)init NS_UNAVAILABLE; // fetch instance via -[AVAudioEngine inputNode].

/*!	@method setManualRenderingInputPCMFormat:inputBlock:
	@abstract
		Supply the data through the input node to the engine operating in the manual rendering mode.
	@param format
		The format of the PCM audio data the block will supply to the engine
	@param block
		The block the engine will call on the input node to get the audio to send to the output,
		when operating in the manual rendering mode. See `AVAudioIONodeInputBlock` for more details
	@return
		YES for success
	@discussion
 		This block must be set if the input node is being used when the engine is operating in 
		manual rendering mode.
		Switching the engine to render to/from an audio device invalidates any previously set block, 
		and makes this method ineffective.
*/
- (BOOL)setManualRenderingInputPCMFormat:(AVAudioFormat *)format inputBlock:(AVAudioIONodeInputBlock)block API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));
@end

/*! @class AVAudioOutputNode
	@abstract 
		A node that performs audio output in the engine.
	@discussion
		When the engine is rendering to/from an audio device, this node connects to the system's 
		audio output.
		When the engine is operating in manual rendering mode, this node performs output in
		response to client's requests.
 
		This node has one element.
		The format of the output scope reflects:
			- the audio hardware sample rate and channel count, when connected to the hardware
			- the engine's manual rendering mode output format (see 
			  `AVAudioEngine(manualRenderingFormat)`), in the manual rendering mode
 
		The format of the input scope is initially the same as that of the
		output, but you may set it to a different format, in which case the node will convert.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioOutputNode : AVAudioIONode
- (instancetype)init NS_UNAVAILABLE; // fetch instance via -[AVAudioEngine outputNode].
@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioPlayerNode.h
/*
	File:		AVAudioPlayerNode.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioNode.h>
#import <AVFAudio/AVAudioFile.h>
#import <AVFAudio/AVAudioMixing.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAudioTime;

/*!
	@enum AVAudioPlayerNodeBufferOptions
	@abstract	Options controlling buffer scheduling.
	
	@constant	AVAudioPlayerNodeBufferLoops
					The buffer loops indefinitely.
	@constant	AVAudioPlayerNodeBufferInterrupts
					The buffer interrupts any buffer already playing.
	@constant	AVAudioPlayerNodeBufferInterruptsAtLoop
					The buffer interrupts any buffer already playing, at its loop point.

	API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
*/
typedef NS_OPTIONS(NSUInteger, AVAudioPlayerNodeBufferOptions) {
    AVAudioPlayerNodeBufferLoops			= 1UL << 0,		// 0x01
	AVAudioPlayerNodeBufferInterrupts		= 1UL << 1,		// 0x02
	AVAudioPlayerNodeBufferInterruptsAtLoop	= 1UL << 2		// 0x04
};

/*!
	@enum AVAudioPlayerNodeCompletionCallbackType
	@abstract	Specifies when the completion handler must be invoked.

	@constant	AVAudioPlayerNodeCompletionDataConsumed
					The buffer or file data has been consumed by the player.
  	@constant	AVAudioPlayerNodeCompletionDataRendered
					The buffer or file data has been rendered (i.e. output) by the player. This
					does not account for any signal processing latencies downstream of the player 
					in the engine (see `AVAudioNode(outputPresentationLatency)`).
	@constant	AVAudioPlayerNodeCompletionDataPlayedBack
					Applicable only when the engine is rendering to/from an audio device.
					The buffer or file has finished playing. This accounts for both (small) signal 
					processing latencies downstream of the player in the engine, as well as
					(possibly significant) latency in the audio playback device.

 	API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));
 */
typedef NS_ENUM(NSInteger, AVAudioPlayerNodeCompletionCallbackType) {
	AVAudioPlayerNodeCompletionDataConsumed		= 0,
	AVAudioPlayerNodeCompletionDataRendered		= 1,
	AVAudioPlayerNodeCompletionDataPlayedBack	= 2,
};

/*! @typedef AVAudioPlayerNodeCompletionHandler
	@abstract Buffer or file completion callback handler.
	@param callbackType
		Indicates the type of buffer or file completion when the callback is invoked.
	@discussion
		AVAudioPlayerNode issues this callback to inform the client about the specific type of
		buffer or file completion. See `AVAudioPlayerNodeCompletionCallbackType` for more details.
 
		Note that the `AVAudioNodeCompletionHandler` callback from some of the player's scheduling
		methods (e.g. `scheduleBuffer:completionHandler:`) is equivalent to the
		AVAudioPlayerNodeCompletionHandler callback for `AVAudioPlayerNodeCompletionDataConsumed`.
 
		In general the callbacks arrive on a non-main thread and it is the client's responsibility
		to handle them in a thread-safe manner.
*/
typedef void (^AVAudioPlayerNodeCompletionHandler)(AVAudioPlayerNodeCompletionCallbackType callbackType) API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!
	@class AVAudioPlayerNode
	@abstract Play buffers or segments of audio files.
	@discussion
		AVAudioPlayerNode supports scheduling the playback of `AVAudioBuffer` instances,
		or segments of audio files opened via `AVAudioFile`. Buffers and segments may be
		scheduled at specific points in time, or to play immediately following preceding segments.
	
		FORMATS
		
		Normally, you will want to configure the node's output format with the same number of
		channels as are in the files and buffers to be played. Otherwise, channels will be dropped
		or added as required. It is usually better to use an `AVAudioMixerNode` to
		do this.
	
		Similarly, when playing file segments, the node will sample rate convert if necessary, but
		it is often preferable to configure the node's output sample rate to match that of the file(s)
		and use a mixer to perform the rate conversion.
		
		When playing buffers, there is an implicit assumption that the buffers are at the same
		sample rate as the node's output format.
		
		TIMELINES
	
		The usual `AVAudioNode` sample times (as observed by `lastRenderTime`)
		have an arbitrary zero point. AVAudioPlayerNode superimposes a second "player timeline" on
		top of this, to reflect when the player was started, and intervals during which it was
		paused. The methods `nodeTimeForPlayerTime:` and `playerTimeForNodeTime:`
		convert between the two.

		This class' `stop` method unschedules all previously scheduled buffers and
		file segments, and returns the player timeline to sample time 0.

		TIMESTAMPS
		
		The "schedule" methods all take an `AVAudioTime` "when" parameter. This is
		interpreted as follows:
		
		1. nil:
			- if there have been previous commands, the new one is played immediately following the
				last one.
			- otherwise, if the node is playing, the event is played in the very near future.
			- otherwise, the command is played at sample time 0.
		2. sample time:
			- relative to the node's start time (which begins at 0 when the node is started).
		3. host time:
			- ignored unless the sample time is invalid when the engine is rendering to an audio 
			  device.
			- ignored in manual rendering mode.
		
		ERRORS
		
		The "schedule" methods can fail if:
		
		1. a buffer's channel count does not match that of the node's output format.
		2. a file can't be accessed.
		3. an AVAudioTime specifies neither a valid sample time or host time.
		4. a segment's start frame or frame count is negative.
 
		BUFFER/FILE COMPLETION HANDLERS
 
		The buffer or file completion handlers (see scheduling methods) are a means to schedule 
		more data if available on the player node. See `AVAudioPlayerNodeCompletionCallbackType` 
		for details on the different buffer/file completion callback types.
		
		Note that a player should not be stopped from within a completion handler callback because
		it can deadlock while trying to unschedule previously scheduled buffers.
 
		OFFLINE RENDERING
	
		When a player node is used with the engine operating in the manual rendering mode, the
		buffer/file completion handlers, `lastRenderTime` and the latencies (`latency` and
		`outputPresentationLatency`) can be used to track how much data the player has rendered and
		how much more data is left to render.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioPlayerNode : AVAudioNode <AVAudioMixing>

- (instancetype)init NS_DESIGNATED_INITIALIZER;

/*! @method scheduleBuffer:completionHandler:
	@abstract Schedule playing samples from an AVAudioBuffer.
	@param buffer
		the buffer to play
	@param completionHandler
		called after the buffer has been consumed by the player or the player is stopped. may be nil.
	@discussion
		Schedules the buffer to be played following any previously scheduled commands.
		
		It is possible for the completionHandler to be called before rendering begins
		or before the buffer is played completely.
*/
- (void)scheduleBuffer:(AVAudioPCMBuffer *)buffer completionHandler:(AVAudioNodeCompletionHandler __nullable)completionHandler;

/*! @method scheduleBuffer:completionCallbackType:completionHandler:
	@abstract Schedule playing samples from an AVAudioBuffer.
	@param buffer
		the buffer to play
	@param callbackType
		option to specify when the completion handler must be called
	@param completionHandler
		called after the buffer has been consumed by the player or has finished playing back or 
		the player is stopped. may be nil.
	@discussion
		Schedules the buffer to be played following any previously scheduled commands.
*/
- (void)scheduleBuffer:(AVAudioPCMBuffer *)buffer completionCallbackType:(AVAudioPlayerNodeCompletionCallbackType)callbackType completionHandler:(AVAudioPlayerNodeCompletionHandler __nullable)completionHandler API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @method scheduleBuffer:atTime:options:completionHandler:
	@abstract Schedule playing samples from an AVAudioBuffer.
	@param buffer
		the buffer to play
	@param when 
		the time at which to play the buffer. see the discussion of timestamps, above.
	@param options
		options for looping, interrupting other buffers, etc.
	@param completionHandler
		called after the buffer has been consumed by the player or the player is stopped. may be nil.
	@discussion
		It is possible for the completionHandler to be called before rendering begins
		or before the buffer is played completely.
*/
- (void)scheduleBuffer:(AVAudioPCMBuffer *)buffer atTime:(AVAudioTime * __nullable)when options:(AVAudioPlayerNodeBufferOptions)options completionHandler:(AVAudioNodeCompletionHandler __nullable)completionHandler;

/*! @method scheduleBuffer:atTime:options:completionCallbackType:completionHandler:
	@abstract Schedule playing samples from an AVAudioBuffer.
	@param buffer
		the buffer to play
	@param when
		the time at which to play the buffer. see the discussion of timestamps, above.
	@param options
		options for looping, interrupting other buffers, etc.
	@param callbackType
		option to specify when the completion handler must be called
	@param completionHandler
		called after the buffer has been consumed by the player or has finished playing back or 
		the player is stopped. may be nil.
*/
- (void)scheduleBuffer:(AVAudioPCMBuffer *)buffer atTime:(AVAudioTime * __nullable)when options:(AVAudioPlayerNodeBufferOptions)options
	 completionCallbackType:(AVAudioPlayerNodeCompletionCallbackType)callbackType completionHandler:(AVAudioPlayerNodeCompletionHandler __nullable)completionHandler API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @method scheduleFile:atTime:completionHandler:
	@abstract Schedule playing of an entire audio file.
	@param file
		the file to play
	@param when 
		the time at which to play the file. see the discussion of timestamps, above.
	@param completionHandler
		called after the file has been consumed by the player or the player is stopped. may be nil.
	@discussion
		It is possible for the completionHandler to be called before rendering begins
		or before the file is played completely.
*/
- (void)scheduleFile:(AVAudioFile *)file atTime:(AVAudioTime * __nullable)when completionHandler:(AVAudioNodeCompletionHandler __nullable)completionHandler;

/*! @method scheduleFile:atTime:completionCallbackType:completionHandler:
	@abstract Schedule playing of an entire audio file.
	@param file
		the file to play
	@param when
		the time at which to play the file. see the discussion of timestamps, above.
	@param callbackType
		option to specify when the completion handler must be called
	@param completionHandler
		called after the file has been consumed by the player or has finished playing back or 
		the player is stopped. may be nil.
*/
- (void)scheduleFile:(AVAudioFile *)file atTime:(AVAudioTime * __nullable)when completionCallbackType:(AVAudioPlayerNodeCompletionCallbackType)callbackType completionHandler:(AVAudioPlayerNodeCompletionHandler __nullable)completionHandler API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*! @method scheduleSegment:startingFrame:frameCount:atTime:completionHandler:
	@abstract Schedule playing a segment of an audio file.
	@param file
		the file to play
	@param startFrame
		the starting frame position in the stream
	@param numberFrames
		the number of frames to play
	@param when
		the time at which to play the region. see the discussion of timestamps, above.
	@param completionHandler
		called after the segment has been consumed by the player or the player is stopped. may be nil.
	@discussion
		It is possible for the completionHandler to be called before rendering begins
		or before the segment is played completely.
*/
- (void)scheduleSegment:(AVAudioFile *)file startingFrame:(AVAudioFramePosition)startFrame frameCount:(AVAudioFrameCount)numberFrames atTime:(AVAudioTime * __nullable)when completionHandler:(AVAudioNodeCompletionHandler __nullable)completionHandler;

/*! @method scheduleSegment:startingFrame:frameCount:atTime:completionCallbackType:completionHandler:
	@abstract Schedule playing a segment of an audio file.
	@param file
		the file to play
	@param startFrame
		the starting frame position in the stream
	@param numberFrames
		the number of frames to play
	@param when
		the time at which to play the region. see the discussion of timestamps, above.
	@param callbackType
		option to specify when the completion handler must be called
	@param completionHandler
		called after the segment has been consumed by the player or has finished playing back or 
		the player is stopped. may be nil.
*/
- (void)scheduleSegment:(AVAudioFile *)file startingFrame:(AVAudioFramePosition)startFrame frameCount:(AVAudioFrameCount)numberFrames atTime:(AVAudioTime * __nullable)when
	completionCallbackType:(AVAudioPlayerNodeCompletionCallbackType)callbackType completionHandler:(AVAudioPlayerNodeCompletionHandler __nullable)completionHandler API_AVAILABLE(macos(10.13), ios(11.0), watchos(4.0), tvos(11.0));

/*!	@method stop
	@abstract Clear all of the node's previously scheduled events and stop playback.
	@discussion
		All of the node's previously scheduled events are cleared, including any that are in the
		middle of playing. The node's sample time (and therefore the times to which new events are 
		to be scheduled) is reset to 0, and will not proceed until the node is started again (via
		play or playAtTime).
 
		Note that pausing or stopping all the players connected to an engine does not pause or stop
		the engine or the underlying hardware. The engine must be explicitly paused or stopped for
		the hardware to stop.
*/
- (void)stop;

/*! @method prepareWithFrameCount:
	@abstract Prepares previously scheduled file regions or buffers for playback.
	@param frameCount
		The number of sample frames of data to be prepared before returning.
	@discussion
*/		
- (void)prepareWithFrameCount:(AVAudioFrameCount)frameCount;

/*!	@method play
	@abstract Start or resume playback immediately.
	@discussion
		equivalent to playAtTime:nil
*/
- (void)play;

/*!	@method playAtTime:
	@abstract Start or resume playback at a specific time.
	@param when
		the node time at which to start or resume playback. nil signifies "now".
	@discussion
		This node is initially paused. Requests to play buffers or file segments are enqueued, and
		any necessary decoding begins immediately. Playback does not begin, however, until the player
		has started playing, via this method.
 
		E.g. To start a player X seconds in future:
<pre>
// start engine and player
NSError *nsErr = nil;
[_engine startAndReturnError:&nsErr];
if (!nsErr) {
	const float kStartDelayTime = 0.5; // sec
	AVAudioFormat *outputFormat = [_player outputFormatForBus:0];
	AVAudioFramePosition startSampleTime = _player.lastRenderTime.sampleTime + kStartDelayTime * outputFormat.sampleRate;
	AVAudioTime *startTime = [AVAudioTime timeWithSampleTime:startSampleTime atRate:outputFormat.sampleRate];
	[_player playAtTime:startTime];
}
</pre>
*/
- (void)playAtTime:(AVAudioTime * __nullable)when;

/*! @method pause
	@abstract Pause playback.
	@discussion
		The player's sample time does not advance while the node is paused.
 
		Note that pausing or stopping all the players connected to an engine does not pause or stop
		the engine or the underlying hardware. The engine must be explicitly paused or stopped for
		the hardware to stop.
*/
- (void)pause;

/*!	@method nodeTimeForPlayerTime:
	@abstract
		Convert from player time to node time.
	@param playerTime
		a time relative to the player's start time
	@return
		a node time
	@discussion
		This method and its inverse `playerTimeForNodeTime:` are discussed in the
		introduction to this class.
	
		If the player is not playing when this method is called, nil is returned.
*/
- (AVAudioTime * __nullable)nodeTimeForPlayerTime:(AVAudioTime *)playerTime;

/*!	@method playerTimeForNodeTime:
	@abstract
		Convert from node time to player time.
	@param nodeTime
		a node time
	@return
		a time relative to the player's start time
	@discussion
		This method and its inverse `nodeTimeForPlayerTime:` are discussed in the
		introduction to this class.
	
		If the player is not playing when this method is called, nil is returned.
*/
- (AVAudioTime * __nullable)playerTimeForNodeTime:(AVAudioTime *)nodeTime;

/*!	@property playing
	@abstract Indicates whether or not the player is playing.
*/
@property(nonatomic, readonly, getter=isPlaying) BOOL playing;


@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioRecorder.h
/*
	File:  AVAudioRecorder.h
	
	Framework:  AVFoundation

	Copyright 2008-2016 Apple Inc. All rights reserved.
*/

#import <AVFoundation/AVBase.h>
#import <AVFoundation/AVAudioFormat.h>
#import <Foundation/Foundation.h>
#import <AVFAudio/AVAudioSettings.h>

#if TARGET_OS_IPHONE
#import <AVFAudio/AVAudioSession.h>
#endif // #if TARGET_OS_EMBEDDED

#import <Availability.h>

NS_ASSUME_NONNULL_BEGIN

@protocol AVAudioRecorderDelegate;
@class NSURL, NSError;


API_AVAILABLE(macos(10.7), ios(3.0), watchos(4.0)) API_UNAVAILABLE(tvos) 
@interface AVAudioRecorder : NSObject {
@private
    void *_impl;
}


/* The file type to create can be set through the corresponding settings key. If not set, it will be inferred from the file extension. Will overwrite a file at the specified url if a file exists. */
- (nullable instancetype)initWithURL:(NSURL *)url settings:(NSDictionary<NSString *, id> *)settings error:(NSError **)outError;

/* The file type to create can be set through the corresponding settings key. If not set, it will be inferred from the file extension. Will overwrite a file at the specified url if a file exists. */
- (nullable instancetype)initWithURL:(NSURL *)url format:(AVAudioFormat *)format error:(NSError **)outError API_AVAILABLE(macos(10.12), ios(10.0), watchos(4.0)) API_UNAVAILABLE(tvos);

/* transport control */
/* methods that return BOOL return YES on success and NO on failure. */
- (BOOL)prepareToRecord; /* creates the file and gets ready to record. happens automatically on record. */
- (BOOL)record; /* start or resume recording to file. */
- (BOOL)recordAtTime:(NSTimeInterval)time API_AVAILABLE(macos(10.9), ios(6.0), watchos(4.0)) API_UNAVAILABLE(tvos); /* start recording at specified time in the future. time is an absolute time based on and greater than deviceCurrentTime. */
- (BOOL)recordForDuration:(NSTimeInterval) duration; /* record a file of a specified duration. the recorder will stop when it has recorded this length of audio */
- (BOOL)recordAtTime:(NSTimeInterval)time forDuration:(NSTimeInterval) duration API_AVAILABLE(macos(10.9), ios(6.0), watchos(4.0)) API_UNAVAILABLE(tvos); /* record a file of a specified duration starting at specified time. time is an absolute time based on and greater than deviceCurrentTime. */
- (void)pause; /* pause recording */
- (void)stop; /* stops recording. closes the file. */

- (BOOL)deleteRecording; /* delete the recorded file. recorder must be stopped. returns NO on failure. */

/* properties */

@property(readonly, getter=isRecording) BOOL recording; /* is it recording or not? */

@property(readonly) NSURL *url; /* URL of the recorded file */

/* these settings are fully valid only when prepareToRecord has been called */
@property(readonly) NSDictionary<NSString *, id> *settings;

/* this object is fully valid only when prepareToRecord has been called */
@property(readonly) AVAudioFormat *format API_AVAILABLE(macos(10.12), ios(10.0), watchos(4.0)) API_UNAVAILABLE(tvos);

/* the delegate will be sent messages from the AVAudioRecorderDelegate protocol */ 
@property(assign, nullable) id<AVAudioRecorderDelegate> delegate;

/* get the current time of the recording - only valid while recording */
@property(readonly) NSTimeInterval currentTime;
/* get the device current time - always valid */
@property(readonly) NSTimeInterval deviceCurrentTime API_AVAILABLE(macos(10.9), ios(6.0), watchos(4.0)) API_UNAVAILABLE(tvos);

/* metering */

@property(getter=isMeteringEnabled) BOOL meteringEnabled; /* turns level metering on or off. default is off. */

- (void)updateMeters; /* call to refresh meter values */

- (float)peakPowerForChannel:(NSUInteger)channelNumber; /* returns peak power in decibels for a given channel */
- (float)averagePowerForChannel:(NSUInteger)channelNumber; /* returns average power in decibels for a given channel */

#if TARGET_OS_IPHONE
/* The channels property lets you assign the output to record specific channels as described by AVAudioSession's channels property */
/* This property is nil valued until set. */
/* The array must have the same number of channels as returned by the numberOfChannels property. */
@property(nonatomic, copy, nullable) NSArray<AVAudioSessionChannelDescription *> *channelAssignments API_AVAILABLE(macos(10.9), ios(7.0), watchos(4.0)) API_UNAVAILABLE(tvos); /* Array of AVAudioSessionChannelDescription objects */
#endif

@end


/* A protocol for delegates of AVAudioRecorder */
API_AVAILABLE(macos(10.7), ios(3.0), watchos(4.0)) API_UNAVAILABLE(tvos) 
@protocol AVAudioRecorderDelegate <NSObject>
@optional 

/* audioRecorderDidFinishRecording:successfully: is called when a recording has been finished or stopped. This method is NOT called if the recorder is stopped due to an interruption. */
- (void)audioRecorderDidFinishRecording:(AVAudioRecorder *)recorder successfully:(BOOL)flag;

/* if an error occurs while encoding it will be reported to the delegate. */
- (void)audioRecorderEncodeErrorDidOccur:(AVAudioRecorder *)recorder error:(NSError * __nullable)error;

#if TARGET_OS_IPHONE

/* AVAudioRecorder INTERRUPTION NOTIFICATIONS ARE DEPRECATED - Use AVAudioSession instead. */

/* audioRecorderBeginInterruption: is called when the audio session has been interrupted while the recorder was recording. The recorded file will be closed. */
- (void)audioRecorderBeginInterruption:(AVAudioRecorder *)recorder NS_DEPRECATED_IOS(2_2, 8_0);

/* audioRecorderEndInterruption:withOptions: is called when the audio session interruption has ended and this recorder had been interrupted while recording. */
/* Currently the only flag is AVAudioSessionInterruptionFlags_ShouldResume. */
- (void)audioRecorderEndInterruption:(AVAudioRecorder *)recorder withOptions:(NSUInteger)flags NS_DEPRECATED_IOS(6_0, 8_0);

- (void)audioRecorderEndInterruption:(AVAudioRecorder *)recorder withFlags:(NSUInteger)flags NS_DEPRECATED_IOS(4_0, 6_0);

/* audioRecorderEndInterruption: is called when the preferred method, audioRecorderEndInterruption:withFlags:, is not implemented. */
- (void)audioRecorderEndInterruption:(AVAudioRecorder *)recorder NS_DEPRECATED_IOS(2_2, 6_0);

#endif // TARGET_OS_IPHONE

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioTypes.h
/*
	File:		AVAudioTypes.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#ifndef __AVAudioTypes_h__
#define __AVAudioTypes_h__

#import <Foundation/Foundation.h>
#import <AVFoundation/AVBase.h>
#import <CoreAudio/CoreAudioTypes.h>

/*! @typedef AVAudioFramePosition
	@abstract A position in an audio file or stream.
*/
typedef int64_t		AVAudioFramePosition;

/*!	@typedef AVAudioFrameCount
	@abstract A number of audio sample frames.
	
	@discussion
		Rationale: making this a potentially larger-than-32-bit type like NSUInteger would open the
		door to a large set of runtime failures due to underlying implementations' use of UInt32.
		
		TODO: Remove rationales.
*/
typedef uint32_t	AVAudioFrameCount;

/*!	@typedef AVAudioPacketCount
	@abstract A number of packets of compressed audio data.
	
	@discussion
		Rationale: making this a potentially larger-than-32-bit type like NSUInteger would open the
		door to a large set of runtime failures due to underlying implementations' use of UInt32.
		
		TODO: Remove rationales.
*/
typedef uint32_t	AVAudioPacketCount;

/*!	@typedef AVAudioChannelCount
	@abstract A number of audio channels.
	
	@discussion
		Rationale: making this a potentially larger-than-32-bit type like NSUInteger would open the
		door to a large set of runtime failures due to underlying implementations' use of UInt32.
*/
typedef uint32_t	AVAudioChannelCount;

/*! @typedef AVAudioNodeCompletionHandler
	@abstract Generic callback handler.
	@discussion
		Various AVAudioEngine objects issue callbacks to generic blocks of this type. In general
		the callback arrives on a non-main thread and it is the client's responsibility to handle it
		in a thread-safe manner.
*/
typedef void (^AVAudioNodeCompletionHandler)(void);

/*!	@typedef AVAudioNodeBus
	@abstract The index of a bus on an AVAudioNode.
	@discussion
		@link AVAudioNode @/link objects potentially have multiple input and/or output busses.
		AVAudioNodeBus represents a bus as a zero-based index.
*/
typedef NSUInteger AVAudioNodeBus;



/*=============================================================================*/
/*!	@struct AVAudio3DPoint
    @abstract Struct representing a point in 3D space
    @discussion
        This struct is used by classes dealing with 3D audio such as `AVAudioMixing`
        and `AVAudioEnvironmentNode` and represents a point in 3D space.
*/
struct AVAudio3DPoint {
    float x;
    float y;
    float z;
};
typedef struct AVAudio3DPoint AVAudio3DPoint;

/*!	@method AVAudioMake3DPoint
    @abstract Creates and returns an AVAudio3DPoint object
*/
NS_INLINE AVAudio3DPoint AVAudioMake3DPoint(float x, float y, float z) {
    AVAudio3DPoint p;
    p.x = x;
    p.y = y;
    p.z = z;
    return p;
}

/*!	@typedef AVAudio3DVector
    @abstract Struct representing a vector in 3D space
    @discussion
        This struct is used by classes dealing with 3D audio such as @link AVAudioMixing @/link
        and @link AVAudioEnvironmentNode @/link and represents a vector in 3D space.
*/
typedef struct AVAudio3DPoint AVAudio3DVector;

/*!	@method AVAudio3DVector
    @abstract Creates and returns an AVAudio3DVector object
*/
NS_INLINE AVAudio3DVector AVAudioMake3DVector(float x, float y, float z) {
    AVAudio3DVector v;
    v.x = x;
    v.y = y;
    v.z = z;
    return v;
}

/*!	@struct AVAudio3DVectorOrientation
    @abstract Struct representing the orientation of the listener in 3D space
    @discussion
        Two orthogonal vectors describe the orientation of the listener. The forward
        vector points in the direction that the listener is facing. The up vector is orthogonal
        to the forward vector and points upwards from the listener's head.
*/
struct AVAudio3DVectorOrientation {
    AVAudio3DVector forward;
    AVAudio3DVector up;
};
typedef struct AVAudio3DVectorOrientation AVAudio3DVectorOrientation;

/*!	@method AVAudioMake3DVectorOrientation
    @abstract Creates and returns an AVAudio3DVectorOrientation object
*/
NS_INLINE AVAudio3DVectorOrientation AVAudioMake3DVectorOrientation(AVAudio3DVector forward, AVAudio3DVector up) {
    AVAudio3DVectorOrientation o;
    o.forward = forward;
    o.up = up;
    return o;
}

/*!	@struct AVAudio3DAngularOrientation
    @abstract Struct representing the orientation of the listener in 3D space
    @discussion
        Three angles describe the orientation of a listener's head - yaw, pitch and roll.
 
        Yaw describes the side to side movement of the listener's head.
        The yaw axis is perpendicular to the plane of the listener's ears with its origin at the 
        center of the listener's head and directed towards the bottom of the listener's head. A 
        positive yaw is in the clockwise direction going from 0 to 180 degrees. A negative yaw is in 
        the counter-clockwise direction going from 0 to -180 degrees.
 
        Pitch describes the up-down movement of the listener's head.
        The pitch axis is perpendicular to the yaw axis and is parallel to the plane of the 
        listener's ears with its origin at the center of the listener's head and directed towards 
        the right ear. A positive pitch is the upwards direction going from 0 to 180 degrees. A 
        negative pitch is in the downwards direction going from 0 to -180 degrees.
 
        Roll describes the tilt of the listener's head.
        The roll axis is perpendicular to the other two axes with its origin at the center of the 
        listener's head and is directed towards the listener's nose. A positive roll is to the right 
        going from 0 to 180 degrees. A negative roll is to the left going from 0 to -180 degrees.
*/
struct AVAudio3DAngularOrientation {
    float yaw;
    float pitch;
    float roll;
};
typedef struct AVAudio3DAngularOrientation AVAudio3DAngularOrientation;

/*!	@method AVAudioMake3DAngularOrientation
    @abstract Creates and returns an AVAudio3DAngularOrientation object
*/
NS_INLINE AVAudio3DAngularOrientation AVAudioMake3DAngularOrientation(float yaw, float pitch, float roll) {
    AVAudio3DAngularOrientation o;
    o.yaw = yaw;
    o.pitch = pitch;
    o.roll = roll;
    return o;
}

#endif // __AVAudioTypes_h__

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioConverter.h
/*
	File:		AVAudioConverter.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>
#import <AVFAudio/AVAudioFormat.h>
#import <AVFAudio/AVAudioBuffer.h>

NS_ASSUME_NONNULL_BEGIN

/*! @enum AVAudioConverterPrimeMethod
    @abstract values for the primeMethod property. See further discussion under AVAudioConverterPrimeInfo.
     
        AVAudioConverterPrimeMethod_Pre
            Primes with leading + trailing input frames.
     
        AVAudioConverterPrimeMethod_Normal
			Only primes with trailing (zero latency). Leading frames are assumed to be silence.
     
        AVAudioConverterPrimeMethod_None
			Acts in "latency" mode. Both leading and trailing frames assumed to be silence.
*/
typedef NS_ENUM(NSInteger, AVAudioConverterPrimeMethod) {
    AVAudioConverterPrimeMethod_Pre       = 0,
    AVAudioConverterPrimeMethod_Normal    = 1,
    AVAudioConverterPrimeMethod_None      = 2
};

/*!
    @struct     AVAudioConverterPrimeInfo
    @abstract   This struct is the value of the primeInfo property and specifies priming information.
    
    @field      leadingFrames
        Specifies the number of leading (previous) input frames, relative to the normal/desired
        start input frame, required by the converter to perform a high quality conversion. If
        using AVAudioConverterPrimeMethod_Pre, the client should "pre-seek" the input stream provided
        through the input proc by leadingFrames. If no frames are available previous to the
        desired input start frame (because, for example, the desired start frame is at the very
        beginning of available audio), then provide "leadingFrames" worth of initial zero frames
        in the input proc.  Do not "pre-seek" in the default case of
        AVAudioConverterPrimeMethod_Normal or when using AVAudioConverterPrimeMethod_None.

    @field      trailingFrames
        Specifies the number of trailing input frames (past the normal/expected end input frame)
        required by the converter to perform a high quality conversion.  The client should be
        prepared to provide this number of additional input frames except when using
        AVAudioConverterPrimeMethod_None. If no more frames of input are available in the input stream
        (because, for example, the desired end frame is at the end of an audio file), then zero
        (silent) trailing frames will be synthesized for the client.
            
    @discussion
        When using convertToBuffer:error:withInputFromBlock: (either a single call or a series of calls), some
        conversions, particularly involving sample-rate conversion, ideally require a certain
        number of input frames previous to the normal start input frame and beyond the end of
        the last expected input frame in order to yield high-quality results.
        
        These are expressed in the leadingFrames and trailingFrames members of the structure.
        
        The very first call to convertToBuffer:error:withInputFromBlock:, or first call after
        reset, will request additional input frames beyond those normally
        expected in the input proc callback to fulfill this first AudioConverterFillComplexBuffer()
        request. The number of additional frames requested, depending on the prime method, will
        be approximately:

        <pre>
            AVAudioConverterPrimeMethod_Pre       leadingFrames + trailingFrames
            AVAudioConverterPrimeMethod_Normal    trailingFrames
            AVAudioConverterPrimeMethod_None      0
        </pre>

        Thus, in effect, the first input proc callback(s) may provide not only the leading
        frames, but also may "read ahead" by an additional number of trailing frames depending
        on the prime method.

        AVAudioConverterPrimeMethod_None is useful in a real-time application processing live input,
        in which case trailingFrames (relative to input sample rate) of through latency will be
        seen at the beginning of the output of the AudioConverter.  In other real-time
        applications such as DAW systems, it may be possible to provide these initial extra
        audio frames since they are stored on disk or in memory somewhere and
        AVAudioConverterPrimeMethod_Pre may be preferable.  The default method is
        AVAudioConverterPrimeMethod_Normal, which requires no pre-seeking of the input stream and
        generates no latency at the output.
*/
typedef struct AVAudioConverterPrimeInfo {
    AVAudioFrameCount      leadingFrames;
    AVAudioFrameCount      trailingFrames;
} AVAudioConverterPrimeInfo;


/*! @enum AVAudioConverterInputStatus
    @abstract You must return one of these codes from your AVAudioConverterInputBlock.
     
        AVAudioConverterInputStatus_HaveData
            This is the normal case where you supply data to the converter.
     
        AVAudioConverterInputStatus_NoDataNow
			If you are out of data for now, set *ioNumberOfPackets = 0 and return AVAudioConverterInputStatus_NoDataNow and the 
			conversion routine will return as much output as could be converted with the input already supplied.
     
        AVAudioConverterInputStatus_EndOfStream
			If you are at the end of stream, set *ioNumberOfPackets = 0 and return AVAudioConverterInputStatus_EndOfStream.
*/
typedef NS_ENUM(NSInteger, AVAudioConverterInputStatus) {
	AVAudioConverterInputStatus_HaveData    = 0,
	AVAudioConverterInputStatus_NoDataNow   = 1,
	AVAudioConverterInputStatus_EndOfStream = 2
}  NS_ENUM_AVAILABLE(10_11, 9_0);

/*! @enum AVAudioConverterOutputStatus
    @abstract These values are returned from convertToBuffer:error:withInputFromBlock:

		AVAudioConverterOutputStatus_HaveData
			All of the requested data was returned.

		AVAudioConverterOutputStatus_InputRanDry
			Not enough input was available to satisfy the request at the current time. The output buffer contains as much as could be converted.
			
		AVAudioConverterOutputStatus_EndOfStream
			The end of stream has been reached. No data was returned.
		
		AVAudioConverterOutputStatus_Error
			An error occurred.
*/
typedef NS_ENUM(NSInteger, AVAudioConverterOutputStatus) {
	AVAudioConverterOutputStatus_HaveData          = 0,
	AVAudioConverterOutputStatus_InputRanDry       = 1,
	AVAudioConverterOutputStatus_EndOfStream       = 2,
	AVAudioConverterOutputStatus_Error             = 3
}  NS_ENUM_AVAILABLE(10_11, 9_0);

/*! @typedef AVAudioConverterInputBlock
    @abstract A block which will be called by convertToBuffer:error:withInputFromBlock: to get input data as needed. 
	@param  inNumberOfPackets
		This will be the number of packets required to complete the request.
		You may supply more or less that this amount. If less, then the input block will get called again.
	@param outStatus
		The block must set the appropriate AVAudioConverterInputStatus enum value.
		If you have supplied data, set outStatus to AVAudioConverterInputStatus_HaveData and return an AVAudioBuffer.
		If you are out of data for now, set outStatus to AVAudioConverterInputStatus_NoDataNow and return nil, and the
		conversion routine will return as much output as could be converted with the input already supplied.
		If you are at the end of stream, set outStatus to AVAudioConverterInputStatus_EndOfStream, and return nil.
	@return
		An AVAudioBuffer containing data to be converted, or nil if at end of stream or no data is available.
		The data in the returned buffer must not be cleared or re-filled until the input block is called again or the conversion has finished.
	@discussion
		convertToBuffer:error:withInputFromBlock: will return as much output as could be converted with the input already supplied.
*/
typedef AVAudioBuffer * __nullable (^AVAudioConverterInputBlock)(AVAudioPacketCount inNumberOfPackets, AVAudioConverterInputStatus* outStatus);

/*!
	@class AVAudioConverter
	@abstract
		AVAudioConverter converts streams of audio between various formats.
	@discussion
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0))
@interface AVAudioConverter : NSObject {
@private
	void *_impl;
}

/*!	@method initFromFormat:toFormat:
	@abstract Initialize from input and output formats.
	@param fromFormat 
		The input format.
	@param toFormat 
		The output format.
	@discussion
		Returns nil if the format conversion is not possible.
*/
- (nullable instancetype)initFromFormat:(AVAudioFormat *)fromFormat toFormat:(AVAudioFormat *)toFormat;


/*! @method reset
    @abstract Resets the converter so that a new stream may be converted.
*/
- (void)reset;

/*! @property inputFormat
    @abstract The format of the input audio stream. (NB. AVAudioFormat includes the channel layout)
*/
@property (nonatomic, readonly) AVAudioFormat *inputFormat;

/*! @property outputFormat
    @abstract The format of the output audio stream. (NB. AVAudioFormat includes the channel layout)
*/
@property (nonatomic, readonly) AVAudioFormat *outputFormat;

/*! @property channelMap
    @abstract An array of integers indicating from which input to derive each output.
	@discussion 
		The array has size equal to the number of output channels. Each element's value is
		the input channel number, starting with zero, that is to be copied to that output. A negative value means 
		that the output channel will have no source and will be silent.
		Setting a channel map overrides channel mapping due to any channel layouts in the input and output formats that may have been supplied.
*/
@property (nonatomic, retain) NSArray<NSNumber *> *channelMap;

/*! @property magicCookie
    @abstract Decoders require some data in the form of a magicCookie in order to decode properly. Encoders will produce a magicCookie.
*/
@property (nonatomic, retain, nullable) NSData *magicCookie;

/*! @property downmix
    @abstract If YES and channel remapping is necessary, then channels will be mixed as appropriate instead of remapped. Default value is NO.
*/
@property (nonatomic) BOOL downmix;

/*! @property dither
    @abstract Setting YES will turn on dither, if dither makes sense in given the current formats and settings. Default value is NO.
*/
@property (nonatomic) BOOL dither;

/*! @property sampleRateConverterQuality
    @abstract An AVAudioQuality value as defined in AVAudioSettings.h.
*/
@property (nonatomic) NSInteger sampleRateConverterQuality;

/*! @property sampleRateConverterAlgorithm
    @abstract An AVSampleRateConverterAlgorithmKey value as defined in AVAudioSettings.h.
*/
@property (nonatomic, retain, nullable) NSString *sampleRateConverterAlgorithm;

/*! @property primeMethod
    @abstract Indicates the priming method to be used by the sample rate converter or decoder.
*/
@property (nonatomic) AVAudioConverterPrimeMethod primeMethod;

/*! @property primeInfo
    @abstract Indicates the the number of priming frames .
*/
@property (nonatomic) AVAudioConverterPrimeInfo primeInfo;


/*! @method convertToBuffer:fromBuffer:error:
    @abstract Perform a simple conversion. That is, a conversion which does not involve codecs or sample rate conversion.
	@param inputBuffer 
		The input buffer.
	@param outputBuffer 
		The output buffer.
	@param outError 
		An error if the conversion fails.
	@return 
		YES is returned on success, NO when an error has occurred.
	@discussion 
		The output buffer's frameCapacity should be at least at large as the inputBuffer's frameLength.
		If the conversion involves a codec or sample rate conversion, you instead must use
		convertToBuffer:error:withInputFromBlock:.
*/
- (BOOL)convertToBuffer:(AVAudioPCMBuffer *)outputBuffer fromBuffer:(const AVAudioPCMBuffer *)inputBuffer error:(NSError **)outError;

/*! @method convertToBuffer:error:withInputFromBlock:
    @abstract Perform any supported conversion. 
	@param inputBlock
		A block which will be called to get input data as needed. See description for AVAudioConverterInputBlock.
	@param outputBuffer 
		The output buffer.
	@param outError 
		An error if the conversion fails.
	@return 
		An AVAudioConverterOutputStatus is returned.
	@discussion 
		It attempts to fill the buffer to its capacity. On return, the buffer's length indicates the number of 
		sample frames successfully converted.
*/
- (AVAudioConverterOutputStatus)convertToBuffer:(AVAudioBuffer *)outputBuffer error:(NSError **)outError withInputFromBlock:(AVAudioConverterInputBlock)inputBlock;

@end


@interface AVAudioConverter (Encoding)

/*! @property bitRate
    @abstract bitRate in bits per second. Only applies when encoding.
*/
@property (nonatomic) NSInteger bitRate;

/*! @property bitRateStrategy
    @abstract When encoding, an AVEncoderBitRateStrategyKey value constant as defined in AVAudioSettings.h. Returns nil if not encoding.
*/
@property (nonatomic, retain, nullable) NSString *bitRateStrategy;

/*! @property maximumOutputPacketSize
	@abstract The maximum size of an output packet, in bytes.
    @discussion When encoding it is useful to know how large a packet can be in order to allocate a buffer to receive the output.
*/
@property (nonatomic, readonly) NSInteger maximumOutputPacketSize;

/*! @property availableEncodeBitRates
    @abstract When encoding, an NSArray of NSNumber of all bit rates provided by the codec. Returns nil if not encoding.
*/
@property (nonatomic, readonly, nullable) NSArray<NSNumber *> *availableEncodeBitRates;

/*! @property applicableEncodeBitRates
    @abstract When encoding, an NSArray of NSNumber of bit rates that can be applied based on the current formats and settings. Returns nil if not encoding.
*/
@property (nonatomic, readonly, nullable) NSArray<NSNumber *> *applicableEncodeBitRates;

/*! @property availableEncodeSampleRates
    @abstract When encoding, an NSArray of NSNumber of all output sample rates provided by the codec. Returns nil if not encoding.
*/
@property (nonatomic, readonly, nullable) NSArray<NSNumber *> *availableEncodeSampleRates;

/*! @property applicableEncodeSampleRates
    @abstract When encoding, an NSArray of NSNumber of output sample rates that can be applied based on the current formats and settings. Returns nil if not encoding.
*/
@property (nonatomic, readonly, nullable) NSArray<NSNumber *> *applicableEncodeSampleRates;

/*! @property availableEncodeChannelLayoutTags
    @abstract When encoding, an NSArray of NSNumber of all output channel layout tags provided by the codec. Returns nil if not encoding.
*/
@property (nonatomic, readonly, nullable) NSArray<NSNumber *> *availableEncodeChannelLayoutTags;

@end


NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitDistortion.h
/*
    File:		AVAudioUnitDistortion.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnitEffect.h>

NS_ASSUME_NONNULL_BEGIN

typedef NS_ENUM(NSInteger, AVAudioUnitDistortionPreset) {
    AVAudioUnitDistortionPresetDrumsBitBrush           = 0,
    AVAudioUnitDistortionPresetDrumsBufferBeats        = 1,
    AVAudioUnitDistortionPresetDrumsLoFi               = 2,
    AVAudioUnitDistortionPresetMultiBrokenSpeaker      = 3,
    AVAudioUnitDistortionPresetMultiCellphoneConcert   = 4,
    AVAudioUnitDistortionPresetMultiDecimated1         = 5,
    AVAudioUnitDistortionPresetMultiDecimated2         = 6,
    AVAudioUnitDistortionPresetMultiDecimated3         = 7,
    AVAudioUnitDistortionPresetMultiDecimated4         = 8,
    AVAudioUnitDistortionPresetMultiDistortedFunk      = 9,
    AVAudioUnitDistortionPresetMultiDistortedCubed     = 10,
    AVAudioUnitDistortionPresetMultiDistortedSquared   = 11,
    AVAudioUnitDistortionPresetMultiEcho1              = 12,
    AVAudioUnitDistortionPresetMultiEcho2              = 13,
    AVAudioUnitDistortionPresetMultiEchoTight1         = 14,
    AVAudioUnitDistortionPresetMultiEchoTight2         = 15,
    AVAudioUnitDistortionPresetMultiEverythingIsBroken = 16,
    AVAudioUnitDistortionPresetSpeechAlienChatter      = 17,
    AVAudioUnitDistortionPresetSpeechCosmicInterference = 18,
    AVAudioUnitDistortionPresetSpeechGoldenPi          = 19,
    AVAudioUnitDistortionPresetSpeechRadioTower        = 20,
    AVAudioUnitDistortionPresetSpeechWaves             = 21
} NS_ENUM_AVAILABLE(10_10, 8_0);

/*! @class AVAudioUnitDistortion
    @abstract An AVAudioUnitEffect that implements a multi-stage distortion effect.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitDistortion : AVAudioUnitEffect

/*! @method loadFactoryPreset:
    @abstract Load a distortion preset.
    Default:    AVAudioUnitDistortionPresetDrumsBitBrush
*/
-(void)loadFactoryPreset:(AVAudioUnitDistortionPreset)preset;

/*! @property preGain
    @abstract
    Gain applied to the signal before being distorted
    Range:      -80 -> 20
    Default:    -6
    Unit:       dB
*/
@property (nonatomic) float preGain;

/*! @property wetDryMix
    @abstract
    Blend of the distorted and dry signals
    Range:      0 (all dry) -> 100 (all distorted)
    Default:    50
    Unit:       Percent
*/
@property (nonatomic) float wetDryMix;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitTimePitch.h
/*
    File:		AVAudioUnitTimePitch.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioUnitTimeEffect.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitTimePitch
    @abstract an AVAudioUnitTimeEffect that provides good quality time stretching and pitch shifting
    @discussion
        In this time effect, the playback rate and pitch parameters function independently of each other
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitTimePitch : AVAudioUnitTimeEffect

/*! @property rate
    @abstract playback rate of the input signal
 
    Range:      1/32 -> 32.0
    Default:    1.0
    Unit:       Generic
*/
@property (nonatomic) float rate;

/*! @property pitch
    @abstract amount by which the input signal is pitch shifted
    @discussion
              1 octave  = 1200 cents
    1 musical semitone  = 100 cents
 
    Range:      -2400 -> 2400
    Default:    0.0
    Unit:       Cents
*/
@property (nonatomic) float pitch;

/*! @property overlap
    @abstract amount of overlap between segments of the input audio signal
    @discussion
    A higher value results in fewer artifacts in the output signal.
    This parameter also impacts the amount of CPU used.
 
    Range:      3.0 -> 32.0
    Default:    8.0
    Unit:       Generic
*/
@property (nonatomic) float overlap;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitEQ.h
/*
    File:		AVAudioUnitEQ.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#import <AVFAudio/AVAudioUnitEffect.h>

NS_ASSUME_NONNULL_BEGIN

/*! @enum AVAudioUnitEQFilterType
    @abstract Filter types available to use with AVAudioUnitEQ.
    @discussion
        Depending on the filter type, a combination of one or all of the filter parameters defined 
        in AVAudioUnitEQFilterParameters are used to set the filter.
     
        AVAudioUnitEQFilterTypeParametric
            Parametric filter based on Butterworth analog prototype.
            Required parameters: frequency (center), bandwidth, gain
     
        AVAudioUnitEQFilterTypeLowPass
            Simple Butterworth 2nd order low pass filter
            Required parameters: frequency (-3 dB cutoff at specified frequency)
        
        AVAudioUnitEQFilterTypeHighPass
            Simple Butterworth 2nd order high pass filter
            Required parameters: frequency (-3 dB cutoff at specified frequency)
     
        AVAudioUnitEQFilterTypeResonantLowPass
            Low pass filter with resonance support (via bandwidth parameter)
            Required parameters: frequency (-3 dB cutoff at specified frequency), bandwidth
     
        AVAudioUnitEQFilterTypeResonantHighPass
            High pass filter with resonance support (via bandwidth parameter)
            Required parameters: frequency (-3 dB cutoff at specified frequency), bandwidth
     
        AVAudioUnitEQFilterTypeBandPass
            Band pass filter
            Required parameters: frequency (center), bandwidth
     
        AVAudioUnitEQFilterTypeBandStop
            Band stop filter (aka "notch filter")
            Required parameters: frequency (center), bandwidth
     
        AVAudioUnitEQFilterTypeLowShelf
            Low shelf filter
            Required parameters: frequency (center), gain
     
        AVAudioUnitEQFilterTypeHighShelf
            High shelf filter
            Required parameters: frequency (center), gain
     
        AVAudioUnitEQFilterTypeResonantLowShelf
            Low shelf filter with resonance support (via bandwidth parameter)
            Required parameters: frequency (center), bandwidth, gain
     
        AVAudioUnitEQFilterTypeResonantHighShelf
            High shelf filter with resonance support (via bandwidth parameter)
            Required parameters: frequency (center), bandwidth, gain
 
*/
typedef NS_ENUM(NSInteger, AVAudioUnitEQFilterType) {
    AVAudioUnitEQFilterTypeParametric        = 0,
    AVAudioUnitEQFilterTypeLowPass           = 1,
    AVAudioUnitEQFilterTypeHighPass          = 2,
    AVAudioUnitEQFilterTypeResonantLowPass   = 3,
    AVAudioUnitEQFilterTypeResonantHighPass  = 4,
    AVAudioUnitEQFilterTypeBandPass          = 5,
    AVAudioUnitEQFilterTypeBandStop          = 6,
    AVAudioUnitEQFilterTypeLowShelf          = 7,
    AVAudioUnitEQFilterTypeHighShelf         = 8,
    AVAudioUnitEQFilterTypeResonantLowShelf  = 9,
    AVAudioUnitEQFilterTypeResonantHighShelf = 10,
} NS_ENUM_AVAILABLE(10_10, 8_0);


/*! @class AVAudioUnitEQFilterParameters
    @abstract Filter parameters used by AVAudioUnitEQ.
    @discussion
        A standalone instance of AVAudioUnitEQFilterParameters cannot be created. Only an instance
        vended out by a source object (e.g. AVAudioUnitEQ) can be used.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioUnitEQFilterParameters : NSObject {
@private
	void *_impl;
}

- (instancetype)init NS_UNAVAILABLE;

/*! @property filterType
    @abstract AVAudioUnitEQFilterType
    @discussion
    Default:    AVAudioUnitEQFilterTypeParametric
*/
@property (nonatomic) AVAudioUnitEQFilterType filterType;

/*! @property frequency
    @abstract Frequency in Hertz.
    @discussion
    Range:      20 -> (SampleRate/2)
    Unit:       Hertz
*/
@property (nonatomic) float frequency;

/*! @property bandwidth
    @abstract Bandwidth in octaves.
    @discussion
    Range:      0.05 -> 5.0
    Unit:       Octaves
*/
@property (nonatomic) float bandwidth;

/*! @property gain
    @abstract Gain in dB.
    @discussion
    Range:      -96 -> 24
    Default:    0
    Unit:       dB
*/
@property (nonatomic) float gain;

/*! @property bypass
    @abstract bypass state of band.
    @discussion
    Default:    YES
*/
@property (nonatomic) BOOL bypass;

@end


/*! @class AVAudioUnitEQ
    @abstract An AVAudioUnitEffect that implements a Multi-Band Equalizer.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitEQ : AVAudioUnitEffect

/*! @method initWithNumberOfBands:
    @abstract Initialize the EQ with number of bands.
    @param numberOfBands
        The number of bands created by the EQ.
*/
- (instancetype)initWithNumberOfBands:(NSUInteger)numberOfBands;

/*! @property bands
    @abstract Array of AVAudioUnitEQFilterParameters objects.
    @discussion
        The number of elements in the array is equal to the number of bands.
*/
@property (nonatomic, readonly) NSArray<AVAudioUnitEQFilterParameters *> *bands;

/*! @property globalGain
    @abstract Overall gain adjustment applied to the signal.
    @discussion
        Range:     -96 -> 24
        Default:   0
        Unit:      dB
*/
@property (nonatomic) float globalGain;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioMixing.h
/*
    File:       AVAudioMixing.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>

NS_ASSUME_NONNULL_BEGIN

@class AVAudioNode, AVAudioConnectionPoint, AVAudioMixingDestination;
@protocol AVAudioStereoMixing;
@protocol AVAudio3DMixing;

/*! @protocol   AVAudioMixing
    @abstract   Protocol that defines properties applicable to the input bus of a mixer
                node
    @discussion
        Nodes that conforms to the AVAudioMixing protocol can talk to a mixer node downstream,
        specifically of type AVAudioMixerNode or AVAudioEnvironmentNode. The properties defined 
        by this protocol apply to the respective input bus of the mixer node that the source node is 
        connected to. Note that effect nodes cannot talk to their downstream mixer.

		Properties can be set either on the source node, or directly on individual mixer connections.
		Source node properties are:
		- applied to all existing mixer connections when set
		- applied to new mixer connections
		- preserved upon disconnection from mixers
		- not affected by connections/disconnections to/from mixers
		- not affected by any direct changes to properties on individual mixer connections

		Individual mixer connection properties, when set, will override any values previously derived 
		from the corresponding source node properties. However, if a source node property is 
		subsequently set, it will override the corresponding property value of all individual mixer 
		connections.
		Unlike source node properties, individual mixer connection properties are not preserved upon
		disconnection (see `AVAudioMixing(destinationForMixer:bus:)` and `AVAudioMixingDestination`).

		Source nodes that are connected to a mixer downstream can be disconnected from
		one mixer and connected to another mixer with source node's mixing settings intact.
		For example, an AVAudioPlayerNode that is being used in a gaming scenario can set up its 
		3D mixing settings and then move from one environment to another.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@protocol AVAudioMixing <AVAudioStereoMixing, AVAudio3DMixing>

/*! @method destinationForMixer:bus:
	@abstract Returns the AVAudioMixingDestination object corresponding to specified mixer node and
		its input bus
	@discussion
		When a source node is connected to multiple mixers downstream, setting AVAudioMixing 
		properties directly on the source node will apply the change to all the mixers downstream. 
		If you want to set/get properties on a specific mixer, use this method to get the 
		corresponding AVAudioMixingDestination and set/get properties on it. 
 
		Note:
		- Properties set on individual AVAudioMixingDestination instances will not reflect at the
			source node level.

		- AVAudioMixingDestination reference returned by this method could become invalid when
			there is any disconnection between the source and the mixer node. Hence this reference
			should not be retained and should be fetched every time you want to set/get properties 
			on a specific mixer.
 
		If the source node is not connected to the specified mixer/input bus, this method
		returns nil.
		Calling this on an AVAudioMixingDestination instance returns self if the specified
		mixer/input bus match its connection point, otherwise returns nil.
*/
- (nullable AVAudioMixingDestination *)destinationForMixer:(AVAudioNode *)mixer bus:(AVAudioNodeBus)bus API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));

/*! @property volume
    @abstract Set a bus's input volume
    @discussion
        Range:      0.0 -> 1.0
        Default:    1.0
        Mixers:     AVAudioMixerNode, AVAudioEnvironmentNode
*/
@property (nonatomic) float volume;

@end


/*! @protocol   AVAudioStereoMixing
    @abstract   Protocol that defines stereo mixing properties
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@protocol AVAudioStereoMixing <NSObject>

/*! @property pan
    @abstract Set a bus's stereo pan
    @discussion
        Range:      -1.0 -> 1.0
        Default:    0.0
        Mixer:      AVAudioMixerNode
*/
@property (nonatomic) float pan;

@end


/*! @enum AVAudio3DMixingRenderingAlgorithm
    @abstract   Types of rendering algorithms available per input bus of the environment node
    @discussion
        The rendering algorithms differ in terms of quality and cpu cost. 
        AVAudio3DMixingRenderingAlgorithmEqualPowerPanning is the simplest panning algorithm and also 
        the least expensive computationally.
 
        With the exception of AVAudio3DMixingRenderingAlgorithmSoundField, while the mixer is
        rendering to multi channel hardware, audio data will only be rendered to channels 1 & 2.
 
        AVAudio3DMixingRenderingAlgorithmEqualPowerPanning
            EqualPowerPanning merely pans the data of the mixer bus into a stereo field. This 
            algorithm is analogous to the pan knob found on a mixing board channel strip. 
 
        AVAudio3DMixingRenderingAlgorithmSphericalHead
            SphericalHead is designed to emulate 3 dimensional space in headphones by simulating 
            inter-aural time delays and other spatial cues. SphericalHead is slightly less CPU 
            intensive than the HRTF algorithm.
 
        AVAudio3DMixingRenderingAlgorithmHRTF
            HRTF (Head Related Transfer Function) is a high quality algorithm using filtering to 
            emulate 3 dimensional space in headphones. HRTF is a cpu intensive algorithm.
 
        AVAudio3DMixingRenderingAlgorithmHRTFHQ
            Higher quality HRTF rendering algorithm compared to AVAudio3DMixingRenderingAlgorithmHRTF.
            Improvements have been made to the overall frequency response and localization of
            sources in a 3D space.
 
        AVAudio3DMixingRenderingAlgorithmSoundField
            SoundField is designed for rendering to multi channel hardware. The mixer takes data 
            being rendered with SoundField and distributes it amongst all the output channels with 
            a weighting toward the location in which the sound derives. It is very effective for 
            ambient sounds, which may derive from a specific location in space, yet should be heard 
            through the listener's entire space.
 
        AVAudio3DMixingRenderingAlgorithmStereoPassThrough
            StereoPassThrough should be used when no localization is desired for the source data. 
            Setting this algorithm tells the mixer to take mono/stereo input and pass it directly to 
            channels 1 & 2 without localization.
 
*/
typedef NS_ENUM(NSInteger, AVAudio3DMixingRenderingAlgorithm) {
    AVAudio3DMixingRenderingAlgorithmEqualPowerPanning      = 0,
    AVAudio3DMixingRenderingAlgorithmSphericalHead          = 1,
    AVAudio3DMixingRenderingAlgorithmHRTF                   = 2,
    AVAudio3DMixingRenderingAlgorithmSoundField             = 3,
    AVAudio3DMixingRenderingAlgorithmStereoPassThrough      = 5,
    AVAudio3DMixingRenderingAlgorithmHRTFHQ                 = 6
} NS_ENUM_AVAILABLE(10_10, 8_0);


/*! @protocol   AVAudio3DMixing
    @abstract   Protocol that defines 3D mixing properties
*/
@protocol AVAudio3DMixing <NSObject>

/*! @property renderingAlgorithm
    @abstract Type of rendering algorithm used
    @discussion
        Depending on the current output format of the AVAudioEnvironmentNode, only a subset of the 
        rendering algorithms may be supported. An array of valid rendering algorithms can be 
        retrieved by calling applicableRenderingAlgorithms on AVAudioEnvironmentNode.
 
        Default:    AVAudio3DMixingRenderingAlgorithmEqualPowerPanning
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) AVAudio3DMixingRenderingAlgorithm renderingAlgorithm API_UNAVAILABLE(watchos);

/*! @property rate
    @abstract Changes the playback rate of the input signal
    @discussion
        A value of 2.0 results in the output audio playing one octave higher.
        A value of 0.5, results in the output audio playing one octave lower.
     
        Range:      0.5 -> 2.0
        Default:    1.0
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) float rate API_UNAVAILABLE(watchos);

/*! @property reverbBlend
    @abstract Controls the blend of dry and reverb processed audio
    @discussion
        This property controls the amount of the source's audio that will be processed by the reverb 
        in AVAudioEnvironmentNode. A value of 0.5 will result in an equal blend of dry and processed 
        (wet) audio.
 
        Range:      0.0 (completely dry) -> 1.0 (completely wet)
        Default:    0.0
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) float reverbBlend API_UNAVAILABLE(watchos);

/*! @property obstruction
    @abstract Simulates filtering of the direct path of sound due to an obstacle
    @discussion
        Only the direct path of sound between the source and listener is blocked.
 
        Range:      -100.0 -> 0.0 dB
        Default:    0.0
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) float obstruction API_UNAVAILABLE(watchos);

/*! @property occlusion
    @abstract Simulates filtering of the direct and reverb paths of sound due to an obstacle
    @discussion
        Both the direct and reverb paths of sound between the source and listener are blocked.
 
        Range:      -100.0 -> 0.0 dB
        Default:    0.0
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) float occlusion API_UNAVAILABLE(watchos);

/*! @property position
    @abstract The location of the source in the 3D environment
    @discussion
        The coordinates are specified in meters.
 
        Mixer:      AVAudioEnvironmentNode
*/
@property (nonatomic) AVAudio3DPoint position API_UNAVAILABLE(watchos);

@end

/*! @class AVAudioMixingDestination
	@abstract An object representing a connection to a mixer node from a node that
		conforms to AVAudioMixing protocol
	@discussion
		A standalone instance of AVAudioMixingDestination cannot be created.
		Only an instance vended by a source node (e.g. AVAudioPlayerNode) can be used
		(see `AVAudioMixing`).
*/
OS_EXPORT API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0))
@interface AVAudioMixingDestination : NSObject <AVAudioMixing> {
@private
	void *_impl;
}

- (instancetype)init NS_UNAVAILABLE;

/*! @property connectionPoint
	@abstract Returns the underlying mixer connection point
*/
@property (nonatomic, readonly) AVAudioConnectionPoint *connectionPoint;

@end

NS_ASSUME_NONNULL_END

// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioMixerNode.h
/*
	File:		AVAudioMixerNode.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioNode.h>
#import <AVFAudio/AVAudioMixing.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioMixerNode
	@abstract A node that mixes its inputs to a single output.
	@discussion
		Mixers may have any number of inputs.
	
		The mixer accepts input at any sample rate and efficiently combines sample rate
		conversions. It also accepts any channel count and will correctly upmix or downmix
		to the output channel count.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioMixerNode : AVAudioNode <AVAudioMixing>

- (instancetype)init NS_DESIGNATED_INITIALIZER;

/*! @property outputVolume
	@abstract The mixer's output volume.
	@discussion
		This accesses the mixer's output volume (0.0-1.0, inclusive).
*/
@property (nonatomic) float outputVolume;

/*! @property nextAvailableInputBus
	@abstract Find an unused input bus.
	@discussion
		This will find and return the first input bus to which no other node is connected.
*/
@property (nonatomic, readonly) AVAudioNodeBus nextAvailableInputBus;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioFormat.h
/*
	File:		AVAudioFormat.h
	Framework:	AVFoundation
	
	Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
*/

#import <AVFAudio/AVAudioTypes.h>
#import <AVFAudio/AVAudioChannelLayout.h>

#if __has_include(<CoreMedia/CMFormatDescription.h>)
#define AVAUDIOFORMAT_HAVE_CMFORMATDESCRIPTION 1
#import <CoreMedia/CMFormatDescription.h>
#endif

NS_ASSUME_NONNULL_BEGIN

/*!	
	@enum		AVAudioCommonFormat
	@constant	AVAudioOtherFormat
					A format other than one of the common ones below.
	@constant	AVAudioPCMFormatFloat32
					Native-endian floats (this is the standard format).
	@constant	AVAudioPCMFormatFloat64
					Native-endian doubles.
	@constant	AVAudioPCMFormatInt16
					Signed 16-bit native-endian integers.
	@constant	AVAudioPCMFormatInt32
					Signed 32-bit native-endian integers.
*/
typedef NS_ENUM(NSUInteger, AVAudioCommonFormat) {
	AVAudioOtherFormat = 0,
	AVAudioPCMFormatFloat32 = 1,
	AVAudioPCMFormatFloat64 = 2,
	AVAudioPCMFormatInt16 = 3,
	AVAudioPCMFormatInt32 = 4
} NS_ENUM_AVAILABLE(10_10, 8_0);


/*! @class AVAudioFormat
	@abstract A representation of an audio format.
	@discussion
		AVAudioFormat wraps a Core Audio AudioStreamBasicDescription struct, with convenience
		initializers and accessors for common formats, including Core Audio's standard deinterleaved
		32-bit floating point.
	
		Instances of this class are immutable.
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), watchos(2.0), tvos(9.0))
@interface AVAudioFormat : NSObject <NSSecureCoding> {
@private
	AudioStreamBasicDescription _asbd;
	AVAudioChannelLayout *_layout;
	AVAudioCommonFormat _commonFormat;
	void * _reserved;
}


/*! @method initWithStreamDescription:
	@abstract Initialize from an AudioStreamBasicDescription.
	@param asbd
		the AudioStreamBasicDescription
	@discussion
		If the format specifies more than 2 channels, this method fails (returns nil).
*/
- (nullable instancetype)initWithStreamDescription:(const AudioStreamBasicDescription *)asbd;

/*! @method initWithStreamDescription:channelLayout:
	@abstract Initialize from an AudioStreamBasicDescription and optional channel layout.
	@param asbd
		the AudioStreamBasicDescription
	@param layout
		the channel layout. Can be nil only if asbd specifies 1 or 2 channels.
	@discussion
		If the format specifies more than 2 channels, this method fails (returns nil) unless layout
		is non-nil.
*/
- (nullable instancetype)initWithStreamDescription:(const AudioStreamBasicDescription *)asbd channelLayout:(AVAudioChannelLayout * __nullable)layout;

/*! @method initStandardFormatWithSampleRate:channels:
	@abstract Initialize to deinterleaved float with the specified sample rate and channel count.
	@param sampleRate
		the sample rate
	@param channels
		the channel count
	@discussion
		If the format specifies more than 2 channels, this method fails (returns nil).
*/
- (nullable instancetype)initStandardFormatWithSampleRate:(double)sampleRate channels:(AVAudioChannelCount)channels;

/*! @method initStandardFormatWithSampleRate:channelLayout:
	@abstract Initialize to deinterleaved float with the specified sample rate and channel layout.
	@param sampleRate
		the sample rate
	@param layout
		the channel layout. must not be nil.
*/
- (instancetype)initStandardFormatWithSampleRate:(double)sampleRate channelLayout:(AVAudioChannelLayout *)layout;

/*! @method initWithCommonFormat:sampleRate:channels:interleaved:
	@abstract Initialize to float with the specified sample rate, channel count and interleavedness.
	@param format
		the common format type
	@param sampleRate
		the sample rate
	@param channels
		the channel count
	@param interleaved
		true if interleaved
	@discussion
		If the format specifies more than 2 channels, this method fails (returns nil).
*/
- (nullable instancetype)initWithCommonFormat:(AVAudioCommonFormat)format sampleRate:(double)sampleRate channels:(AVAudioChannelCount)channels interleaved:(BOOL)interleaved;

/*! @method initWithCommonFormat:sampleRate:interleaved:channelLayout:
	@abstract Initialize to float with the specified sample rate, channel layout and interleavedness.
	@param format
		the common format type
	@param sampleRate
		the sample rate
	@param interleaved
		true if interleaved
	@param layout
		the channel layout. must not be nil.
*/
- (instancetype)initWithCommonFormat:(AVAudioCommonFormat)format sampleRate:(double)sampleRate interleaved:(BOOL)interleaved channelLayout:(AVAudioChannelLayout *)layout;

/*! @method initWithSettings:
	@abstract Initialize using a settings dictionary.
	@discussion
		See AVAudioSettings.h. Note that many settings dictionary elements pertain to encoder
		settings, not the basic format, and will be ignored.

 		Returns nil if a format cannot be constructed with the provided settings, e.g. when:
			- AVNumberOfChannelsKey specifies more than 2 channels, but AVChannelLayoutKey hasn't 
			  been specified or the layout does not match
			- AVLinearPCMBitDepthKey for linear PCM format specifies less than 8 or greater
			  than 32 bits
			- values for the keys are not of the expected types
*/
- (nullable instancetype)initWithSettings:(NSDictionary<NSString *, id> *)settings;

#if AVAUDIOFORMAT_HAVE_CMFORMATDESCRIPTION
/*!
 	@method initWithCMAudioFormatDescription:
 	@abstract initialize from a CMAudioFormatDescriptionRef.
 	@param formatDescription
 		the CMAudioFormatDescriptionRef.
 	@discussion
 		If formatDescription is invalid, this method fails (returns nil).
 */
- (instancetype)initWithCMAudioFormatDescription:(CMAudioFormatDescriptionRef)formatDescription API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));
#endif

/*!	@method isEqual:
	@abstract Determine whether another format is functionally equivalent.
	@param object
		the format to compare against
	@discussion
		For PCM, interleavedness is ignored for mono. Differences in the AudioStreamBasicDescription
		alignment and packedness are ignored when they are not significant (e.g. with 1 channel, 2
		bytes per frame and 16 bits per channel, neither alignment, the format is implicitly packed
		and can be interpreted as either high- or low-aligned.)
		For AVAudioChannelLayout, a layout with standard mono/stereo tag is considered to be 
		equivalent to a nil layout. Otherwise, the layouts are compared for equality.
*/
- (BOOL)isEqual:(id)object;

/*!	@property standard
	@abstract Describes whether the format is deinterleaved native-endian float.
*/
@property (nonatomic, readonly, getter=isStandard) BOOL standard;

/*!	@property commonFormat
	@abstract An `AVAudioCommonFormat` identifying the format
*/
@property (nonatomic, readonly) AVAudioCommonFormat commonFormat;

/*! @property channelCount
	@abstract The number of channels of audio data.
*/
@property (nonatomic, readonly) AVAudioChannelCount channelCount;

/*! @property sampleRate
	@abstract A sampling rate in Hertz.
*/
@property (nonatomic, readonly) double sampleRate;

/*!	@property interleaved
	@abstract Describes whether the samples are interleaved.
	@discussion
		For non-PCM formats, the value is undefined.
*/
@property (nonatomic, readonly, getter=isInterleaved) BOOL interleaved;

/*!	@property streamDescription
	@abstract Returns the AudioStreamBasicDescription, for use with lower-level audio API's.
*/
@property (nonatomic, readonly) const AudioStreamBasicDescription *streamDescription;

/*!	@property channelLayout
	@abstract The underlying AVAudioChannelLayout, if any.
	@discussion
		Only formats with more than 2 channels are required to have channel layouts.
*/
@property (nonatomic, readonly, nullable) AVAudioChannelLayout *channelLayout;

/*! @property magicCookie
	@abstract The underlying magic cookie, if any.
	@discussion
		A magic cookie contains metadata associated with encoders and decoders.
		Encoders produce a magic cookie, and some decoders require a magic cookie to decode properly.
*/
@property (nonatomic, retain, nullable) NSData *magicCookie API_AVAILABLE(macos(10.12), ios(10.0), watchos(3.0), tvos(10.0));

/*!	@property settings
	@abstract Returns the format represented as a dictionary with keys from AVAudioSettings.h.
*/
@property (nonatomic, readonly) NSDictionary<NSString *, id> *settings;

#if AVAUDIOFORMAT_HAVE_CMFORMATDESCRIPTION
/*!
	 @property formatDescription
	 @abstract Converts to a CMAudioFormatDescriptionRef, for use with Core Media API's.
 */
@property (nonatomic, readonly) CMAudioFormatDescriptionRef formatDescription API_AVAILABLE(macos(10.11), ios(9.0), watchos(2.0), tvos(9.0));
#endif

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitVarispeed.h
/*
    File:		AVAudioUnitVarispeed.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#import <AVFAudio/AVAudioUnitTimeEffect.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitVarispeed
    @abstract an AVAudioUnitTimeEffect that can be used to control the playback rate 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitVarispeed : AVAudioUnitTimeEffect

/*! @property rate
    @abstract controls the playback rate of the audio signal
    @discussion
    Since this unit resamples the input signal, changing the playback rate also changes the pitch.
    
    i.e. changing the rate to 2.0 results in the output audio playing one octave higher.
    Similarly changing the rate to 0.5, results in the output audio playing one octave lower.
 
    The playback rate and pitch can be calculated as
                  rate  = pow(2, cents/1200.0)
        pitch in cents  = 1200.0 * log2(rate)
    
    Where,    1 octave  = 1200 cents
    1 musical semitone  = 100 cents
 
    Range:      0.25 -> 4.0
    Default:    1.0
    Unit:       Generic
*/
@property (nonatomic) float rate;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitReverb.h
/*
    File:		AVAudioUnitReverb.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#import <AVFAudio/AVAudioUnitEffect.h>

NS_ASSUME_NONNULL_BEGIN

typedef NS_ENUM(NSInteger, AVAudioUnitReverbPreset) {
    AVAudioUnitReverbPresetSmallRoom       = 0,
    AVAudioUnitReverbPresetMediumRoom      = 1,
    AVAudioUnitReverbPresetLargeRoom       = 2,
    AVAudioUnitReverbPresetMediumHall      = 3,
    AVAudioUnitReverbPresetLargeHall       = 4,
    AVAudioUnitReverbPresetPlate           = 5,
    AVAudioUnitReverbPresetMediumChamber   = 6,
    AVAudioUnitReverbPresetLargeChamber    = 7,
    AVAudioUnitReverbPresetCathedral       = 8,
    AVAudioUnitReverbPresetLargeRoom2      = 9,
    AVAudioUnitReverbPresetMediumHall2     = 10,
    AVAudioUnitReverbPresetMediumHall3     = 11,
    AVAudioUnitReverbPresetLargeHall2      = 12
} NS_ENUM_AVAILABLE(10_10, 8_0);

/*! @class AVAudioUnitReverb
    @abstract an AVAudioUnitEffect that implements a reverb
    @discussion
        A reverb simulates the acoustic characteristics of a particular environment.
        Use the different presets to simulate a particular space and blend it in with
        the original signal using the wetDryMix parameter.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos) 
@interface AVAudioUnitReverb : AVAudioUnitEffect

/*! @method loadFactoryPreset:
    @abstract load a reverb preset
    Default:    AVAudioUnitReverbPresetMediumHall
*/
- (void)loadFactoryPreset:(AVAudioUnitReverbPreset)preset;

/*! @property wetDryMix
    @abstract
    Blend of the wet and dry signals
    Range:      0 (all dry) -> 100 (all wet)
    Unit:       Percent
*/
@property (nonatomic) float wetDryMix;

@end

NS_ASSUME_NONNULL_END
// ==========  AVFoundation.framework/Frameworks/AVFAudio.framework/Headers/AVAudioUnitTimeEffect.h
/*
    File:		AVAudioUnitTimeEffect.h
    Framework:	AVFoundation
 
    Copyright (c) 2014-2015 Apple Inc. All Rights Reserved.
 */

#import <AVFAudio/AVAudioUnit.h>

NS_ASSUME_NONNULL_BEGIN

/*! @class AVAudioUnitTimeEffect
    @abstract an AVAudioUnit that processes audio in non real-time
    @discussion
    An AVAudioUnitTimeEffect represents an audio unit of type aufc.
    These effects do not process audio in real-time. The varispeed
    unit is an example of a time effect unit.
 
*/
OS_EXPORT API_AVAILABLE(macos(10.10), ios(8.0), tvos(9.0)) API_UNAVAILABLE(watchos)
@interface AVAudioUnitTimeEffect : AVAudioUnit

#if AVAUDIOUNIT_HAVE_AUDIOUNIT
/*! @method initWithAudioComponentDescription:
    @abstract create an AVAudioUnitTimeEffect object
    
    @param audioComponentDescription
    @abstract AudioComponentDescription of the audio unit to be initialized
    @discussion 
    The componentType must be kAudioUnitType_FormatConverter
*/
- (instancetype)initWithAudioComponentDescription:(AudioComponentDescription)audioComponentDescription;
#endif

/*! @property bypass
    @abstract bypass state of the audio unit
*/
@property (nonatomic) BOOL bypass;

@end

NS_ASSUME_NONNULL_END
